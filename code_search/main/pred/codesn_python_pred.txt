Q_0	specify the input data source format	C_0_0	def format ( self , source ) self . _jreader = self . _jreader . format ( source ) return self	1
Q_0	specify the input data source format	C_0_1	def get_name ( self ) if self . asset_type = = asset_type . governing_token return string elif self . asset_type = = asset_type . utility_token return string if type ( self . name ) is bytes return self . name . decode ( string ) return self . name	0
Q_0	specify the input data source format	C_0_2	def merge_size_fill ( tsize = number , tfill = number ) assert tsize > = number , string assert tfill > = number , string def fmerge ( ccp , cck , np , nk ) fill = ( ccp _ ( cck _ nk ) ) * nk if fill < = tfill or ( nk < = tsize and np < = tsize ) return true else return false fmerge . _doc_ = ( tsize , tfill ) return fmerge	0
Q_1	return number of observation over request axis	C_1_0	def count ( self , axis = string ) i = self . _get_axis_number ( axis ) values = self . values mask = np . isfinite ( values ) result = mask . sum ( axis = i , dtype = string ) return self . _wrap_result ( result , axis )	1
Q_1	return number of observation over request axis	C_1_1	def calc_shape_statistics ( self , stat_names ) stats = { } try all_props = [ regionprops ( var ) for var in self . masks ] except type_error print ( self . masks ) exit ( ) for stat in stat_names stats [ stat ] = np . mean ( [ p [ number ] [ stat ] for p in all_props ] ) return stats	0
Q_1	return number of observation over request axis	C_1_2	def watch ( self , flag ) lib . env_set_defmessage_handler_watch ( self . _env , int ( flag ) , self . _cls , self . _idx )	0
Q_2	export the minimal state and save it in fn to load an empty version for inference	C_2_0	def export ( self , fn path_or_str , * * kwargs ) string pickle . dump ( self . get_state ( * * kwargs ) , open ( fn , string ) )	1
Q_2	export the minimal state and save it in fn to load an empty version for inference	C_2_1	def create_app_cookie_stickiness_policy ( self , name , lb_name , policy_name ) params = { string name , string lb_name , string policy_name , } return self . get_status ( string , params )	0
Q_2	export the minimal state and save it in fn to load an empty version for inference	C_2_2	def get ( cls , keyval , key = string , user_id = none ) if keyval is none return none if ( key in cls . _table_ . columns and cls . _table_ . columns [ key ] . primary_key ) return cls . query . get ( keyval ) else result = cls . query . filter ( getattr ( cls , key ) = = keyval ) return result . first ( )	0
Q_3	find element by a partial match of their link text	C_3_0	def find_elements_by_partial_link_text ( self , link_text ) return self . find_elements ( by = by . partial_link_text , value = link_text )	1
Q_3	find element by a partial match of their link text	C_3_1	def overlaps ( self , canvas , exclude = [ ] ) try exclude = list ( exclude ) except type_error exclude = [ exclude ] exclude . append ( self ) for var , var in enumerate ( self . image . image ( ) ) for self_x , pixel in enumerate ( var ) canvas_pixel_on = canvas . test_pixel ( ( self_x + self . position [ number ] , var + self . position [ number ] ) , excluded_sprites = exclude ) if pixel and canvas_pixel_on return true return false	0
Q_3	find element by a partial match of their link text	C_3_2	def gather_repositories ( ) for ( root , dirs , files ) in os . walk ( string , topdown = true ) if string not in dirs continue for var in list ( dirs ) dirs . remove ( var ) path = os . path . split ( root ) [ number ] repo = os . path . basename ( path ) yield ( repo , root )	0
Q_4	return datasource with column and metric	C_4_0	def get_eager_datasource ( cls , session , datasource_type , datasource_id ) datasource_class = connector_registry . sources [ datasource_type ] return ( session . query ( datasource_class ) . options ( subqueryload ( datasource_class . columns ) , subqueryload ( datasource_class . metrics ) , ) . filter_by ( id = datasource_id ) . one ( ) )	1
Q_4	return datasource with column and metric	C_4_1	def _create_tokens_for_next_line_dent ( self , newline_token ) indent_delta = self . _get_next_line_indent_delta ( newline_token ) if indent_delta is none or indent_delta = = number return none dent_type = string if indent_delta > number else string dent_token = _create_token ( dent_type , string , newline_token . lineno + number , newline_token . lexpos + len ( newline_token . value ) ) tokens = [ dent_token ] * abs ( indent_delta ) self . cur_indent + = indent_delta return multi_token ( tokens )	0
Q_4	return datasource with column and metric	C_4_2	def insert_bulk_block ( self , block_dump ) frst = true if ( block_dump [ string ] [ number ] [ string ] [ number ] ) . get ( string ) = = none frst = false red_flag = false if frst = = true event_ct = ( fl . get ( string ) = = none for var in block_dump [ string ] for fl in var [ string ] ) else event_ct = ( fl . get ( string ) = none for var in block_dump [ string ] for fl in var [ string ] ) red_flag = any ( event_ct ) if red_flag for var in block_dump [ string ] for fl in var [ string ] if string in fl del fl [ string ] result = self . _call_server ( string , data = block_dump , callmethod = string ) if red_flag raise dbs_client_exception ( string block_dump [ string ] [ string ] , string ) else return result	0
Q_5	endpoint that refresh druid datasources metadata	C_5_0	def refresh_datasources ( self , refresh_all = true ) session = db . session ( ) druid_cluster = connector_registry . sources [ string ] . cluster_class for var in session . query ( druid_cluster ) . all ( ) cluster_name = var . cluster_name valid_cluster = true try var . refresh_datasources ( refresh_all = refresh_all ) except exception as e valid_cluster = false flash ( string { } string . format ( cluster_name , utils . error_msg_from_exception ( e ) ) , string ) logging . exception ( e ) pass if valid_cluster var . metadata_last_refreshed = datetime . now ( ) flash ( _ ( string ) . format ( var . cluster_name ) , string ) session . commit ( ) return redirect ( string )	1
Q_5	endpoint that refresh druid datasources metadata	C_5_1	def chirp ( t , f0 = number . , t1 = number . , f1 = number . , form = string , phase = number ) r valid_forms = [ string , string , string ] if form not in valid_forms raise value_error ( string valid_forms ) t = numpy . array ( t ) phase = number . * pi * phase / number . if form = = string a = pi * ( f1 _ f0 ) / t1 b = number . * pi * f0 y = numpy . cos ( a * t * * number + b * t + phase ) elif form = = string a = ( number / number . * pi * ( f1 _ f0 ) / t1 / t1 ) b = number . * pi * f0 y = numpy . cos ( a * t * * number + b * t + phase ) elif form = = string a = number . * pi * t1 / numpy . log ( f1 _ f0 ) b = number . * pi * f0 x = ( f1 _ f0 ) * * ( number . / t1 ) y = numpy . cos ( a * x * * t + b * t + phase ) return y	0
Q_5	endpoint that refresh druid datasources metadata	C_5_2	def _add_record ( self , record_set_class , name , values , ttl = number , weight = none , region = none , set_identifier = none , alias_hosted_zone_id = none , alias_dns_name = none ) self . _halt_if_already_deleted ( ) rrset_kwargs = dict ( connection = self . connection , zone_id = self . id , name = name , ttl = ttl , records = values , weight = weight , region = region , set_identifier = set_identifier , ) if alias_hosted_zone_id or alias_dns_name rrset_kwargs . update ( dict ( alias_hosted_zone_id = alias_hosted_zone_id , alias_dns_name = alias_dns_name ) ) rrset = record_set_class ( * * rrset_kwargs ) cset = change_set ( connection = self . connection , hosted_zone_id = self . id ) cset . add_change ( string , rrset ) change_info = self . connection . _change_resource_record_sets ( cset ) return rrset , change_info	0
Q_6	export give checkpoint as tfhub module with give spec	C_6_0	def export_module_spec_with_checkpoint ( module_spec , checkpoint_path , export_path , scope_prefix = string ) with tf . graph ( ) . as_default ( ) m = hub . module ( module_spec ) assign_map = { scope_prefix + var var for var , var in m . variable_map . items ( ) } tf . train . init_from_checkpoint ( checkpoint_path , assign_map ) init_op = tf . initializers . global_variables ( ) with tf . session ( ) as session session . run ( init_op ) m . export ( export_path , session )	1
Q_6	export give checkpoint as tfhub module with give spec	C_6_1	def save_activity_profile ( self , profile ) request = http_request ( method = string , resource = string , content = profile . content ) if profile . content_type is not none request . headers [ string ] = profile . content_type else request . headers [ string ] = string if profile . etag is not none request . headers [ string ] = profile . etag request . query_params = { string profile . id , string profile . activity . id } lrs_response = self . _send_request ( request ) lrs_response . content = profile return lrs_response	0
Q_6	export give checkpoint as tfhub module with give spec	C_6_2	def _send_pending_messages ( self ) if len ( self . _queue ) = = number time . sleep ( number . number ) return msg = self . _queue . pop ( number ) if msg . can_send ( ) self . _send_msg ( msg ) msg . refresh ( ) if not ( msg . is_finished ( ) ) self . _queue . append ( msg ) else self . _queue . append ( msg ) time . sleep ( number . number number )	0
Q_7	generate wikipedia_articles from gc that be part of shard shard_id	C_7_0	def _wiki_articles ( shard_id , wikis_dir = none ) if not wikis_dir wikis_dir = wiki_content_dir with tf . graph ( ) . as_default ( ) dataset = tf . data . tf_record_dataset ( cc_utils . readahead ( os . path . join ( wikis_dir , wiki_content_file shard_id ) ) , buffer_size = number * number * number ) def _parse_example ( ex_ser ) features = { string tf . var_len_feature ( tf . string ) , string tf . var_len_feature ( tf . string ) , string tf . var_len_feature ( tf . string ) , string tf . var_len_feature ( tf . string ) , } ex = tf . parse_single_example ( ex_ser , features ) for var in ex . keys ( ) ex [ var ] = ex [ var ] . values ex [ string ] = ex [ string ] [ number ] ex [ string ] = ex [ string ] [ number ] return ex dataset = dataset . map ( _parse_example , num_parallel_calls = number ) dataset = dataset . prefetch ( number ) record_it = dataset . make_one_shot_iterator ( ) . get_next ( ) with tf . session ( ) as sess while true try ex = sess . run ( record_it ) except tf . errors . out_of_range_error break sections = [ wikipedia_section ( title = text_encoder . to_unicode ( title ) , text = text_encoder . to_unicode ( text ) ) for title , text in zip ( ex [ string ] , ex [ string ] ) ] yield wikipedia_article ( url = text_encoder . to_unicode ( ex [ string ] ) , title = text_encoder . to_unicode ( ex [ string ] ) , sections = sections )	1
Q_7	generate wikipedia_articles from gc that be part of shard shard_id	C_7_1	def by_group_and_perm ( cls , group_id , perm_name , db_session = none ) db_session = get_db_session ( db_session ) query = db_session . query ( cls . model ) . filter ( cls . model . group_id = = group_id ) query = query . filter ( cls . model . perm_name = = perm_name ) return query . first ( )	0
Q_7	generate wikipedia_articles from gc that be part of shard shard_id	C_7_2	def create_subword_function ( subword_function_name , * * kwargs ) create_ = registry . get_create_func ( subword_function , string ) return create_ ( subword_function_name , * * kwargs )	0
Q_8	a default set of length_bucket boundary	C_8_0	def _bucket_boundaries ( max_length , min_length = number , length_bucket_step = number . number ) assert length_bucket_step > number . number x = min_length boundaries = [ ] while x < max_length boundaries . append ( x ) x = max ( x + number , int ( x * length_bucket_step ) ) return boundaries	1
Q_8	a default set of length_bucket boundary	C_8_1	def dipole_moment ( r_array , charge_array ) return np . sum ( r_array * charge_array [ , np . newaxis ] , axis = number )	0
Q_8	a default set of length_bucket boundary	C_8_2	def target_types_from_build_file_aliases ( aliases ) target_types = dict ( aliases . target_types ) for var , var in aliases . target_macro_factories . items ( ) target_type , = var . target_types target_types [ var ] = target_type return target_types	0
Q_9	log metric to summary writer and history	C_9_0	def log_metrics ( metrics , summ_writer , log_prefix , step , history = none ) rjust_len = max ( [ len ( var ) for var in metrics ] ) for var , value in six . iteritems ( metrics ) step_log ( step , string ( log_prefix . ljust ( number ) , var . rjust ( rjust_len ) , value ) ) full_name = string + var if history history . append ( log_prefix , full_name , step , value ) if summ_writer summ_writer . scalar ( full_name , value , step )	1
Q_9	log metric to summary writer and history	C_9_1	def begin ( self ) self . out_f . write ( string ) for var , var in enumerate ( self . names ) self . out_f . write ( string . format ( self . prefix ( var ) , var ) )	0
Q_9	log metric to summary writer and history	C_9_2	def use ( app = none , gl = none ) if app is none and gl is none raise type_error ( string app string gl string ) if app = = string app = string gl = string if app = = string from . util . osmesa_gl import fix_osmesa_gl_lib fix_osmesa_gl_lib ( ) if gl is not none raise value_error ( string ) if gl from . import gloo , config config [ string ] = gl gloo . gl . use_gl ( gl ) if app from . app import use_app use_app ( app )	0
Q_10	apply layer postprocessing	C_10_0	def layer_postprocess ( layer_input , layer_output , hparams ) return layer_prepostprocess ( layer_input , layer_output , sequence = hparams . layer_postprocess_sequence , dropout_rate = hparams . layer_prepostprocess_dropout , norm_type = hparams . norm_type , depth = none , epsilon = hparams . norm_epsilon , dropout_broadcast_dims = comma_separated_string_to_integer_list ( getattr ( hparams , string , string ) ) , default_name = string )	1
Q_10	apply layer postprocessing	C_10_1	def column_width ( self , width ) if isinstance ( width , int ) if width > = number self . _column_width = width else raise value_error ( string ) else raise type_error ( string )	0
Q_10	apply layer postprocessing	C_10_2	def send_cached_media ( self , chat_id union [ int , str ] , file_id str , caption str = string , parse_mode str = string , disable_notification bool = none , reply_to_message_id int = none , reply_markup union [ string , string , string , string ] = none ) _ > union [ string , none ] style = self . html if parse_mode . lower ( ) = = string else self . markdown try decoded = utils . decode ( file_id ) fmt = string if len ( decoded ) > number else string unpacked = struct . unpack ( fmt , decoded ) except ( assertion_error , binascii . error , struct . error ) raise file_id_invalid from none else media_type = base_client . media_type_id . get ( unpacked [ number ] , none ) if not media_type raise file_id_invalid ( string . format ( unpacked [ number ] ) ) if media_type = = string media = types . input_media_photo ( id = types . input_photo ( id = unpacked [ number ] , access_hash = unpacked [ number ] , file_reference = b string ) ) else media = types . input_media_document ( id = types . input_document ( id = unpacked [ number ] , access_hash = unpacked [ number ] , file_reference = b string ) ) r = self . send ( functions . messages . send_media ( peer = self . resolve_peer ( chat_id ) , media = media , silent = disable_notification or none , reply_to_msg_id = reply_to_message_id , random_id = self . rnd_id ( ) , reply_markup = reply_markup . write ( ) if reply_markup else none , * * style . parse ( caption ) ) ) for var in r . updates if isinstance ( var , ( types . update_new_message , types . update_new_channel_message ) ) return pyrogram . message . _parse ( self , var . message , { var . id var for var in r . users } , { var . id var for var in r . chats } )	0
Q_11	mix start with x2 mix mix go towards x1	C_11_0	def mix ( x1 , x2 , steps , is_training , min_prob = number . number , max_prob = number . number , mode = string , simple = false , broadcast_last = false ) with tf . name_scope ( string ) if not is_training if max_prob > = number . number return x1 alpha_shape = shape_list ( x1 ) if broadcast_last alpha_shape = alpha_shape [ _ number ] + [ number ] alpha = tf . random_uniform ( alpha_shape ) alpha = to_float ( tf . less ( alpha , max_prob ) ) return alpha * x1 + ( number . number _ alpha ) * x2 def get_res ( ) if mode = = string alpha_p = inverse_lin_decay ( steps ) else alpha_p = inverse_exp_decay ( steps ) alpha_p = alpha_p * ( max_prob _ min_prob ) + min_prob if simple return alpha_p * x1 + ( number . number _ alpha_p ) * x2 alpha_shape = shape_list ( x1 ) if broadcast_last alpha_shape = alpha_shape [ _ number ] + [ number ] alpha = tf . random_uniform ( alpha_shape ) alpha = to_float ( tf . less ( alpha , alpha_p ) ) return alpha * x1 + ( number . number _ alpha ) * x2 if max_prob < number . number return get_res ( ) if is_xla_compiled ( ) return get_res ( ) else cur_step = tf . train . get_global_step ( ) if cur_step is none return x1 return tf . cond ( tf . less ( cur_step , steps ) , get_res , lambda x1 )	1
Q_11	mix start with x2 mix mix go towards x1	C_11_1	def wheel_delta ( self ) delta = self . _libinput . libinput_event_tablet_tool_get_wheel_delta ( self . _handle ) changed = self . _libinput . libinput_event_tablet_tool_wheel_has_changed ( self . _handle ) return delta , changed	0
Q_11	mix start with x2 mix mix go towards x1	C_11_2	def start ( self , file_size = number , maximum_pending_files = number , use_external_stores = false ) super ( multi_get_file_logic , self ) . start ( ) self . state . files_hashed = number self . state . use_external_stores = use_external_stores self . state . file_size = file_size self . state . files_to_fetch = number self . state . files_fetched = number self . state . files_skipped = number self . state . files_hashed_since_check = number self . state . pending_hashes = { } self . state . pending_files = { } self . state . maximum_pending_files = maximum_pending_files self . state . indexed_pathspecs = [ ] self . state . request_data_list = [ ] self . state . next_pathspec_to_start = number self . state . blob_hashes_pending = number	0
Q_12	return the token type of the input	C_12_0	def _token_of ( self , input ) if isinstance ( input , dict ) if is_intrinsics ( input ) return self . token . primitive else return self . token . dict elif isinstance ( input , list ) return self . token . list else return self . token . primitive	1
Q_12	return the token type of the input	C_12_1	def os_paste ( self ) from . input_emulation import keyboard k = keyboard ( ) k . key_down ( string ) k . type ( string ) k . key_up ( string )	0
Q_12	return the token type of the input	C_12_2	def watch_file ( path , func , * args , * * kwargs ) if not path raise value_error ( string ) print ( string { } string . format ( path ) ) last_modification_time = os . path . getmtime ( path ) try while true time . sleep ( number ) new_modification_time = os . path . getmtime ( path ) if new_modification_time = = last_modification_time continue func ( * args , * * kwargs ) last_modification_time = new_modification_time except keyboard_interrupt pass	0
Q_13	return a view of this array with a new shape without alter any data	C_13_0	def reshape ( self , * shape , * * kwargs ) if len ( shape ) = = number and isinstance ( shape [ number ] , ( list , tuple ) ) shape = shape [ number ] elif not shape shape = kwargs . get ( string ) assert shape , string if not all ( var in [ string , string ] for var in kwargs ) raise type_error ( string string shape string reverse string . format ( string . join ( [ var for var in kwargs if var not in [ string , string ] ] ) ) ) reverse = kwargs . get ( string , false ) handle = nd_array_handle ( ) check_call ( _lib . mxnd_array_reshape64 ( self . handle , len ( shape ) , c_array ( ctypes . c_int64 , shape ) , reverse , ctypes . byref ( handle ) ) ) return nd_array ( handle = handle , writable = self . writable )	1
Q_13	return a view of this array with a new shape without alter any data	C_13_1	def url ( url_pattern , view , kwargs = none , name = none ) if isinstance ( url_pattern , url_pattern ) and isinstance ( view , tuple ) url_pattern = url_pattern . for_include ( ) if name is none name = resolve_name ( view ) if callable ( view ) and hasattr ( view , string ) and callable ( view . as_view ) view = view . as_view ( ) return urls . url ( regex = url_pattern , view = view , kwargs = kwargs , name = name )	0
Q_13	return a view of this array with a new shape without alter any data	C_13_2	def _strategy ( codes , context ) if not isinstance ( codes , tuple ) strategies = [ ] i = number while i < len ( codes ) if codes [ i ] [ number ] = = sre . literal and not ( context . flags re . ignorecase ) j = i + number while j < len ( codes ) and codes [ j ] [ number ] = = sre . literal j + = number strategies . append ( hs . just ( u string . join ( [ six . unichr ( charcode ) for ( _ , charcode ) in codes [ i j ] ] ) ) ) i = j else strategies . append ( _strategy ( codes [ i ] , context ) ) i + = number return hs . tuples ( * strategies ) . map ( u string . join ) else code , value = codes if code = = sre . literal c = six . unichr ( value ) if context . flags re . ignorecase return hs . sampled_from ( [ c . lower ( ) , c . upper ( ) ] ) else return hs . just ( c ) elif code = = sre . not_literal c = six . unichr ( value ) blacklist = set ( [ c . lower ( ) , c . upper ( ) ] ) if context . flags re . ignorecase else [ c ] return hs . characters ( blacklist_characters = blacklist ) elif code = = sre . in charsets = value builder = characters_builder ( negate = charsets [ number ] [ number ] = = sre . negate , flags = context . flags ) for var , var in charsets if var = = sre . negate pass elif var = = sre . literal builder . add_chars ( six . unichr ( var ) ) elif var = = sre . range low , high = var for x in six . moves . range ( low , high + number ) builder . add_chars ( six . unichr ( x ) ) elif var = = sre . category builder . add_category ( var ) else raise he . invalid_argument ( string var ) return builder . strategy elif code = = sre . any if context . flags re . dotall return hs . characters ( ) else return hs . characters ( blacklist_characters = string ) elif code = = sre . at if value = = sre . at_end return hs . one_of ( hs . just ( u string ) , hs . just ( u string ) ) return hs . just ( string ) elif code = = sre . subpattern old_flags = context . flags if has_subpattern_flags context . flags = ( context . flags value [ number ] ) value [ number ] strat = _strategy ( value [ _ number ] , context ) context . flags = old_flags if value [ number ] context . groups [ value [ number ] ] = strat strat = hs . shared ( strat , key = value [ number ] ) return strat elif code = = sre . groupref return hs . shared ( context . groups [ value ] , key = value ) elif code = = sre . assert return _strategy ( value [ number ] , context ) elif code = = sre . assert_not return hs . just ( string ) elif code = = sre . branch return hs . one_of ( [ _strategy ( branch , context ) for branch in value [ number ] ] ) elif code in [ sre . min_repeat , sre . max_repeat ] at_least , at_most , regex = value if at_most = = number at_most = none return hs . lists ( _strategy ( regex , context ) , min_size = at_least , max_size = at_most ) . map ( string . join ) elif code = = sre . groupref_exists return hs . one_of ( _strategy ( value [ number ] , context ) , _strategy ( value [ number ] , context ) if value [ number ] else hs . just ( u string ) , ) else raise he . invalid_argument ( string repr ( code ) )	0
Q_14	reset the iterator to the beginning of the data	C_14_0	def reset ( self ) if self . shuffle self . _shuffle_data ( ) if self . last_batch_handle = = string and self . num_data _ self . batch_size < self . cursor < self . num_data self . cursor = self . cursor _ self . num_data _ self . batch_size else self . cursor = _ self . batch_size	1
Q_14	reset the iterator to the beginning of the data	C_14_1	def find_raw ( self , file_id ) raw_handler = lambda resp , data data return self . transport . get ( url = string file_id , handler = raw_handler )	0
Q_14	reset the iterator to the beginning of the data	C_14_2	def delete_by_query ( self , indices , doc_types , query , * * query_params ) path = self . _make_path ( indices , doc_types , string ) body = { string query . serialize ( ) } return self . _send_request ( string , path , body , query_params )	0
Q_15	give a str as a path the symbol	C_15_0	def _load_sym ( sym , logger = logging ) if isinstance ( sym , str ) cur_path = os . path . dirname ( os . path . realpath ( _file_ ) ) symbol_file_path = os . path . join ( cur_path , sym ) logger . info ( string symbol_file_path ) return sym_load ( symbol_file_path ) elif isinstance ( sym , symbol ) return sym else raise value_error ( string string str ( type ( sym ) ) )	1
Q_15	give a str as a path the symbol	C_15_1	def update_state ( self ) response = self . api_interface . get_device_state ( self , type_override = string ) return self . _update_state_from_response ( response )	0
Q_15	give a str as a path the symbol	C_15_2	"def check_file_names ( samples , raw_dir , options ) file_names = { } for var in samples the_sample = none try the_sample = var [ number ] except index_error msg = ( string string ) raise program_error ( msg ) if options . use_full_ids the_sample = options . full_ids_delimiter . join ( var ) file_name = os . path . join ( raw_dir , string . format ( the_sample ) ) if not os . path . isfile ( file_name ) file_name + = string if not os . path . isfile ( file_name ) msg = string t find file for var { } "" . format ( the_sample ) raise program_error ( msg ) file_names [ the_sample ] = file_name return file_names"	0
Q_16	alternative to the common request method which send the	C_16_0	def request_chunked ( self , method , url , body = none , headers = none ) headers = http_header_dict ( headers if headers is not none else { } ) skip_accept_encoding = string in headers skip_host = string in headers self . putrequest ( method , url , skip_accept_encoding = skip_accept_encoding , skip_host = skip_host ) for var , var in headers . items ( ) self . putheader ( var , var ) if string not in headers self . putheader ( string , string ) self . endheaders ( ) if body is not none stringish_types = six . string_types + ( bytes , ) if isinstance ( body , stringish_types ) body = ( body , ) for chunk in body if not chunk continue if not isinstance ( chunk , bytes ) chunk = chunk . encode ( string ) len_str = hex ( len ( chunk ) ) [ number ] self . send ( len_str . encode ( string ) ) self . send ( b string ) self . send ( chunk ) self . send ( b string ) self . send ( b string )	1
Q_16	alternative to the common request method which send the	C_16_1	def cummin ( expr , sort = none , ascending = true , unique = false , preceding = none , following = none ) return _cumulative_op ( expr , cum_min , sort = sort , ascending = ascending , unique = unique , preceding = preceding , following = following )	0
Q_16	alternative to the common request method which send the	C_16_2	def chunk_sequence ( sequence , chunk_length , allow_incomplete = true ) ( complete , leftover ) = divmod ( len ( sequence ) , chunk_length ) if not allow_incomplete leftover = number chunk_count = complete + min ( leftover , number ) chunks = [ ] for var in range ( chunk_count ) left = chunk_length * var right = left + chunk_length chunks . append ( sequence [ left right ] ) return chunks	0
Q_17	give a requirement return it cache key	C_17_0	def as_cache_key ( self , ireq ) extras = tuple ( sorted ( ireq . extras ) ) if not extras extras_string = string else extras_string = string . format ( string . join ( extras ) ) name = key_from_req ( ireq . req ) version = get_pinned_version ( ireq ) return name , string . format ( version , extras_string )	1
Q_17	give a requirement return it cache key	C_17_1	def init_app ( self , app ) if ( string not in app . config and string not in app . config ) warnings . warn ( string string sqlite / / / memory string ) app . config . setdefault ( string , string ) app . config . setdefault ( string , none ) app . config . setdefault ( string , none ) app . config . setdefault ( string , false ) app . config . setdefault ( string , none ) app . config . setdefault ( string , none ) app . config . setdefault ( string , none ) app . config . setdefault ( string , none ) app . config . setdefault ( string , none ) app . config . setdefault ( string , false ) track_modifications = app . config . setdefault ( string , none ) app . config . setdefault ( string , { } ) if track_modifications is none warnings . warn ( fsa_deprecation_warning ( string string string ) ) utils . engine_config_warning ( app . config , string , string , string ) utils . engine_config_warning ( app . config , string , string , string ) utils . engine_config_warning ( app . config , string , string , string ) utils . engine_config_warning ( app . config , string , string , string ) app . extensions [ string ] = _sql_alchemy_state ( self ) app . teardown_appcontext def shutdown_session ( response_or_exc ) if app . config [ string ] if response_or_exc is none self . session . commit ( ) self . session . remove ( ) return response_or_exc	0
Q_17	give a requirement return it cache key	C_17_2	def verb_chain_texts ( self ) if not self . is_tagged ( verb_chains ) self . tag_verb_chains ( ) return self . texts ( verb_chains )	0
Q_18	formula for compute the current backoff	C_18_0	def get_backoff_time ( self ) consecutive_errors_len = len ( list ( takewhile ( lambda x x . redirect_location is none , reversed ( self . history ) ) ) ) if consecutive_errors_len < = number return number backoff_value = self . backoff_factor * ( number * * ( consecutive_errors_len _ number ) ) return min ( self . backoff_max , backoff_value )	1
Q_18	formula for compute the current backoff	C_18_1	def start_jobs ( session ) js = saga . job . service ( string + address , session = session ) batches = range ( number ) jobs = [ ] for var in batches print ( string var ) jd = saga . job . description ( ) jd . executable = string jd . arguments = [ string + str ( var ) ] jd . output = string + str ( var ) jd . error = string + str ( var ) jd . working_directory = working_dir myjob = js . create_job ( jd ) print ( string ( myjob . id ) ) print ( string ( myjob . state ) ) print ( string ) myjob . run ( ) jobs . append ( myjob ) for myjob in jobs print ( string ( myjob . id ) ) print ( string ( myjob . state ) ) print ( string ) myjob . wait ( ) print ( string ( myjob . state ) ) print ( string ( myjob . exit_code ) )	0
Q_18	formula for compute the current backoff	C_18_2	def histogram ( data , return_edges = true , * * kwargs ) r p , bin_edges = np . histogram ( data , * * kwargs ) if return_edges return p , bin_edges x = np . diff ( bin_edges ) / number . + bin_edges [ _ number ] return p , x	0
Q_19	get annotation for cl	C_19_0	def _get_annotations ( cls ) anns = getattr ( cls , string , none ) if anns is none return { } for var in cls . _mro_ [ number ] if anns is getattr ( var , string , none ) return { } return anns	1
Q_19	get annotation for cl	C_19_1	def message ( self , value ) if value is not none assert type ( value ) in ( unicode , q_string ) , string { number } string { number } string unicode string q_string string . format ( string , value ) self . _message = value	0
Q_19	get annotation for cl	C_19_2	def update_media_file ( upload_file ) temp_directory = tempfile . mkdtemp ( ) temp_file = tempfile . temporary_file ( ) temp_media_file = os . path . join ( temp_directory , string ) try for var in upload_file . chunks ( ) temp_file . write ( var ) with zipfile . zip_file ( temp_file , string ) as z z . extractall ( temp_directory ) if os . path . exists ( temp_media_file ) return distutils . dir_util . copy_tree ( temp_media_file , settings . media_root ) else raise exception ( string string media string ) except exception as e raise e finally temp_file . close ( ) if os . path . exists ( temp_directory ) shutil . rmtree ( temp_directory )	0
Q_20	find the distribution which can fulfill a requirement	C_20_0	def find_providers ( self , reqt ) matcher = self . get_matcher ( reqt ) name = matcher . key result = set ( ) provided = self . provided if name in provided for var , var in provided [ name ] try match = matcher . match ( var ) except unsupported_version_error match = false if match result . add ( var ) break return result	1
Q_20	find the distribution which can fulfill a requirement	C_20_1	def leftsibling ( node ) if node . parent pchildren = node . parent . children idx = pchildren . index ( node ) if idx return pchildren [ idx _ number ] else return none else return none	0
Q_20	find the distribution which can fulfill a requirement	C_20_2	def ensure_loopback_device ( path , size ) for var , var in six . iteritems ( loopback_devices ( ) ) if var = = path return var if not os . path . exists ( path ) cmd = [ string , string , size , path ] check_call ( cmd ) return create_loopback ( path )	0
Q_21	ensure that the environment be good for unicode on python tagint	C_21_0	def _verify_python3_env ( ) if py2 return try import locale fs_enc = codecs . lookup ( locale . getpreferredencoding ( ) ) . name except exception fs_enc = string if fs_enc = string return extra = string if os . name = = string import subprocess try rv = subprocess . popen ( [ string , string ] , stdout = subprocess . pipe , stderr = subprocess . pipe ) . communicate ( ) [ number ] except os_error rv = b string good_locales = set ( ) has_c_utf8 = false if isinstance ( rv , bytes ) rv = rv . decode ( string , string ) for var in rv . splitlines ( ) locale = var . strip ( ) if locale . lower ( ) . endswith ( ( string , string ) ) good_locales . add ( locale ) if locale . lower ( ) in ( string , string ) has_c_utf8 = true extra + = string if not good_locales extra + = ( string string string ) elif has_c_utf8 extra + = ( string string string string string ) else extra + = ( string string string ) string . join ( sorted ( good_locales ) ) bad_locale = none for locale in os . environ . get ( string ) , os . environ . get ( string ) if locale and locale . lower ( ) . endswith ( ( string , string ) ) bad_locale = locale if locale is not none break if bad_locale is not none extra + = ( string string string s string string ) bad_locale raise runtime_error ( string string string string + extra )	1
Q_21	ensure that the environment be good for unicode on python tagint	C_21_1	def bind ( nodemask ) mask = set_to_numa_nodemask ( nodemask ) bitmask = libnuma . numa_allocate_nodemask ( ) libnuma . copy_nodemask_to_bitmask ( byref ( mask ) , bitmask ) libnuma . numa_bind ( bitmask ) libnuma . numa_bitmask_free ( bitmask )	0
Q_21	ensure that the environment be good for unicode on python tagint	C_21_2	def _get_blocks_containing_index ( self , axis , index ) if not axis error_message . catch_bugs_and_request_email ( index > sum ( self . block_widths ) ) cumulative_column_widths = np . array ( self . block_widths ) . cumsum ( ) block_idx = int ( np . digitize ( index , cumulative_column_widths ) ) if block_idx = = len ( cumulative_column_widths ) block_idx _ = number internal_idx = ( index if not block_idx else index _ cumulative_column_widths [ block_idx _ number ] ) else error_message . catch_bugs_and_request_email ( index > sum ( self . block_lengths ) ) cumulative_row_lengths = np . array ( self . block_lengths ) . cumsum ( ) block_idx = int ( np . digitize ( index , cumulative_row_lengths ) ) internal_idx = ( index if not block_idx else index _ cumulative_row_lengths [ block_idx _ number ] ) return block_idx , internal_idx	0
Q_22	insert an option_group at a give position	C_22_0	def insert_option_group ( self , idx , * args , * * kwargs ) group = self . add_option_group ( * args , * * kwargs ) self . option_groups . pop ( ) self . option_groups . insert ( idx , group ) return group	1
Q_22	insert an option_group at a give position	C_22_1	def get_simulations ( self , times , * * kwargs ) for var in times yield self . get_simulation ( var , * * kwargs )	0
Q_22	insert an option_group at a give position	C_22_2	def email ( email_address ) if not email_address click . secho ( dtool_config . utils . get_user_email ( config_path ) ) else click . secho ( dtool_config . utils . set_user_email ( config_path , email_address ) )	0
Q_23	generate cifar example as dicts	C_23_0	def _generate_examples ( self , filepaths ) label_keys = self . _cifar_info . label_keys for var in filepaths for labels , np_image in _load_data ( var , len ( label_keys ) ) row = dict ( zip ( label_keys , labels ) ) row [ string ] = np_image yield row	1
Q_23	generate cifar example as dicts	C_23_1	def _put_overlay ( self , overlay_name , overlay ) if not isinstance ( overlay , dict ) raise type_error ( string ) if set ( self . _identifiers ( ) ) = set ( overlay . keys ( ) ) raise value_error ( string ) self . _storage_broker . put_overlay ( overlay_name , overlay )	0
Q_23	generate cifar example as dicts	C_23_2	def cmd_as_file ( cmd , * args , * * kwargs ) kwargs [ string ] = subprocess . pipe stdin = kwargs . pop ( string , none ) if isinstance ( stdin , basestring ) with tempfile . temporary_file ( ) as stdin_file stdin_file . write ( stdin ) stdin_file . seek ( number ) kwargs [ string ] = stdin_file p = subprocess . popen ( cmd , * args , * * kwargs ) else p = subprocess . popen ( cmd , * args , * * kwargs ) try yield p . stdout finally p . stdout . close ( ) if p . wait ( ) raise subprocess . called_process_error ( p . returncode , cmd )	0
Q_24	give two node i_ds return all the pool layer between them	C_24_0	def _get_pooling_layers ( self , start_node_id , end_node_id ) layer_list = [ ] node_list = [ start_node_id ] assert self . _depth_first_search ( end_node_id , layer_list , node_list ) ret = [ ] for var in layer_list layer = self . layer_list [ var ] if is_layer ( layer , string ) ret . append ( layer ) elif is_layer ( layer , string ) and layer . stride = number ret . append ( layer ) return ret	1
Q_24	give two node i_ds return all the pool layer between them	C_24_1	def read_molden ( inputfile , start_index = number , get_bonds = true ) from chemcoord . cartesian_coordinates . cartesian_class_main import cartesian with open ( inputfile , string ) as f found = false while not found line = f . readline ( ) if string in line found = true number_of_molecules = int ( f . readline ( ) . strip ( ) ) energies = [ ] found = false while not found line = f . readline ( ) if string in line found = true for var in range ( number_of_molecules ) energies . append ( float ( f . readline ( ) . strip ( ) ) ) found = false while not found line = f . readline ( ) if string in line found = true current_line = f . tell ( ) number_of_atoms = int ( f . readline ( ) . strip ( ) ) f . seek ( current_line ) cartesians = [ ] for energy in energies cartesian = cartesian . read_xyz ( f , start_index = start_index , get_bonds = get_bonds , nrows = number_of_atoms , engine = string ) cartesian . metadata [ string ] = energy cartesians . append ( cartesian ) return cartesians	0
Q_24	give two node i_ds return all the pool layer between them	C_24_2	def make_worksheet_data ( headers , worksheet ) data = [ ] row_idx = number while row_idx < worksheet . nrows cell_idx = number row_dict = { } while cell_idx < worksheet . ncols cell_type = worksheet . cell_type ( row_idx , cell_idx ) if cell_type in valid_cell_types cell_value = worksheet . cell_value ( row_idx , cell_idx ) try if cell_type = = number and cell_value . is_integer ( ) cell_value = int ( cell_value ) row_dict [ headers [ cell_idx ] ] = cell_value except key_error try column = ascii_uppercase [ cell_idx ] except index_error column = cell_idx puts ( string { number } string { number } string { number } string . format ( cell_value , column , worksheet . name ) ) cell_idx + = number data . append ( row_dict ) row_idx + = number if string in headers . values ( ) keyed_data = { } for var in data if string in var . keys ( ) key = slughifi ( var [ string ] ) if keyed_data . get ( key ) puts ( string { number } string string { number } string { number } string string { number } string . format ( key , keyed_data . get ( key ) , worksheet . name , var ) ) if worksheet . name = = string value = var . get ( string ) if value not in ( string , none ) keyed_data [ key ] = value else keyed_data [ key ] = var data = keyed_data return data	0
Q_25	get a list of tensor in the default graph by a list of name	C_25_0	def get_tensors_by_names ( names ) ret = [ ] g = tfv1 . get_default_graph ( ) for var in names opn , varn = get_op_tensor_name ( var ) ret . append ( g . get_tensor_by_name ( varn ) ) return ret	1
Q_25	get a list of tensor in the default graph by a list of name	C_25_1	def login_required ( method ) def wrapper ( self , * arg , * * karg ) if not self . user if self . request . method = = string self . redirect ( settings . login_path ) else self . error ( number ) else method ( self , * arg , * * karg ) return wrapper	0
Q_25	get a list of tensor in the default graph by a list of name	C_25_2	def _merge_layout ( x go . layout , y go . layout ) _ > go . layout xjson = x . to_plotly_json ( ) yjson = y . to_plotly_json ( ) if string in yjson and string in xjson xjson [ string ] + = yjson [ string ] yjson . update ( xjson ) return go . layout ( yjson )	0
Q_26	api call delete all entry in a logger via a delete request	C_26_0	def logger_delete ( self , project , logger_name ) path = string ( project , logger_name ) self . _gapic_api . delete_log ( path )	1
Q_26	api call delete all entry in a logger via a delete request	C_26_1	def _action ( action = string , search = none , one = true , force = false ) vms = { } matched_vms = [ ] client = salt . client . get_local_client ( _opts_ [ string ] ) try vmadm_args = { } vmadm_args [ string ] = string if string in search vmadm_args [ string ] = search for var in client . cmd_iter ( string , string , kwarg = vmadm_args , tgt_type = string ) if not var continue node = next ( six . iterkeys ( var ) ) if not isinstance ( var [ node ] , dict ) or string not in var [ node ] or not isinstance ( var [ node ] [ string ] , dict ) continue for vm in var [ node ] [ string ] vmcfg = var [ node ] [ string ] [ vm ] vmcfg [ string ] = node vms [ vm ] = vmcfg except salt_client_error as client_error pass if not vms return { string string } if string not in search loop_pass = number while loop_pass < number if loop_pass = = number field = string elif loop_pass = = number field = string else field = string for vm in vms if field = = string and vm = = search matched_vms . append ( vm ) break elif field in vms [ vm ] and vms [ vm ] [ field ] = = search matched_vms . append ( vm ) if matched_vms break else loop_pass + = number else for vm in vms matched_vms . append ( vm ) if not matched_vms return { string string } if one and len ( matched_vms ) > number return { string string . format ( len ( matched_vms ) ) , string matched_vms } ret = { } if action in [ string , string , string , string ] for vm in matched_vms vmadm_args = { string string , string vm } try for vmadm_res in client . cmd_iter ( vms [ vm ] [ string ] , string . format ( action ) , kwarg = vmadm_args ) if not vmadm_res continue if vms [ vm ] [ string ] in vmadm_res ret [ vm ] = vmadm_res [ vms [ vm ] [ string ] ] [ string ] except salt_client_error as client_error ret [ vm ] = false elif action in [ string ] ret = true for vm in matched_vms if vms [ vm ] [ string ] = string ret = false break return ret	0
Q_26	api call delete all entry in a logger via a delete request	C_26_2	def get_indels ( sbjct_seq , qry_seq , start_pos ) seqs = [ sbjct_seq , qry_seq ] indels = [ ] gap_obj = re . compile ( r string ) for var in range ( len ( seqs ) ) for match in gap_obj . finditer ( seqs [ var ] ) pos = int ( match . start ( ) ) gap = match . group ( ) sbj_pos = len ( sbjct_seq [ pos ] . replace ( string , string ) ) + start_pos indel = seqs [ abs ( var _ number ) ] [ pos pos + len ( gap ) ] codon_no = int ( math . ceil ( ( sbj_pos ) / number ) ) qry_pos = len ( qry_seq [ pos ] . replace ( string , string ) ) + start_pos qry_codon = int ( math . ceil ( ( qry_pos ) / number ) ) if var = = number mut = string else mut = string indels . append ( [ mut , codon_no , sbj_pos , indel , qry_codon ] ) indels = sorted ( indels , key = lambda x ( x [ number ] , x [ number ] ) ) return indels	0
Q_27	make write protobufs for set method	C_27_0	def pbs_for_set_no_merge ( document_path , document_data ) extractor = document_extractor ( document_data ) if extractor . deleted_fields raise value_error ( string string merge = true string merge = [ field_paths ] string ) write_pbs = [ extractor . get_update_pb ( document_path ) ] if extractor . has_transforms transform_pb = extractor . get_transform_pb ( document_path ) write_pbs . append ( transform_pb ) return write_pbs	1
Q_27	make write protobufs for set method	C_27_1	def register_to_data_types ( cls ) from projexui . xdatatype import register_data_type register_data_type ( cls . _name_ , lambda pyvalue pyvalue . to_string ( ) , lambda qvariant cls . from_string ( unwrap_variant ( qvariant ) ) )	0
Q_27	make write protobufs for set method	C_27_2	def _make_thumbnail ( self , width , height ) ( w , h ) = self . size factor = max ( ( float ( w ) / width ) , ( float ( h ) / height ) ) w / = factor h / = factor return self . get_image ( ( round ( w ) , round ( h ) ) )	0
Q_28	send new trace to stackdriver trace or update exist trace	C_28_0	def patch_traces ( self , project_id , traces , retry = google . api_core . gapic_v1 . method . default , timeout = google . api_core . gapic_v1 . method . default , metadata = none , ) if string not in self . _inner_api_calls self . _inner_api_calls [ string ] = google . api_core . gapic_v1 . method . wrap_method ( self . transport . patch_traces , default_retry = self . _method_configs [ string ] . retry , default_timeout = self . _method_configs [ string ] . timeout , client_info = self . _client_info , ) request = trace_pb2 . patch_traces_request ( project_id = project_id , traces = traces ) if metadata is none metadata = [ ] metadata = list ( metadata ) try routing_header = [ ( string , project_id ) ] except attribute_error pass else routing_metadata = google . api_core . gapic_v1 . routing_header . to_grpc_metadata ( routing_header ) metadata . append ( routing_metadata ) self . _inner_api_calls [ string ] ( request , retry = retry , timeout = timeout , metadata = metadata )	1
Q_28	send new trace to stackdriver trace or update exist trace	C_28_1	def _get_flags ( osm_obj ) flags = [ ] if osm_obj . visible flags . append ( string ) if osm_obj . user flags . append ( string osm_obj . user ) if osm_obj . timestamp flags . append ( string osm_obj . timestamp . isoformat ( ) ) if osm_obj . tags flags . append ( string . join ( string ( var , var ) for var , var in sorted ( osm_obj . tags . items ( ) ) ) ) return flags	0
Q_28	send new trace to stackdriver trace or update exist trace	C_28_2	def fwhm2sigma ( fwhm ) fwhm = np . asarray ( fwhm ) return fwhm / np . sqrt ( number * np . log ( number ) )	0
Q_29	decline to acknowldge the give message	C_29_0	def nack ( self ) self . _request_queue . put ( requests . nack_request ( ack_id = self . _ack_id , byte_size = self . size ) )	1
Q_29	decline to acknowldge the give message	C_29_1	def iterative_limited_depth_first ( problem , graph_search = false , viewer = none ) solution = none limit = number while not solution solution = limited_depth_first ( problem , depth_limit = limit , graph_search = graph_search , viewer = viewer ) limit + = number if viewer viewer . event ( string , solution , string limit ) return solution	0
Q_29	decline to acknowldge the give message	C_29_2	def find_all_bycolumn ( self , target ) column_matches = [ ] for var in range ( self . _raster [ number ] ) column = self . get_row ( var ) column_matches [ var ] = column . find_all ( target ) return column_matches	0
Q_30	update work piece in datastore as complete	C_30_0	def update_work_as_completed ( self , worker_id , work_id , other_values = none , error = none ) client = self . _datastore_client try with client . transaction ( ) as transaction work_key = client . key ( kind_work_type , self . _work_type_entity_id , kind_work , work_id ) work_entity = client . get ( work_key , transaction = transaction ) if work_entity [ string ] = worker_id return false work_entity [ string ] = true if other_values work_entity . update ( other_values ) if error work_entity [ string ] = text_type ( error ) transaction . put ( work_entity ) except exception return false return true	1
Q_30	update work piece in datastore as complete	C_30_1	def combine_keys ( pks iterable [ ed25519_public_point ] ) _ > ed25519_public_point p = [ _ed25519 . decodepoint ( var ) for var in pks ] combine = reduce ( _ed25519 . edwards_add , p ) return ed25519_public_point ( _ed25519 . encodepoint ( combine ) )	0
Q_30	update work piece in datastore as complete	C_30_2	def unsnooze_alert ( self , id , * * kwargs ) kwargs [ string ] = true if kwargs . get ( string ) return self . unsnooze_alert_with_http_info ( id , * * kwargs ) else ( data ) = self . unsnooze_alert_with_http_info ( id , * * kwargs ) return data	0
Q_31	create a random s_frame with num_rows row and randomly	C_31_0	def generate_random_sframe ( num_rows , column_codes , random_seed = number ) from . extensions import _generate_random_sframe assert isinstance ( column_codes , str ) assert isinstance ( num_rows , int ) assert isinstance ( random_seed , int ) x = _generate_random_sframe ( num_rows , column_codes , random_seed , false , number ) x . _materialize_ ( ) return x	1
Q_31	create a random s_frame with num_rows row and randomly	C_31_1	def _prepare_body ( self ) content_type = self . _consume if not content_type content_type = self . _op . consumes [ number ] if self . _op . consumes else string if self . _op . consumes and content_type not in self . _op . consumes raise errs . schema_error ( string . format ( content_type , self . _op . consumes ) ) for var in self . _op . parameters var = final ( var ) if getattr ( var , string ) = = string schema = deref ( var . schema ) _type = schema . type _format = schema . format name = schema . name body = self . _p [ string ] [ var . name ] return content_type , self . _op . _mime_codec . marshal ( content_type , body , _type = _type , _format = _format , name = name ) return none , none	0
Q_31	create a random s_frame with num_rows row and randomly	C_31_2	def getnodefor ( self , name ) string node = self . _getnodenamefor ( name ) return { node self . cluster [ string ] [ node ] }	0
Q_32	create an xmltext_reader for a preparsed document	C_32_0	def reader_walker ( self ) ret = libxml2mod . xml_reader_walker ( self . _o ) if ret is none raise tree_error ( string ) _tmp = xml_text_reader ( _obj = ret ) return _tmp	1
Q_32	create an xmltext_reader for a preparsed document	C_32_1	def _register ( self , target , endpoint = none , blueprint = none , resource_class_args = none , resource_class_kwargs = none ) if isinstance ( target , types . function_type ) paths = self . view_converter . convert ( target , endpoint , blueprint ) elif isinstance ( target , resource_meta ) paths = self . resource_converter . convert ( target , endpoint , blueprint , resource_class_args = resource_class_args , resource_class_kwargs = resource_class_kwargs , ) else raise type_error ( ) for var in paths self . spec . var ( * * var )	0
Q_32	create an xmltext_reader for a preparsed document	C_32_2	def to_task ( self ) from google . appengine . api . taskqueue import task from google . appengine . api . taskqueue import task_retry_options self . _increment_recursion_level ( ) self . check_recursion_depth ( ) url = string ( async_endpoint , self . function_path ) kwargs = { string url , string self . get_headers ( ) . copy ( ) , string json . dumps ( self . to_dict ( ) ) } kwargs . update ( copy . deepcopy ( self . get_task_args ( ) ) ) retry_options = copy . deepcopy ( default_retry_options ) retry_options . update ( kwargs . pop ( string , { } ) ) kwargs [ string ] = task_retry_options ( * * retry_options ) return task ( * * kwargs )	0
Q_33	return configuration of linear_annealed_policy	C_33_0	def get_config ( self ) config = super ( linear_annealed_policy , self ) . get_config ( ) config [ string ] = self . attr config [ string ] = self . value_max config [ string ] = self . value_min config [ string ] = self . value_test config [ string ] = self . nb_steps config [ string ] = get_object_config ( self . inner_policy ) return config	1
Q_33	return configuration of linear_annealed_policy	C_33_1	def context_serve ( context , configfile , listen_addr , listen_port , logfile , debug , daemon , uid , gid , pidfile , umask , rundir ) global global_config server = none try with context level = logging . debug if debug else logging . info init_logging ( logfile = logfile , loglevel = level , configfile = configfile ) server = server_from_config ( ) logger . info ( string server . server_address ) if debug poll_interval = float ( global_config . get ( string , string ) ) if poll_interval def diagnostic_loop ( server ) log = logger while true log . debug ( string ) store = server . queue_manager . store for var in store . destinations ( ) log . debug ( string ( var , store . size ( var ) , server . queue_manager . subscriber_count ( var ) ) ) time . sleep ( poll_interval ) diagnostic_thread = threading . thread ( target = diagnostic_loop , name = string , args = ( server , ) ) diagnostic_thread . daemon = true diagnostic_thread . start ( ) server . serve_forever ( ) except ( keyboard_interrupt , system_exit ) logger . info ( string ) raise system_exit ( ) except exception as e logger . error ( string e ) logger . exception ( e ) raise system_exit ( ) finally if server server . server_close ( )	0
Q_33	return configuration of linear_annealed_policy	C_33_2	def set_attachments_order ( self , order ) if isinstance ( order , basestring ) new_order = self . storage . get ( string , [ ] ) new_order . append ( order ) order = new_order self . storage . update ( { string order } )	0
Q_34	source http github	C_34_0	def cast_bytes ( s , encoding = none ) if not isinstance ( s , bytes ) return encode ( s , encoding ) return s	1
Q_34	source http github	C_34_1	def init_transformers ( self ) self . _transformers = [ ] for var in _default_transformers var ( shell = self . shell , prefilter_manager = self , config = self . config )	0
Q_34	source http github	C_34_2	def t_prepro_define_pragma_defargs_defargsopt_continue ( self , t ) r string t . lexer . lineno + = number t . value = t . value [ number ] t . type = string return t	0
Q_35	create a saved_model_handler from a saved_model in path	C_35_0	def load ( path ) proto = _parse_saved_model ( path ) _merge_assets_key_collection ( proto , path ) handler = saved_model_handler ( ) handler . _proto = proto return handler	1
Q_35	create a saved_model_handler from a saved_model in path	C_35_1	def cachedir_index_del ( minion_id , base = none ) base = init_cachedir ( base ) index_file = os . path . join ( base , string ) lock_file ( index_file ) if os . path . exists ( index_file ) mode = string if six . py3 else string with salt . utils . files . fopen ( index_file , mode ) as fh_ index = salt . utils . data . decode ( salt . utils . msgpack . load ( fh_ , encoding = msgpack_encoding ) ) else return if minion_id in index del index [ minion_id ] mode = string if six . py3 else string with salt . utils . files . fopen ( index_file , mode ) as fh_ salt . utils . msgpack . dump ( index , fh_ , encoding = msgpack_encoding ) unlock_file ( index_file )	0
Q_35	create a saved_model_handler from a saved_model in path	C_35_2	def down_by_arrival ( * filters , local_dir = string , remote_dir = default_remote_dir ) remote_monitor = watch_remote_files ( * filters , remote_dir = remote_dir ) _ , var = next ( remote_monitor ) _notify_sync_ready ( len ( var ) , remote_dir , local_dir ) for var , var in remote_monitor if var _notify_sync ( direction . down , var ) down_by_files ( var , local_dir ) _notify_sync_ready ( len ( var ) , remote_dir , local_dir ) yield direction . down , var	0
Q_36	setup main window	C_36_0	"def setup ( self ) string string string logger . info ( string ) logger . info ( string ) ui_theme = conf . get ( string , string ) color_scheme = conf . get ( string , string ) if ui_theme = = string dark_qss = qdarkstyle . load_stylesheet_from_environment ( ) self . set_style_sheet ( dark_qss ) self . status_bar ( ) . set_style_sheet ( dark_qss ) css_path = dark_css_path elif ui_theme = = string if not is_dark_font_color ( color_scheme ) dark_qss = qdarkstyle . load_stylesheet_from_environment ( ) self . set_style_sheet ( dark_qss ) self . status_bar ( ) . set_style_sheet ( dark_qss ) css_path = dark_css_path else css_path = css_path else css_path = css_path logger . info ( string ) self . close_dockwidget_action = create_action ( self , icon = ima . icon ( string ) , text = _ ( string ) , triggered = self . close_current_dockwidget , context = qt . application_shortcut ) self . register_shortcut ( self . close_dockwidget_action , string , string ) self . lock_interface_action = create_action ( self , _ ( string ) , toggled = self . toggle_lock , context = qt . application_shortcut ) self . register_shortcut ( self . lock_interface_action , string , string ) self . toggle_next_layout_action = create_action ( self , _ ( string ) , triggered = self . toggle_next_layout , context = qt . application_shortcut ) self . toggle_previous_layout_action = create_action ( self , _ ( string ) , triggered = self . toggle_previous_layout , context = qt . application_shortcut ) self . register_shortcut ( self . toggle_next_layout_action , string , string ) self . register_shortcut ( self . toggle_previous_layout_action , string , string ) self . file_switcher_action = create_action ( self , _ ( string ) , icon = ima . icon ( string ) , tip = _ ( string ) , triggered = self . open_fileswitcher , context = qt . application_shortcut ) self . register_shortcut ( self . file_switcher_action , context = string , var = string ) self . symbol_finder_action = create_action ( self , _ ( string ) , icon = ima . icon ( string ) , tip = _ ( string ) , triggered = self . open_symbolfinder , context = qt . application_shortcut ) self . register_shortcut ( self . symbol_finder_action , context = string , var = string , add_sc_to_tip = true ) self . file_toolbar_actions = [ self . file_switcher_action , self . symbol_finder_action ] def create_edit_action ( text , tr_text , icon ) textseq = text . split ( string ) method_name = textseq [ number ] . lower ( ) + string . join ( textseq [ number ] ) action = create_action ( self , tr_text , icon = icon , triggered = self . global_callback , data = method_name , context = qt . widget_shortcut ) self . register_shortcut ( action , string , text ) return action self . undo_action = create_edit_action ( string , _ ( string ) , ima . icon ( string ) ) self . redo_action = create_edit_action ( string , _ ( string ) , ima . icon ( string ) ) self . copy_action = create_edit_action ( string , _ ( string ) , ima . icon ( string ) ) self . cut_action = create_edit_action ( string , _ ( string ) , ima . icon ( string ) ) self . paste_action = create_edit_action ( string , _ ( string ) , ima . icon ( string ) ) self . selectall_action = create_edit_action ( string , _ ( string ) , ima . icon ( string ) ) self . edit_menu_actions = [ self . undo_action , self . redo_action , none , self . cut_action , self . copy_action , self . paste_action , self . selectall_action ] namespace = none logger . info ( string ) self . file_menu = self . menu_bar ( ) . add_menu ( _ ( string ) ) self . file_toolbar = self . create_toolbar ( _ ( string ) , string ) self . edit_menu = self . menu_bar ( ) . add_menu ( _ ( string ) ) self . edit_toolbar = self . create_toolbar ( _ ( string ) , string ) self . search_menu = self . menu_bar ( ) . add_menu ( _ ( string ) ) self . search_toolbar = self . create_toolbar ( _ ( string ) , string ) self . source_menu = self . menu_bar ( ) . add_menu ( _ ( string ) ) self . source_toolbar = self . create_toolbar ( _ ( string ) , string ) self . run_menu = self . menu_bar ( ) . add_menu ( _ ( string ) ) self . run_toolbar = self . create_toolbar ( _ ( string ) , string ) self . debug_menu = self . menu_bar ( ) . add_menu ( _ ( string ) ) self . debug_toolbar = self . create_toolbar ( _ ( string ) , string ) self . consoles_menu = self . menu_bar ( ) . add_menu ( _ ( string ) ) self . consoles_menu . about_to_show . connect ( self . update_execution_state_kernel ) self . projects_menu = self . menu_bar ( ) . add_menu ( _ ( string ) ) self . projects_menu . about_to_show . connect ( self . valid_project ) self . tools_menu = self . menu_bar ( ) . add_menu ( _ ( string ) ) self . view_menu = self . menu_bar ( ) . add_menu ( _ ( string ) ) self . help_menu = self . menu_bar ( ) . add_menu ( _ ( string ) ) status = self . status_bar ( ) status . set_object_name ( string ) status . show_message ( _ ( string ) , number ) logger . info ( string ) prefs_action = create_action ( self , _ ( string ) , icon = ima . icon ( string ) , triggered = self . edit_preferences , context = qt . application_shortcut ) self . register_shortcut ( prefs_action , string , string , add_sc_to_tip = true ) spyder_path_action = create_action ( self , _ ( string ) , none , icon = ima . icon ( string ) , triggered = self . path_manager_callback , tip = _ ( string ) , menurole = q_action . application_specific_role ) reset_spyder_action = create_action ( self , _ ( string ) , triggered = self . reset_spyder ) self . tools_menu_actions = [ prefs_action , spyder_path_action ] if win_user_env_dialog is not none winenv_action = create_action ( self , _ ( string ) , icon = string , tip = _ ( string string string ) , triggered = self . win_env ) self . tools_menu_actions . append ( winenv_action ) self . tools_menu_actions + = [ menu_separator , reset_spyder_action ] self . external_tools_menu = q_menu ( _ ( string ) ) self . external_tools_menu_actions = [ ] self . wp_action = create_action ( self , _ ( string ) , icon = get_icon ( string ) , triggered = lambda programs . run_python_script ( string , string ) ) if os . var = = string and is_module_installed ( string ) self . external_tools_menu_actions . append ( self . wp_action ) additact = [ ] for var in ( string , string ) qtdact = create_program_action ( self , _ ( string ) , var ) if qtdact break for var in ( string , string ) qtlact = create_program_action ( self , _ ( string ) , string ) if qtlact break args = [ string ] if os . var = = string else [ ] for act in ( qtdact , qtlact ) if act additact . append ( act ) if additact and is_module_installed ( string ) self . external_tools_menu_actions + = [ none ] + additact logger . info ( string ) gdgq_act = [ ] try from guidata import configtools from guidata import config guidata_icon = configtools . get_icon ( string ) guidata_act = create_python_script_action ( self , _ ( string ) , guidata_icon , string , osp . join ( string , string ) ) gdgq_act + = [ guidata_act ] except pass try from guidata import configtools from guiqwt import config guiqwt_icon = configtools . get_icon ( string ) guiqwt_act = create_python_script_action ( self , _ ( string ) , guiqwt_icon , string , osp . join ( string , string ) ) if guiqwt_act gdgq_act + = [ guiqwt_act ] sift_icon = configtools . get_icon ( string ) sift_act = create_python_script_action ( self , _ ( string ) , sift_icon , string , osp . join ( string , string ) ) if sift_act gdgq_act + = [ sift_act ] except pass if gdgq_act self . external_tools_menu_actions + = [ none ] + gdgq_act self . maximize_action = create_action ( self , string , triggered = self . maximize_dockwidget , context = qt . application_shortcut ) self . register_shortcut ( self . maximize_action , string , string ) self . _update_maximize_action ( ) self . fullscreen_action = create_action ( self , _ ( string ) , triggered = self . toggle_fullscreen , context = qt . application_shortcut ) self . register_shortcut ( self . fullscreen_action , string , string , add_sc_to_tip = true ) self . main_toolbar_actions = [ self . maximize_action , self . fullscreen_action , none , prefs_action , spyder_path_action ] self . main_toolbar = self . create_toolbar ( _ ( string ) , string ) logger . info ( string ) from spyder . plugins . console . plugin import console self . console = console ( self , namespace , exitfunc = self . closing , profile = self . profile , multithreaded = self . multithreaded , message = _ ( string string string string string string t use it to run your code string starting language server protocol manager . string loading working directory . string help string enable string loading help . string outline_explorer string enable string loading outline explorer . string loading editor . string launching lsp client for python . string python string quit string exit string quit string _ string quit string restart string restart string restart string _ string restart string string loading namespace browser . string loading figure browser . string historylog string enable string loading history plugin . string loading i_python console . string explorer string enable string loading file explorer . string onlinehelp string enable string loading online help . string loading project explorer . string find_in_files string enable string breakpoints string profiler string pylint string enable string spyder . plugins . { } string loading third _ party plugins . string s s string setting up main window . string troubleshooting . string dependencies . string advanced string report issue . string bug string spyder support . string check for updates . string https / / docs . spyder _ ide . org / string spyder documentation string dialog_help_button string _ string spyder documentation string spyder tutorial string shortcuts summary string meta + f1 string interactive tours string last string var string string python documentation string i_python documentation string intro to i_python string quick reference string console help string string add installed python module doc action to help submenu string string pyside string s . png string doc string ( [ a _ z_a _ z _ ] * ) ( doc ) ? ( _ dev ) ? ( _ ref ) ? ( _ user ) ? . ( chm pdf ) string python string installed python modules string online documentation string linux string assistant _ qt4 string assistant string qt documentation string about s . string spyder string message_box_information string panes string toolbars string window layouts string show toolbars string _ string show toolbars string attached console window ( debugging ) string external tools string setting up window . string darwin string file string edit string search string source string run string debug string projects string tools string plugins string _menu string main string tear_off_menus string * * * end of main_window setup * * * "" ) self . is_starting_up = false"	1
Q_36	setup main window	C_36_1	def cookies ( self ) return { var self . raw_cookies [ var ] . value for var in self . raw_cookies . keys ( ) }	0
Q_36	setup main window	C_36_2	def usersettings ( request ) if hasattr ( request , string ) usersettings = request . usersettings else from . shortcuts import get_current_usersettings usersettings = get_current_usersettings ( ) return { string usersettings }	0
Q_37	file be rename in file explorer widget or in project explorer	C_37_0	def renamed ( self , source , dest ) string string string filename = osp . abspath ( to_text_string ( source ) ) index = self . editorstacks [ number ] . has_filename ( filename ) if index is not none for var in self . editorstacks var . rename_in_data ( filename , new_filename = to_text_string ( dest ) )	1
Q_37	file be rename in file explorer widget or in project explorer	C_37_1	def register_all ( module , exclude = [ ] , admin_class = admin . model_admin ) member_names = dir ( module ) for var in member_names obj = getattr ( module , var , none ) if isinstance ( obj , model_base ) and obj not in exclude and not obj . _meta . abstract try if hasattr ( obj , string ) admin . site . register ( obj , l10n ( obj , admin_class ) ) else admin . site . register ( obj , admin_class ) except already_registered pass	0
Q_37	file be rename in file explorer widget or in project explorer	C_37_2	def to_astropy_table ( self , column_names = none , selection = none , strings = true , virtual = false , index = none ) from astropy . table import table , column , masked_column meta = dict ( ) meta [ string ] = self . var meta [ string ] = self . description table = table ( meta = meta ) for var , var in self . to_items ( column_names = column_names , selection = selection , strings = strings , virtual = virtual ) if self . dtype ( var ) = = str_type var = np . array ( var ) . astype ( string ) meta = dict ( ) if var in self . ucds meta [ string ] = self . ucds [ var ] if np . ma . is_masked_array ( var ) cls = masked_column else cls = column table [ var ] = cls ( var , unit = self . unit ( var ) , description = self . descriptions . get ( var ) , meta = meta ) return table	0
Q_38	all logins be do over post this be a park endpoint	C_38_0	def get ( self ) self . set_status ( number ) self . set_header ( string , string ) ret = { string string , string string } self . write ( self . serialize ( ret ) )	1
Q_38	all logins be do over post this be a park endpoint	C_38_1	def publish ( self , tag , message ) payload = self . build_payload ( tag , message ) self . socket . send ( payload )	0
Q_38	all logins be do over post this be a park endpoint	C_38_2	def anchor_idx ( self ) anchor = self . anchor if anchor in [ string , string ] return anchor return self . valid_elements . get_by_id ( anchor ) . index_in_valids	0
Q_39	process a ret event return by salt for a particular minion	C_39_0	def process_ret_job_event ( self , event_data ) tag = event_data [ string ] event_info = event_data [ string ] _ , _ , jid , _ , var = tag . split ( string ) job = self . jobs . setdefault ( jid , { } ) var = job . setdefault ( string , { } ) . setdefault ( var , { } ) var . update ( { string event_info [ string ] } ) var . update ( { string event_info [ string ] } ) var . update ( { string event_info [ string ] } ) job_complete = all ( [ var [ string ] for var , var in six . iteritems ( job [ string ] ) ] ) if job_complete job [ string ] = string self . publish ( string , self . jobs )	1
Q_39	process a ret event return by salt for a particular minion	C_39_1	def _infer_type ( value , element_kind , element_name ) if value is none raise value_error ( _format ( string string , element_kind , element_name ) ) try return cimtype ( value ) except type_error as exc raise value_error ( _format ( string , element_kind , element_name , exc ) )	0
Q_39	process a ret event return by salt for a particular minion	C_39_2	def update_file ( self , file_id , upload_id ) put_data = { string upload_id , } return self . _put ( string + file_id , put_data , content_type = content_type . form )	0
Q_40	encrypt a message or file	C_40_0	def encrypt ( user = none , recipients = none , text = none , filename = none , output = none , sign = none , use_passphrase = false , gnupghome = none , bare = false ) ret = { string true , string string } gpg = _create_gpg ( user , gnupghome ) if use_passphrase gpg_passphrase = _salt_ [ string ] ( string ) if not gpg_passphrase raise salt_invocation_error ( string ) gpg_passphrase = gpg_passphrase [ string ] else gpg_passphrase = none if text result = gpg . encrypt ( text , recipients , passphrase = gpg_passphrase ) elif filename if gpg_1_3_1 with salt . utils . files . flopen ( filename , string ) as _fp _contents = _fp . read ( ) result = gpg . encrypt ( _contents , recipients , passphrase = gpg_passphrase , output = output ) else with salt . utils . files . flopen ( filename , string ) as _fp if output result = gpg . encrypt_file ( _fp , recipients , passphrase = gpg_passphrase , output = output , sign = sign ) else result = gpg . encrypt_file ( _fp , recipients , passphrase = gpg_passphrase , sign = sign ) else raise salt_invocation_error ( string ) if result . ok if not bare if output ret [ string ] = string . format ( output ) else ret [ string ] = result . data else ret = result . data else if not bare ret [ string ] = false ret [ string ] = string . format ( result . status ) else ret = false log . error ( result . stderr ) return ret	1
Q_40	encrypt a message or file	C_40_1	def plot_returns ( returns , live_start_date = none , ax = none ) if ax is none ax = plt . gca ( ) ax . set_label ( string ) ax . set_ylabel ( string ) if live_start_date is not none live_start_date = ep . utils . get_utc_timestamp ( live_start_date ) is_returns = returns . loc [ returns . index < live_start_date ] oos_returns = returns . loc [ returns . index > = live_start_date ] is_returns . plot ( ax = ax , color = string ) oos_returns . plot ( ax = ax , color = string ) else returns . plot ( ax = ax , color = string ) return ax	0
Q_40	encrypt a message or file	C_40_2	def add_automatic_comment ( self , ref ) if self . fixed is true text = ( downtime_fixed_message ( ref . my_type , time . strftime ( string , time . localtime ( self . start_time ) ) , time . strftime ( string , time . localtime ( self . end_time ) ) , ref . my_type ) ) else hours , remainder = divmod ( self . duration , number ) minutes , _ = divmod ( remainder , number ) text = ( downtime_flexible_message ( ref . my_type , time . strftime ( string , time . localtime ( self . start_time ) ) , time . strftime ( string , time . localtime ( self . end_time ) ) , hours , minutes , ref . my_type ) ) data = { string text , string number if ref . my_type = = string else number , string number , string number , string false , string ref . uuid } comment = comment ( data ) self . comment_id = comment . uuid ref . comments [ comment . uuid ] = comment return comment	0
Q_41	"run command through switch "" s cli"	C_41_0	def sendline ( command ) if ping ( ) is false init ( ) out , _ = details [ _worker_name ( ) ] . sendline ( command ) return out	1
Q_41	"run command through switch "" s cli"	C_41_1	def _build_metric_list_to_collect ( self , additional_metrics ) metrics_to_collect = { } for var in itervalues ( self . default_metrics ) metrics_to_collect . update ( var ) for option in additional_metrics additional_metrics = self . available_metrics . get ( option ) if not additional_metrics if option in self . default_metrics self . log . warning ( u string , option ) else self . log . warning ( u string , option ) continue self . log . debug ( u string , option ) metrics_to_collect . update ( additional_metrics ) return metrics_to_collect	0
Q_41	"run command through switch "" s cli"	C_41_2	def data_not_in ( db_data , user_data ) if isinstance ( user_data , list ) if db_data not in user_data return true return false	0
Q_42	return a vim	C_42_0	def get_vsan_cluster_config_system ( service_instance ) context = none if sys . version_info [ number ] > ( number , number , number ) context = ssl . create_default_context ( ) context . check_hostname = false context . verify_mode = ssl . cert_none stub = service_instance . _stub vc_mos = vsanapiutils . get_vsan_vc_mos ( stub , context = context ) return vc_mos [ string ]	1
Q_42	return a vim	C_42_1	def main ( ) x = [ _ number . number , _ number . number , _ number . number , number . number ] y = [ number . number , number . number , _ number . number , number . number ] n = [ number . number , number . number , number . number , number . number ] t = [ number . , number . number , number . , number . number ] dt = [ var _ min ( t ) for var in t ] plot = plot ( ) plot . scatter ( [ number ] , [ number ] , mark = string ) plot . add_pin_at_xy ( number , number , string , use_arrow = false , location = string ) plot . scatter_table ( x , y , dt , n ) plot . set_scalebar ( location = string ) plot . set_colorbar ( string ) plot . set_axis_equal ( ) plot . set_mlimits ( max = number . ) plot . set_slimits ( min = number . , max = number . ) plot . set_xlabel ( string ) plot . set_ylabel ( string ) plot . save ( string ) x508 = [ number . number , number . number , _ number . number , number . number ] y508 = [ _ number . number , _ number . number , _ number . number , number . number ] n508 = [ number . number , number . number , number . number , number . number ] t508 = [ number . , number . number , number . number , number . ] dt508 = [ var _ min ( t508 ) for var in t508 ] plot = multi_plot ( number , number , width = r string ) plot . set_xlimits_for_all ( min = _ number , max = number ) plot . set_ylimits_for_all ( min = _ number , max = number ) plot . set_mlimits_for_all ( min = number . , max = number . ) plot . set_colorbar ( string , false ) plot . set_colormap ( string ) plot . set_scalebar_for_all ( location = string ) p = plot . get_subplot_at ( number , number ) p . scatter ( [ number ] , [ number ] , mark = string ) p . add_pin_at_xy ( number , number , string , use_arrow = false , location = string ) p . scatter_table ( x , y , dt , n ) p . set_axis_equal ( ) p = plot . get_subplot_at ( number , number ) p . scatter ( [ number ] , [ number ] , mark = string ) p . add_pin_at_xy ( number , number , string , use_arrow = false , location = string ) p . scatter_table ( x508 , y508 , dt508 , n508 ) p . set_axis_equal ( ) plot . show_yticklabels_for_all ( [ ( number , number ) ] ) plot . show_xticklabels_for_all ( [ ( number , number ) , ( number , number ) ] ) plot . set_xlabel ( string ) plot . set_ylabel ( string ) plot . save ( string )	0
Q_42	return a vim	C_42_2	def run ( self , repo str , branch str , task task , git_repo repo , repo_path path ) from fabric import api from fabric . exceptions import command_timeout vm_location = self . get_vm_location ( ) self . ensure_vm_running ( vm_location ) logger . info ( string , vm_location ) self . check_docker_access ( ) self . get_image_for_repo ( repo , branch , git_repo , repo_path ) requirements_option , requirements_hash = self . get_requirements_information ( repo_path ) image_tag = self . get_image_tag ( requirements_option , requirements_hash , self . get_dependencies ( ) ) image_name = self . use_registry_name task_filename , task_json = self . serialized_task ( task ) ( vm_location / task_filename ) . write_text ( task_json ) container_name = self . get_container_name ( repo , branch , git_repo ) api . env . hosts = [ self . vagrant . user_hostname_port ( ) ] api . env . key_filename = self . vagrant . keyfile ( ) api . env . disable_known_hosts = true api . env . abort_exception = build_error api . env . shell = string if self . quiet api . output . everything = false else api . output . everything = true try res = api . execute ( self . fabric_task , container_name = container_name , definition_filename = task_filename , image_name = image_name , image_tag = image_tag , repository = str ( repo_path . relative_to ( path ( self . _arca . base_dir ) . resolve ( ) / string ) ) , timeout = task . timeout ) return result ( res [ self . vagrant . user_hostname_port ( ) ] . stdout ) except command_timeout raise build_timeout_error ( f string ) except build_error raise except exception as e logger . exception ( e ) raise build_error ( string , extra_info = { string e } ) finally if not self . keep_vm_running if self . destroy self . vagrant . destroy ( ) shutil . rmtree ( self . vagrant . root , ignore_errors = true ) self . vagrant = none else self . vagrant . halt ( )	0
Q_43	check to see if an rds parameter group exist	C_43_0	def parameter_group_exists ( name , tags = none , region = none , key = none , keyid = none , profile = none ) conn = _get_conn ( region = region , key = key , keyid = keyid , profile = profile ) try rds = conn . describe_db_parameter_groups ( db_parameter_group_name = name ) return { string bool ( rds ) , string none } except client_error as e resp = { } if e . response [ string ] [ string ] = = string resp [ string ] = false resp [ string ] = _utils_ [ string ] ( e ) return resp	1
Q_43	check to see if an rds parameter group exist	C_43_1	def validate ( cls , policy ) return policy in [ cls . public , cls . members , cls . admins ]	0
Q_43	check to see if an rds parameter group exist	C_43_2	def _set_show_system_monitor ( self , v , load = false ) if hasattr ( v , string ) v = v . _utype ( v ) try t = yang_dyn_class ( v , base = show_system_monitor . show_system_monitor , is_leaf = true , yang_name = string , rest_name = string , parent = self , path_helper = self . _path_helper , extmethods = self . _extmethods , register_paths = false , extensions = { u string { u string u string , u string u string } } , namespace = string , defining_module = string , yang_type = string , is_config = true ) except ( type_error , value_error ) raise value_error ( { string string string string , string string , string string string show _ system _ monitor string show _ system _ monitor string tailf _ common string hidden string rpccmd string actionpoint string system _ monitor _ show string urn brocade . com mgmt brocade _ system _ monitor _ ext string brocade _ system _ monitor _ ext string rpc string string , } ) self . _show_system_monitor = t if hasattr ( self , string ) self . _set ( )	0
Q_44	return the team of a distribute virtual portgroup	C_44_0	"def _get_dvportgroup_teaming ( pg_name , pg_default_port_config ) log . trace ( string s string teaming config string notify_switches string policy string reverse_policy string rolling_order string failure_criteria string check_beacon string check_duplex string check_error_percent string check_speed string full_duplex string percentage string speed string port_order string active string standby "" uplink_order . standby_uplink_port } } ) return ret_dict"	1
Q_44	return the team of a distribute virtual portgroup	C_44_1	def assemble ( ops , target = none ) target = get_py_internals ( target ) opmap = target [ string ] hasjrel = target [ string ] hasjabs = target [ string ] hasjump = set ( hasjrel ) set ( hasjabs ) have_argument = target [ string ] extended_arg = target [ string ] wordcode = target [ string ] if not wordcode def encode_op ( output , op_code , op_arg = none ) n = number if op_arg is none output . append ( op_code ) else n + = number ext_arg = op_arg number if ext_arg n + = number output . extend ( [ extended_arg , ext_arg number , ext_arg number ] ) op_arg = number output . extend ( [ op_code , op_arg number , op_arg number ] ) return n else def encode_op ( output , op_code , op_arg = none ) n = number if op_arg is none output . extend ( [ op_code , number ] ) else ext_arg = op_arg number if ext_arg n + = encode_op ( extended_arg , ext_arg ) output . extend ( [ op_code , op_arg number ] ) return n label_address = { } while true retry = false output = bytearray ( ) address = number for var in ops if isinstance ( var , label ) if label_address . get ( var ) = address retry = true label_address [ var ] = address continue op_code = opmap [ var . name ] op_arg = var . arg if op_code > = have_argument and op_arg is none raise value_error ( string var ) elif op_code < have_argument and op_arg is not none raise value_error ( string var ) elif isinstance ( op_arg , label ) if op_code not in hasjump raise value_error ( string var ) if op_arg not in ops raise value_error ( string ) op_arg = label_address . get ( op_arg ) if op_arg is none address + = encode_op ( output , op_code , number ) continue if op_code in hasjrel op_arg _ = address elif op_code in hasjump raise value_error ( string var ) n = encode_op ( output , op_code , op_arg ) address + = n if op_code in hasjrel if not wordcode op_arg = output [ _ number ] + ( output [ _ number ] < < number ) if op_arg < n ext_arg = output [ _ number ] + ( output [ _ number ] < < number ) _ number output [ _ number ] , output [ _ number ] = ext_arg number , ext_arg number op_arg + = number op_arg _ = n output [ _ number ] , output [ _ number ] = op_arg number , op_arg number else for i in itertools . count ( number , number ) if n < = output [ _ i ] output [ _ i ] _ = n break output [ _ i ] + = number _ n n = number if not retry return bytes ( output )	0
Q_44	return the team of a distribute virtual portgroup	C_44_2	def liftover ( pass_pos , matures ) fixed_pos = [ ] _print_header ( pass_pos ) for var in pass_pos mir = var [ string ] db_pos = matures [ var [ string ] ] mut = _parse_mut ( var [ string ] ) print ( [ db_pos [ mir ] , mut , var [ string ] ] ) var [ string ] = db_pos [ mir ] [ number ] + mut [ number ] _ number var [ string ] = list ( mut [ number ] ) fixed_pos . append ( var ) print_vcf ( var ) return fixed_pos	0
Q_45	return a vim	C_45_0	def _create_network_backing ( network_name , switch_type , parent_ref ) log . trace ( string string , network_name , switch_type , salt . utils . vmware . get_managed_object_name ( parent_ref ) ) backing = { } if network_name if switch_type = = string networks = salt . utils . vmware . get_networks ( parent_ref , network_names = [ network_name ] ) if not networks raise salt . exceptions . v_mware_object_retrieval_error ( string { number } string string . format ( network_name ) ) network_ref = networks [ number ] backing = vim . vm . device . virtual_ethernet_card . network_backing_info ( ) backing . device_name = network_name backing . network = network_ref elif switch_type = = string networks = salt . utils . vmware . get_dvportgroups ( parent_ref , portgroup_names = [ network_name ] ) if not networks raise salt . exceptions . v_mware_object_retrieval_error ( string { number } string string . format ( network_name ) ) network_ref = networks [ number ] dvs_port_connection = vim . dvs . port_connection ( portgroup_key = network_ref . key , switch_uuid = network_ref . config . distributed_virtual_switch . uuid ) backing = vim . vm . device . virtual_ethernet_card . distributed_virtual_port_backing_info ( ) backing . port = dvs_port_connection return backing	1
Q_45	return a vim	C_45_1	def filter_conflicts ( conflicts_list , fields ) for var in fields conflicts_list = filter_conflicts_by_path ( conflicts_list , var ) return conflicts_list	0
Q_45	return a vim	C_45_2	async def handle_client_hello ( self , client_addr , _ client_hello ) self . _logger . info ( string , client_addr ) self . _registered_clients . add ( client_addr ) await self . send_container_update_to_client ( [ client_addr ] )	0
Q_46	call tagstr on the instance	C_46_0	def destroy ( vm_name , call = none ) if call and call = string raise salt_cloud_system_exit ( string _ a destroy string ) conn = get_conn ( ) try node = conn . ex_get_node ( vm_name ) except exception as exc log . error ( string string string , vm_name , exc , exc_info_on_loglevel = logging . debug ) raise salt_cloud_system_exit ( string . format ( vm_name ) ) _utils_ [ string ] ( string , string , string . format ( vm_name ) , args = { string vm_name } , sock_dir = _opts_ [ string ] , transport = _opts_ [ string ] ) profile = none if node . extra [ string ] and string in node . extra [ string ] for var in node . extra [ string ] [ string ] if var [ string ] = = string profile = var [ string ] vm_ = get_configured_provider ( ) delete_boot_pd = false if profile and profile in vm_ [ string ] and string in vm_ [ string ] [ profile ] delete_boot_pd = vm_ [ string ] [ profile ] [ string ] try inst_deleted = conn . destroy_node ( node ) except exception as exc log . error ( string string string , vm_name , exc , exc_info_on_loglevel = logging . debug ) raise salt_cloud_system_exit ( string . format ( vm_name ) ) _utils_ [ string ] ( string , string , string . format ( vm_name ) , args = { string vm_name } , sock_dir = _opts_ [ string ] , transport = _opts_ [ string ] ) if delete_boot_pd log . info ( string string ) _utils_ [ string ] ( string , string , string , args = { string vm_name } , sock_dir = _opts_ [ string ] , transport = _opts_ [ string ] ) try conn . destroy_volume ( conn . ex_get_volume ( vm_name ) ) except exception as exc log . error ( string string string , vm_name , exc , exc_info_on_loglevel = logging . debug ) _utils_ [ string ] ( string , string , string , args = { string vm_name } , sock_dir = _opts_ [ string ] , transport = _opts_ [ string ] ) if _opts_ . get ( string , false ) is true _utils_ [ string ] ( vm_name , _active_provider_name_ . split ( string ) [ number ] , _opts_ ) return inst_deleted	1
Q_46	call tagstr on the instance	C_46_1	def _order_pases ( self , passes ) passes = set ( passes ) pass_deps = { } for var in passes _ , before , after = self . _known_passes [ var ] if var not in pass_deps pass_deps [ var ] = set ( ) for after_pass in after pass_deps [ var ] . add ( after_pass ) for other in before if other not in passes continue if other not in pass_deps pass_deps [ other ] = set ( ) pass_deps [ other ] . add ( var ) return toposort_flatten ( pass_deps )	0
Q_46	call tagstr on the instance	C_46_2	def optgroups ( self , name , value , attrs = none ) default = ( none , [ ] , number ) groups = [ default ] has_selected = false selected_choices = { str ( var ) for var in value } if not self . is_required and not self . allow_multiple_selected default [ number ] . append ( self . create_option ( name , string , string , false , number ) ) if not isinstance ( self . choices , model_choice_iterator ) return super ( model_select2_mixin , self ) . optgroups ( name , value , attrs = attrs ) selected_choices = { c for c in selected_choices if c not in self . choices . field . empty_values } field_name = self . choices . field . to_field_name or string query = q ( * * { string field_name selected_choices } ) for obj in self . choices . queryset . filter ( query ) option_value = self . choices . choice ( obj ) [ number ] option_label = self . label_from_instance ( obj ) selected = ( str ( option_value ) in value and ( has_selected is false or self . allow_multiple_selected ) ) if selected is true and has_selected is false has_selected = true index = len ( default [ number ] ) subgroup = default [ number ] subgroup . append ( self . create_option ( name , option_value , option_label , selected_choices , index ) ) return groups	0
Q_47	check if a computer be join to the domain	C_47_0	def join_domain ( name , username = none , password = none , account_ou = none , account_exists = false , restart = false ) ret = { string name , string { } , string true , string string { number } string . format ( name ) } current_domain_dic = _salt_ [ string ] ( ) if string in current_domain_dic current_domain = current_domain_dic [ string ] elif string in current_domain_dic current_domain = string else current_domain = none if name . lower ( ) = = current_domain . lower ( ) ret [ string ] = string { number } string . format ( name ) return ret if _opts_ [ string ] ret [ string ] = none ret [ string ] = string { number } string . format ( name ) return ret result = _salt_ [ string ] ( domain = name , username = username , password = password , account_ou = account_ou , account_exists = account_exists , restart = restart ) if result is not false ret [ string ] = string { number } string . format ( name ) if restart ret [ string ] + = string else ret [ string ] + = string ret [ string ] = { string current_domain , string name } else ret [ string ] = string { number } string . format ( name ) ret [ string ] = false return ret	1
Q_47	check if a computer be join to the domain	C_47_1	def _parse_value ( self , stats , field ) if field = = string value = self . _parse_conference ( stats ) elif field = = string value = self . _parse_team_abbreviation ( stats ) else value = utils . _parse_field ( player_scheme , stats , field ) return value	0
Q_47	check if a computer be join to the domain	C_47_2	def friends_description ( base_ur_ls ) friends = element ( etree . q_name ( ns_friends [ none ] , string ) , nsmap = ns_friends ) friends . set ( etree . q_name ( ns [ string ] , string ) , string . format ( friends_schema_location , friends_schema_location_xsd ) ) for var in base_ur_ls friends . append ( e ( string , var ) ) return etree . tostring ( friends , pretty_print = true )	0
Q_48	create a certificate authority	C_48_0	"def create_ca ( ca_name , bits = number , days = number , cn = string , c = string , st = string , l = string , o = string , ou = none , email_address = none , fixmode = false , cacert_path = none , ca_filename = none , digest = string , onlyif = none , unless = none , replace = false ) status = _check_onlyif_unless ( onlyif , unless ) if status is not none return none set_ca_path ( cacert_path ) if not ca_filename ca_filename = string . format ( ca_name ) certp = string . format ( cert_base_path ( ) , ca_name , ca_filename ) ca_keyp = string . format ( cert_base_path ( ) , ca_name , ca_filename ) if not replace and not fixmode and ca_exists ( ca_name , ca_filename = ca_filename ) return string { number } string . format ( ca_name ) if fixmode and not os . path . exists ( certp ) raise value_error ( string t fix string { number } / { number } string { number } / { number } string error loading existing private key string s , generating a new key s string { number } . unloadable . { number } string y m d h m s string saving unloadable ca ssl key in s string basic_constraints string ca true , pathlen number string key_usage string key_cert_sign , c_rl_sign string subject_key_identifier string hash string authority_key_identifier string issuer always , keyid always string { number } . { number } string y m d h m s string saving old ca ssl key in s string w string wb string wb string created private key string string created ca string string "" ) . format ( ca_name , cert_base_path ( ) , ca_filename ) return ret"	1
Q_48	create a certificate authority	C_48_1	def list_solvers ( args = none ) parser = argparse . argument_parser ( description = string string string ) parser . add_argument ( string , nargs = string , type = str , help = string ) parsed_args = parser . parse_args ( args ) requirements = { } for var in parsed_args . requirement try key , value = parse_solver_setting ( var ) except value_error as e parser . error ( str ( e ) ) else requirements [ key ] = value solvers = list ( filter_solvers ( _solvers , requirements ) ) solver_names = set ( solver [ string ] for solver in solvers ) priority = { } if string in os . environ names = os . environ [ string ] . split ( string ) for i , solver_name in enumerate ( names ) priority [ solver_name ] = len ( names ) _ i solvers = [ s for s in solvers if s [ string ] in priority ] solver_names = set ( priority ) else for solver in solvers priority [ solver [ string ] ] = solver [ string ] solvers = sorted ( solvers , key = lambda s priority . get ( s [ string ] , number ) , reverse = true ) status = number if len ( solvers ) > number print ( string ) for solver in solvers print ( string . format ( solver [ string ] ) ) print ( string . format ( solver [ string ] ) ) print ( string . format ( solver [ string ] ) ) print ( string . format ( solver [ string ] ) ) print ( string . format ( solver [ string ] ) ) print ( string . format ( solver [ string ] ) ) print ( ) else status = number print ( string ) print ( ) filtered_solvers_count = len ( _solvers ) _ len ( solvers ) if filtered_solvers_count > number or len ( _solver_import_errors ) > number print ( string ) for solver in _solvers if solver [ string ] not in solver_names print ( string . format ( solver [ string ] ) ) for solver , error in iteritems ( _solver_import_errors ) print ( string . format ( solver , error ) ) if status = number parser . exit ( status )	0
Q_48	create a certificate authority	C_48_2	def convert_reshape ( params , w_name , scope_name , inputs , layers , weights , names ) print ( string ) if names = = string tf_name = string + random_string ( number ) elif names = = string tf_name = w_name else tf_name = w_name + str ( random . random ( ) ) if len ( inputs ) > number if layers [ inputs [ number ] ] [ number ] = = _ number print ( string ) reshape = keras . layers . reshape ( layers [ inputs [ number ] + string ] , name = tf_name ) layers [ scope_name ] = reshape ( layers [ inputs [ number ] ] ) else if inputs [ number ] in layers reshape = keras . layers . reshape ( params [ string ] [ number ] , name = tf_name ) layers [ scope_name ] = reshape ( layers [ inputs [ number ] ] ) else print ( string )	0
Q_49	find an dictionary in a list of dictionary give it main key	C_49_0	def _lookup_element ( lst , key ) if not lst return { } for var in lst if not var or not isinstance ( var , dict ) continue if var . keys ( ) [ number ] = = key return var . values ( ) [ number ] return { }	1
Q_49	find an dictionary in a list of dictionary give it main key	C_49_1	def write_crc32 ( fo , bytes ) data = crc32 ( bytes ) number fo . write ( pack ( string , data ) )	0
Q_49	find an dictionary in a list of dictionary give it main key	C_49_2	def get_model ( app_label , model_name ) try from django . apps import apps from django . core . exceptions import app_registry_not_ready except import_error from django . db import models return models . get_model ( app_label , model_name ) try return apps . get_model ( app_label , model_name ) except app_registry_not_ready if apps . apps_ready and not apps . models_ready app_config = apps . get_app_config ( app_label ) import_module ( string ( app_config . name , string ) ) return apps . get_registered_model ( app_label , model_name ) else raise	0
Q_50	describe all tag match the filter criterion or all tag in the account otherwise	C_50_0	def get_all_tags ( filters = none , region = none , key = none , keyid = none , profile = none ) conn = _get_conn ( region = region , key = key , keyid = keyid , profile = profile ) try ret = conn . get_all_tags ( filters ) tags = { } for var in ret if var . res_id not in tags tags [ var . res_id ] = { } tags [ var . res_id ] [ var . name ] = var . value return tags except boto . exception . boto_server_error as e log . error ( e ) return { }	1
Q_50	describe all tag match the filter criterion or all tag in the account otherwise	C_50_1	def agm ( agm_file , dir_path = string , input_dir_path = string , meas_outfile = string , spec_outfile = string , samp_outfile = string , site_outfile = string , loc_outfile = string , spec_infile = string , samp_infile = string , site_infile = string , specimen = string , specnum = number , samp_con = string , location = string , instrument = string , institution = string , bak = false , syn = false , syntype = string , units = string , fmt = string , user = string ) citations = string meth = string version_num = pmag . get_version ( ) samps , sites = [ ] , [ ] input_dir_path , output_dir_path = pmag . fix_directories ( input_dir_path , dir_path ) specnum = _ int ( specnum ) if not specimen specimen = agm_file . split ( string ) [ number ] if not meas_outfile meas_outfile = specimen + string if not spec_outfile spec_outfile = specimen + string if not samp_outfile samp_file = specimen + string if not site_outfile site_file = specimen + string if not loc_outfile loc_outfile = specimen + string if bak meth = string output = output_dir_path + string if string = = samp_con [ number ] if string not in samp_con print ( string ) print ( string ) return false , string else z = samp_con . split ( string ) [ number ] samp_con = string if string = = samp_con [ number ] if string not in samp_con print ( string ) return false , string else z = samp_con . split ( string ) [ number ] samp_con = string else z = number if site_infile sites , file_type = pmag . magic_read ( site_infile ) if samp_infile samps , file_type = pmag . magic_read ( samp_infile ) if spec_infile specs , file_type = pmag . magic_read ( spec_infile ) if agm_file agm_file = pmag . resolve_file_name ( agm_file , input_dir_path ) data = pmag . open_file ( agm_file ) if not data print ( string ) return false , string if not agm_file print ( _doc_ ) print ( string ) return false , string if string not in data [ number ] and fmt = string fmt = string measnum , start , end = number , number , number if fmt = = string end = number for var in range ( len ( data ) ) line = data [ var ] rec = line . strip ( string ) . strip ( string ) . split ( ) if string in line units = rec [ _ number ] if string in rec start = var + number if ( string in rec ) and ( string in rec ) and ( not start ) start = var + number break elif fmt = = string start = number end = number meas_recs , spec_recs , samp_recs , site_recs , loc_recs = [ ] , [ ] , [ ] , [ ] , [ ] version_num = pmag . get_version ( ) stop = len ( data ) _ end for line in data [ start stop ] meas_rec , spec_rec , samp_rec , site_rec , loc_rec = { } , { } , { } , { } , { } if not syn meas_rec [ string ] = specimen if specnum = number sample = specimen [ specnum ] else sample = specimen if samp_infile and samps samp = pmag . get_dictitem ( samps , string , sample , string ) if len ( samp ) > number site = samp [ number ] [ string ] else site = string if site_infile and sites sites = pmag . get_dictitem ( sites , string , sample , string ) if len ( sites ) > number site = sites [ number ] [ string ] else site = string else site = pmag . parse_site ( sample , samp_con , z ) if location = string and location not in [ x [ string ] if string in list ( x . keys ( ) ) else string for x in loc_recs ] loc_rec [ string ] = location loc_recs . append ( loc_rec ) if site = string and site not in [ x [ string ] if string in list ( x . keys ( ) ) else string for x in site_recs ] site_rec [ string ] = location site_rec [ string ] = site site_recs . append ( site_rec ) if sample = string and sample not in [ x [ string ] if string in list ( x . keys ( ) ) else string for x in samp_recs ] samp_rec [ string ] = site samp_rec [ string ] = sample samp_recs . append ( samp_rec ) if specimen = string and specimen not in [ x [ string ] if string in list ( x . keys ( ) ) else string for x in spec_recs ] spec_rec [ string ] = specimen spec_rec [ string ] = sample spec_recs . append ( spec_rec ) else samp_rec [ string ] = syntype meas_rec [ string ] = specimen meas_rec [ string ] = user if specnum = number sample = specimen [ specnum ] else sample = specimen site = pmag . parse_site ( sample , samp_con , z ) if location = string and location not in [ x [ string ] if string in list ( x . keys ( ) ) else string for x in loc_recs ] loc_rec [ string ] = location loc_recs . append ( loc_rec ) if site = string and site not in [ x [ string ] if string in list ( x . keys ( ) ) else string for x in site_recs ] site_rec [ string ] = location site_rec [ string ] = site site_recs . append ( site_rec ) if sample = string and sample not in [ x [ string ] if string in list ( x . keys ( ) ) else string for x in samp_recs ] samp_rec [ string ] = site samp_rec [ string ] = sample samp_recs . append ( samp_rec ) if specimen = string and specimen not in [ x [ string ] if string in list ( x . keys ( ) ) else string for x in spec_recs ] spec_rec [ string ] = specimen spec_rec [ string ] = sample spec_recs . append ( spec_rec ) meas_rec [ string ] = instrument meas_rec [ string ] = institution meas_rec [ string ] = meth meas_rec [ string ] = specimen + string + meth if fmt = = string rec = list ( line . strip ( string ) . split ( ) ) else rec = list ( line . strip ( string ) . strip ( string ) . split ( string ) ) if rec [ number ] = string if units = = string field = float ( rec [ number ] ) * number e _ number else field = float ( rec [ number ] ) if meth = = string meas_rec [ string ] = string ( field ) meas_rec [ string ] = string else meas_rec [ string ] = string meas_rec [ string ] = string ( field ) if units = = string meas_rec [ string ] = string ( float ( rec [ number ] ) * number e _ number ) else meas_rec [ string ] = string ( float ( rec [ number ] ) ) meas_rec [ string ] = string meas_rec [ string ] = string meas_rec [ string ] = string meas_rec [ string ] = string meas_rec [ string ] = string ( measnum ) meas_rec [ string ] = specimen + string + meth + string ( measnum ) measnum + = number meas_rec [ string ] = version_num meas_rec [ string ] = string meas_recs . append ( meas_rec ) if meth = = string recnum = number while float ( meas_recs [ recnum ] [ string ] ) < float ( meas_recs [ recnum + number ] [ string ] ) and recnum + number < len ( meas_recs ) meas_recs [ recnum ] [ string ] = string meas_recs [ recnum ] [ string ] = meas_recs [ recnum ] [ string ] + string + string recnum + = number con = cb . contribution ( output_dir_path , read_tables = [ ] ) con . add_magic_table_from_data ( dtype = string , data = spec_recs ) con . add_magic_table_from_data ( dtype = string , data = samp_recs ) con . add_magic_table_from_data ( dtype = string , data = site_recs ) con . add_magic_table_from_data ( dtype = string , data = loc_recs ) con . add_magic_table_from_data ( dtype = string , data = meas_recs ) con . write_table_to_file ( string , custom_name = spec_outfile ) con . write_table_to_file ( string , custom_name = samp_outfile ) con . write_table_to_file ( string , custom_name = site_outfile ) con . write_table_to_file ( string , custom_name = loc_outfile ) con . write_table_to_file ( string , custom_name = meas_outfile ) return true , meas_outfile	0
Q_50	describe all tag match the filter criterion or all tag in the account otherwise	C_50_2	def add_summary_page ( self ) s = portfolio_summary ( ) s . include_long_short ( ) pieces = [ ] for var in self . results tmp = s ( var . port , portfolio_summary . analyze_returns ) tmp [ string ] = var . desc tmp [ string ] = var . sid tmp = tmp . set_index ( [ string , string ] , append = number ) . reorder_levels ( [ number , number , number ] ) pieces . append ( tmp ) frame = pd . concat ( pieces ) tf = self . pdf . table_formatter ( frame ) tf . apply_basic_style ( cmap = self . table_style ) tf . cells . match_column_labels ( [ string , string , string , string , string ] ) . int_format ( ) tf . cells . match_column_labels ( [ string , string , string ] ) . float_format ( precision = number ) tf . cells . match_column_labels ( [ string ] ) . apply_format ( new_datetime_formatter ( string ) ) tf . cells . match_column_labels ( [ string , string , string , string , string , string , string , string , string , string , string ] ) . percent_format ( ) self . pdf . build_page ( string , { string tf . build ( ) } )	0
Q_51	revoke a privilege on a database from a user	C_51_0	def revoke_privilege ( database , privilege , username , * * client_args ) client = _client ( * * client_args ) client . revoke_privilege ( privilege , database , username ) return true	1
Q_51	revoke a privilege on a database from a user	C_51_1	def plot_front ( self ) u_angle = np . linspace ( _ np . pi , number , number ) v_angle = np . linspace ( number , np . pi , number ) x_dir = np . outer ( np . cos ( u_angle ) , np . sin ( v_angle ) ) y_dir = np . outer ( np . sin ( u_angle ) , np . sin ( v_angle ) ) z_dir = np . outer ( np . ones ( u_angle . shape [ number ] ) , np . cos ( v_angle ) ) self . axes . plot_surface ( x_dir , y_dir , z_dir , rstride = number , cstride = number , color = self . sphere_color , linewidth = number , alpha = self . sphere_alpha ) self . axes . plot_wireframe ( x_dir , y_dir , z_dir , rstride = number , cstride = number , color = self . frame_color , alpha = self . frame_alpha ) self . axes . plot ( number . number * np . cos ( u_angle ) , number . number * np . sin ( u_angle ) , zs = number , zdir = string , lw = self . frame_width , color = self . frame_color ) self . axes . plot ( number . number * np . cos ( u_angle ) , number . number * np . sin ( u_angle ) , zs = number , zdir = string , lw = self . frame_width , color = self . frame_color )	0
Q_51	revoke a privilege on a database from a user	C_51_2	def vcf2cytosure ( institute_id , case_name , individual_id ) ( display_name , vcf2cytosure ) = controllers . vcf2cytosure ( store , institute_id , case_name , individual_id ) outdir = os . path . abspath ( os . path . dirname ( vcf2cytosure ) ) filename = os . path . basename ( vcf2cytosure ) log . debug ( string . format ( filename , outdir ) ) attachment_filename = display_name + string return send_from_directory ( outdir , filename , attachment_filename = attachment_filename , as_attachment = true )	0
Q_52	ensure a load balancer be absent	C_52_0	def balancer_absent ( name , profile , * * libcloud_kwargs ) balancers = _salt_ [ string ] ( profile ) match = [ var for var in balancers if var [ string ] = = name ] if not match return state_result ( true , string , name ) else result = _salt_ [ string ] ( match [ number ] [ string ] , profile , * * libcloud_kwargs ) return state_result ( result , string , name )	1
Q_52	ensure a load balancer be absent	C_52_1	def loadnetcdf ( filename , copy = true ) filename = str ( path ( filename ) . expanduser ( ) ) if copy dataarray = xr . open_dataarray ( filename ) . copy ( ) else dataarray = xr . open_dataarray ( filename , chunks = { } ) if dataarray . name is none dataarray . name = filename . rstrip ( string ) for var , var in dataarray . coords . items ( ) if var . dtype . kind = = string dataarray [ var ] = var . astype ( string ) elif var . dtype = = np . int32 dataarray [ var ] = var . astype ( string ) return dataarray	0
Q_52	ensure a load balancer be absent	C_52_2	def waitforcardevent ( self ) abstract_card_request . waitforcardevent ( self ) presentcards = [ ] evt = threading . event ( ) if infinite = = self . timeout timertimeout = number else timertimeout = self . timeout timer = threading . timer ( timertimeout , signal_event , [ evt , infinite = = self . timeout ] ) readerstates = { } timerstarted = false while not evt . is_set ( ) if not timerstarted timerstarted = true timer . start ( ) time . sleep ( self . pollinginterval ) readernames = self . get_reader_names ( ) for var in readernames if not var in readerstates readerstates [ var ] = ( var , scard_state_unaware ) for oldreader in list ( readerstates . keys ( ) ) if oldreader not in readernames del readerstates [ oldreader ] if { } = readerstates hresult , newstates = s_card_get_status_change ( self . hcontext , number , list ( readerstates . values ( ) ) ) else hresult = number newstates = [ ] if scard_e_timeout = = hresult if evt . is_set ( ) raise card_request_timeout_exception ( ) elif scard_e_unknown_reader = = hresult pass elif number = hresult timer . cancel ( ) raise card_request_exception ( string + s_card_get_error_message ( hresult ) ) else timer . cancel ( ) for state in newstates readername , eventstate , atr = state r , oldstate = readerstates [ readername ] if oldstate scard_state_present and eventstate ( scard_state_changed scard_state_present ) eventstate = eventstate ( number ^ scard_state_changed ) if eventstate scard_state_present and eventstate scard_state_changed presentcards . append ( card . card ( readername , atr ) ) return presentcards if evt . is_set ( ) raise card_request_timeout_exception ( )	0
Q_53	return a boolean indicate whether give document exist	C_53_0	def document_exists ( index , id , doc_type = string , hosts = none , profile = none ) es = _get_instance ( hosts , profile ) try return es . exists ( index = index , id = id , doc_type = doc_type ) except elasticsearch . exceptions . not_found_error return false except elasticsearch . transport_error as e raise command_execution_error ( string . format ( id , index , e . status_code , e . error ) )	1
Q_53	return a boolean indicate whether give document exist	C_53_1	def cli ( env , identifier ) vsi = soft_layer . vs_manager ( env . client ) vs_id = helpers . resolve_id ( vsi . resolve_ids , identifier , string ) if not ( env . skip_confirmations or formatting . no_going_back ( vs_id ) ) raise exceptions . cli_abort ( string ) vsi . cancel_instance ( vs_id )	0
Q_53	return a boolean indicate whether give document exist	C_53_2	def reduce_list_size ( li ) size = len ( repr ( li ) ) keep = li toss = [ ] n = len ( li ) decrement_by = max ( n / number , number ) while ( size > = max_size ) and ( n > number ) n _ = decrement_by toss = li [ _ n ] keep = li [ _ n ] size = len ( repr ( keep ) ) return keep , toss	0
Q_54	create a rule in a security group	C_54_0	def security_group_rule_create ( auth = none , * * kwargs ) cloud = get_operator_cloud ( auth ) kwargs = _clean_kwargs ( * * kwargs ) return cloud . create_security_group_rule ( * * kwargs )	1
Q_54	create a rule in a security group	C_54_1	def resources ( argv = sys . argv [ number ] ) eps = iter_entry_points ( string ) ep_map = { var . name var . load ( ) for var in eps } parser = argparse . argument_parser ( ) if string in argv print ( string ) return number subparsers = { } subparser_factory = parser . add_subparsers ( ) subparsers [ string ] = subparser_factory . add_parser ( string , help = string ) subparsers [ string ] . add_argument ( string , nargs = string ) subparsers [ string ] . set_defaults ( subcommand = string ) for name , subcommand in ep_map . items ( ) subparsers [ name ] = subparser_factory . add_parser ( name , help = subcommand . _doc_ ) subparsers [ name ] . set_defaults ( subcommand = subcommand ) for args , kwargs in getattr ( subcommand , string , [ ] ) subparsers [ name ] . add_argument ( * args , * * kwargs ) for argset in getattr ( subcommand , string , { } ) . values ( ) group = subparsers [ name ] . add_mutually_exclusive_group ( required = true ) for args , kwargs in argset group . add_argument ( * args , * * kwargs ) opts = parser . parse_args ( argv ) if opts . subcommand = = string if opts . command subparsers [ opts . command ] . print_help ( ) else parser . print_help ( ) else return _exit ( opts . subcommand ( opts ) or number )	0
Q_54	create a rule in a security group	C_54_2	def _load ( cls , path ) assert os . path . basename ( path ) = = string bytes_io = bytes_io ( ) with zip_file ( file = bytes_io , mode = string ) as zip_file for dir_name , _ , file_list in os . walk ( path ) zip_file . write ( dir_name ) for var in file_list full_path = os . path . join ( dir_name , var ) zip_file . write ( full_path , os . path . relpath ( full_path , path ) ) bytes_io . seek ( number ) return bytes_io	0
Q_55	convert a dictionary to a list of dictionary to facilitate ordering	C_55_0	def _dict_to_name_value ( data ) if isinstance ( data , dict ) sorted_data = sorted ( data . items ( ) , key = lambda s s [ number ] ) result = [ ] for var , var in sorted_data if isinstance ( var , dict ) result . append ( { var _dict_to_name_value ( var ) } ) else result . append ( { var var } ) else result = data return result	1
Q_55	convert a dictionary to a list of dictionary to facilitate ordering	C_55_1	def clone ( self , folder , git_repository ) os . makedirs ( folder ) git . git ( ) . clone ( git_repository , folder )	0
Q_55	convert a dictionary to a list of dictionary to facilitate ordering	C_55_2	def _wrap_with_tuple ( self ) _ > tuple l = list ( ) length = len ( self . data ) while self . idx < length l . append ( self . _parse ( ) ) return tuple ( l )	0
Q_56	return the image object to use	C_56_0	def get_image ( server_ ) images = avail_images ( ) server_image = six . text_type ( config . get_cloud_config_value ( string , server_ , _opts_ , search_global = false ) ) for var in images if server_image in ( images [ var ] [ string ] , images [ var ] [ string ] ) return images [ var ] [ string ] raise salt_cloud_not_found ( string { number } string . format ( server_image ) )	1
Q_56	return the image object to use	C_56_1	def lithospheric_stress ( step , trench , ridge , time ) timestep = step . isnap base_lith = step . geom . rcmb + number _ number . number stressfld = step . fields [ string ] [ number , , , number ] stressfld = np . ma . masked_where ( step . geom . r_mesh [ number ] < base_lith , stressfld ) dzm = ( step . geom . r_coord [ number ] _ step . geom . r_coord [ _ number ] ) stress_lith = np . sum ( ( stressfld [ , number ] * dzm . t ) , axis = number ) ph_coord = step . geom . p_coord fig , axis , _ , _ = field . plot_scalar ( step , string , stressfld , cmap = string , vmin = number , vmax = number ) axis . text ( number . , number . number , str ( round ( time , number ) ) + string , transform = axis . trans_axes ) axis . text ( number . , number . number , str ( timestep ) , transform = axis . trans_axes ) misc . saveplot ( fig , string , timestep ) vphi = step . fields [ string ] [ number , , , number ] vph2 = number . number * ( vphi + np . roll ( vphi , number , number ) ) concfld = step . fields [ string ] [ number , , , number ] if step . sdat . par [ string ] [ string ] dsa = step . sdat . par [ string ] [ string ] indcont = np . argmin ( abs ( ( number _ dsa ) _ step . geom . r_coord ) ) _ number else indcont = _ number if step . sdat . par [ string ] [ string ] and not step . sdat . par [ string ] [ string ] continents = np . ma . masked_where ( np . logical_or ( concfld [ _ number , indcont ] < number , concfld [ _ number , indcont ] > number ) , concfld [ _ number , indcont ] ) elif step . sdat . par [ string ] [ string ] and step . sdat . par [ string ] [ string ] continents = np . ma . masked_where ( np . logical_or ( concfld [ _ number , indcont ] < number , concfld [ _ number , indcont ] > number ) , concfld [ _ number , indcont ] ) elif step . sdat . par [ string ] [ string ] continents = np . ma . masked_where ( concfld [ _ number , indcont ] < number , concfld [ _ number , indcont ] ) else continents = np . ma . masked_where ( concfld [ _ number , indcont ] < number , concfld [ _ number , indcont ] ) continentsall = continents / continents fig0 , ( ax1 , ax2 ) = plt . subplots ( number , number , sharex = true , figsize = ( number , number ) ) ax1 . plot ( ph_coord [ _ number ] , vph2 [ _ number , _ number ] , label = string ) ax1 . axhline ( y = number , xmin = number , xmax = number * np . pi , color = string , ls = string , alpha = number . number ) ax1 . set_ylabel ( string ) ax1 . text ( number . number , number . number number , str ( round ( time , number ) ) + string , transform = ax1 . trans_axes ) ax1 . text ( number . number number , number . number number , str ( round ( step . geom . ti_ad , number ) ) , transform = ax1 . trans_axes ) intstr_scale = step . sdat . scales . stress * step . sdat . scales . length / number . e12 ax2 . plot ( ph_coord , stress_lith * intstr_scale , color = string , label = string ) ax2 . set_ylabel ( r string ) plot_plate_limits ( ax1 , ridge , trench , conf . plates . vmin , conf . plates . vmax ) plot_plate_limits ( ax2 , ridge , trench , conf . plates . stressmin , conf . plates . lstressmax ) ax1 . set_xlim ( number , number * np . pi ) ax1 . set_title ( timestep ) ax1 . fill_between ( ph_coord [ _ number ] , continentsall * conf . plates . vmin , conf . plates . vmax , facecolor = string ax2 . set_ylim ( conf . plates . stressmin , conf . plates . lstressmax ) misc . saveplot ( fig0 , string , timestep )	0
Q_56	return the image object to use	C_56_2	def depsolve ( self ) requires = { } provides = { } for var , var in self . recipes . items ( ) l = self . load_recipe_object ( var ) for field , vv in l . requirements ( ) . items ( ) if vv . type . isproduct ( ) name = vv . type . name ( ) pe = product_entry ( name , var , field ) requires [ name ] = pe for field , vv in l . products ( ) . items ( ) if vv . type . isproduct ( ) name = vv . type . name ( ) pe = product_entry ( name , var , field ) provides [ name ] = pe return requires , provides	0
Q_57	handle a fire_master event	C_57_0	def _handle_tag_fire_master ( self , tag , data ) if self . connected log . debug ( string , data [ string ] ) self . _fire_master ( data [ string ] , data [ string ] , data [ string ] , data [ string ] )	1
Q_57	handle a fire_master event	C_57_1	async def post_heartbeat ( self , msg , _context ) name = msg . get ( string ) await self . service_manager . send_heartbeat ( name )	0
Q_57	handle a fire_master event	C_57_2	def package_releases ( self , package_name ) if self . debug self . logger . debug ( string + package_name ) return self . xmlrpc . package_releases ( package_name )	0
Q_58	call an execution module with the give argument and keyword argument	C_58_0	def cmd ( self , fun , * args , * * kwargs ) func = self . sminion . functions [ fun ] data = { string args , string fun } data . update ( kwargs ) executors = getattr ( self . sminion , string , [ ] ) or self . opts . get ( string , [ string ] ) if isinstance ( executors , six . string_types ) executors = [ executors ] for var in executors fname = string . format ( var ) if fname not in self . sminion . executors raise salt_invocation_error ( string { number } string . format ( var ) ) return_data = self . sminion . executors [ fname ] ( self . opts , data , func , args , kwargs ) if return_data is not none break return return_data	1
Q_58	call an execution module with the give argument and keyword argument	C_58_1	def print_synthese ( fct , df ) res_count = dict ( ) polluant , res = fct ( df ) print ( string polluant ) print ( string ) for var , var in res . items ( ) comp = compresse ( var ) if not comp . empty comp . index . name = var print ( comp . to_string ( na_rep = string , float_format = lambda x string x ) ) else print ( string var ) res_count [ var ] = var . count ( ) res_count = pd . data_frame ( res_count ) . t print ( string ) print ( res_count )	0
Q_58	call an execution module with the give argument and keyword argument	C_58_2	def check_section ( node , section , keys = none ) if keys for var in keys if var not in node raise value_error ( string ( var , section ) )	0
Q_59	resolve this address	C_59_0	def _translate_addr ( self , a ) if isinstance ( a , claripy . ast . base ) and not a . singlevalued raise sim_fast_memory_error ( string ) return self . state . solver . eval ( a )	1
Q_59	resolve this address	C_59_1	def duel_command ( f ) functools . wraps ( f ) def func_wrapper ( self , * argv , * * kwargs ) commands = _get_commands ( f , self , * argv , * * kwargs ) return self . execute_dual ( commands ) return func_wrapper	0
Q_59	resolve this address	C_59_2	def set_sort_data ( self , column , data ) string string string self . set_data ( column , self . sort_role , wrap_variant ( data ) )	0
Q_60	preconstrain the content of a file	C_60_0	"def preconstrain_file ( self , content , simfile , set_length = false ) repair_entry_state_opts = false if o . track_action_history in self . state . options repair_entry_state_opts = true self . state . options _ = { o . track_action_history } if set_length simfile . has_end = false pos = number for var in content if type ( var ) is int var = bytes ( [ var ] ) data , length , pos = simfile . read ( pos , len ( var ) , disable_actions = true , inspect = false , short_reads = false ) if not claripy . is_true ( length = = len ( var ) ) raise angr_error ( string t get requested data from file "" ) self . preconstrain ( var , data ) if simfile . pos is not none simfile . pos = number if set_length simfile . has_end = true if repair_entry_state_opts self . state . options = { o . track_action_history }"	1
Q_60	preconstrain the content of a file	C_60_1	def rename_variables ( expression expression , renaming dict [ str , str ] ) _ > expression if isinstance ( expression , operation ) if hasattr ( expression , string ) variable_name = renaming . get ( expression . variable_name , expression . variable_name ) return create_operation_expression ( expression , [ rename_variables ( var , renaming ) for var in op_iter ( expression ) ] , variable_name = variable_name ) operands = [ rename_variables ( var , renaming ) for var in op_iter ( expression ) ] return create_operation_expression ( expression , operands ) elif isinstance ( expression , expression ) expression = expression . _copy_ ( ) expression . variable_name = renaming . get ( expression . variable_name , expression . variable_name ) return expression	0
Q_60	preconstrain the content of a file	C_60_2	def maxs ( self , value ) self . x_max , self . y_max , self . z_max = value	0
Q_61	store the result into the result key of the amp	C_61_0	def set_result ( self , result , separator = string ) if self . one_line ( ) self . configs [ string ] = str ( result ) . replace ( string , separator ) else self . configs [ string ] = str ( result )	1
Q_61	store the result into the result key of the amp	C_61_1	def get_example ( cls ) _ > dict if cls . example is not none return cls . example return { var var . get_example ( ) for var , var in cls . properties . items ( ) }	0
Q_61	store the result into the result key of the amp	C_61_2	def compose_item_handle ( tokens ) if len ( tokens ) < number raise coconut_internal_exception ( string , tokens ) elif len ( tokens ) = = number return tokens [ number ] else return string + string . join ( reversed ( tokens ) ) + string	0
Q_62	transform a list of port number to the list of string with port range	C_62_0	def get_ports_strings_from_list ( data ) if len ( data ) = = number return [ ] first = number result = [ ] for var in range ( number , len ( data ) ) if data [ first ] = = data [ var ] _ ( var _ first ) continue result . append ( ports_range_helper . ports_range ( start = data [ first ] , end = data [ var _ number ] ) ) first = var result . append ( ports_range_helper . ports_range ( start = data [ first ] , end = data [ _ number ] ) ) result = [ str ( x . start ) if x . start = = x . end else string ( x . start , x . end ) for x in result ] return result	1
Q_62	transform a list of port number to the list of string with port range	C_62_1	def isns_isns_vrf_isns_vrf_instance ( self , * * kwargs ) config = et . element ( string ) isns = et . sub_element ( config , string , xmlns = string ) isns_vrf = et . sub_element ( isns , string ) isns_vrf_instance = et . sub_element ( isns_vrf , string ) isns_vrf_instance . text = kwargs . pop ( string ) callback = kwargs . pop ( string , self . _callback ) return callback ( config )	0
Q_62	transform a list of port number to the list of string with port range	C_62_2	def get_object_directory ( self , obj ) if not hasattr ( obj , string ) return none module = sys . modules [ obj . _module_ ] if not hasattr ( module , string ) return none path = module . _file_ return os . path . dirname ( path )	0
Q_63	re_enable paging globally	C_63_0	def cleanup ( self ) if self . allow_disable_global output_mode_cmd = string . format ( self . _output_mode ) enable_paging_commands = [ string , output_mode_cmd , string ] if self . vdoms enable_paging_commands . insert ( number , string ) for var in enable_paging_commands self . send_command_timing ( var )	1
Q_63	re_enable paging globally	C_63_1	def none_missing ( df , columns = none ) if columns is none columns = df . columns try assert not df [ columns ] . isnull ( ) . any ( ) . any ( ) except assertion_error as e missing = df [ columns ] . isnull ( ) msg = generic . bad_locations ( missing ) e . args = msg raise return df	0
Q_63	re_enable paging globally	C_63_2	def dump_json_data ( page ) def content_langs_ordered ( ) params = { string page } if page . freeze_date params [ string ] = page . freeze_date cqs = content . objects . filter ( * * params ) cqs = cqs . values ( string ) . annotate ( latest = max ( string ) ) return [ var [ string ] for var in cqs . order_by ( string ) ] languages = content_langs_ordered ( ) def language_content ( ctype ) return dict ( ( lang , page . get_content ( lang , ctype , language_fallback = false ) ) for lang in languages ) def placeholder_content ( ) out = { } for p in get_placeholders ( page . get_template ( ) ) if p . ctype in ( string , string ) continue out [ p . name ] = language_content ( p . name ) return out def isoformat ( d ) return none if d is none else d . strftime ( isodate_format ) def custom_email ( user ) return user . email tags = [ ] if settings . page_tagging tags = [ tag . name for tag in page . tags . all ( ) ] return { string dict ( ( lang , page . get_complete_slug ( lang , hideroot = false ) ) for lang in languages ) , string language_content ( string ) , string custom_email ( page . author ) , string isoformat ( page . creation_date ) , string isoformat ( page . publication_date ) , string isoformat ( page . publication_end_date ) , string isoformat ( page . last_modification_date ) , string { page . published string , page . hidden string , page . draft string } [ page . status ] , string page . template , string ( [ site . domain for site in page . sites . all ( ) ] if settings . page_use_site_id else [ ] ) , string page . redirect_to_url , string dict ( ( lang , page . redirect_to . get_complete_slug ( lang , hideroot = false ) ) for lang in page . redirect_to . get_languages ( ) ) if page . redirect_to is not none else none , string placeholder_content ( ) , string languages , string tags , }	0
Q_64	gracefully exit the ssh session	C_64_0	def cleanup ( self ) self . exit_config_mode ( ) self . write_channel ( string + self . return ) count = number while count < = number time . sleep ( number . number ) output = self . read_channel ( ) if string in output self . _session_log_fin = true self . write_channel ( string + self . return ) elif string in output self . _session_log_fin = true self . write_channel ( string + self . return ) try self . write_channel ( self . return ) except socket . error break count + = number	1
Q_64	gracefully exit the ssh session	C_64_1	def register ( host = dflt_address [ number ] , port = dflt_address [ number ] , signum = signal . sigusr1 ) _pdbhandler . _register ( host , port , signum )	0
Q_64	gracefully exit the ssh session	C_64_2	def ip_hide_as_path_holder_as_path_access_list_seq_keyword ( self , * * kwargs ) config = et . element ( string ) ip = et . sub_element ( config , string , xmlns = string ) hide_as_path_holder = et . sub_element ( ip , string , xmlns = string ) as_path = et . sub_element ( hide_as_path_holder , string ) access_list = et . sub_element ( as_path , string ) name_key = et . sub_element ( access_list , string ) name_key . text = kwargs . pop ( string ) instance_key = et . sub_element ( access_list , string ) instance_key . text = kwargs . pop ( string ) seq_keyword = et . sub_element ( access_list , string ) seq_keyword . text = kwargs . pop ( string ) callback = kwargs . pop ( string , self . _callback ) return callback ( config )	0
Q_65	read a notebook from the file with give name	C_65_0	def readf ( nb_file , fmt = none ) if nb_file = = string text = sys . stdin . read ( ) fmt = fmt or divine_format ( text ) return reads ( text , fmt ) _ , ext = os . path . splitext ( nb_file ) fmt = copy ( fmt or { } ) fmt . update ( { string ext } ) with io . open ( nb_file , encoding = string ) as stream return read ( stream , fmt , as_version = number )	1
Q_65	read a notebook from the file with give name	C_65_1	def inverse ( self ) result = g2_element ( ) librelic . g2_neg_abi ( byref ( result ) , byref ( self ) ) return result	0
Q_65	read a notebook from the file with give name	C_65_2	def sync_readmes ( ) print ( string ) with open ( string , string ) as reader file_text = reader . read ( ) with open ( string , string ) as writer writer . write ( file_text )	0
Q_66	compute period wise factor quantiles	C_66_0	def quantize_factor ( factor_data , quantiles = number , bins = none , by_group = false , no_raise = false , zero_aware = false ) if not ( ( quantiles is not none and bins is none ) or ( quantiles is none and bins is not none ) ) raise value_error ( string ) if zero_aware and not ( isinstance ( quantiles , int ) or isinstance ( bins , int ) ) msg = ( string string ) raise value_error ( msg ) def quantile_calc ( x , _quantiles , _bins , _zero_aware , _no_raise ) try if _quantiles is not none and _bins is none and not _zero_aware return pd . qcut ( x , _quantiles , labels = false ) + number elif _quantiles is not none and _bins is none and _zero_aware pos_quantiles = pd . qcut ( x [ x > = number ] , _quantiles / / number , labels = false ) + _quantiles / / number + number neg_quantiles = pd . qcut ( x [ x < number ] , _quantiles / / number , labels = false ) + number return pd . concat ( [ pos_quantiles , neg_quantiles ] ) . sort_index ( ) elif _bins is not none and _quantiles is none and not _zero_aware return pd . cut ( x , _bins , labels = false ) + number elif _bins is not none and _quantiles is none and _zero_aware pos_bins = pd . cut ( x [ x > = number ] , _bins / / number , labels = false ) + _bins / / number + number neg_bins = pd . cut ( x [ x < number ] , _bins / / number , labels = false ) + number return pd . concat ( [ pos_bins , neg_bins ] ) . sort_index ( ) except exception as e if _no_raise return pd . series ( index = x . index ) raise e grouper = [ factor_data . index . get_level_values ( string ) ] if by_group grouper . append ( string ) factor_quantile = factor_data . groupby ( grouper ) [ string ] . apply ( quantile_calc , quantiles , bins , zero_aware , no_raise ) factor_quantile . name = string return factor_quantile . dropna ( )	1
Q_66	compute period wise factor quantiles	C_66_1	def values ( self , * args str , * * kwargs str ) _ > string fields_for_select = { } for var in args if var in fields_for_select raise field_error ( string . format ( var ) ) fields_for_select [ var ] = var for return_as , var in kwargs . items ( ) if return_as in fields_for_select raise field_error ( string . format ( return_as ) ) fields_for_select [ return_as ] = var return values_query ( db = self . _db , model = self . model , q_objects = self . _q_objects , fields_for_select = fields_for_select , distinct = self . _distinct , limit = self . _limit , offset = self . _offset , orderings = self . _orderings , annotations = self . _annotations , custom_filters = self . _custom_filters , )	0
Q_66	compute period wise factor quantiles	C_66_2	def get_pin_codes ( self , refresh = false ) if refresh self . refresh ( ) val = self . get_value ( string ) raw_code_list = [ ] try raw_code_list = val . rstrip ( ) . split ( string ) [ number ] except exception as ex logger . error ( string . format ( val , ex ) ) codes = [ ] for var in raw_code_list try code_addrs = var . split ( string ) [ number ] . split ( string ) slot , active = code_addrs [ number ] if active = string _ , _ , pin , name = code_addrs [ number ] codes . append ( ( slot , name , pin ) ) except exception as ex logger . error ( string . format ( var , ex ) ) return codes	0
Q_67	return a function_call give some validation for the function and args	C_67_0	def init_with_validation ( cls , function , arguments ) func = functions [ function ] args = [ ] for var , var in zip ( arguments , func . args ) if var . values if isinstance ( var , six . string_types ) try args . append ( [ var . values [ var ] ] ) except key_error raise key_error ( string ( var , [ v . name for v in var . values ] ) ) else if isinstance ( var , ( list , tuple ) ) var = var [ number ] try args . append ( [ var . values ( var ) ] ) except value_error raise value_error ( string ( var , list ( var . values ) ) ) elif isinstance ( var , int ) args . append ( [ var ] ) else args . append ( list ( var ) ) return cls ( func . id , args )	1
Q_67	return a function_call give some validation for the function and args	C_67_1	def enable_global_typelogged_decorator ( flag = true , retrospective = true ) global global_typelogged_decorator global_typelogged_decorator = flag if import_hook_enabled _install_import_hook ( ) if global_typelogged_decorator and retrospective _catch_up_global_typelogged_decorator ( ) return global_typelogged_decorator	0
Q_67	return a function_call give some validation for the function and args	C_67_2	def set_input ( self , data ) for ( name , value ) in data . items ( ) i = self . form . find ( string , { string name } ) if not i raise invalid_form_method ( string + name ) i [ string ] = value	0
Q_68	stream statistic for a specific container	C_68_0	def stats ( self , container , decode = none , stream = true ) url = self . _url ( string , container ) if stream return self . _stream_helper ( self . _get ( url , stream = true ) , decode = decode ) else if decode raise errors . invalid_argument ( string ) return self . _result ( self . _get ( url , params = { string false } ) , json = true )	1
Q_68	stream statistic for a specific container	C_68_1	def on_dn_d_mode_change ( self , dnd_mode ) if not isinstance ( dnd_mode , dn_d_mode ) raise type_error ( string ) self . _call ( string , in_p = [ dnd_mode ] )	0
Q_68	stream statistic for a specific container	C_68_2	def get_json ( self ) port = self . get_basic_json ( ) port . update ( { string self . boot . boot_protocol , string self . boot . boot_prio , } ) boot_env = self . boot . get_json ( ) if boot_env port . update ( boot_env ) if self . use_virtual_addresses and self . mac port [ string ] = { string self . mac } return port	0
Q_69	generate the index file for the specify folder	C_69_0	def _generate_index ( root , folder , paths , bots_index = false , bots_index_paths = ( ) ) namespaces = [ ] files = [ ] index = string bot_index = string for var in ( bots_index_paths or folder . iterdir ( ) ) if var . is_dir ( ) namespaces . append ( var ) elif var . name not in ( index , bot_index ) files . append ( var ) filename = folder / ( bot_index if bots_index else index ) with docs_writer ( root , filename , _get_path_for_type ) as docs docs . write_head ( str ( folder ) . replace ( os . path . sep , string ) . title ( ) , css_path = paths [ string ] , default_css = paths [ string ] ) docs . set_menu_separator ( paths [ string ] ) _build_menu ( docs ) docs . write_title ( str ( filename . parent . relative_to ( root ) ) . replace ( os . path . sep , string ) . title ( ) ) if bots_index docs . write_text ( string string { } string string . format ( index ) ) else docs . write_text ( string { } string string . format ( bot_index ) ) if namespaces docs . write_title ( string , level = number ) docs . begin_table ( number ) namespaces . sort ( ) for namespace in namespaces namespace_paths = [ ] if bots_index for var in bots_index_paths if var . parent = = namespace namespace_paths . append ( var ) _generate_index ( root , namespace , paths , bots_index , namespace_paths ) docs . add_row ( namespace . stem . title ( ) , link = namespace / ( bot_index if bots_index else index ) ) docs . end_table ( ) docs . write_title ( string ) docs . begin_table ( number ) files = [ ( f , _find_title ( f ) ) for f in files ] files . sort ( key = lambda t t [ number ] ) for file , title in files docs . add_row ( title , link = file ) docs . end_table ( ) docs . end_body ( )	1
Q_69	generate the index file for the specify folder	C_69_1	def add_debugged_source_file ( self , debugged_source_file ) key = debugged_source_file . file_path self . _source_file_host [ key ] = debugged_source_file . host self . _source_file_last_modified [ key ] = debugged_source_file . last_modified self . _source_file_bytes [ key ] = debugged_source_file . bytes self . _source_file_content [ key ] = debugged_source_file . lines	0
Q_69	generate the index file for the specify folder	C_69_2	def export ( self ) data = { } for var , var in self . items ( ) data [ var ] = var return data	0
Q_70	make a generic function which call a validator with the right argument	C_70_0	def make_generic_validator ( validator any_callable ) _ > string sig = signature ( validator ) args = list ( sig . parameters . keys ( ) ) first_arg = args . pop ( number ) if first_arg = = string raise config_error ( f string self string f string values string config string field string ) elif first_arg = = string return wraps ( validator ) ( _generic_validator_cls ( validator , sig , set ( args [ number ] ) ) ) else return wraps ( validator ) ( _generic_validator_basic ( validator , sig , set ( args ) ) )	1
Q_70	make a generic function which call a validator with the right argument	C_70_1	def batch_renorm ( x , rmax , dmax , momentum = number . number , epsilon = number e _ number , center = true , scale = true , gamma_initializer = none , data_format = string ) shape = x . get_shape ( ) . as_list ( ) ndims = len ( shape ) assert ndims in [ number , number ] if ndims = = number data_format = string ctx = get_current_tower_context ( ) coll_bk = backup_collection ( [ tf . graph_keys . update_ops ] ) layer = tf . layers . batch_normalization ( axis = number if data_format = = string else number , momentum = momentum , epsilon = epsilon , center = center , scale = scale , renorm = true , renorm_clipping = { string number . number / rmax , string rmax , string dmax } , renorm_momentum = number . number , gamma_initializer = gamma_initializer , fused = false , _reuse = tf . get_variable_scope ( ) . reuse ) xn = layer . apply ( x , training = ctx . is_training , scope = tf . get_variable_scope ( ) ) if ctx . is_main_training_tower for var in layer . non_trainable_variables if isinstance ( var , tf . variable ) tf . add_to_collection ( tf . graph_keys . model_variables , var ) else restore_collection ( coll_bk ) if ndims = = number xn = tf . squeeze ( xn , [ number , number ] ) ret = tf . identity ( xn , name = string ) vh = ret . variables = variable_holder ( ) if scale vh . gamma = layer . gamma if center vh . beta = layer . beta return ret	0
Q_70	make a generic function which call a validator with the right argument	C_70_2	def get_host_ip ( self , env_with_dig = string ) try container = self . _docker . containers . create ( env_with_dig , command = string ) container . start ( ) response = container . wait ( ) assert response [ string ] = = number if isinstance ( response , dict ) else response = = number answer = container . logs ( stdout = true , stderr = false ) . decode ( string ) . strip ( ) container . remove ( v = true , link = false , force = true ) return answer except return none	0
Q_71	set a prior for the class balance	C_71_0	def _set_class_balance ( self , class_balance , y_dev ) if class_balance is not none self . p = np . array ( class_balance ) elif y_dev is not none class_counts = counter ( y_dev ) sorted_counts = np . array ( [ var for var , var in sorted ( class_counts . items ( ) ) ] ) self . p = sorted_counts / sum ( sorted_counts ) else self . p = ( number / self . var ) * np . ones ( self . var ) self . p = torch . diag ( torch . from_numpy ( self . p ) ) . float ( )	1
Q_71	set a prior for the class balance	C_71_1	def iterable ( self , iterable_name , * , collection , attribute , word , func = none , operation = none ) if func is none and operation is none raise value_error ( string ) elif func is not none and operation is not none raise value_error ( string ) current_att = self . _attribute self . _attribute = iterable_name word = self . _parse_filter_word ( word ) collection = self . _get_mapping ( collection ) attribute = self . _get_mapping ( attribute ) if func is not none sentence = self . _prepare_function ( func , attribute , word ) else sentence = self . _prepare_sentence ( attribute , operation , word ) filter_str , attrs = sentence filter_data = string . format ( collection , iterable_name , filter_str ) , attrs self . _add_filter ( * filter_data ) self . _attribute = current_att return self	0
Q_71	set a prior for the class balance	C_71_2	def lenv ( self ) _env = type ( env ) ( ) for var , var in six . iteritems ( env ) if var . startswith ( self . name + string ) _env [ var [ len ( self . name ) + number ] ] = var return _env	0
Q_72	perform two series of snmp request one for all that have mib asociated	C_72_0	def _check ( self , instance ) ( snmp_engine , mib_view_controller , ip_address , tags , metrics , timeout , retries , enforce_constraints , ) = self . _load_conf ( instance ) tags + = [ string . format ( ip_address ) ] table_oids , raw_oids , mibs_to_load = self . parse_metrics ( metrics , enforce_constraints ) try if table_oids self . log . debug ( string , ip_address , len ( table_oids ) ) table_results = self . check_table ( instance , snmp_engine , mib_view_controller , table_oids , true , timeout , retries , enforce_constraints = enforce_constraints , mibs_to_load = mibs_to_load , ) self . report_table_metrics ( metrics , table_results , tags ) if raw_oids self . log . debug ( string , ip_address , len ( raw_oids ) ) raw_results = self . check_table ( instance , snmp_engine , mib_view_controller , raw_oids , false , timeout , retries , enforce_constraints = false , ) self . report_raw_metrics ( metrics , raw_results , tags ) except exception as e if string not in instance instance [ string ] = string . format ( instance [ string ] , e ) self . warning ( instance [ string ] ) finally if string in instance status = status . down if string in instance status = instance [ string ] return [ ( self . sc_status , status , instance [ string ] ) ] return [ ( self . sc_status , status . up , none ) ]	1
Q_72	perform two series of snmp request one for all that have mib asociated	C_72_1	def get_content_macro_by_hash ( self , content_id , version , macro_hash , callback = none ) return self . _service_get_request ( string string . format ( id = content_id , version = version , hash = macro_hash ) , callback = callback )	0
Q_72	perform two series of snmp request one for all that have mib asociated	C_72_2	async def _check_last_ping ( self , run_listen ) if self . _last_ping < time . time ( ) _ number self . _logger . warning ( string ) run_listen . cancel ( ) self . _cancel_remaining_safe_tasks ( ) else self . _loop . call_later ( number , self . _create_safe_task , self . _check_last_ping ( run_listen ) )	0
Q_73	finalize execute any subclass_specific ax finalization step	C_73_0	def finalize ( self , * * kwargs ) self . set_title ( string . format ( len ( self . features_ ) ) ) if self . show_vlines for var in self . _increments self . ax . axvline ( var , * * self . vlines_kwds ) self . ax . set_xticks ( self . _increments ) self . ax . set_xticklabels ( self . features_ ) self . ax . set_xlim ( self . _increments [ number ] , self . _increments [ _ number ] ) labels = sorted ( list ( self . _colors . keys ( ) ) ) colors = [ self . _colors [ lbl ] for lbl in labels ] manual_legend ( self , labels , colors , loc = string , frameon = true ) self . ax . grid ( )	1
Q_73	finalize execute any subclass_specific ax finalization step	C_73_1	def write_referrers_to_file ( self , file_path = string , date = str ( datetime . date . today ( ) ) , organization = string ) self . remove_date ( file_path = file_path , date = date ) referrers_exists = os . path . isfile ( file_path ) with open ( file_path , string ) as out if not referrers_exists out . write ( string + string ) sorted_referrers = sorted ( self . referrers_lower ) for var in sorted_referrers ref_name = self . referrers_lower [ var ] count = self . referrers [ ref_name ] [ number ] uniques = self . referrers [ ref_name ] [ number ] if count = = number count = number . number if uniques = = number uniques = number . number count_logged = math . log ( count ) uniques_logged = math . log ( uniques ) out . write ( date + string + organization + string + ref_name + string + str ( count ) + string + str ( count_logged ) + string + str ( uniques ) + string + str ( uniques_logged ) + string ) out . close ( )	0
Q_73	finalize execute any subclass_specific ax finalization step	C_73_2	def _table_cell ( args , cell_body ) if args [ string ] = = string filter_ = args [ string ] if args [ string ] else string if args [ string ] if args [ string ] is none datasets = [ bigquery . dataset ( args [ string ] ) ] else context = google . datalab . context ( args [ string ] , google . datalab . context . default ( ) . credentials ) datasets = [ bigquery . dataset ( args [ string ] , context ) ] else default_context = google . datalab . context . default ( ) context = google . datalab . context ( default_context . project_id , default_context . credentials ) if args [ string ] context . set_project_id ( args [ string ] ) datasets = bigquery . datasets ( context ) tables = [ ] for var in datasets tables . extend ( [ table . full_name for table in var if fnmatch . fnmatch ( table . full_name , filter_ ) ] ) return _render_list ( tables ) elif args [ string ] = = string if cell_body is none print ( string args [ string ] ) else try record = google . datalab . utils . commands . parse_config ( cell_body , google . datalab . utils . commands . notebook_environment ( ) , as_dict = false ) jsonschema . validate ( record , big_query_schema . table_schema_schema ) schema = bigquery . schema ( record [ string ] ) bigquery . table ( args [ string ] ) . create ( schema = schema , overwrite = args [ string ] ) except exception as e print ( string ( args [ string ] , e ) ) elif args [ string ] = = string name = args [ string ] table = _get_table ( name ) if not table raise exception ( string name ) html = _repr_html_table_schema ( table . schema ) return i_python . core . display . html ( html ) elif args [ string ] = = string try bigquery . table ( args [ string ] ) . delete ( ) except exception as e print ( string ( args [ string ] , e ) ) elif args [ string ] = = string name = args [ string ] table = _get_table ( name ) if not table raise exception ( string name ) return table	0
Q_74	return a q_circuit_based latex diagram of the give circuit	C_74_0	def circuit_to_latex_using_qcircuit ( circuit circuits . circuit , qubit_order ops . qubit_order_or_list = ops . qubit_order . default ) _ > str diagram = circuit . to_text_diagram_drawer ( qubit_namer = qcircuit_qubit_namer , qubit_order = qubit_order , get_circuit_diagram_info = get_qcircuit_diagram_info ) return _render ( diagram )	1
Q_74	return a q_circuit_based latex diagram of the give circuit	C_74_1	def is_valid ( self , value ) if not self . is_array return self . _valid ( value ) if isinstance ( value , ( list , set , tuple ) ) return all ( [ self . _valid ( var ) for var in value ] ) return self . _valid ( value )	0
Q_74	return a q_circuit_based latex diagram of the give circuit	C_74_2	def get_value ( self , class_name , attr , default_value = none , state = string , base_name = string ) styles = self . get_dict_for_class ( class_name , state , base_name ) try return styles [ attr ] except key_error return default_value	0
Q_75	return the density matrix at this step in the simulation	C_75_0	def density_matrix ( self ) size = number * * len ( self . _qubit_map ) return np . reshape ( self . _density_matrix , ( size , size ) )	1
Q_75	return the density matrix at this step in the simulation	C_75_1	def count_lines ( filename , buf_size = number ) f = open ( filename ) try lines = number read_f = f . read buf = read_f ( buf_size ) if not buf return number while buf lines + = buf . count ( string ) buf = read_f ( buf_size ) return lines finally f . close ( )	0
Q_75	return the density matrix at this step in the simulation	C_75_2	def slow_augment ( n , ii , jj , idx , count , x , y , u , v , c ) inf = np . sum ( c ) + number d = np . zeros ( n ) cc = np . zeros ( ( n , n ) ) cc [ , ] = inf for var in range ( n ) cc [ var , jj [ idx [ var ] ( idx [ var ] + count [ var ] ) ] ] = c [ idx [ var ] ( idx [ var ] + count [ var ] ) ] c = cc for var in ii print ( string var ) j = jj [ idx [ var ] ( idx [ var ] + count [ var ] ) ] d = c [ var , ] _ v pred = np . ones ( n , int ) * var on_deck = [ ] ready = [ ] scan = [ ] to_do = list ( range ( n ) ) try while true print ( string ( var , len ( scan ) ) ) if len ( scan ) = = number ready + = on_deck on_deck = [ ] umin = np . min ( [ d [ jjj ] for jjj in to_do ] ) print ( string umin ) scan = [ jjj for jjj in to_do if d [ jjj ] = = umin ] to_do = [ jjj for jjj in to_do if d [ jjj ] = umin ] for j1 in scan if y [ j1 ] = = n raise stop_iteration ( ) j1 = scan [ number ] iii = y [ j1 ] print ( string ( iii , j1 ) ) scan = scan [ number ] on_deck + = [ j1 ] u1 = c [ iii , j1 ] _ v [ j1 ] _ umin for j1 in list ( to_do ) h = c [ iii , j1 ] _ v [ j1 ] _ u1 print ( string ( j1 , iii , j1 , c [ iii , j1 ] , j1 , v [ j1 ] , h , d [ j1 ] ) ) if h < d [ j1 ] print ( string ) pred [ j1 ] = iii if h = = umin if y [ j1 ] = = n raise stop_iteration ( ) print ( string ) scan + = [ j1 ] to_do . remove ( j1 ) d [ j1 ] = h except stop_iteration print ( string j1 ) for k in ready temp = v [ k ] v [ k ] = v [ k ] + d [ k ] _ umin print ( string ( k , temp , v [ k ] ) ) while true iii = pred [ j1 ] print ( string ( j1 , y [ j1 ] , iii ) ) y [ j1 ] = iii j1 , x [ iii ] = x [ iii ] , j1 if iii = = var break for var in range ( n ) j = x [ var ] u [ var ] = c [ var , j ] _ v [ j ]	0
Q_76	greedily assign operation to moment	C_76_0	def _pick_inserted_ops_moment_indices ( operations sequence [ ops . operation ] , start int = number , frontier dict [ ops . qid , int ] = none ) _ > tuple [ sequence [ int ] , dict [ ops . qid , int ] ] if frontier is none frontier = defaultdict ( lambda number ) moment_indices = [ ] for var in operations op_start = max ( start , max ( frontier [ q ] for q in var . qubits ) ) moment_indices . append ( op_start ) for q in var . qubits frontier [ q ] = max ( frontier [ q ] , op_start + number ) return moment_indices , frontier	1
Q_76	greedily assign operation to moment	C_76_1	def convert_ini ( config_dict ) config_lines = [ ] for var , var in sorted ( config_dict . items ( ) ) for resource , app_properties in sorted ( var . items ( ) ) try for app_property , value in sorted ( app_properties . items ( ) ) variable = string . format ( var = var , resource = resource , app_property = app_property ) . upper ( ) if isinstance ( value , ( dict , deep_chain_map ) ) safe_value = string { number } string . format ( json . dumps ( dict ( value ) ) ) else safe_value = json . dumps ( value ) line = string . format ( variable = variable , value = safe_value ) log . debug ( string , line ) config_lines . append ( line ) except attribute_error resource = resource . upper ( ) app_properties = string { } string . format ( json . dumps ( app_properties ) ) line = string . format ( resource , app_properties ) log . debug ( string , line ) config_lines . append ( line ) return config_lines	0
Q_76	greedily assign operation to moment	C_76_2	def create_element ( self , token ) name = token [ string ] namespace = token . get ( string , self . default_namespace ) element = self . element_class ( name , namespace ) element . attributes = token [ string ] return element	0
Q_77	convert network_connection to exported_network_connection	C_77_0	def convert ( self , metadata , conn , token = none ) result = exported_network_connection ( metadata = metadata , family = conn . family , type = conn . type , local_address = conn . local_address , remote_address = conn . remote_address , state = conn . state , pid = conn . pid , ctime = conn . ctime ) return [ result ]	1
Q_77	convert network_connection to exported_network_connection	C_77_1	def getlist ( self , key string ) _ > sequence [ object ] r if not ( isinstance ( key , type ( self ) ) and key . type is entity_type . property ) return [ ] claims_map = self . attributes . get ( string ) or { } assert isinstance ( claims_map , collections . abc . mapping ) claims = claims_map . get ( key . id , [ ] ) claims . sort ( key = lambda claim claim [ string ] , reverse = true ) logger = logging . get_logger ( _name_ + string ) if logger . is_enabled_for ( logging . debug ) logger . debug ( string , _import_ ( string ) . pformat ( claims ) ) decode = self . client . decode_datavalue return [ decode ( var [ string ] , var [ string ] ) for var in ( claim [ string ] for claim in claims ) ]	0
Q_77	convert network_connection to exported_network_connection	C_77_2	def transform ( self , x ) utils . validation . check_is_fitted ( self , string ) if self . check_input utils . check_array ( x , dtype = [ str , np . number ] ) return self . row_coordinates ( x )	0
Q_78	check if flow can be start on a particular client	C_78_0	"def check_flow_can_be_started_on_client ( flow_name ) flow_cls = flow . grr_flow . get_plugin ( flow_name ) if flow_cls . category return true else raise access_control . unauthorized_access ( string t be started on a client by non _ suid users . "" flow_name )"	1
Q_78	check if flow can be start on a particular client	C_78_1	def parse_band_set_name ( self , message ) if message . get ( string ) self . _service_name = message [ string ] else self . log . warning ( string , str ( message ) )	0
Q_78	check if flow can be start on a particular client	C_78_2	def _parse_value_data ( self , knowledge_base , value_data ) if not isinstance ( value_data , py2to3 . unicode_type ) raise errors . pre_process_fail ( string string . format ( type ( value_data ) , self . artifact_definition_name ) ) lookup_key = value_data . replace ( string , string ) time_zone = time_zones . time_zones . get ( lookup_key , value_data ) if time_zone try knowledge_base . set_time_zone ( time_zone ) except value_error time_zone = value_data logger . warning ( string { number s } string . format ( value_data ) )	0
Q_79	write a new client crash record	C_79_0	def write_client_crash_info ( self , client_id , crash_info ) if client_id not in self . metadatas raise db . unknown_client_error ( client_id ) ts = rdfvalue . rdf_datetime . now ( ) self . metadatas [ client_id ] [ string ] = ts history = self . crash_history . setdefault ( client_id , { } ) history [ ts ] = crash_info . serialize_to_string ( )	1
Q_79	write a new client crash record	C_79_1	def debug_log ( self , no_tail = false , exclude_module = none , include_module = none , include = none , level = none , limit = number , lines = number , replay = false , exclude = none ) raise not_implemented_error ( )	0
Q_79	write a new client crash record	C_79_2	def appndc ( item , cell ) assert isinstance ( cell , stypes . spice_cell ) if isinstance ( item , list ) for var in item libspice . appndc_c ( stypes . string_to_char_p ( var ) , cell ) else item = stypes . string_to_char_p ( item ) libspice . appndc_c ( item , cell )	0
Q_80	add signal method argument to an argparse	C_80_0	def add_signaling_arguments ( parser ) parser . add_argument ( string , string , choices = [ string , string , string ] ) parser . add_argument ( string , default = string , help = string ) parser . add_argument ( string , default = number , help = string ) parser . add_argument ( string , default = string , help = string )	1
Q_80	add signal method argument to an argparse	C_80_1	def register_for_reply_by_message_id ( self , message_id , callback , * args , * * kwargs ) if message_id in self . reply_handlers . keys ( ) self . reply_handlers [ message_id ] . append ( handler ( callback , * args , * * kwargs ) ) else self . reply_handlers [ message_id ] = [ handler ( callback , * args , * * kwargs ) ] if self . reply_saver is not none self . reply_saver . start_save_timer ( )	0
Q_80	add signal method argument to an argparse	C_80_2	def create ( type_dict , * type_parameters ) assert len ( type_parameters ) = = number klazz = type_factory . new ( type_dict , * type_parameters [ number ] ) assert isclass ( klazz ) assert issubclass ( klazz , object ) return type_metaclass ( string klazz . _name_ , ( list_container , ) , { string klazz } )	0
Q_81	use serial access to detect the chip type	C_81_0	def detect_chip ( port = default_port , baud = esp_rom_baud , connect_mode = string , trace_enabled = false ) detect_port = esp_loader ( port , baud , trace_enabled = trace_enabled ) detect_port . connect ( connect_mode ) try print ( string , end = string ) sys . stdout . flush ( ) date_reg = detect_port . read_reg ( esp_loader . uart_data_reg_addr ) for var in [ esp8266_rom , esp32_rom ] if date_reg = = var . date_reg_value inst = var ( detect_port . _port , baud , trace_enabled = trace_enabled ) print ( string inst . chip_name , end = string ) return inst finally print ( string ) raise fatal_error ( string date_reg )	1
Q_81	use serial access to detect the chip type	C_81_1	def get_event_noblock ( self ) assert self . _run_io_loop_sync if not self . cpub if not self . connect_pub ( ) return none raw = self . subscriber . read_sync ( timeout = number ) if raw is none return none mtag , data = self . unpack ( raw , self . serial ) return { string data , string mtag }	0
Q_81	use serial access to detect the chip type	C_81_2	def receives ( self , id_a , id_b ) obj_a = self . chart . get ( id_a ) obj_b = self . chart . get ( id_b ) asp = aspects . is_aspecting ( obj_b , obj_a , const . major_aspects ) return self . in_dignities ( id_b , id_a ) if asp else [ ]	0
Q_82	retrieve a single commit identify by it id	C_82_0	def get_commit_info ( self , project , repository , commit , path = none ) url = string . format ( project = project , repository = repository , commit_id = commit ) params = { } if path params [ string ] = path return self . get ( url , params = params )	1
Q_82	retrieve a single commit identify by it id	C_82_1	def properties ( obj , type = none , set = none ) if type and type not in [ string , string , string , string , string , string , string , string ] raise command_execution_error ( string { number } string . format ( type ) ) cmd = [ string ] cmd . append ( string ) cmd . append ( set and string or string ) if type cmd . append ( string . format ( type ) ) cmd . append ( obj ) if set try for var , var in [ [ item . strip ( ) for item in keyset . split ( string ) ] for keyset in set . split ( string ) ] cmd . append ( var ) cmd . append ( var ) except exception as ex raise command_execution_error ( ex ) out = _salt_ [ string ] ( string . join ( cmd ) ) salt . utils . fsutils . _verify_run ( out ) if not set ret = { } for prop , descr in six . iteritems ( _parse_proplist ( out [ string ] ) ) ret [ prop ] = { string descr } var = _salt_ [ string ] ( string . format ( obj , prop ) ) [ string ] ret [ prop ] [ string ] = var and var . split ( string ) [ _ number ] or string return ret	0
Q_82	retrieve a single commit identify by it id	C_82_2	def surf_flux ( self , forc , parameter , sim_time , hum_ref , temp_ref , wind_ref , bound_cond , int_flux ) dens = forc . pres / ( number * number . number * temp_ref * ( number . + number . number * hum_ref ) ) self . aero_cond = number . number + number . number * wind_ref if ( self . horizontal ) if not self . is_near_zero ( self . water_storage ) and self . water_storage > number . number qtsat = self . qsat ( [ self . layer_temp [ number ] ] , [ forc . pres ] , parameter ) [ number ] eg = self . aero_cond * parameter . colburn * dens * ( qtsat _ hum_ref ) / parameter . water_dens / parameter . cp self . water_storage = min ( self . water_storage + sim_time . dt * ( forc . prec _ eg ) , parameter . wgmax ) self . water_storage = max ( self . water_storage , number . ) else eg = number . soil_lat = eg * parameter . water_dens * parameter . lv if sim_time . month < parameter . veg_start and sim_time . month > parameter . veg_end self . sol_abs = ( number . _ self . albedo ) * self . sol_rec veg_lat = number . veg_sens = number . else self . sol_abs = ( ( number . _ self . veg_coverage ) * ( number . _ self . albedo ) + self . veg_coverage * ( number . _ parameter . veg_albedo ) ) * self . sol_rec veg_lat = self . veg_coverage * parameter . grass_f_lat * ( number . _ parameter . veg_albedo ) * self . sol_rec veg_sens = self . veg_coverage * ( number . _ parameter . grass_f_lat ) * ( number . _ parameter . veg_albedo ) * self . sol_rec self . lat = soil_lat + veg_lat self . sens = veg_sens + self . aero_cond * ( self . layer_temp [ number ] _ temp_ref ) self . flux = _ self . sens + self . sol_abs + self . infra _ self . lat else self . sol_abs = ( number . _ self . albedo ) * self . sol_rec self . lat = number . self . sens = self . aero_cond * ( self . layer_temp [ number ] _ temp_ref ) self . flux = _ self . sens + self . sol_abs + self . infra _ self . lat self . layer_temp = self . conduction ( sim_time . dt , self . flux , bound_cond , forc . deep_temp , int_flux ) self . t_ext = self . layer_temp [ number ] self . t_int = self . layer_temp [ _ number ]	0
Q_83	get a json_ready representation of this spam_check	C_83_0	def get ( self ) spam_check = { } if self . enable is not none spam_check [ string ] = self . enable if self . threshold is not none spam_check [ string ] = self . threshold . get ( ) if self . post_to_url is not none spam_check [ string ] = self . post_to_url . get ( ) return spam_check	1
Q_83	get a json_ready representation of this spam_check	C_83_1	def execute ( self , input_data ) view = input_data [ string ] extracted_files = input_data [ string ] [ string ] del view [ string ] view [ string ] = [ self . workbench . work_request ( string , var , [ string , string , string , string , string , string ] ) for var in extracted_files ] return view	0
Q_83	get a json_ready representation of this spam_check	C_83_2	def build_cycle_graph ( num_nodes ) graph = undirected_graph ( ) if num_nodes > number first_node = graph . new_node ( ) if num_nodes > number previous_node = first_node for var in range ( num_nodes _ number ) new_node = graph . new_node ( ) graph . new_edge ( previous_node , new_node ) previous_node = new_node graph . new_edge ( previous_node , first_node ) return graph	0
Q_84	get a json_ready representation of this custom_arg	C_84_0	def get ( self ) custom_arg = { } if self . key is not none and self . value is not none custom_arg [ self . key ] = self . value return custom_arg	1
Q_84	get a json_ready representation of this custom_arg	C_84_1	def plot_iv_curve ( a , hold_v , i , * plt_args , * * plt_kwargs ) grid = plt_kwargs . pop ( string , true ) same_fig = plt_kwargs . pop ( string , false ) if not len ( plt_args ) plt_args = ( string , ) if string not in plt_kwargs plt_kwargs [ string ] = string if not same_fig make_iv_curve_fig ( a , grid = grid ) if type ( i ) is dict i = [ i [ var ] for var in hold_v ] plt . plot ( [ var * number for var in hold_v ] , [ ii * number for ii in i ] , * plt_args , * * plt_kwargs ) plt . legend ( loc = number )	0
Q_84	get a json_ready representation of this custom_arg	C_84_2	def stdio ( filters = none , search_dirs = none , data_dir = true , sys_path = true , panfl_ = false , input_stream = none , output_stream = none ) doc = load ( input_stream ) verbose = doc . get_metadata ( string , false ) if search_dirs is none search_dirs = doc . get_metadata ( string , [ ] ) if type ( search_dirs ) = list search_dirs = [ search_dirs ] if string in search_dirs data_dir = true if string in search_dirs sys_path = false search_dirs = [ var for var in search_dirs if var not in ( string , string ) ] if verbose debug ( string . format ( data_dir , sys_path ) ) search_dirs = [ p . normpath ( p . expanduser ( p . expandvars ( var ) ) ) for var in search_dirs ] if not panfl_ search_dirs . append ( string ) if data_dir search_dirs . append ( get_filter_dir ( ) ) if sys_path search_dirs + = sys . path else if data_dir search_dirs . append ( get_filter_dir ( ) ) if sys_path search_dirs + = reduced_sys_path msg = doc . get_metadata ( string , false ) if msg debug ( msg ) if filters is none filters = doc . get_metadata ( string , [ ] ) if type ( filters ) = list filters = [ filters ] if filters if verbose msg = string debug ( msg , string . join ( filters ) ) doc = autorun_filters ( filters , doc , search_dirs , verbose ) elif verbose debug ( string ) dump ( doc , output_stream )	0
Q_85	check whether this structure be similar to another structure	C_85_0	def matches ( self , other , * * kwargs ) from pymatgen . analysis . structure_matcher import structure_matcher m = structure_matcher ( * * kwargs ) return m . fit ( structure . from_sites ( self ) , structure . from_sites ( other ) )	1
Q_85	check whether this structure be similar to another structure	C_85_1	def hard_reset ( self ) if self . shuffle self . _shuffle_data ( ) self . cursor = _ self . batch_size self . _cache_data = none self . _cache_label = none	0
Q_85	check whether this structure be similar to another structure	C_85_2	def require ( self , lock , guard_func , * guard_args , * * guard_kw ) def decorator ( f ) wraps ( f ) def wrapper ( * args , * * kw ) if self . granted ( lock ) self . msg ( string . format ( lock ) ) return f ( * args , * * kw ) if guard_func ( * guard_args , * * guard_kw ) and self . acquire ( lock ) return f ( * args , * * kw ) return none return wrapper return decorator	0
Q_86	constant volume specific heat c_v at temperature t obtain from the integration of the do	C_86_0	def cv ( self , t , structure = none ) if t = = number return number freqs = self . _positive_frequencies dens = self . _positive_densities csch2 = lambda x number . number / ( np . sinh ( x ) * * number ) wd2kt = freqs / ( number * boltz_thz_per_k * t ) cv = np . trapz ( wd2kt * * number * csch2 ( wd2kt ) * dens , x = freqs ) cv * = const . boltzmann * const . avogadro if structure formula_units = structure . composition . num_atoms / structure . composition . reduced_composition . num_atoms cv / = formula_units return cv	1
Q_86	constant volume specific heat c_v at temperature t obtain from the integration of the do	C_86_1	def days_since_last_snowfall ( self , value = number ) if value is not none try value = int ( value ) except value_error raise value_error ( string string . format ( value ) ) self . _days_since_last_snowfall = value	0
Q_86	constant volume specific heat c_v at temperature t obtain from the integration of the do	C_86_2	def _update_card_file_location ( self , card_name , new_directory ) with tmp_chdir ( self . gssha_directory ) file_card = self . project_manager . get_card ( card_name ) if file_card if file_card . value original_location = file_card . value . strip ( string string string ) new_location = os . path . join ( new_directory , os . path . basename ( original_location ) ) file_card . value = string { number } string . format ( os . path . basename ( original_location ) ) try move ( original_location , new_location ) except os_error as ex log . warning ( ex ) pass	0
Q_87	add edge to graph	C_87_0	def add_edge ( self , from_index , to_index , from_jimage = ( number , number , number ) , to_jimage = none , weight = none , warn_duplicates = true , edge_properties = none ) if to_index < from_index to_index , from_index = from_index , to_index to_jimage , from_jimage = from_jimage , to_jimage if not np . array_equal ( from_jimage , ( number , number , number ) ) shift = from_jimage from_jimage = np . subtract ( from_jimage , shift ) to_jimage = np . subtract ( to_jimage , shift ) if to_jimage is none warnings . warn ( string string ) dist , to_jimage = self . structure [ from_index ] . distance_and_image ( self . structure [ to_index ] ) if dist = = number images = [ number , number , number ] , [ number , number , number ] , [ number , number , number ] dists = [ ] for var in images dists . append ( self . structure [ from_index ] . distance_and_image ( self . structure [ from_index ] , jimage = var ) [ number ] ) dist = min ( dists ) equiv_sites = self . structure . get_neighbors_in_shell ( self . structure [ from_index ] . coords , dist , dist * number . number number , include_index = true ) for site , dist , to_index in equiv_sites to_jimage = np . subtract ( site . frac_coords , self . structure [ from_index ] . frac_coords ) to_jimage = to_jimage . astype ( int ) self . add_edge ( from_index = from_index , from_jimage = ( number , number , number ) , to_jimage = to_jimage , to_index = to_index ) return from_jimage , to_jimage = tuple ( map ( int , from_jimage ) ) , tuple ( map ( int , to_jimage ) ) from_index , to_index = int ( from_index ) , int ( to_index ) existing_edge_data = self . graph . get_edge_data ( from_index , to_index ) if existing_edge_data for key , d in existing_edge_data . items ( ) if d [ string ] = = to_jimage if warn_duplicates warnings . warn ( string string . format ( from_index , to_index , to_jimage ) ) return edge_properties = edge_properties or { } if weight self . graph . add_edge ( from_index , to_index , to_jimage = to_jimage , weight = weight , * * edge_properties ) else self . graph . add_edge ( from_index , to_index , to_jimage = to_jimage , * * edge_properties )	1
Q_87	add edge to graph	C_87_1	def get_access_tokens ( self , authorization_code ) response = self . box_request . get_access_token ( authorization_code ) try att = response . json ( ) except exception , ex raise box_http_response_error ( ex ) if response . status_code > = number raise box_error ( response . status_code , att ) return att [ string ] , att [ string ]	0
Q_87	add edge to graph	C_87_2	def nl_object_alloc ( ops ) new = nl_object ( ) nl_init_list_head ( new . ce_list ) new . ce_ops = ops if ops . oo_constructor ops . oo_constructor ( new ) _logger . debug ( string , id ( new ) ) return new	0
Q_88	create a ctrl file object from a string	C_88_0	def from_string ( cls , data , sigfigs = number ) lines = data . split ( string ) [ _ number ] struc_lines = { string [ ] , string [ ] , string [ ] , string [ ] , string [ ] , string [ ] } for var in lines if var = string and not var . isspace ( ) if not var [ number ] . isspace ( ) cat = var . split ( ) [ number ] if cat in struc_lines struc_lines [ cat ] . append ( var ) else pass for cat in struc_lines struc_lines [ cat ] = string . join ( struc_lines [ cat ] ) . replace ( string , string ) structure_tokens = { string none , string [ ] , string [ ] , string [ ] } for cat in [ string , string , string ] fields = struc_lines [ cat ] . split ( string ) for f , field in enumerate ( fields ) token = field . split ( ) [ _ number ] if token = = string alat = round ( float ( fields [ f + number ] . split ( ) [ number ] ) , sigfigs ) structure_tokens [ string ] = alat elif token = = string atom = fields [ f + number ] . split ( ) [ number ] if not bool ( re . match ( string , atom ) ) if cat = = string structure_tokens [ string ] . append ( atom ) else structure_tokens [ string ] . append ( { string atom } ) else pass elif token in [ string , string ] try arr = np . array ( [ round ( float ( i ) , sigfigs ) for i in fields [ f + number ] . split ( ) ] ) except value_error arr = np . array ( [ round ( float ( i ) , sigfigs ) for i in fields [ f + number ] . split ( ) [ _ number ] ] ) if token = = string structure_tokens [ string ] = arr . reshape ( [ number , number ] ) elif not bool ( re . match ( string , atom ) ) structure_tokens [ string ] [ _ number ] [ string ] = arr else pass else pass try spcgrp_index = struc_lines [ string ] . index ( string ) spcgrp = struc_lines [ string ] [ spcgrp_index spcgrp_index + number ] structure_tokens [ string ] = spcgrp . split ( string ) [ number ] . split ( ) [ number ] except value_error pass for token in [ string , string ] try value = re . split ( token + r string , struc_lines [ token ] ) [ number ] structure_tokens [ token ] = value . strip ( ) except index_error pass return lmto_ctrl . from_dict ( structure_tokens )	1
Q_88	create a ctrl file object from a string	C_88_1	def remove ( self , transport ) recipients = copy . copy ( self . recipients ) for var , var in recipients . items ( ) var . remove ( transport ) if not len ( var . transports ) del self . recipients [ var ]	0
Q_88	create a ctrl file object from a string	C_88_2	def knowledge_kbtype_formatter ( view , context , model , name ) value = model . kbtype return next ( ( var for var , var in knw_kb . knwkb_types . items ( ) if var = = value ) , none )	0
Q_89	an rgb color line for plot	C_89_0	def _rgbline ( ax , k , e , red , green , blue , alpha = number , linestyles = string ) from matplotlib . collections import line_collection pts = np . array ( [ k , e ] ) . t . reshape ( _ number , number , number ) seg = np . concatenate ( [ pts [ _ number ] , pts [ number ] ] , axis = number ) nseg = len ( k ) _ number r = [ number . number * ( red [ var ] + red [ var + number ] ) for var in range ( nseg ) ] g = [ number . number * ( green [ var ] + green [ var + number ] ) for var in range ( nseg ) ] b = [ number . number * ( blue [ var ] + blue [ var + number ] ) for var in range ( nseg ) ] a = np . ones ( nseg , np . float ) * alpha lc = line_collection ( seg , colors = list ( zip ( r , g , b , a ) ) , linewidth = number , linestyles = linestyles ) ax . add_collection ( lc )	1
Q_89	an rgb color line for plot	C_89_1	def payload ( self ) if self . _content_type is not none try return self . _payload [ self . _content_type ] except key_error raise key_error ( string ) else if defines . content_types [ string ] in self . _payload return self . _payload [ defines . content_types [ string ] ] else val = list ( self . _payload . keys ( ) ) return val [ number ] , self . _payload [ val [ number ] ]	0
Q_89	an rgb color line for plot	C_89_2	def pdf ( self , x ) string string string return invwishart . pdf ( x , df = self . v , scale = self . psi )	0
Q_90	bind the give socket to the give interface	C_90_0	def bind_socket ( sock , channel = string ) log . debug ( string , channel ) if has_native_support sock . bind ( ( channel , ) ) else addr = get_addr ( sock , channel ) libc . bind ( sock . fileno ( ) , addr , len ( addr ) ) log . debug ( string )	1
Q_90	bind the give socket to the give interface	C_90_1	def sim_minkowski ( src , tar , qval = number , pval = number , alphabet = none ) return minkowski ( ) . sim ( src , tar , qval , pval , alphabet )	0
Q_90	bind the give socket to the give interface	C_90_2	def _get_purecn_dx_files ( paired , out ) out_base = string utils . splitext_plus ( out [ string ] ) [ number ] all_files = [ ] for var , var in [ [ ( string , ) , string ] , [ ( string , string ) , string ] , [ ( string , ) , string ] ] cur_file = string ( out_base , var ) out = tz . update_in ( out , var , lambda x cur_file ) all_files . append ( os . path . basename ( cur_file ) ) return out_base , out , all_files	0
Q_91	test to see if a pauli_term or pauli_sum be zero	C_91_0	def is_zero ( pauli_object ) if isinstance ( pauli_object , pauli_term ) return np . isclose ( pauli_object . coefficient , number ) elif isinstance ( pauli_object , pauli_sum ) return len ( pauli_object . terms ) = = number and np . isclose ( pauli_object . terms [ number ] . coefficient , number ) else raise type_error ( string )	1
Q_91	test to see if a pauli_term or pauli_sum be zero	C_91_1	def load_top_hat ( self , wave_min , wave_max , pixels_per_bin = number ) self . pixels_per_bin = pixels_per_bin self . n_bins = number self . _wave_units = q . aa wave_min = wave_min . to ( self . wave_units ) wave_max = wave_max . to ( self . wave_units ) self . _wave = np . linspace ( wave_min , wave_max , pixels_per_bin ) self . _throughput = np . ones_like ( self . wave ) self . raw = np . array ( [ self . wave . value , self . throughput ] ) wave_eff = ( ( wave_min + wave_max ) / number . ) . value width = ( wave_max _ wave_min ) . value self . path = string self . refs = string self . band = string self . calibration_reference = string self . fwhm = width self . facility = string self . filter_profile_service = string self . mag_sys = string self . phot_cal_id = string self . phot_system = string self . profile_reference = string self . wavelength_min = wave_min . value self . wavelength_max = wave_max . value self . wavelength_cen = wave_eff self . wavelength_eff = wave_eff self . wavelength_mean = wave_eff self . wavelength_peak = wave_eff self . wavelength_phot = wave_eff self . wavelength_pivot = wave_eff self . wavelength_ucd = string self . width_eff = width self . zero_point = number self . zero_point_type = string self . zero_point_unit = string self . filter_id = string	0
Q_91	test to see if a pauli_term or pauli_sum be zero	C_91_2	def overlay_gateway_map_vlan_vni_mapping_vid ( self , * * kwargs ) config = et . element ( string ) overlay_gateway = et . sub_element ( config , string , xmlns = string ) name_key = et . sub_element ( overlay_gateway , string ) name_key . text = kwargs . pop ( string ) map = et . sub_element ( overlay_gateway , string ) vlan_vni_mapping = et . sub_element ( map , string ) vid = et . sub_element ( vlan_vni_mapping , string ) vid . text = kwargs . pop ( string ) callback = kwargs . pop ( string , self . _callback ) return callback ( config )	0
Q_92	take the output of _parse_name to create a canonical name	C_92_0	def _canonicalize_name ( prefix , qvm_type , noisy ) if noisy noise_suffix = string else noise_suffix = string if qvm_type is none qvm_suffix = string elif qvm_type = = string qvm_suffix = string elif qvm_type = = string qvm_suffix = string else raise value_error ( f string ) name = f string return name	1
Q_92	take the output of _parse_name to create a canonical name	C_92_1	def get_repositories ( self ) if self . srcdir and not self . duplicate return self . srcdir . get_all_rdirs ( ) + self . repositories return self . repositories	0
Q_92	take the output of _parse_name to create a canonical name	C_92_2	def main ( ) print ( string . format ( _version_ ) ) try args = setup_argparse ( ) except argparse . argument_error as error raise system_exit ( error ) try serial_port = setup_serial_port ( args . loopback , args . port ) except serial . serial_exception as error raise system_exit ( error ) tun_name = string . format ( args . callsign . upper ( ) , args . id ) is_running = threading . event ( ) is_running . set ( ) try tun = monitor ( serial_port = serial_port , name = tun_name , is_running = is_running ) tun . start ( ) except pytun . error as error print ( string ) raise system_exit ( error ) try while true time . sleep ( number . number ) except keyboard_interrupt tun . is_running . clear ( ) tun . join ( )	0
Q_93	convert validator dictionary to a recipient list for transaction	C_93_0	def recipients ( cls , bigchain ) recipients = [ ] for var , var in cls . get_validators ( bigchain ) . items ( ) recipients . append ( ( [ var ] , var ) ) return recipients	1
Q_93	convert validator dictionary to a recipient list for transaction	C_93_1	def smart_convert ( original , colorkey , pixelalpha ) tile_size = original . get_size ( ) threshold = number try px = pygame . mask . from_surface ( original , threshold ) . count ( ) except return original . convert_alpha ( ) if px = = tile_size [ number ] * tile_size [ number ] tile = original . convert ( ) elif colorkey tile = original . convert ( ) tile . set_colorkey ( colorkey , pygame . rleaccel ) elif pixelalpha tile = original . convert_alpha ( ) else tile = original . convert ( ) return tile	0
Q_93	convert validator dictionary to a recipient list for transaction	C_93_2	def less_than ( self , value ) newq = self . copy ( ) newq . set_op ( query . op . less_than ) newq . set_value ( value ) return newq	0
Q_94	add token to vocabulary	C_94_0	def add_token ( self , token ) token = self . process_token ( token ) self . _token_count . update ( [ token ] )	1
Q_94	add token to vocabulary	C_94_1	def configure_uploads ( app , upload_sets ) if isinstance ( upload_sets , upload_set ) upload_sets = ( upload_sets , ) if not hasattr ( app , string ) app . upload_set_config = { } set_config = app . upload_set_config defaults = dict ( dest = app . config . get ( string ) , url = app . config . get ( string ) ) for var in upload_sets config = config_for_set ( var , app , defaults ) set_config [ var . name ] = config should_serve = any ( s . base_url is none for s in set_config . values ( ) ) if string not in app . blueprints and should_serve app . register_blueprint ( uploads_mod )	0
Q_94	add token to vocabulary	C_94_2	def _send_offset_requests ( self , timestamps ) timestamps_by_node = collections . defaultdict ( dict ) for var , var in six . iteritems ( timestamps ) node_id = self . _client . cluster . leader_for_partition ( var ) if node_id is none self . _client . add_topic ( var . topic ) log . debug ( string string , var ) return future ( ) . failure ( errors . stale_metadata ( var ) ) elif node_id = = _ number log . debug ( string string , var ) return future ( ) . failure ( errors . leader_not_available_error ( var ) ) else timestamps_by_node [ node_id ] [ var ] = var list_offsets_future = future ( ) responses = [ ] node_count = len ( timestamps_by_node ) def on_success ( value ) responses . append ( value ) if len ( responses ) = = node_count offsets = { } for r in responses offsets . update ( r ) list_offsets_future . success ( offsets ) def on_fail ( err ) if not list_offsets_future . is_done list_offsets_future . failure ( err ) for node_id , timestamps in six . iteritems ( timestamps_by_node ) _f = self . _send_offset_request ( node_id , timestamps ) _f . add_callback ( on_success ) _f . add_errback ( on_fail ) return list_offsets_future	0
Q_95	set metadata when an exception be raise	C_95_0	def cell_exception ( self , cell , cell_index = none , * * kwargs ) cell . metadata . papermill [ string ] = true cell . metadata . papermill [ string ] = self . failed self . nb . metadata . papermill [ string ] = true	1
Q_95	set metadata when an exception be raise	C_95_1	def default_callback ( pending , timeout ) def echo ( msg ) sys . stderr . write ( msg + string ) echo ( string pending ) echo ( string timeout ) echo ( string ( os . name = = string and string or string ) ) sys . stderr . flush ( )	0
Q_95	set metadata when an exception be raise	C_95_2	def edit ( self , entity , id , payload , sync = true ) url = urljoin ( self . host , entity . value + string ) url = urljoin ( url , id + string ) params = { string str ( sync ) . lower ( ) } url = utils . add_url_parameters ( url , params ) r = requests . put ( url , auth = self . auth , data = json . dumps ( payload ) , headers = self . headers ) if r . status_code = = number error_message = r . json ( ) [ string ] raise coredata_error ( string . format ( error = error_message ) )	0
Q_96	invoke callback for every key_up event	C_96_0	def on_release ( callback , suppress = false ) return hook ( lambda e e . event_type = = key_down or callback ( e ) , suppress = suppress )	1
Q_96	invoke callback for every key_up event	C_96_1	def export ( request , page_id , export_unpublished = false ) try if export_unpublished root_page = page . objects . get ( id = page_id ) else root_page = page . objects . get ( id = page_id , live = true ) except page . does_not_exist return json_response ( { string _ ( string ) } ) payload = export_pages ( root_page , export_unpublished = export_unpublished ) return json_response ( payload )	0
Q_96	invoke callback for every key_up event	C_96_2	def to_mask ( obj , m = none , indices = none ) if not pimms . is_map ( obj ) and pimms . is_vector ( obj ) and len ( obj ) < number and m is none if len ( obj ) = = number obj = obj [ number ] elif len ( obj ) = = number ( obj , m ) = obj else ( obj , m , q ) = obj if indices is none if pimms . is_map ( q ) indices = q . get ( string , false ) else indices = q if indices is none indices = false if is_vset ( obj ) lbls = obj . labels idcs = obj . indices obj = obj . properties else obj = pimms . itable ( obj ) lbls = np . arange ( number , obj . row_count , number , dtype = np . int ) idcs = lbls if m is none return idcs if indices else lbls if is_tuple ( m ) if len ( m ) = = number return np . asarray ( [ ] , dtype = np . int ) p = to_property ( obj , m [ number ] ) if len ( m ) = = number and hasattr ( m [ number ] , string ) m = reduce ( lambda q , var np . logical_or ( q , p = = var ) , m [ number ] , np . zeros ( len ( p ) , dtype = np . bool ) ) elif len ( m ) = = number m = ( p = = m [ number ] ) elif len ( m ) = = number m = np . logical_and ( m [ number ] < p , p < = m [ number ] ) elif pimms . is_str ( m ) m = np . asarray ( obj [ m ] , dtype = np . bool ) elif pimms . is_map ( m ) if len ( m ) = number raise value_error ( string ) ( k , v ) = next ( six . iteritems ( m ) ) if not hasattr ( v , string ) raise value_error ( string ) if not pimms . is_str ( k ) raise value_error ( string or string and string ) v = [ to_mask ( obj , var , indices = indices ) for var in v ] if k in ( string , string , string , string , string , string ) return reduce ( np . intersect1d , v ) elif k in ( string , string , string , string string ) return reduce ( np . union1d , v ) return idcs [ m ] if indices else lbls [ m ]	0
Q_97	launch malmo minecraft mod in one or more client from	C_97_0	def launch_minecraft ( ports = [ ] , wait_timeout = number ) if string not in os . environ print ( string ) return cwd = os . getcwd ( ) try os . chdir ( malmo_install_dir + string ) launch_minecraft_in_background ( os . getcwd ( ) , ports , wait_timeout ) finally os . chdir ( cwd )	1
Q_97	launch malmo minecraft mod in one or more client from	C_97_1	def add_folder ( self , path , title , description = none , language = none , thumbnail = none , source_id = none , * * node_data ) self . _parse_path ( path ) path = path if path . endswith ( title ) else string . format ( path , title ) self . _commit ( path , title , description = description , language = language , thumbnail = thumbnail , source_id = source_id )	0
Q_97	launch malmo minecraft mod in one or more client from	C_97_2	def _populate_warcinfo ( self , extra_fields = none ) self . _warcinfo_record . set_common_fields ( warc_record . warcinfo , warc_record . warc_fields ) info_fields = name_value_record ( wrap_width = number ) info_fields [ string ] = self . _params . software_string or self . default_software_string info_fields [ string ] = string info_fields [ string ] = string if extra_fields for var , var in extra_fields info_fields . add ( var , var ) self . _warcinfo_record . block_file = io . bytes_io ( bytes ( info_fields ) + b string ) self . _warcinfo_record . compute_checksum ( )	0
Q_98	bound method reply_chat_action of obj message pyrogram	C_98_0	def reply_chat_action ( self , action union [ chat_action , str ] , progress int = number ) _ > string return self . _client . send_chat_action ( chat_id = self . chat . id , action = action , progress = progress )	1
Q_98	bound method reply_chat_action of obj message pyrogram	C_98_1	def get_table ( self , table_name , db = string ) if db = = string and string in table_name db , table_name = table_name . split ( string ) [ number ] with self . metastore as client return client . get_table ( dbname = db , tbl_name = table_name )	0
Q_98	bound method reply_chat_action of obj message pyrogram	C_98_2	def connect ( self ) nodes = self . element . inlets total = self . sequences . inlets . total if total . shape = ( len ( nodes ) , ) total . shape = len ( nodes ) for var , var in enumerate ( nodes ) double = var . get_double ( string ) total . set_pointer ( double , var ) for ( var , name ) in enumerate ( self . nodenames ) try outlet = getattr ( self . element . outlets , name ) double = outlet . get_double ( string ) except attribute_error raise runtime_error ( f string f string f string f string ) self . sequences . outlets . branched . set_pointer ( double , var )	0
Q_99	re_download folder data	C_99_0	def refresh_folder ( self , update_parent_if_changed = false ) folder_id = getattr ( self , string , none ) if self . root or folder_id is none return false folder = self . get_folder ( folder_id = folder_id ) if folder is none return false self . name = folder . name if folder . parent_id and self . parent_id if folder . parent_id = self . parent_id self . parent_id = folder . parent_id self . parent = ( self . get_parent_folder ( ) if update_parent_if_changed else none ) self . child_folders_count = folder . child_folders_count self . unread_items_count = folder . unread_items_count self . total_items_count = folder . total_items_count self . updated_at = folder . updated_at return true	1
Q_99	re_download folder data	C_99_1	def new_process ( self , consumer_name ) process_name = string ( consumer_name , self . new_process_number ( consumer_name ) ) kwargs = { string self . config . application , string consumer_name , string self . profile , string false , string self . stats_queue , string self . config . logging } return process_name , process . process ( name = process_name , kwargs = kwargs )	0
Q_99	re_download folder data	C_99_2	async def finish ( self , * , chat typing . union [ str , int , none ] = none , user typing . union [ str , int , none ] = none ) await self . reset_state ( chat = chat , user = user , with_data = true )	0
Q_100	get the project version from whatever source be available	C_100_0	def get_versions ( verbose = false ) if string in sys . modules del sys . modules [ string ] root = get_root ( ) cfg = get_config_from_root ( root ) assert cfg . vcs is not none , string handlers = handlers . get ( cfg . vcs ) assert handlers , string s string cfg . vcs verbose = verbose or cfg . verbose assert cfg . versionfile_source is not none , string assert cfg . tag_prefix is not none , string versionfile_abs = os . path . join ( root , cfg . versionfile_source ) get_keywords_f = handlers . get ( string ) from_keywords_f = handlers . get ( string ) if get_keywords_f and from_keywords_f try keywords = get_keywords_f ( versionfile_abs ) ver = from_keywords_f ( keywords , cfg . tag_prefix , verbose ) if verbose print ( string ver ) return ver except not_this_method pass try ver = versions_from_file ( versionfile_abs ) if verbose print ( string ( versionfile_abs , ver ) ) return ver except not_this_method pass from_vcs_f = handlers . get ( string ) if from_vcs_f try pieces = from_vcs_f ( cfg . tag_prefix , root , verbose ) ver = render ( pieces , cfg . style ) if verbose print ( string ver ) return ver except not_this_method pass try if cfg . parentdir_prefix ver = versions_from_parentdir ( cfg . parentdir_prefix , root , verbose ) if verbose print ( string ver ) return ver except not_this_method pass if verbose print ( string ) return { string string , string none , string none , string string }	1
Q_100	get the project version from whatever source be available	C_100_1	def parse_boolargs ( self , args ) if not isinstance ( args , list ) args = [ args ] return_vals = [ ] bool_args = [ ] for var in args if not isinstance ( var , tuple ) return_val = var bool_arg = none elif len ( var ) = = number return_val = var [ number ] bool_arg = none elif len ( var ) = = number return_val , bool_arg = var else raise value_error ( string ) return_vals . append ( return_val ) bool_args . append ( bool_arg ) outdtype = numpy . array ( return_vals ) . dtype out = numpy . zeros ( self . size , dtype = outdtype ) mask = numpy . zeros ( self . size , dtype = bool ) leftovers = numpy . ones ( self . size , dtype = bool ) for ii , ( boolarg , val ) in enumerate ( zip ( bool_args , return_vals ) ) if boolarg is none or boolarg = = string or boolarg . lower ( ) = = string if ii + number = len ( bool_args ) raise value_error ( string string ) mask = leftovers else mask = leftovers self [ boolarg ] out [ mask ] = val leftovers = mask return out , numpy . where ( leftovers ) [ number ]	0
Q_100	get the project version from whatever source be available	C_100_2	def mean_pairwise_difference ( ac , an = none , fill = np . nan ) ac = asarray_ndim ( ac , number ) if an is none an = np . sum ( ac , axis = number ) else an = asarray_ndim ( an , number ) check_dim0_aligned ( ac , an ) n_pairs = an * ( an _ number ) / number n_same = np . sum ( ac * ( ac _ number ) / number , axis = number ) n_diff = n_pairs _ n_same with ignore_invalid ( ) mpd = np . where ( n_pairs > number , n_diff / n_pairs , fill ) return mpd	0
Q_101	test if a building can be place in the give location	C_101_0	async def can_place ( self , building union [ ability_data , ability_id , unit_type_id ] , position point2 ) _ > bool assert isinstance ( building , ( ability_data , ability_id , unit_type_id ) ) if isinstance ( building , unit_type_id ) building = self . _game_data . units [ building . value ] . creation_ability elif isinstance ( building , ability_id ) building = self . _game_data . abilities [ building . value ] r = await self . _client . query_building_placement ( building , [ position ] ) return r [ number ] = = action_result . success	1
Q_101	test if a building can be place in the give location	C_101_1	def relax_ax ( self ) self . a_xnr = self . cnst_a ( self . x , self . xf ) if self . rlx = = number . number self . ax = self . a_xnr else if not hasattr ( self , string ) self . _cnst_c = self . cnst_c ( ) alpha = self . rlx self . ax = alpha * self . a_xnr _ ( number _ alpha ) * ( self . cnst_b ( self . y ) _ self . _cnst_c )	0
Q_101	test if a building can be place in the give location	C_101_2	def export_coreml ( self , filename , include_non_maximum_suppression = true , iou_threshold = none , confidence_threshold = none ) import mxnet as _mx from . _mxnet . _mxnet_to_coreml import _mxnet_converter import coremltools from coremltools . models import datatypes , neural_network if iou_threshold is none iou_threshold = self . non_maximum_suppression_threshold if confidence_threshold is none confidence_threshold = number . number preds_per_box = number + self . num_classes num_anchors = len ( self . anchors ) num_classes = self . num_classes batch_size = number image_shape = ( batch_size , ) + tuple ( self . input_image_shape ) s_image_uint8 = _mx . sym . variable ( self . feature , shape = image_shape , dtype = _np . float32 ) s_image = s_image_uint8 / number from copy import copy net = copy ( self . _model ) net . _children = copy ( self . _model . _children ) from . _model import _special_darknet_maxpool_block op = _special_darknet_maxpool_block ( name = string ) assert ( self . _model [ number ] . name = = string and self . _model [ number ] . name = = string ) del net . _children [ number ] net . _children [ number ] = op s_ymap = net ( s_image ) mod = _mx . mod . module ( symbol = s_ymap , label_names = none , data_names = [ self . feature ] ) mod . bind ( for_training = false , data_shapes = [ ( self . feature , image_shape ) ] ) mod . init_params ( ) arg_params , aux_params = mod . get_params ( ) net_params = net . collect_params ( ) new_arg_params = { } for var , var in arg_params . items ( ) new_arg_params [ var ] = net_params [ var ] . data ( net_params [ var ] . list_ctx ( ) [ number ] ) new_aux_params = { } for var , var in aux_params . items ( ) new_aux_params [ var ] = net_params [ var ] . data ( net_params [ var ] . list_ctx ( ) [ number ] ) mod . set_params ( new_arg_params , new_aux_params ) input_names = [ self . feature ] input_dims = [ list ( self . input_image_shape ) ] input_types = [ datatypes . array ( * dim ) for dim in input_dims ] input_features = list ( zip ( input_names , input_types ) ) num_spatial = self . _grid_shape [ number ] * self . _grid_shape [ number ] num_bounding_boxes = num_anchors * num_spatial confidence_str = ( string if include_non_maximum_suppression else string ) coordinates_str = ( string if include_non_maximum_suppression else string ) output_names = [ confidence_str , coordinates_str ] output_dims = [ ( num_bounding_boxes , num_classes ) , ( num_bounding_boxes , number ) , ] output_types = [ datatypes . array ( * dim ) for dim in output_dims ] output_features = list ( zip ( output_names , output_types ) ) mode = none builder = neural_network . neural_network_builder ( input_features , output_features , mode ) _mxnet_converter . convert ( mod , mode = none , input_shape = [ ( self . feature , image_shape ) ] , builder = builder , verbose = false ) prefix = string builder . add_reshape ( name = prefix + string , target_shape = [ batch_size , num_anchors , preds_per_box , num_spatial ] , mode = number , input_name = string , output_name = prefix + string ) builder . add_permute ( name = prefix + string , dim = [ number , number , number , number ] , input_name = prefix + string , output_name = prefix + string ) builder . add_slice ( name = prefix + string , axis = string , start_index = number , end_index = number , stride = number , input_name = prefix + string , output_name = prefix + string ) builder . add_activation ( name = prefix + string , non_linearity = string , input_name = prefix + string , output_name = prefix + string ) builder . add_reshape ( name = prefix + string , target_shape = [ batch_size , number , num_bounding_boxes , number ] , mode = number , input_name = prefix + string , output_name = prefix + string ) c_xy = _np . array ( _np . meshgrid ( _np . arange ( self . _grid_shape [ number ] ) , _np . arange ( self . _grid_shape [ number ] ) ) , dtype = _np . float32 ) c_xy_reshaped = ( _np . tile ( c_xy [ , _np . newaxis ] , ( num_anchors , number , number ) ) . reshape ( number , _ number ) ) [ _np . newaxis , . , _np . newaxis ] builder . add_load_constant ( prefix + string , constant_value = c_xy_reshaped , shape = c_xy_reshaped . shape [ number ] , output_name = prefix + string ) builder . add_elementwise ( name = prefix + string , mode = string , input_names = [ prefix + string , prefix + string ] , output_name = prefix + string ) builder . add_slice ( name = prefix + string , axis = string , start_index = number , end_index = number , stride = number , input_name = prefix + string , output_name = prefix + string ) builder . add_unary ( name = prefix + string , mode = string , input_name = prefix + string , output_name = prefix + string ) builder . add_reshape ( name = prefix + string , target_shape = [ batch_size , number * num_anchors ] + list ( self . _grid_shape ) , mode = number , input_name = prefix + string , output_name = prefix + string ) np_anchors = _np . asarray ( self . anchors , dtype = _np . float32 ) . t anchors_0 = _np . tile ( np_anchors . reshape ( [ number * num_anchors , number , number ] ) , self . _grid_shape ) builder . add_load_constant ( name = prefix + string , constant_value = anchors_0 , shape = anchors_0 . shape , output_name = prefix + string ) builder . add_elementwise ( name = prefix + string , mode = string , input_names = [ prefix + string , prefix + string ] , output_name = prefix + string ) builder . add_reshape ( name = prefix + string , target_shape = [ number , number , num_bounding_boxes , number ] , mode = number , input_name = prefix + string , output_name = prefix + string ) builder . add_elementwise ( name = prefix + string , mode = string , input_names = [ prefix + string , prefix + string ] , output_name = prefix + string ) builder . add_permute ( name = prefix + string , dim = [ number , number , number , number ] , input_name = prefix + string , output_name = prefix + string ) scale = _np . zeros ( ( num_bounding_boxes , number , number ) ) scale [ , number number ] = number . number / self . _grid_shape [ number ] scale [ , number number ] = number . number / self . _grid_shape [ number ] builder . add_scale ( name = coordinates_str , w = scale , b = number , has_bias = false , shape_scale = ( num_bounding_boxes , number , number ) , input_name = prefix + string , output_name = coordinates_str ) builder . add_slice ( name = prefix + string , axis = string , start_index = number , end_index = preds_per_box , stride = number , input_name = prefix + string , output_name = prefix + string ) builder . add_softmax ( name = prefix + string , input_name = prefix + string , output_name = prefix + string ) builder . add_slice ( name = prefix + string , axis = string , start_index = number , end_index = number , stride = number , input_name = prefix + string , output_name = prefix + string ) builder . add_activation ( name = prefix + string , non_linearity = string , input_name = prefix + string , output_name = prefix + string ) if num_classes > number conf = prefix + string builder . add_elementwise ( name = prefix + string , mode = string , input_names = [ prefix + string ] * num_classes , output_name = conf ) else conf = prefix + string builder . add_elementwise ( name = prefix + string , mode = string , input_names = [ conf , prefix + string ] , output_name = prefix + string ) builder . add_reshape ( name = prefix + string , target_shape = [ number , num_classes , num_bounding_boxes , number ] , mode = number , input_name = prefix + string , output_name = prefix + string ) builder . add_permute ( name = confidence_str , dim = [ number , number , number , number ] , input_name = prefix + string , output_name = confidence_str ) _mxnet_converter . _set_input_output_layers ( builder , input_names , output_names ) builder . set_input ( input_names , input_dims ) builder . set_output ( output_names , output_dims ) builder . set_pre_processing_parameters ( image_input_names = self . feature ) model = builder . spec if include_non_maximum_suppression model_neural_network = model . neural_network model . specification_version = number model . pipeline . parse_from_string ( b string ) model . pipeline . models . add ( ) model . pipeline . models [ number ] . neural_network . parse_from_string ( b string ) model . pipeline . models . add ( ) model . pipeline . models [ number ] . non_maximum_suppression . parse_from_string ( b string ) nn_model = model . pipeline . models [ number ] nn_model . description . parse_from_string ( b string ) input_image = model . description . input [ number ] input_image . type . image_type . width = self . input_image_shape [ number ] input_image . type . image_type . height = self . input_image_shape [ number ] nn_model . description . input . add ( ) nn_model . description . input [ number ] . parse_from_string ( input_image . serialize_to_string ( ) ) for i in range ( number ) del model . description . output [ i ] . type . multi_array_type . shape [ ] names = [ string , string ] bounds = [ self . num_classes , number ] for i in range ( number ) output_i = model . description . output [ i ] output_i . name = names [ i ] for j in range ( number ) ma_type = output_i . type . multi_array_type ma_type . shape_range . size_ranges . add ( ) ma_type . shape_range . size_ranges [ j ] . lower_bound = ( bounds [ i ] if j = = number else number ) ma_type . shape_range . size_ranges [ j ] . upper_bound = ( bounds [ i ] if j = = number else _ number ) nn_model . description . output . add ( ) nn_model . description . output [ i ] . parse_from_string ( output_i . serialize_to_string ( ) ) ma_type = nn_model . description . output [ i ] . type . multi_array_type ma_type . shape . append ( num_bounding_boxes ) ma_type . shape . append ( bounds [ i ] ) nn_model . neural_network . parse_from_string ( model_neural_network . serialize_to_string ( ) ) nn_model . specification_version = model . specification_version nms_model = model . pipeline . models [ number ] nms_model_non_max_sup = nms_model . non_maximum_suppression for i in range ( number ) output_i = model . description . output [ i ] nms_model . description . input . add ( ) nms_model . description . input [ i ] . parse_from_string ( output_i . serialize_to_string ( ) ) nms_model . description . output . add ( ) nms_model . description . output [ i ] . parse_from_string ( output_i . serialize_to_string ( ) ) nms_model . description . output [ i ] . name = ( string if i = = number else string ) nms_model_non_max_sup . iou_threshold = iou_threshold nms_model_non_max_sup . confidence_threshold = confidence_threshold nms_model_non_max_sup . confidence_input_feature_name = string nms_model_non_max_sup . coordinates_input_feature_name = string nms_model_non_max_sup . confidence_output_feature_name = string nms_model_non_max_sup . coordinates_output_feature_name = string nms_model . specification_version = model . specification_version nms_model_non_max_sup . string_class_labels . vector . extend ( self . classes ) for i in range ( number ) nms_model . description . input [ i ] . parse_from_string ( nn_model . description . output [ i ] . serialize_to_string ( ) ) if include_non_maximum_suppression iou_threshold_string = string model . description . input . add ( ) model . description . input [ number ] . type . double_type . parse_from_string ( b string ) model . description . input [ number ] . name = iou_threshold_string nms_model . description . input . add ( ) nms_model . description . input [ number ] . parse_from_string ( model . description . input [ number ] . serialize_to_string ( ) ) nms_model_non_max_sup . iou_threshold_input_feature_name = iou_threshold_string confidence_threshold_string = string model . description . input . add ( ) model . description . input [ number ] . type . double_type . parse_from_string ( b string ) model . description . input [ number ] . name = confidence_threshold_string nms_model . description . input . add ( ) nms_model . description . input [ number ] . parse_from_string ( model . description . input [ number ] . serialize_to_string ( ) ) nms_model_non_max_sup . confidence_threshold_input_feature_name = confidence_threshold_string model . description . output [ number ] . name = string model . description . output [ number ] . name = string iou_threshold_string = string confidence_threshold_string = ( string + string ) model_type = string self . model if include_non_maximum_suppression model_type + = string model . description . metadata . short_description = _coreml_utils . _mlmodel_short_description ( model_type ) model . description . input [ number ] . short_description = string if include_non_maximum_suppression iou_threshold_string = string model . description . input [ number ] . short_description = iou_threshold_string . format ( iou_threshold ) confidence_threshold_string = ( string + string ) model . description . input [ number ] . short_description = confidence_threshold_string . format ( confidence_threshold ) model . description . output [ number ] . short_description = u string classes string model . description . output [ number ] . short_description = u string version = object_detector . _python_object_detector_version partial_user_defined_metadata = { string self . model , string str ( self . max_iterations ) , string str ( self . training_iterations ) , string str ( include_non_maximum_suppression ) , string str ( iou_threshold ) , string str ( confidence_threshold ) , string str ( iou_threshold ) , string self . feature , string self . annotations , string string . join ( self . classes ) } user_defined_metadata = _coreml_utils . _get_model_metadata ( self . _class_ . _name_ , partial_user_defined_metadata , version ) model . description . metadata . user_defined . update ( user_defined_metadata ) from coremltools . models . utils import save_spec as _save_spec _save_spec ( model , filename )	0
Q_102	do not include upgrade	C_102_0	def can_attack_air ( self ) _ > bool if self . _weapons var = next ( ( var for var in self . _weapons if var . type in { target_type . air . value , target_type . any . value } ) , none , ) return var is not none return false	1
Q_102	do not include upgrade	C_102_1	def patch_namespaced_config_map ( self , name , namespace , body , * * kwargs ) kwargs [ string ] = true if kwargs . get ( string ) return self . patch_namespaced_config_map_with_http_info ( name , namespace , body , * * kwargs ) else ( data ) = self . patch_namespaced_config_map_with_http_info ( name , namespace , body , * * kwargs ) return data	0
Q_102	do not include upgrade	C_102_2	def to_json ( self , drop_null = true , camel = false , indent = none , sort_keys = false ) return json . dumps ( self . to_dict ( drop_null , camel ) , indent = indent , sort_keys = sort_keys )	0
Q_103	draw a line from p0 to p1	C_103_0	def debug_line_out ( self , p0 union [ unit , point2 , point3 ] , p1 union [ unit , point2 , point3 ] , color = none ) self . _debug_lines . append ( debug_pb . debug_line ( line = debug_pb . line ( p0 = self . to_debug_point ( p0 ) , p1 = self . to_debug_point ( p1 ) ) , color = self . to_debug_color ( color ) , ) )	1
Q_103	draw a line from p0 to p1	C_103_1	def build ( self , bug bug ) r = self . _api . post ( string . format ( bug . name ) ) if r . status_code = = number return if r . status_code = = number raise exception ( string . format ( bug . name ) ) if r . status_code = = number raise exception ( string ) if r . status_code = = number raise key_error ( string . format ( bug . name ) ) self . _api . handle_erroneous_response ( r )	0
Q_103	draw a line from p0 to p1	C_103_2	def disable_radio_button ( self ) checked = self . default_input_button_group . checked_button ( ) if checked self . default_input_button_group . set_exclusive ( false ) checked . set_checked ( false ) self . default_input_button_group . set_exclusive ( true ) for var in self . default_input_button_group . buttons ( ) var . set_disabled ( true ) self . custom_value . set_disabled ( true )	0
Q_104	list the merge request relate to this milestone	C_104_0	def merge_requests ( self , * * kwargs ) path = string ( self . manager . path , self . get_id ( ) ) data_list = self . manager . gitlab . http_list ( path , as_list = false , * * kwargs ) manager = project_merge_request_manager ( self . manager . gitlab , parent = self . manager . _parent ) return rest_object_list ( manager , project_merge_request , data_list )	1
Q_104	list the merge request relate to this milestone	C_104_1	def ensure_data ( ) if not os . path . exists ( garuda_dir ) os . makedirs ( garuda_dir ) path ( f string ) . touch ( )	0
Q_104	list the merge request relate to this milestone	C_104_2	def get_most_recent_network_by_name ( self , name str ) _ > optional [ network ] return self . session . query ( network ) . filter ( network . name = = name ) . order_by ( network . created . desc ( ) ) . first ( )	0
Q_105	importantly this agree with what the kaplan_meier_fitter produce	C_105_0	def _compute_baseline_survival ( self ) survival_df = np . exp ( _ self . baseline_cumulative_hazard_ ) if self . strata is none survival_df . columns = [ string ] return survival_df	1
Q_105	importantly this agree with what the kaplan_meier_fitter produce	C_105_1	def list ( self , all_my_agents = false , limit = number , offset = number ) logger . info ( string , all_my_agents , limit , offset ) if all_my_agents evt = self . _request_entity_list_all ( limit = limit , offset = offset ) else evt = self . _request_entity_list ( limit = limit , offset = offset ) self . _wait_and_except_if_failed ( evt ) return evt . payload [ string ]	0
Q_105	importantly this agree with what the kaplan_meier_fitter produce	C_105_2	def set_mac ( self , x_eui ) print string self . port address64 = string try if not x_eui address64 = self . mac if not isinstance ( x_eui , str ) address64 = self . _convert_long_to_string ( x_eui ) if len ( address64 ) < number address64 = address64 . zfill ( number ) print address64 else address64 = x_eui cmd = wpanctl_cmd + string address64 if self . _send_command ( cmd ) [ number ] = string self . mac = address64 return true else return false except exception , e module_helper . write_into_debug_logger ( string + str ( e ) )	0
Q_106	equivalent to pickle	C_106_0	def load ( cls , filename ) cls . load_counter_offset = store_options . id_offset ( ) val = pickle . load ( filename ) cls . load_counter_offset = none return val	1
Q_106	equivalent to pickle	C_106_1	def create_objective ( self , objective_form ) collection = json_client_validated ( string , collection = string , runtime = self . _runtime ) if not isinstance ( objective_form , abc_objective_form ) raise errors . invalid_argument ( string ) if objective_form . is_for_update ( ) raise errors . invalid_argument ( string ) try if self . _forms [ objective_form . get_id ( ) . get_identifier ( ) ] = = created raise errors . illegal_state ( string ) except key_error raise errors . unsupported ( string ) if not objective_form . is_valid ( ) raise errors . invalid_argument ( string ) insert_result = collection . insert_one ( objective_form . _my_map ) self . _forms [ objective_form . get_id ( ) . get_identifier ( ) ] = created result = objects . objective ( osid_object_map = collection . find_one ( { string insert_result . inserted_id } ) , runtime = self . _runtime , proxy = self . _proxy ) return result	0
Q_106	equivalent to pickle	C_106_2	def delete_persistent_volume ( self , name , * * kwargs ) kwargs [ string ] = true if kwargs . get ( string ) return self . delete_persistent_volume_with_http_info ( name , * * kwargs ) else ( data ) = self . delete_persistent_volume_with_http_info ( name , * * kwargs ) return data	0
Q_107	safely find either the numerical min and max of	C_107_0	def find_range ( values , soft_range = [ ] ) try values = np . array ( values ) values = np . squeeze ( values ) if len ( values . shape ) > number else values if len ( soft_range ) values = np . concatenate ( [ values , soft_range ] ) if values . dtype . kind = = string return values . min ( ) , values . max ( ) return np . nanmin ( values ) , np . nanmax ( values ) except try values = sorted ( values ) return ( values [ number ] , values [ _ number ] ) except return ( none , none )	1
Q_107	safely find either the numerical min and max of	C_107_1	def get_upload_status ( self ) if self . id return self . connection . post ( string , video_id = self . id )	0
Q_107	safely find either the numerical min and max of	C_107_2	def set_title ( self , title ) self . _my_map [ string ] = self . _get_display_text ( title , self . get_title_metadata ( ) )	0
Q_108	return the maximum index of pauli matrix in the term	C_108_0	def max_n ( self ) return max ( var . max_n ( ) for var in self . terms if var . ops )	1
Q_108	return the maximum index of pauli matrix in the term	C_108_1	"def verify_link_ed25519_cot_signature ( chain , link , unsigned_path , signature_path ) if chain . context . config [ string ] log . debug ( string . format ( link . name , link . task_id , link . worker_impl ) ) signature = read_from_file ( signature_path , file_type = string , exception = co_t_error ) binary_contents = read_from_file ( unsigned_path , file_type = string , exception = co_t_error ) errors = [ ] verify_key_seeds = chain . context . config [ string ] . get ( link . worker_impl , [ ] ) for var in verify_key_seeds try verify_key = ed25519_public_key_from_string ( var ) verify_ed25519_signature ( verify_key , binary_contents , signature , string t verify against { } ( exc ) s string { } { } ed25519 cot signature verified . string { } { } unknown error verifying ed25519 cot signature . worker_impl { } verify_keys { } string string { } { } invalid unsigned cot json body ( exc ) s "" . format ( link . name , link . task_id ) )"	0
Q_108	return the maximum index of pauli matrix in the term	C_108_2	def getquerydict ( self , sep = string , encoding = string , errors = string ) dict = collections . defaultdict ( list ) for var , var in self . getquerylist ( sep , encoding , errors ) dict [ var ] . append ( var ) return dict	0
Q_109	event for the item change	C_109_0	def on_item_changed ( self , item , new_value , row , column ) return ( item , new_value , row , column )	1
Q_109	event for the item change	C_109_1	def get_mods ( package ) pkgdir = package . _path_ [ number ] matches = filter ( none , [ pymod_re . match ( var ) for var in os . listdir ( pkgdir ) ] ) parse_match = lambda groups ( groups [ number ] , int ( groups [ number ] ) , int ( groups [ number ] ) ) return sorted ( list ( set ( [ parse_match ( m . groups ( ) ) for m in matches ] ) ) , key = lambda x ( x [ number ] , x [ number ] ) )	0
Q_109	event for the item change	C_109_2	def _extract_file_entry ( self , path_spec , destination_path , output_writer , skip_duplicates = true ) file_entry = path_spec_resolver . resolver . open_file_entry ( path_spec ) if not file_entry logger . warning ( string . format ( path_spec . comparable ) ) return if not self . _filter_collection . matches ( file_entry ) return file_entry_processed = false for var in file_entry . data_streams if self . _abort break self . _extract_data_stream ( file_entry , var . name , destination_path , output_writer , skip_duplicates = skip_duplicates ) file_entry_processed = true if not file_entry_processed self . _extract_data_stream ( file_entry , string , destination_path , output_writer , skip_duplicates = skip_duplicates )	0
Q_110	take a tagint _bit monochrome or tagint _bit rgb image and render it	C_110_0	def display ( self , image ) assert ( image . mode = = self . mode ) assert ( image . size = = self . size ) image = self . preprocess ( image ) if self . framebuffer . redraw_required ( image ) left , top , right , bottom = self . framebuffer . inflate_bbox ( ) width = right _ left height = bottom _ top buf = bytearray ( width * height number ) self . _set_position ( top , right , bottom , left ) self . _populate ( buf , self . framebuffer . getdata ( ) ) self . data ( list ( buf ) )	1
Q_110	take a tagint _bit monochrome or tagint _bit rgb image and render it	C_110_1	def parse_changelog ( path , * * kwargs ) app , doctree = get_doctree ( path , * * kwargs ) first_list = none for var in doctree [ number ] if isinstance ( var , bullet_list ) first_list = var break releases , manager = construct_releases ( first_list . children , app ) ret = changelog2dict ( releases ) for key in ret . copy ( ) if key . startswith ( string ) del ret [ key ] for family in manager manager [ family ] . pop ( string , none ) unreleased = manager [ family ] . pop ( string , none ) if unreleased is not none ret [ string . format ( family ) ] = unreleased ret . update ( manager [ family ] ) return ret	0
Q_110	take a tagint _bit monochrome or tagint _bit rgb image and render it	C_110_2	def _extract ( self , raw str , station str ) _ > str report = raw [ raw . find ( station . upper ( ) + string ) ] report = report [ report . find ( string ) ] return report	0
Q_111	"change work directory to contract "" s dir in order to avoid symbol"	C_111_0	def compile_files_cwd ( * args , * * kwargs ) compile_wd = os . path . commonprefix ( args [ number ] ) if os . path . isfile ( compile_wd ) compile_wd = os . path . dirname ( compile_wd ) if compile_wd [ _ number ] = string compile_wd + = string file_list = [ var . replace ( compile_wd , string ) for var in args [ number ] ] cwd = os . getcwd ( ) try os . chdir ( compile_wd ) compiled_contracts = compile_files ( source_files = file_list , output_values = ( string , string , string , string , string ) , * * kwargs , ) finally os . chdir ( cwd ) return compiled_contracts	1
Q_111	"change work directory to contract "" s dir in order to avoid symbol"	C_111_1	def quote ( self , s ) if six . py2 from pipes import quote else from shlex import quote return quote ( s )	0
Q_111	"change work directory to contract "" s dir in order to avoid symbol"	C_111_2	def get ( self , table , sys_id ) record = self . api . get ( table , sys_id ) return record	0
Q_112	check out give branch	C_112_0	def checkout_branch ( self , branch ) _ , stdout , stderr = self . git_exec ( [ string , branch ] , with_extended_output = true ) return string . join ( [ stderr , stdout ] )	1
Q_112	check out give branch	C_112_1	def get_ciphertext_len ( self , ciphertext ) plaintext_length = self . get_plaintext_len ( ciphertext ) ciphertext_length = plaintext_length + encrypter . _ctxt_expansion return ciphertext_length	0
Q_112	check out give branch	C_112_2	def blacklist ( self , src , dst , duration = number , * * kw ) self . make_request ( method = string , resource = string , json = prepare_blacklist ( src , dst , duration , * * kw ) )	0
Q_113	move dkfz filter information into info field	C_113_0	def _filter_to_info ( in_file , data ) header = ( string string bias estimation based on unequal read support from dkfz_bias_filter_variant depth string string ) out_file = string utils . splitext_plus ( in_file ) [ number ] if not utils . file_uptodate ( out_file , in_file ) and not utils . file_uptodate ( out_file + string , in_file ) with file_transaction ( data , out_file ) as tx_out_file with utils . open_gzipsafe ( in_file ) as in_handle with open ( tx_out_file , string ) as out_handle for var in in_handle if var . startswith ( string out_handle . write ( var ) else out_handle . write ( _rec_filter_to_info ( var ) ) return vcfutils . bgzip_and_index ( out_file , data [ string ] )	1
Q_113	move dkfz filter information into info field	C_113_1	def match ( self , path ) match = self . _re . search ( path ) if match is none return none args = [ ] kwargs = { } for var , var in enumerate ( self . _wildcards ) if var . name = = string continue value = var . value ( match . groups ( ) [ var ] ) if not var . name args . append ( value ) else kwargs [ var . name ] = value return self . _callback , args , kwargs	0
Q_113	move dkfz filter information into info field	C_113_2	"def handle ( self , * args , * * options ) quickstart = quickstart ( ) try quickstart . create_app ( os . path . join ( settings . base_dir , string ) , options . get ( string ) ) self . stdout . write ( self . style . success ( string t forget to add string to installed_apps string name string app with same name already exists "" )"	0
Q_114	run tumor only sm_counter2 call	C_114_0	def run ( align_bams , items , ref_file , assoc_files , region = none , out_file = none ) paired = vcfutils . get_paired_bams ( align_bams , items ) assert paired and not paired . normal_bam , ( string ( string . join ( [ dd . get_sample_name ( var ) for var in items ] ) ) ) vrs = bedutils . population_variant_regions ( items ) target = shared . subset_variant_regions ( vrs , region , out_file , items = items , do_merge = true ) out_file = out_file . replace ( string , string ) out_prefix = utils . splitext_plus ( os . path . basename ( out_file ) ) [ number ] if not utils . file_exists ( out_file ) and not utils . file_exists ( out_file + string ) with file_transaction ( paired . tumor_data , out_file ) as tx_out_file cmd = [ string , string , os . path . dirname ( tx_out_file ) , string , out_prefix , string , target , string , ref_file , string , paired . tumor_bam , string , string , string , dd . get_num_cores ( paired . tumor_data ) ] do . run ( cmd , string ) for fname in glob . glob ( os . path . join ( os . path . dirname ( tx_out_file ) , string ) ) shutil . move ( fname , os . path . join ( os . path . dirname ( out_file ) , os . path . basename ( fname ) ) ) utils . symlink_plus ( os . path . join ( os . path . dirname ( out_file ) , string out_prefix ) , out_file ) return vcfutils . bgzip_and_index ( out_file , paired . tumor_data [ string ] , remove_orig = false , prep_cmd = string s ( out_prefix , dd . get_sample_name ( paired . tumor_data ) , vcfutils . add_contig_to_header_cl ( dd . get_ref_file ( paired . tumor_data ) , out_file ) ) )	1
Q_114	run tumor only sm_counter2 call	C_114_1	def remote_data_received ( self , connection , data ) proto = self . get_local_protocol ( connection ) proto . transport . write ( data ) return { }	0
Q_114	run tumor only sm_counter2 call	C_114_2	def preformat_call ( self , api_call ) api_call_formatted = api_call . lstrip ( string ) api_call_formatted = api_call_formatted . rstrip ( string ) if api_call = api_call_formatted logger . debug ( string api_call_formatted ) return api_call_formatted	0
Q_115	give a string e	C_115_0	def to_css_class ( s ) if css_class_re . match ( s ) return s md5_suffix = hashlib . md5 ( s . encode ( string ) ) . hexdigest ( ) [ number ] s = s . lstrip ( string ) . lstrip ( string ) s = string . join ( s . split ( ) ) s = css_invalid_chars_re . sub ( string , s ) bits = [ var for var in ( s , md5_suffix ) if var ] return string . join ( bits )	1
Q_115	give a string e	C_115_1	def _save ( self ) if os . path . isfile ( self . _full_filename ) shutil . copyfile ( self . _full_filename , self . _full_backup_filename ) with open ( self . _full_filename , string ) as file writer = csv . dict_writer ( file , fieldnames = self . _headers ) writer . writeheader ( ) for var in self . _table writer . writerow ( var )	0
Q_115	give a string e	C_115_2	def get_handlers ( self , event str ) _ > t . list [ t . callable ] return list ( self . _events . get ( event , [ ] ) )	0
Q_116	use this method to set the score of the specify user in a game	C_116_0	async def set_game_score ( self , user_id base . integer , score base . integer , force typing . union [ base . boolean , none ] = none , disable_edit_message typing . union [ base . boolean , none ] = none , chat_id typing . union [ base . integer , none ] = none , message_id typing . union [ base . integer , none ] = none , inline_message_id typing . union [ base . string , none ] = none ) _ > types . message or base . boolean payload = generate_payload ( * * locals ( ) ) result = await self . request ( api . methods . set_game_score , payload ) if isinstance ( result , bool ) return result return types . message ( * * result )	1
Q_116	use this method to set the score of the specify user in a game	C_116_1	def set_border_color ( self , color ) color = q_color ( color ) if self . _palette is none self . _palette = x_node_palette ( self . _scene_palette ) self . _palette . set_color ( self . _palette . node_border , color ) self . set_dirty ( )	0
Q_116	use this method to set the score of the specify user in a game	C_116_2	def _sample_in_stratum ( self , stratum_idx , replace = true ) if replace stratum_loc = np . random . choice ( self . sizes_ [ stratum_idx ] ) else stratum_locs = np . where ( self . _sampled [ stratum_idx ] ) [ number ] stratum_loc = np . random . choice ( stratum_locs ) self . _sampled [ stratum_idx ] [ stratum_loc ] = true self . _n_sampled [ stratum_idx ] + = number loc = self . allocations_ [ stratum_idx ] [ stratum_loc ] return loc	0
Q_117	generate user data	C_117_0	def process_user ( self , user types . user ) if not user return yield string , user . id if self . include_content yield string , user . full_name if user . username yield string , f string	1
Q_117	generate user data	C_117_1	def from_inet_ptoi ( bgp_id ) four_byte_id = none try four_byte_id = ip . ipv4_to_int ( bgp_id ) except value_error log . debug ( string , bgp_id ) return four_byte_id	0
Q_117	generate user data	C_117_2	def prepare_feats ( feat_type , org_wav_dir = org_wav_dir , feat_dir = feat_dir , tgt_wav_dir = tgt_wav_dir , org_xml_dir = org_xml_dir , label_dir = label_dir ) if not os . path . isdir ( tgt_dir ) os . makedirs ( tgt_dir ) if not os . path . isdir ( feat_dir ) os . makedirs ( feat_dir ) if not os . path . isdir ( os . path . join ( feat_dir , string ) ) os . makedirs ( os . path . join ( feat_dir , string ) ) if not os . path . isdir ( os . path . join ( feat_dir , string ) ) os . makedirs ( os . path . join ( feat_dir , string ) ) trim_wavs ( org_wav_dir = org_wav_dir , tgt_wav_dir = tgt_wav_dir , org_xml_dir = org_xml_dir ) prefixes = [ ] for var in os . listdir ( os . path . join ( tgt_wav_dir , string ) ) if var . endswith ( string ) pre , _ = os . path . splitext ( var ) prefixes . append ( os . path . join ( string , pre ) ) for var in os . listdir ( os . path . join ( tgt_wav_dir , string ) ) if var . endswith ( string ) pre , _ = os . path . splitext ( var ) prefixes . append ( os . path . join ( string , pre ) ) if feat_type = = string import numpy as np for prefix in prefixes label_fn = os . path . join ( label_dir , string prefix ) out_fn = os . path . join ( feat_dir , string prefix ) try with open ( label_fn ) as label_f labels = label_f . readlines ( ) [ number ] . split ( ) except file_not_found_error continue indices = [ phonemes_to_indices [ label ] for label in labels ] one_hots = [ [ number ] * len ( phonemes ) for _ in labels ] for i , index in enumerate ( indices ) one_hots [ i ] [ index ] = number one_hots = np . array ( one_hots ) np . save ( out_fn , one_hots ) else for prefix in prefixes wav_fn = os . path . join ( tgt_wav_dir , string prefix ) mono16k_wav_fn = os . path . join ( feat_dir , string prefix ) if not os . path . isfile ( mono16k_wav_fn ) logging . info ( string . format ( wav_fn , mono16k_wav_fn ) ) feat_extract . convert_wav ( wav_fn , mono16k_wav_fn ) feat_extract . from_dir ( path ( os . path . join ( feat_dir , string ) ) , feat_type = feat_type ) feat_extract . from_dir ( path ( os . path . join ( feat_dir , string ) ) , feat_type = feat_type )	0
Q_118	remove all stop time from this trip	C_118_0	def clear_stop_times ( self ) cursor = self . _schedule . _connection . cursor ( ) cursor . execute ( string , ( self . trip_id , ) )	1
Q_118	remove all stop time from this trip	C_118_1	def hypervolume ( pareto_set , reference_point = none ) def dominates ( var , q , k = none ) if k is none k = len ( var ) d = true while d and k < len ( var ) d = not ( q [ k ] > var [ k ] ) k + = number return d def insert ( var , k , pl ) ql = [ ] while pl and pl [ number ] [ k ] > var [ k ] ql . append ( pl [ number ] ) pl = pl [ number ] ql . append ( var ) while pl if not dominates ( var , pl [ number ] , k ) ql . append ( pl [ number ] ) pl = pl [ number ] return ql def slice ( pl , k , ref ) var = pl [ number ] pl = pl [ number ] ql = [ ] s = [ ] while pl ql = insert ( var , k + number , ql ) p_prime = pl [ number ] s . append ( ( math . fabs ( var [ k ] _ p_prime [ k ] ) , ql ) ) var = p_prime pl = pl [ number ] ql = insert ( var , k + number , ql ) s . append ( ( math . fabs ( var [ k ] _ ref [ k ] ) , ql ) ) return s ps = pareto_set ref = reference_point n = min ( [ len ( var ) for var in ps ] ) if ref is none ref = [ max ( ps , key = lambda x x [ o ] ) [ o ] for o in range ( n ) ] pl = ps [ ] pl . sort ( key = lambda x x [ number ] , reverse = true ) s = [ ( number , pl ) ] for k in range ( n _ number ) s_prime = [ ] for x , ql in s for x_prime , ql_prime in slice ( ql , k , ref ) s_prime . append ( ( x * x_prime , ql_prime ) ) s = s_prime vol = number for x , ql in s vol = vol + x * math . fabs ( ql [ number ] [ n _ number ] _ ref [ n _ number ] ) return vol	0
Q_118	remove all stop time from this trip	C_118_2	def get ( self , arg , value ) if arg = = string response = self . _swimlane . request ( string , string . format ( value ) ) try var = response . json ( ) except value_error raise value_error ( string { } string . format ( value ) ) return user ( self . _swimlane , var ) else response = self . _swimlane . request ( string , string . format ( quote_plus ( value ) ) ) matched_users = response . json ( ) target_matches = [ ] for var in matched_users user_display_name = var . get ( string ) if user_display_name = = value target_matches . append ( var ) if not target_matches raise value_error ( string { } string . format ( value ) ) if len ( target_matches ) > number raise value_error ( string { } string . format ( value , string . join ( [ string { } string . format ( r [ string ] ) for r in target_matches ] ) ) ) return user ( self . _swimlane , target_matches [ number ] )	0
Q_119	reseed the internal random generator	C_119_0	def reseed ( self , seed seed = none ) _ > none if self . random is random self . random = random ( ) self . seed = seed self . random . seed ( self . seed )	1
Q_119	reseed the internal random generator	C_119_1	def generate ( env ) java_file = s_cons . tool . create_java_file_builder ( env ) java_class = s_cons . tool . create_java_class_file_builder ( env ) java_class_dir = s_cons . tool . create_java_class_dir_builder ( env ) java_class . add_emitter ( none , emit_java_classes ) java_class . add_emitter ( env . subst ( string ) , emit_java_classes ) java_class_dir . emitter = emit_java_classes env . add_method ( java ) env [ string ] = string env [ string ] = s_cons . util . cl_var ( string ) env [ string ] = [ ] env [ string ] = [ ] env [ string ] = [ ] env [ string ] = pathopt env [ string ] = string _ bootclasspath string javabootclasspath string env [ string ] = string _ classpath string javaclasspath string env [ string ] = string _ sourcepath string javasourcepath string _javasourcepathdefault string env [ string ] = string env [ string ] = string env [ string ] = string _javaccom string javaccomstr string env [ string ] = string env [ string ] = string	0
Q_119	reseed the internal random generator	C_119_2	def get_img_data ( f , maxsize = ( number , number ) , first = false ) img = image . open ( f ) img . thumbnail ( maxsize ) if first bio = io . bytes_io ( ) img . save ( bio , format = string ) del img return bio . getvalue ( ) return image_tk . photo_image ( img )	0
Q_120	this method disconnect an iom session to allow for reconnecting when switch network	C_120_0	def disconnect ( self ) if not self . sascfg . reconnect return string pgm = b string + b string self . stdin [ number ] . send ( pgm ) while true try log = self . stderr [ number ] . recv ( number ) . decode ( errors = string ) except ( blocking_io_error ) log = b string if len ( log ) > number if log . count ( string ) > = number break return log . rstrip ( string )	1
Q_120	this method disconnect an iom session to allow for reconnecting when switch network	C_120_1	def on_activate_reader ( self , event ) item = event . get_item ( ) if item itemdata = self . readertreepanel . readertreectrl . get_item_py_data ( item ) if isinstance ( itemdata , smartcard . card . card ) self . activate_card ( itemdata ) elif isinstance ( itemdata , smartcard . reader . reader . reader ) self . dialogpanel . on_activate_reader ( itemdata ) event . skip ( )	0
Q_120	this method disconnect an iom session to allow for reconnecting when switch network	C_120_2	def add_lv_load_area_group ( self , lv_load_area_group ) if lv_load_area_group not in self . lv_load_area_groups ( ) self . _lv_load_area_groups . append ( lv_load_area_group )	0
Q_121	integrate expect improvement	C_121_0	def _compute_acq ( self , x ) means , stds = self . model . predict ( x ) fmins = self . model . get_fmin ( ) f_acqu = number for m , s , fmin in zip ( means , stds , fmins ) _ , phi , _ = get_quantiles ( self . jitter , fmin , m , s ) f_acqu + = phi return f_acqu / len ( means )	1
Q_121	integrate expect improvement	C_121_1	def polynomial_reduce_mod ( poly , polymod , p ) assert polymod [ _ number ] = = number assert len ( polymod ) > number while len ( poly ) > = len ( polymod ) if poly [ _ number ] = number for var in range ( number , len ( polymod ) + number ) poly [ _ var ] = ( poly [ _ var ] _ poly [ _ number ] * polymod [ _ var ] ) p poly = poly [ number _ number ] return poly	0
Q_121	integrate expect improvement	C_121_2	def search ( self , query , locations list = none ) cas_number = re . search ( r string , str ( query ) ) if cas_number query = cas_number [ number ] search_type = string else try query = int ( query ) search_type = string except value_error query = f string search_type = string if not locations locations = self . get_locations ( filter_to_my_group = true ) locations = [ var . inventory_id for var in locations ] data = { string self . groupid , string search_type , string query , string locations . append ( number ) } r = self . _post ( string , referer_path = string , data = data ) if r [ string ] [ string ] containers = [ ] for container in r [ string ] [ string ] var = location ( name = container . get ( string ) ) ct = container ( inventory_id = container . get ( string ) , compound_id = container . get ( string ) , name = container . get ( string ) , location = var , size = container . get ( string ) , smiles = container . get ( string ) , cas = container . get ( string ) , comments = container . get ( string ) , barcode = container . get ( string ) , supplier = container . get ( string ) , date_acquired = container . get ( string ) , owner = container . get ( string ) ) containers . append ( ct ) return containers else return [ ]	0
Q_122	get a list of namedtuples one for each annotation	C_122_0	def _init_w_godag ( self ) nts = [ ] ntobj = cx . namedtuple ( string , self . flds + [ string ] ) for var , var in self . id2gos . items ( ) for goid in var goobj = self . godag . get ( goid , string ) nts . append ( ntobj ( db_id = var , go_id = goid , ns = namespace2_ns [ goobj . namespace ] if goobj else string ) ) return nts	1
Q_122	get a list of namedtuples one for each annotation	C_122_1	def output ( self ) data = self . len_bytes ( ) + self . data + self . checksum ( ) if self . escaped and len ( self . raw_data ) < number self . raw_data = api_frame . escape ( data ) if self . escaped data = self . raw_data return api_frame . start_byte + data	0
Q_122	get a list of namedtuples one for each annotation	C_122_2	def trigger_created ( self , filepath ) if os . path . exists ( filepath ) self . _trigger ( string , filepath )	0
Q_123	save network definition inference training execution	C_123_0	def save ( filename , contents , include_params = false , variable_batch_size = true ) _ , ext = os . path . splitext ( filename ) if ext = = string or ext = = string logger . info ( string . format ( filename ) ) proto = create_proto ( contents , include_params , variable_batch_size ) with open ( filename , string ) as file text_format . print_message ( proto , file ) elif ext = = string logger . info ( string . format ( filename ) ) proto = create_proto ( contents , include_params , variable_batch_size ) with open ( filename , string ) as file file . write ( proto . serialize_to_string ( ) ) elif ext = = string logger . info ( string . format ( filename ) ) try tmpdir = tempfile . mkdtemp ( ) save ( string . format ( tmpdir ) , contents , include_params = false , variable_batch_size = variable_batch_size ) with open ( string . format ( tmpdir ) , string ) as file file . write ( string . format ( nnp_version ( ) ) ) save_parameters ( string . format ( tmpdir ) ) with zipfile . zip_file ( filename , string ) as nnp nnp . write ( string . format ( tmpdir ) , string ) nnp . write ( string . format ( tmpdir ) , string ) nnp . write ( string . format ( tmpdir ) , string ) finally shutil . rmtree ( tmpdir )	1
Q_123	save network definition inference training execution	C_123_1	def top_games ( self , limit = number , offset = number ) r = self . kraken_request ( string , string , params = { string limit , string offset } ) return models . game . wrap_topgames ( r )	0
Q_123	save network definition inference training execution	C_123_2	def get_action_by_id ( op , action_id ) actions = get_actions ( op ) if actions and number < = action_id < len ( actions ) return actions [ action_id _ number ]	0
Q_124	basic function for read a symmetric problem in the tsplib format	C_124_0	def read_tsplib ( filename ) string string string if filename [ _ number ] = = string f = gzip . open ( filename , string ) else f = open ( filename ) line = f . readline ( ) while line . find ( string ) = = _ number line = f . readline ( ) n = int ( line . split ( ) [ _ number ] ) while line . find ( string ) = = _ number line = f . readline ( ) if line . find ( string ) = _ number dist = dist_l2 elif line . find ( string ) = _ number dist = dist_l1 elif line . find ( string ) = _ number dist = dist_linf elif line . find ( string ) = _ number dist = dist_att elif line . find ( string ) = _ number dist = dist_ceil2_d elif line . find ( string ) = _ number while line . find ( string ) = = _ number line = f . readline ( ) if line . find ( string ) = _ number while line . find ( string ) = = _ number line = f . readline ( ) return read_explicit_lowerdiag ( f , n ) if line . find ( string ) = _ number while line . find ( string ) = = _ number line = f . readline ( ) return read_explicit_upper ( f , n ) if line . find ( string ) = _ number while line . find ( string ) = = _ number line = f . readline ( ) return read_explicit_upperdiag ( f , n ) if line . find ( string ) = _ number while line . find ( string ) = = _ number line = f . readline ( ) return read_explicit_matrix ( f , n ) print ( string + line ) raise ( exception ) else print ( string s string line ) raise exception while line . find ( string ) = = _ number line = f . readline ( ) x , y = { } , { } while number line = f . readline ( ) if line . find ( string ) = _ number or not line break ( var , xi , yi ) = line . split ( ) x [ var ] = float ( xi ) y [ var ] = float ( yi ) v = x . keys ( ) c = { } for var in v for j in v c [ var , j ] = dist ( x [ var ] , y [ var ] , x [ j ] , y [ j ] ) return v , c , x , y	1
Q_124	basic function for read a symmetric problem in the tsplib format	C_124_1	def set_mock_engine ( self , engine ) if not engine raise type_error ( string ) mock_engine = engine ( self ) methods = ( string , string ) if not all ( [ hasattr ( mock_engine , var ) for var in methods ] ) raise not_implemented_error ( string string ) self . mock_engine = mock_engine if self . active self . mock_engine . activate ( )	0
Q_124	basic function for read a symmetric problem in the tsplib format	C_124_2	def _call_method_from_namespace ( obj , method_name , namespace ) method = getattr ( obj , method_name ) method_parser = method . parser arg_names = _get_args_name_from_parser ( method_parser ) if method_name = = string return _call ( obj , arg_names , namespace ) return _call ( method , arg_names , namespace )	0
Q_125	find plugins in give directory	C_125_0	def register_plugin_dir ( path ) import glob for var in glob . glob ( path + string ) for k , v in load_plugins_from_module ( var ) . items ( ) if k global_registry [ k ] = v	1
Q_125	find plugins in give directory	C_125_1	def get_dict_column ( dict_ , colx ) r retdict_ = { var util_list . list_take ( var , colx ) for var , var in six . iteritems ( dict_ ) } return retdict_	0
Q_125	find plugins in give directory	C_125_2	def parker_weighting ( ray_trafo , q = number . number ) src_radius = ray_trafo . geometry . src_radius det_radius = ray_trafo . geometry . det_radius ndim = ray_trafo . geometry . ndim angles = ray_trafo . range . meshgrid [ number ] min_rot_angle = ray_trafo . geometry . motion_partition . min_pt alen = ray_trafo . geometry . motion_params . length if ray_trafo . geometry . ndim = number pitch = ray_trafo . geometry . pitch if pitch = number raise value_error ( string string ) if ndim = = number dx = ray_trafo . range . meshgrid [ number ] elif ndim = = number rot_dir = _rotation_direction_in_detector ( ray_trafo . geometry ) if rot_dir [ number ] = = number dx = rot_dir [ number ] * ray_trafo . range . meshgrid [ number ] elif rot_dir [ number ] = = number dx = rot_dir [ number ] * ray_trafo . range . meshgrid [ number ] else dx = ( rot_dir [ number ] * ray_trafo . range . meshgrid [ number ] + rot_dir [ number ] * ray_trafo . range . meshgrid [ number ] ) dx_abs_max = np . max ( np . abs ( dx ) ) max_fan_angle = number * np . arctan2 ( dx_abs_max , src_radius + det_radius ) delta = max_fan_angle / number epsilon = alen _ np . pi _ max_fan_angle if epsilon < number raise exception ( string ) def s ( betap ) return ( number . number * ( number . number + np . sin ( np . pi * betap ) ) * ( np . abs ( betap ) < number . number ) + ( betap > = number . number ) ) def b ( alpha ) return q * ( number * delta _ number * alpha + epsilon ) beta = np . asarray ( angles _ min_rot_angle , dtype = ray_trafo . range . dtype ) alpha = np . asarray ( np . arctan2 ( dx , src_radius + det_radius ) , dtype = ray_trafo . range . dtype ) s_sum = s ( beta / b ( alpha ) _ number . number ) s_sum + = s ( ( beta _ number * delta + number * alpha _ epsilon ) / b ( alpha ) + number . number ) s_sum _ = s ( ( beta _ np . pi + number * alpha ) / b ( _ alpha ) _ number . number ) s_sum _ = s ( ( beta _ np . pi _ number * delta _ epsilon ) / b ( _ alpha ) + number . number ) scale = number . number * alen / np . pi return ray_trafo . range . element ( np . broadcast_to ( s_sum * scale , ray_trafo . range . shape ) )	0
Q_126	pull an environment variable out of the environment and cast it to an	C_126_0	def env_float ( name str , required bool = false , default union [ type [ empty ] , float ] = empty ) _ > float value = get_env_value ( name , required = required , default = default ) if value is empty raise value_error ( string string ) return float ( value )	1
Q_126	pull an environment variable out of the environment and cast it to an	C_126_1	def find ( name ) if op . exists ( name ) return name var = op . dirname ( _file_ ) or string paths = [ var ] + config [ string ] for var in paths filename = op . abspath ( op . join ( var , name ) ) if op . exists ( filename ) return filename for d in os . listdir ( var ) fullpath = op . abspath ( op . join ( var , d ) ) if op . isdir ( fullpath ) filename = op . abspath ( op . join ( fullpath , name ) ) if op . exists ( filename ) return filename return none	0
Q_126	pull an environment variable out of the environment and cast it to an	C_126_2	def get_flatpages_i18n ( parser , token ) bits = token . split_contents ( ) syntax_message = ( string string url_starts_with string dict ( tag_name = bits [ number ] ) ) if number < = len ( bits ) < = number containing = none excluding = none if len ( bits ) number = = number prefix = bits [ number ] else prefix = none if bits [ number ] = = string containing = bits [ number ] else if bits [ number ] = = string excluding = bits [ number ] if bits [ _ number ] = string raise template . template_syntax_error ( syntax_message ) context_name = bits [ _ number ] if len ( bits ) > = number if bits [ _ number ] = string raise template . template_syntax_error ( syntax_message ) user = bits [ _ number ] else user = none return flatpage_node ( context_name , starts_with = prefix , contains = containing , excludes = excluding , user = user ) else raise template . template_syntax_error ( syntax_message )	0
Q_127	get the balance of a particular token	C_127_0	def get_account_balance ( self , path_info , account_addr , token_type ) if not check_account_address ( account_addr ) return self . _reply_json ( { string string } , status_code = number ) if not check_token_type ( token_type ) return self . _reply_json ( { string string } , status_code = number ) blockstackd_url = get_blockstackd_url ( ) res = blockstackd_client . get_account_balance ( account_addr , token_type , hostport = blockstackd_url ) if json_is_error ( res ) log . error ( string . format ( account_addr , token_type , res [ string ] ) ) return self . _reply_json ( { string string . format ( token_type , account_addr , res [ string ] ) } , status_code = res . get ( string , number ) ) self . _reply_json ( { string str ( res ) } ) return	1
Q_127	get the balance of a particular token	C_127_1	def node ( self , * args , * * kwargs ) return cell_node ( get_node ( self . _impl , * convert_args ( args , kwargs ) ) )	0
Q_127	get the balance of a particular token	C_127_2	def dump ( stuff , filename , verbose = true , protocol = number ) filename = os . path . normcase ( filename ) dir_path = os . path . dirname ( filename ) if not os . path . exists ( dir_path ) os . makedirs ( dir_path ) with open ( filename , string ) as f p = pickle . pickler ( f , protocol = protocol ) p . dump ( stuff ) if verbose print ( string . format ( len ( stuff ) , filename ) ) return filename	0
Q_128	get the price for a namespace	C_128_0	def get_prices_namespace ( self , path_info , namespace_id ) if not check_namespace ( namespace_id ) return self . _reply_json ( { string string } , status_code = number ) blockstackd_url = get_blockstackd_url ( ) price_info = blockstackd_client . get_namespace_cost ( namespace_id , hostport = blockstackd_url ) if json_is_error ( price_info ) status_code = price_info . get ( string , number ) return self . _reply_json ( { string price_info [ string ] } , status_code = status_code ) ret = { string str ( price_info [ string ] ) , string price_info [ string ] , } if ret [ string ] = = string ret [ string ] = price_info [ string ] return self . _reply_json ( ret )	1
Q_128	get the price for a namespace	C_128_1	def enable ( dataframe = true , series = true ) try from i_python . core . getipython import get_ipython except import_error raise import_error ( string ) ip = get_ipython ( ) ip_formatter = ip . display_formatter . ipython_display_formatter if dataframe ip_formatter . for_type ( pd . data_frame , _display_as_qgrid ) else ip_formatter . type_printers . pop ( pd . data_frame , none ) if series ip_formatter . for_type ( pd . series , _display_as_qgrid ) else ip_formatter . type_printers . pop ( pd . series )	0
Q_128	get the price for a namespace	C_128_2	def describe_security_groups ( self , * names ) group_names = { } if names group_names = dict ( [ ( string ( var + number ) , var ) for var , var in enumerate ( names ) ] ) query = self . query_factory ( action = string , creds = self . creds , endpoint = self . endpoint , other_params = group_names ) d = query . submit ( ) return d . add_callback ( self . parser . describe_security_groups )	0
Q_129	open an arbitrary activity during a test	C_129_0	def start_activity ( self , app_package , app_activity , * * opts ) arguments = { string string , string string , string string , string string , string string , string string , string string } data = { } for var , var in arguments . items ( ) if var in opts data [ var ] = opts [ var ] driver = self . _current_application ( ) driver . start_activity ( app_package = app_package , app_activity = app_activity , * * data )	1
Q_129	open an arbitrary activity during a test	C_129_1	def system_information_type7 ( ) a = l2_pseudo_length ( l2p_length = number ) b = tp_pd ( pd = number ) c = message_type ( mes_type = number ) d = si7_rest_octets ( ) packet = a / b / c / d return packet	0
Q_129	open an arbitrary activity during a test	C_129_2	"def is_requirement ( line ) return not ( line = = string or line . startswith ( string ) or line . startswith ( string _ e string git + "" ) )"	0
Q_130	check if the path match this rule	C_130_0	def match ( self , path str ) _ > tuple [ optional [ dict [ str , any ] ] , bool ] match = self . _pattern . match ( path ) if match is not none needs_slash = ( self . strict_slashes and not self . is_leaf and match . groupdict ( ) [ string ] = string ) try converted_varaibles = { var self . _converters [ var ] . to_python ( var ) for var , var in match . groupdict ( ) . items ( ) if var = string } except validation_error return none , false else return { * * self . defaults , * * converted_varaibles } , needs_slash else return none , false	1
Q_130	check if the path match this rule	C_130_1	def to_bitstream ( self ) self . sanitize ( ) bitstream = bit_array ( string self . ttl ) bitstream + = bit_array ( string len ( self . locator_records ) ) bitstream + = bit_array ( string self . eid_prefix . prefixlen ) bitstream + = bit_array ( string self . action ) bitstream + = bit_array ( string self . authoritative ) bitstream + = self . _reserved1 bitstream + = bit_array ( string self . map_version ) bitstream + = get_bitstream_for_afi_address ( self . eid_prefix ) for var in self . locator_records bitstream + = var . to_bitstream ( ) return bitstream	0
Q_130	check if the path match this rule	C_130_2	def plot_elbo ( self , figsize = ( number , number ) ) import matplotlib . pyplot as plt plt . figure ( figsize = figsize ) plt . plot ( self . elbo_records ) plt . xlabel ( string ) plt . ylabel ( string ) plt . show ( )	0
Q_131	load data for a single step	C_131_0	def _load_mapping ( self , mapping ) mapping [ string ] = bool ( mapping . get ( string , { } ) . get ( string ) ) job_id , local_ids_for_batch = self . _create_job ( mapping ) result = self . _wait_for_job ( job_id ) self . _store_inserted_ids ( mapping , job_id , local_ids_for_batch ) return result	1
Q_131	load data for a single step	C_131_1	def close ( self ) files = self . _dict_ . get ( string ) for var , var in iter_multi_items ( files or ( ) ) var . close ( )	0
Q_131	load data for a single step	C_131_2	def start_of_month ( val ) if type ( val ) = = date val = datetime . fromordinal ( val . toordinal ( ) ) return start_of_day ( val ) . replace ( day = number )	0
Q_132	decode a buffer as a mav_link message	C_132_0	def decode ( self , msgbuf ) if msgbuf [ number ] = protocol_marker_v1 headerlen = number try magic , mlen , incompat_flags , compat_flags , seq , src_system , src_component , msg_idlow , msg_idhigh = struct . unpack ( string , msgbuf [ headerlen ] ) except struct . error as emsg raise mav_error ( string emsg ) msg_id = msg_idlow ( msg_idhigh < < number ) mapkey = msg_id else headerlen = number try magic , mlen , seq , src_system , src_component , msg_id = struct . unpack ( string , msgbuf [ headerlen ] ) incompat_flags = number compat_flags = number except struct . error as emsg raise mav_error ( string emsg ) mapkey = msg_id if ( incompat_flags mavlink_iflag_signed ) = number signature_len = mavlink_signature_block_len else signature_len = number if ord ( magic ) = protocol_marker_v1 and ord ( magic ) = protocol_marker_v2 raise mav_error ( string s string magic ) if mlen = len ( msgbuf ) _ ( headerlen + number + signature_len ) raise mav_error ( string ( len ( msgbuf ) _ ( headerlen + number + signature_len ) , mlen , msg_id , headerlen ) ) if not mapkey in mavlink_map raise mav_error ( string str ( mapkey ) ) type = mavlink_map [ mapkey ] fmt = type . format order_map = type . orders len_map = type . lengths crc_extra = type . crc_extra try crc , = struct . unpack ( string , msgbuf [ _ ( number + signature_len ) ] [ number ] ) except struct . error as emsg raise mav_error ( string emsg ) crcbuf = msgbuf [ number _ ( number + signature_len ) ] if true crcbuf . append ( crc_extra ) crc2 = x25crc ( crcbuf ) if crc = crc2 . crc raise mav_error ( string ( msg_id , crc , crc2 . crc ) ) sig_ok = false if self . signing . secret_key is not none accept_signature = false if signature_len = = mavlink_signature_block_len sig_ok = self . check_signature ( msgbuf , src_system , src_component ) accept_signature = sig_ok if sig_ok self . signing . goodsig_count + = number else self . signing . badsig_count + = number if not accept_signature and self . signing . allow_unsigned_callback is not none accept_signature = self . signing . allow_unsigned_callback ( self , msg_id ) if accept_signature self . signing . unsigned_count + = number else self . signing . reject_count + = number elif self . signing . allow_unsigned_callback is not none accept_signature = self . signing . allow_unsigned_callback ( self , msg_id ) if accept_signature self . signing . unsigned_count + = number else self . signing . reject_count + = number if not accept_signature raise mav_error ( string ) csize = struct . calcsize ( fmt ) mbuf = msgbuf [ headerlen _ ( number + signature_len ) ] if len ( mbuf ) < csize mbuf . extend ( [ number ] * ( csize _ len ( mbuf ) ) ) if len ( mbuf ) < csize raise mav_error ( string ( type , len ( mbuf ) , csize ) ) mbuf = mbuf [ csize ] try t = struct . unpack ( fmt , mbuf ) except struct . error as emsg raise mav_error ( string ( type , fmt , len ( mbuf ) , emsg ) ) tlist = list ( t ) if true t = tlist [ ] if sum ( len_map ) = = len ( len_map ) for var in range ( number , len ( tlist ) ) tlist [ var ] = t [ order_map [ var ] ] else tlist = [ ] for var in range ( number , len ( order_map ) ) order = order_map [ var ] l = len_map [ order ] tip = sum ( len_map [ order ] ) field = t [ tip ] if l = = number or isinstance ( field , str ) tlist . append ( field ) else tlist . append ( t [ tip ( tip + l ) ] ) for var in range ( number , len ( tlist ) ) if isinstance ( tlist [ var ] , str ) tlist [ var ] = str ( mav_string ( tlist [ var ] ) ) t = tuple ( tlist ) try m = type ( * t ) except exception as emsg raise mav_error ( string ( type , emsg ) ) m . _signed = sig_ok if m . _signed m . _link_id = msgbuf [ _ number ] m . _msgbuf = msgbuf m . _payload = msgbuf [ number _ ( number + signature_len ) ] m . _crc = crc m . _header = mav_link_header ( msg_id , incompat_flags , compat_flags , mlen , seq , src_system , src_component ) return m	1
Q_132	decode a buffer as a mav_link message	C_132_1	def command_health ( self ) if len ( self . args ) = = number and self . args [ number ] = = string package_health ( mode = string ) . test ( ) elif ( len ( self . args ) = = number and self . args [ number ] = = string and self . args [ number ] = = string ) package_health ( mode = self . args [ number ] ) . test ( ) else usage ( string )	0
Q_132	decode a buffer as a mav_link message	C_132_2	def getdirs ( self , section , option , raw = false , vars = none , fallback = [ ] ) globs = self . getlist ( section , option , fallback = [ ] ) return [ f for var in globs for f in glob . glob ( var ) if os . path . isdir ( f ) ]	0
Q_133	execute supply command_int	C_133_0	def cmd_command_int ( self , args ) if len ( args ) = number print ( string . format ( len ( args ) ) ) print ( string ) print ( string ) print ( string ) return if args [ number ] . isdigit ( ) frame = int ( args [ number ] ) else try frame = eval ( string + args [ number ] ) except attribute_error as e try frame = eval ( string + args [ number ] ) except attribute_error as e pass if frame is none print ( string . format ( args [ number ] ) ) return command = none if args [ number ] . isdigit ( ) command = int ( args [ number ] ) else try command = eval ( string + args [ number ] ) except attribute_error as e try command = eval ( string + args [ number ] ) except attribute_error as e pass current = int ( args [ number ] ) autocontinue = int ( args [ number ] ) param1 = float ( args [ number ] ) param2 = float ( args [ number ] ) param3 = float ( args [ number ] ) param4 = float ( args [ number ] ) x = int ( args [ number ] ) y = int ( args [ number ] ) z = float ( args [ number ] ) self . master . mav . command_int_send ( self . settings . target_system , self . settings . target_component , frame , command , number , number , param1 , param2 , param3 , param4 , x , y , z )	1
Q_133	execute supply command_int	C_133_1	def compute_content ( self , layout ) out = self . out try for var in layout . children stream = string_io ( ) self . out = stream var . accept ( self ) yield stream . getvalue ( ) finally self . out = out	0
Q_133	execute supply command_int	C_133_2	def all ( self ) if not self . sas . batch for var in self . _names if var . upper ( ) = string x = self . _getattr_ ( var ) if isinstance ( x , pd . data_frame ) if self . sas . sascfg . display . lower ( ) = = string print ( string + var + string + str ( x ) + string ) else self . sas . display ( x ) else ret = [ ] for var in self . _names if var . upper ( ) = string ret . append ( self . _getattr_ ( var ) ) return ret	0
Q_134	download xml file for parameter	C_134_0	def param_help_download ( self ) files = [ ] for var in [ string , string , string , string , string ] url = string var path = mp_util . dot_mavproxy ( string var ) files . append ( ( url , path ) ) url = string var if var = string path = mp_util . dot_mavproxy ( string var ) files . append ( ( url , path ) ) try child = multiproc . process ( target = mp_util . download_files , args = ( files , ) ) child . start ( ) except exception as e print ( e )	1
Q_134	download xml file for parameter	C_134_1	def _get_longest_hit_at_qry_start ( self , nucmer_hits ) hits_at_start = [ var for var in nucmer_hits if self . _is_at_qry_start ( var ) ] return self . _get_longest_hit_by_ref_length ( hits_at_start )	0
Q_134	download xml file for parameter	C_134_2	def translated ( structure , values , lang_spec ) indentation = string endline = string object_code = string stack = [ ] push = lambda x stack . append ( x ) pop = lambda stack . pop ( ) last = lambda stack [ _ number ] if len ( stack ) > number else string def indented_code ( s , level , end ) return lang_spec [ indentation ] * level + s + end level = number conditions = [ lexem_type_predicat , lexem_type_condition ] action = lexem_type_action downlevel = lexem_type_downlevel for var in structure if var is action if last ( ) in conditions value , values = values [ number len ( stack ) ] , values [ len ( stack ) ] object_code + = ( indented_code ( lang_spec [ beg_condition ] + lang_spec [ logical_and ] . join ( value ) + lang_spec [ end_condition ] , level , lang_spec [ end_line ] ) ) if len ( lang_spec [ beg_block ] ) > number object_code + = indented_code ( lang_spec [ beg_block ] , level , lang_spec [ end_line ] ) stack = [ ] level + = number object_code + = indented_code ( lang_spec [ beg_action ] + values [ number ] , level , lang_spec [ end_action ] + lang_spec [ end_line ] ) values = values [ number ] elif var in conditions push ( var ) elif var is downlevel if last ( ) not in conditions level _ = number if level > = number object_code + = indented_code ( lang_spec [ end_block ] , level , lang_spec [ end_line ] ) else level = number while level > number level _ = number if level > = number object_code + = indented_code ( lang_spec [ end_block ] , level , lang_spec [ end_line ] ) else level = number return object_code	0
Q_135	filter sensitive detail like password from a configuration	C_135_0	def _filter_config ( self , config , fields = none ) filter_fields_default = [ string , string ] filter_fields = fields or filter_fields_default return dict ( map ( lambda key ( key , string if key in filter_fields else config [ key ] ) , config . keys ( ) ) )	1
Q_135	filter sensitive detail like password from a configuration	C_135_1	def release_mouse ( wait_time float = operation_wait_time ) _ > none x , y = get_cursor_pos ( ) screen_width , screen_height = get_screen_size ( ) mouse_event ( mouse_event_flag . left_up mouse_event_flag . absolute , x * number / / screen_width , y * number / / screen_height , number , number ) time . sleep ( wait_time )	0
Q_135	filter sensitive detail like password from a configuration	C_135_2	def get_redirect_target ( ) for var in request . values . get ( string ) , request . referrer if var and is_local_url ( var ) return var	0
Q_136	concatlist concatlist comma expression	C_136_0	def p_concatlist ( self , p ) string p [ number ] = p [ number ] + ( p [ number ] , ) p . set_lineno ( number , p . lineno ( number ) )	1
Q_136	concatlist concatlist comma expression	C_136_1	def gini_coefficient ( y ) r n = len ( y ) i_sum = np . zeros ( n ) for var in prange ( n ) for j in range ( n ) i_sum [ var ] + = abs ( y [ var ] _ y [ j ] ) return np . sum ( i_sum ) / ( number * n * np . sum ( y ) )	0
Q_136	concatlist concatlist comma expression	C_136_2	def dom_debugger_remove_instrumentation_breakpoint ( self , event_name ) assert isinstance ( event_name , ( str , ) ) , string event_name string [ string ] string s string type ( event_name ) subdom_funcs = self . synchronous_command ( string , event_name = event_name ) return subdom_funcs	0
Q_137	return model step after apply noise	C_137_0	def forward ( self , actions , batch_info ) while len ( self . processes ) < actions . shape [ number ] len_action_space = self . action_space . shape [ _ number ] self . processes . append ( ornstein_uhlenbeck_noise_process ( np . zeros ( len_action_space ) , float ( self . std_dev ) * np . ones ( len_action_space ) ) ) noise = torch . from_numpy ( np . stack ( [ var ( ) for var in self . processes ] ) ) . float ( ) . to ( actions . device ) return torch . min ( torch . max ( actions + noise , self . low_tensor ) , self . high_tensor )	1
Q_137	return model step after apply noise	C_137_1	def replace_broker ( self , source_id , dest_id ) try source = self . brokers [ source_id ] dest = self . brokers [ dest_id ] for var in source . partitions . copy ( ) source . partitions . remove ( var ) dest . partitions . add ( var ) var . replace ( source , dest ) except key_error as e self . log . error ( string , e . args [ number ] ) raise invalid_broker_id_error ( string . format ( e . args [ number ] ) )	0
Q_137	return model step after apply noise	C_137_2	def from_config ( config ) matrix = { } variables = config . keys ( ) for var in product ( * config . values ( ) ) combination = dict ( zip ( variables , var ) ) include = true for value in combination . values ( ) for reducer in value . reducers if reducer . pattern = = string match = not combination [ reducer . variable ] . value else match = fnmatch ( combination [ reducer . variable ] . value , reducer . pattern ) if match if reducer . is_exclude else not match include = false if include key = string . join ( entry . alias for entry in var if entry . alias ) data = dict ( zip ( variables , ( entry . value for entry in var ) ) ) if key in matrix and data = matrix [ key ] raise duplicate_environment ( key , data , matrix [ key ] ) matrix [ key ] = data return matrix	0
Q_138	return the type of the give column base on it row value on the give run_set_result	C_138_0	"def get_column_type ( column , column_values ) try return _get_column_type_heur ( column , column_values ) except util . table_definition_error as e logging . error ( string t be determined { } "" . format ( e . message ) ) return column_type . text , none , none , number"	1
Q_138	return the type of the give column base on it row value on the give run_set_result	C_138_1	def get_subscription_labels ( self , user_pk ) r = self . _request ( string + str ( user_pk ) ) if r s = r . json ( ) return s return [ ]	0
Q_138	return the type of the give column base on it row value on the give run_set_result	C_138_2	def _update_data ( self , new_data ) log . debug ( string ( len ( new_data ) , self . path ) ) if self . _data is none self . _data = new_data else if self . data_type . nptype is not none data_pos = ( self . _data_insert_position , self . _data_insert_position + len ( new_data ) ) self . _data_insert_position + = len ( new_data ) self . _data [ data_pos [ number ] data_pos [ number ] ] = new_data else self . _data . extend ( new_data )	0
Q_139	init azure blob lease from exist	C_139_0	def with_source ( self , lease ) super ( ) . with_source ( lease ) self . offset = lease . offset self . sequence_number = lease . sequence_number	1
Q_139	init azure blob lease from exist	C_139_1	def pre_render ( self ) self . add_styles ( ) self . add_scripts ( ) self . root . set ( string , string ( self . graph . width , self . graph . height ) ) if self . graph . explicit_size self . root . set ( string , str ( self . graph . width ) ) self . root . set ( string , str ( self . graph . height ) )	0
Q_139	init azure blob lease from exist	C_139_2	def check_routes ( feed string , * , as_df bool = false , include_warnings bool = false ) _ > list table = string problems = [ ] if feed . routes is none problems . append ( [ string , string , table , [ ] ] ) else f = feed . routes . copy ( ) problems = check_for_required_columns ( problems , table , f ) if problems return format_problems ( problems , as_df = as_df ) if include_warnings problems = check_for_invalid_columns ( problems , table , f ) problems = check_column_id ( problems , table , f , string ) if string in f if string not in feed . agency . columns problems . append ( [ string , string , table , [ ] , ] ) else g = f . dropna ( subset = [ string ] ) cond = g [ string ] . isin ( feed . agency [ string ] ) problems = check_table ( problems , table , g , cond , string ) for var in [ string , string ] problems = check_column ( problems , table , f , var , valid_str , column_required = false ) cond = ( f [ string ] . notnull ( ) f [ string ] . notnull ( ) ) problems = check_table ( problems , table , f , cond , string , ) v = lambda x x in range ( number ) problems = check_column ( problems , table , f , string , v ) problems = check_column ( problems , table , f , string , valid_url , column_required = false ) for col in [ string , string ] problems = check_column ( problems , table , f , col , valid_color , column_required = false ) if include_warnings cond = f [ [ string , string ] ] . duplicated ( ) problems = check_table ( problems , table , f , cond , string , string , ) s = feed . trips [ string ] cond = f [ string ] . isin ( s ) problems = check_table ( problems , table , f , cond , string , string ) return format_problems ( problems , as_df = as_df )	0
Q_140	set the quantity of this order_line_item	C_140_0	def quantity ( self , quantity ) if quantity is none raise value_error ( string ) if len ( quantity ) > number raise value_error ( string ) if len ( quantity ) < number raise value_error ( string ) self . _quantity = quantity	1
Q_140	set the quantity of this order_line_item	C_140_1	def entitlements ( self , request , pk = none ) enterprise_customer_user = self . get_object ( ) instance = { string enterprise_customer_user . entitlements } serializer = serializers . enterprise_customer_user_entitlement_serializer ( instance , context = { string request } ) return response ( serializer . data )	0
Q_140	set the quantity of this order_line_item	C_140_2	def rain_series ( self ) return [ ( var , self . _station_history . get_measurements ( ) [ var ] [ string ] ) for var in self . _station_history . get_measurements ( ) ]	0
Q_141	get the normalize type value base on the cortex data model	C_141_0	async def get_type_norm ( self , name , valu ) tobj = self . model . type ( name ) if tobj is none raise s_exc . no_such_type ( mesg = f string , name = name ) norm , info = tobj . norm ( valu ) return norm , info	1
Q_141	get the normalize type value base on the cortex data model	C_141_1	def _tm ( self , theta , phi , psi , dx , dy , dz ) matrix = self . get_matrix ( theta , phi , psi , dx , dy , dz ) coord = matrix . dot ( self . coord2 ) dist = coord _ self . coord1 d_i2 = ( dist * dist ) . sum ( axis = number ) tm = _ ( number / ( number + ( d_i2 / self . d02 ) ) ) return tm	0
Q_141	get the normalize type value base on the cortex data model	C_141_2	def get_compositions_by_query ( self , composition_query ) and_list = list ( ) or_list = list ( ) for var in composition_query . _query_terms if string in composition_query . _query_terms [ var ] and string in composition_query . _query_terms [ var ] and_list . append ( { string [ { var { string composition_query . _query_terms [ var ] [ string ] } } , { var { string composition_query . _query_terms [ var ] [ string ] } } ] } ) else and_list . append ( { var composition_query . _query_terms [ var ] } ) for var in composition_query . _keyword_terms or_list . append ( { var composition_query . _keyword_terms [ var ] } ) if or_list and_list . append ( { string or_list } ) view_filter = self . _view_filter ( ) if view_filter and_list . append ( view_filter ) if and_list query_terms = { string and_list } collection = json_client_validated ( string , collection = string , runtime = self . _runtime ) result = collection . find ( query_terms ) . sort ( string , descending ) else result = [ ] return objects . composition_list ( result , runtime = self . _runtime , proxy = self . _proxy )	0
Q_142	register sigint signal handler with the ioloop to cancel the currently run cmdloop task	C_142_0	async def add_signal_handlers ( self ) def sigint ( ) self . printf ( string ) if self . cmdtask is not none self . cmdtask . cancel ( ) self . loop . add_signal_handler ( signal . sigint , sigint )	1
Q_142	register sigint signal handler with the ioloop to cancel the currently run cmdloop task	C_142_1	def unpack ( self , format_text ) data = _unpack_from ( format_text , self . data , self . offset ) self . offset + = _calcsize ( format_text ) return data	0
Q_142	register sigint signal handler with the ioloop to cancel the currently run cmdloop task	C_142_2	def visit_call ( self , node , * * kwargs ) line_num = node . lineno _ number match = ast_walker . _implements_re . match ( self . lines [ line_num ] ) if match self . lines [ line_num ] = string linesep ) ) self . generic_visit ( node , containing_nodes = kwargs [ string ] )	0
Q_143	main entry point for set up list multiplier	C_143_0	def setup_list_pars ( self ) tdf = self . setup_temporal_list_pars ( ) sdf = self . setup_spatial_list_pars ( ) if tdf is none and sdf is none return os . chdir ( self . m . model_ws ) try apply_list_pars ( ) except exception as e os . chdir ( string ) self . logger . lraise ( string . format ( str ( e ) ) ) os . chdir ( string ) line = string self . logger . statement ( string . format ( line ) ) self . frun_pre_lines . append ( line )	1
Q_143	main entry point for set up list multiplier	C_143_1	def add ( self , task , multiprocess = false , processes = number ) if self . _first_task is none and hasattr ( task , string ) self . _first_task = task . task_id self . add_succeeded = true if multiprocess queue = multiprocessing . manager ( ) . queue ( ) pool = multiprocessing . pool ( processes = processes if processes > number else none ) else queue = deque_queue ( ) pool = single_process_pool ( ) self . _validate_task ( task ) pool . apply_async ( check_complete , [ task , queue ] ) queue_size = number try seen = { task . task_id } while queue_size current = queue . get ( ) queue_size _ = number item , is_complete = current for var in self . _add ( item , is_complete ) if var . task_id not in seen self . _validate_task ( var ) seen . add ( var . task_id ) pool . apply_async ( check_complete , [ var , queue ] ) queue_size + = number except ( keyboard_interrupt , task_exception ) raise except exception as ex self . add_succeeded = false formatted_traceback = traceback . format_exc ( ) self . _log_unexpected_error ( task ) task . trigger_event ( event . broken_task , task , ex ) self . _email_unexpected_error ( task , formatted_traceback ) raise finally pool . close ( ) pool . join ( ) return self . add_succeeded	0
Q_143	main entry point for set up list multiplier	C_143_2	def tmatrix_sampler ( c , reversible = false , mu = none , t0 = none , nsteps = none , prior = string ) r if issparse ( c ) _show_sparse_conversion_warning ( ) c = c . toarray ( ) from . dense . tmatrix_sampler import transition_matrix_sampler sampler = transition_matrix_sampler ( c , reversible = reversible , mu = mu , p0 = t0 , nsteps = nsteps , prior = prior ) return sampler	0
Q_144	return true if the first category in this collection have a date	C_144_0	def are_dates ( self ) if self . depth = number return false first_cat_label = self [ number ] . label date_types = ( datetime . date , datetime . datetime ) if isinstance ( first_cat_label , date_types ) return true return false	1
Q_144	return true if the first category in this collection have a date	C_144_1	def compare_name_component ( list1 , list2 , settings , use_ratio = false ) if not list1 [ number ] or not list2 [ number ] not_required = not settings [ string ] return not_required * number if use_ratio else not_required if len ( list1 ) = len ( list2 ) return false compare_func = _ratio_compare if use_ratio else _normal_compare return compare_func ( list1 , list2 , settings )	0
Q_144	return true if the first category in this collection have a date	C_144_2	def handle_event ( self , packet ) if packet . get ( string ) task = self . send_command_ack ( packet [ string ] , packet [ string ] ) self . loop . create_task ( task )	0
Q_145	vocabulary of available department	C_145_0	def _departments_voc ( self ) query = { string string , string true } results = api . search ( query , string ) items = map ( lambda var ( api . get_uid ( var ) , api . get_title ( var ) ) , results ) dept_uids = map ( api . get_uid , results ) depts = self . get_departments ( ) for var in depts uid = api . get_uid ( var ) if uid in dept_uids continue items . append ( ( uid , api . get_title ( var ) ) ) return api . to_display_list ( items , sort_by = string , allow_empty = false )	1
Q_145	vocabulary of available department	C_145_1	def _find_configs ( self ) if self . config_files is not none return self . config_files result = [ ] for path , _ , files in os . walk ( self . app . root_path ) if string in files result . append ( os . path . join ( path , string ) ) return result	0
Q_145	vocabulary of available department	C_145_2	def uv_to_color ( uv , image ) if image is none or uv is none return none uv = np . asanyarray ( uv , dtype = np . float64 ) x = ( uv [ , number ] * ( image . width _ number ) ) y = ( ( number _ uv [ , number ] ) * ( image . height _ number ) ) x = x . round ( ) . astype ( np . int64 ) image . width y = y . round ( ) . astype ( np . int64 ) image . height colors = np . asanyarray ( image . convert ( string ) ) [ y , x ] assert colors . ndim = = number and colors . shape [ number ] = = number return colors	0
Q_146	remove the view tagstr from inside ar	C_146_0	def remove_not_requested_analyses_view ( portal ) logger . info ( string analyses not requested string ) ar_ptype = portal . portal_types . analysis_request ar_ptype . _actions = filter ( lambda act act . id = string , ar_ptype . list_actions ( ) )	1
Q_146	remove the view tagstr from inside ar	C_146_1	def _train_multi_device ( symbol , ctx , arg_names , param_names , aux_names , arg_params , aux_params , begin_epoch , end_epoch , epoch_size , optimizer , kvstore , update_on_kvstore , train_data , eval_data = none , eval_metric = none , epoch_end_callback = none , batch_end_callback = none , logger = none , work_load_list = none , monitor = none , eval_end_callback = none , eval_batch_end_callback = none , sym_gen = none ) if logger is none logger = logging executor_manager = data_parallel_executor_manager ( symbol = symbol , sym_gen = sym_gen , ctx = ctx , train_data = train_data , param_names = param_names , arg_names = arg_names , aux_names = aux_names , work_load_list = work_load_list , logger = logger ) if monitor executor_manager . install_monitor ( monitor ) executor_manager . set_params ( arg_params , aux_params ) if not update_on_kvstore updater = get_updater ( optimizer ) else kvstore . set_optimizer ( optimizer ) if kvstore _initialize_kvstore ( kvstore = kvstore , param_arrays = executor_manager . param_arrays , arg_params = arg_params , param_names = executor_manager . param_names , update_on_kvstore = update_on_kvstore ) train_data . reset ( ) for var in range ( begin_epoch , end_epoch ) tic = time . time ( ) eval_metric . reset ( ) nbatch = number while true do_reset = true for data_batch in train_data executor_manager . load_data_batch ( data_batch ) if monitor is not none monitor . tic ( ) executor_manager . forward ( is_train = true ) executor_manager . backward ( ) if update_on_kvstore if string in kvstore . type _update_params_on_kvstore_nccl ( executor_manager . param_arrays , executor_manager . grad_arrays , kvstore , executor_manager . param_names ) else _update_params_on_kvstore ( executor_manager . param_arrays , executor_manager . grad_arrays , kvstore , executor_manager . param_names ) else _update_params ( executor_manager . param_arrays , executor_manager . grad_arrays , updater = updater , num_device = len ( ctx ) , kvstore = kvstore , param_names = executor_manager . param_names ) if monitor is not none monitor . toc_print ( ) executor_manager . update_metric ( eval_metric , data_batch . label ) nbatch + = number if batch_end_callback is not none batch_end_params = batch_end_param ( var = var , nbatch = nbatch , eval_metric = eval_metric , locals = locals ( ) ) _multiple_callbacks ( batch_end_callback , batch_end_params ) if epoch_size is not none and nbatch > = epoch_size do_reset = false break if do_reset logger . info ( string , var ) train_data . reset ( ) if epoch_size is none or nbatch > = epoch_size break toc = time . time ( ) logger . info ( string , var , ( toc _ tic ) ) if epoch_end_callback or var + number = = end_epoch executor_manager . copy_to ( arg_params , aux_params ) _multiple_callbacks ( epoch_end_callback , var , symbol , arg_params , aux_params ) if eval_data eval_metric . reset ( ) eval_data . reset ( ) total_num_batch = number for i , eval_batch in enumerate ( eval_data ) executor_manager . load_data_batch ( eval_batch ) executor_manager . forward ( is_train = false ) executor_manager . update_metric ( eval_metric , eval_batch . label ) if eval_batch_end_callback is not none batch_end_params = batch_end_param ( var = var , nbatch = i , eval_metric = eval_metric , locals = locals ( ) ) _multiple_callbacks ( eval_batch_end_callback , batch_end_params ) total_num_batch + = number if eval_end_callback is not none eval_end_params = batch_end_param ( var = var , nbatch = total_num_batch , eval_metric = eval_metric , locals = locals ( ) ) _multiple_callbacks ( eval_end_callback , eval_end_params ) eval_data . reset ( )	0
Q_146	remove the view tagstr from inside ar	C_146_2	def _flds_shape ( fieldname , header ) shp = list ( header [ string ] ) shp . append ( header [ string ] ) if fieldname = = string shp . insert ( number , number ) header [ string ] = int ( header [ string ] [ number ] = number ) shp [ number ] + = header [ string ] header [ string ] = int ( header [ string ] [ number ] = number ) shp [ number ] + = header [ string ] header [ string ] = number header [ string ] = number else shp . insert ( number , number ) header [ string ] = number header [ string ] = number header [ string ] = number header [ string ] = number return shp	0
Q_147	return the copies_count number request parameter	C_147_0	def get_copies_count ( self ) setup = api . get_setup ( ) default_num = setup . get_default_number_of_copies ( ) request_num = self . request . form . get ( string ) return to_int ( request_num , default_num )	1
Q_147	return the copies_count number request parameter	C_147_1	def data ( self , data ) assert type ( data ) is np . ndarray assert data . shape [ number ] = = self . n_cols for var in range ( self . n_cols ) self . col_data [ var ] = data [ , var ] . tolist ( )	0
Q_147	return the copies_count number request parameter	C_147_2	def routes_collector ( gatherer ) def hat_func ( rule , * * options ) def decorator ( f ) rule_dict = { string rule , string f } rule_dict . update ( options ) gatherer . append ( rule_dict ) return decorator return hat_func	0
Q_148	"return configuration file as bytes_io _ we only access it through the respective blob "" s data"	C_148_0	def _sio_modules ( cls , parent_commit ) sio = bytes_io ( parent_commit . tree [ cls . k_modules_file ] . data_stream . read ( ) ) sio . name = cls . k_modules_file return sio	1
Q_148	"return configuration file as bytes_io _ we only access it through the respective blob "" s data"	C_148_1	def idle_task ( self ) try datagram = self . port . recvfrom ( self . buffer_size ) data = json . loads ( datagram [ number ] ) except socket . error as e if e . errno in [ errno . eagain , errno . ewouldblock ] return raise for var in data . keys ( ) self . data [ var ] = data [ var ] try self . master . mav . gps_input_send ( self . data [ string ] , self . data [ string ] , self . data [ string ] , self . data [ string ] , self . data [ string ] , self . data [ string ] , self . data [ string ] , self . data [ string ] , self . data [ string ] , self . data [ string ] , self . data [ string ] , self . data [ string ] , self . data [ string ] , self . data [ string ] , self . data [ string ] , self . data [ string ] , self . data [ string ] , self . data [ string ] ) except exception as e print ( string , e )	0
Q_148	"return configuration file as bytes_io _ we only access it through the respective blob "" s data"	C_148_2	def resolve ( self , lineno ) if not self . defined error ( lineno , string s string self . name ) if isinstance ( self . value , expr ) return self . value . eval ( ) return self . value	0
Q_149	create a new version of a secret at the specify location	C_149_0	def create_or_update_secret ( self , path , secret , cas = none , mount_point = default_mount_point ) params = { string { } , string secret , } if cas is not none params [ string ] [ string ] = cas api_path = string . format ( mount_point = mount_point , path = path ) response = self . _adapter . post ( url = api_path , json = params , ) return response . json ( )	1
Q_149	create a new version of a secret at the specify location	C_149_1	def get_date_fields ( fc ) if arcpy_found = = false raise exception ( string ) return [ var . name for var in arcpy . list_fields ( fc , field_type = string ) ]	0
Q_149	create a new version of a secret at the specify location	C_149_2	def next_iteration ( self , ref_point , bounds = none ) if bounds self . problem . points = reachable_points ( self . problem . points , self . problem . ideal , bounds ) if not utils . isin ( self . fh , self . problem . points ) or ref_point = self . ref_point self . ref_point = list ( ref_point ) self . _update_fh ( ) self . _update_zh ( self . zh , self . fh ) self . fh_lo = list ( self . lower_bounds_factory . result ( self . zh ) ) self . fh_up = list ( self . upper_bounds_factory . result ( self . zh ) ) logging . debug ( f string ) logging . debug ( f string ) if not np . all ( np . array ( self . fh_up ) > np . array ( self . fh_lo ) ) warn ( self . negative_interval_warning ( ) ) assert utils . isin ( self . fh_up , self . problem . points ) assert utils . isin ( self . fh_lo , self . problem . points ) dist = self . distance ( self . zh , self . fh ) self . update_points ( ) l_p = len ( self . problem . points ) self . current_iter _ = number return dist , self . fh , self . zh , self . fh_lo , self . fh_up , l_p	0
Q_150	ping the server	C_150_0	def ping ( self , message = _notset , * , encoding = _notset ) if message is not _notset args = ( message , ) else args = ( ) return self . execute ( string , * args , encoding = encoding )	1
Q_150	ping the server	C_150_1	def wait_until_page_source_contains ( self , text , timeout = none ) timeout = timeout if timeout is not none else self . timeout def wait ( ) web_driver_wait ( self . driver , timeout ) . until ( lambda d text in self . page_source ( ) ) return self . page_source ( ) return self . execute_and_handle_webdriver_exceptions ( wait , timeout , text , string . format ( text ) )	0
Q_150	ping the server	C_150_2	def _create_dict_reader ( self , line_reader ) for var in line_reader if isinstance ( var , py2to3 . bytes_type ) try var = codecs . decode ( var , self . _encoding ) except unicode_decode_error as exception raise errors . unable_to_parse_file ( string . format ( exception ) ) stripped_line = var . strip ( ) values = stripped_line . split ( self . delimiter ) number_of_values = len ( values ) number_of_columns = len ( self . columns ) if number_of_values < self . min_columns raise errors . unable_to_parse_file ( string . format ( self . min_columns , number_of_values ) ) if number_of_values > number_of_columns raise errors . unable_to_parse_file ( string . format ( number_of_columns , number_of_values ) ) yield dict ( zip ( self . columns , values ) )	0
Q_151	incrementally iterate set element	C_151_0	def sscan ( self , key , cursor = number , match = none , count = none ) tokens = [ key , cursor ] match is not none and tokens . extend ( [ b string , match ] ) count is not none and tokens . extend ( [ b string , count ] ) fut = self . execute ( b string , * tokens ) return wait_convert ( fut , lambda obj ( int ( obj [ number ] ) , obj [ number ] ) )	1
Q_151	incrementally iterate set element	C_151_1	def pid ( self , pidnum ) try with open ( self . pidfile , string ) as pidfile pidfile . write ( string . format ( pidnum ) ) except io_error log . exception ( string . format ( self . pidfile ) ) sys . exit ( exit . pidfile_inaccessible )	0
Q_151	incrementally iterate set element	C_151_2	def add_plugin_arguments ( self , parser ) for var in self . hook_managers . values ( ) if len ( list ( var ) ) = = number continue var . map ( self . _add_hook_extension_arguments , parser ) for namespace , var in self . driver_managers . items ( ) choices = list ( sorted ( var . names ( ) ) ) if len ( choices ) = = number continue option , dest = self . _namespace_to_option ( namespace ) parser . add_argument ( option , help = self . _help [ namespace ] , dest = dest , choices = choices , default = string ) option_prefix = string . format ( option ) dest_prefix = string . format ( dest ) var . map ( self . _add_driver_extension_arguments , parser , option_prefix , dest_prefix )	0
Q_152	a sparse matrix representation of the face angle	C_152_0	def face_angles_sparse ( mesh ) matrix = coo_matrix ( ( mesh . face_angles . flatten ( ) , ( mesh . faces_sparse . row , mesh . faces_sparse . col ) ) , mesh . faces_sparse . shape ) return matrix	1
Q_152	a sparse matrix representation of the face angle	C_152_1	def to_json_file ( self , json_file_path ) with open ( json_file_path , string , encoding = string ) as writer writer . write ( self . to_json_string ( ) )	0
Q_152	a sparse matrix representation of the face angle	C_152_2	def create_layout_params ( self , child , layout ) from . android_fragment import android_fragment if isinstance ( child , android_fragment ) return super ( android_view_pager , self ) . create_layout_params ( child , layout ) dp = self . dp w , h = ( coerce_size ( layout . get ( string , string ) ) , coerce_size ( layout . get ( string , string ) ) ) w = w if w < number else int ( w * dp ) h = h if h < number else int ( h * dp ) params = view_pager_layout_params ( ) params . width = w params . height = h params . is_decor = true return params	0
Q_153	return a correctly transform polygon soup of the	C_153_0	def triangles ( self ) triangles = collections . deque ( ) triangles_node = collections . deque ( ) for var in self . graph . nodes_geometry transform , geometry_name = self . graph [ var ] geometry = self . geometry [ geometry_name ] if not hasattr ( geometry , string ) continue triangles . append ( transformations . transform_points ( geometry . triangles . copy ( ) . reshape ( ( _ number , number ) ) , matrix = transform ) ) triangles_node . append ( np . tile ( var , len ( geometry . triangles ) ) ) self . _cache [ string ] = np . hstack ( triangles_node ) triangles = np . vstack ( triangles ) . reshape ( ( _ number , number , number ) ) return triangles	1
Q_153	return a correctly transform polygon soup of the	C_153_1	def _parse_bool ( value ) if isinstance ( value , bool ) return value elif isinstance ( value , str ) if value = = string return true elif value = = string return false raise exception ( string value )	0
Q_153	return a correctly transform polygon soup of the	C_153_2	def compare_config ( self ) diff = self . device . cu . diff ( ) if diff is none return string else return diff . strip ( )	0
Q_154	"r tagstr "" scan a resource directory for colortable file and add them to the registry"	C_154_0	def scan_resource ( self , pkg , path ) r for var in resource_listdir ( pkg , path ) if var . endswith ( table_ext ) table_path = posixpath . join ( path , var ) with contextlib . closing ( resource_stream ( pkg , table_path ) ) as stream self . add_colortable ( stream , posixpath . splitext ( posixpath . basename ( var ) ) [ number ] )	1
Q_154	"r tagstr "" scan a resource directory for colortable file and add them to the registry"	C_154_1	def has ( * permissions , * * kwargs ) target = kwargs [ string ] kwargs [ string ] = type_for ( target ) return target in filter_ ( * permissions , * * kwargs )	0
Q_154	"r tagstr "" scan a resource directory for colortable file and add them to the registry"	C_154_2	def select_assemblies ( pth , manifest = none ) rv = [ ] if not os . path . isfile ( pth ) pth = check_extract_from_egg ( pth ) [ number ] [ number ] if manifest _dep_names = set ( [ var . name for var in manifest . dependent_assemblies ] ) for assembly in get_assemblies ( pth ) if seen . get ( assembly . getid ( ) . upper ( ) , number ) continue if manifest and not assembly . name in _dep_names logger . info ( string string , assembly . name ) manifest . dependent_assemblies . append ( assembly ) _dep_names . add ( assembly . name ) if not dylib . include_library ( assembly . name ) logger . debug ( string , assembly . getid ( ) ) continue if assembly . optional logger . debug ( string , assembly . getid ( ) ) continue files = assembly . find_files ( ) if files seen [ assembly . getid ( ) . upper ( ) ] = number for fn in files fname , fext = os . path . splitext ( fn ) if fext . lower ( ) = = string nm = assembly . name + fext else nm = os . path . basename ( fn ) ftocnm = nm if assembly . language not in ( none , string , string , string ) ftocnm = os . path . join ( assembly . getlanguage ( ) , ftocnm ) nm , ftocnm , fn = [ item . encode ( sys . getfilesystemencoding ( ) ) for item in ( nm , ftocnm , fn ) ] if not seen . get ( fn . upper ( ) , number ) logger . debug ( string , ftocnm ) seen [ nm . upper ( ) ] = number seen [ fn . upper ( ) ] = number rv . append ( ( ftocnm , fn ) ) else pass else logger . error ( string , assembly . getid ( ) ) return rv	0
Q_155	get expect and actual repeat cumulative transaction	C_155_0	def expected_cumulative_transactions ( model , transactions , datetime_col , customer_id_col , t , datetime_format = none , freq = string , set_index_date = false , freq_multiplier = number , ) start_date = pd . to_datetime ( transactions [ datetime_col ] , format = datetime_format ) . min ( ) start_period = start_date . to_period ( freq ) observation_period_end = start_period + t repeated_and_first_transactions = _find_first_transactions ( transactions , customer_id_col , datetime_col , datetime_format = datetime_format , observation_period_end = observation_period_end , freq = freq , ) first_trans_mask = repeated_and_first_transactions [ string ] repeated_transactions = repeated_and_first_transactions [ first_trans_mask ] first_transactions = repeated_and_first_transactions [ first_trans_mask ] date_range = pd . date_range ( start_date , periods = t + number , freq = freq ) date_periods = date_range . to_period ( freq ) pred_cum_transactions = [ ] first_trans_size = first_transactions . groupby ( datetime_col ) . size ( ) for var , var in enumerate ( date_periods ) if var freq_multiplier = = number and var > number times = np . array ( [ d . n for d in var _ first_trans_size . index ] ) times = times [ times > number ] . astype ( float ) / freq_multiplier expected_trans_agg = model . expected_number_of_purchases_up_to_time ( times ) mask = first_trans_size . index < var expected_trans = sum ( expected_trans_agg * first_trans_size [ mask ] ) pred_cum_transactions . append ( expected_trans ) act_trans = repeated_transactions . groupby ( datetime_col ) . size ( ) act_tracking_transactions = act_trans . reindex ( date_periods , fill_value = number ) act_cum_transactions = [ ] for j in range ( number , t / / freq_multiplier + number ) sum_trans = sum ( act_tracking_transactions . iloc [ j * freq_multiplier ] ) act_cum_transactions . append ( sum_trans ) if set_index_date index = date_periods [ freq_multiplier _ number _ number freq_multiplier ] else index = range ( number , t / / freq_multiplier ) df_cum_transactions = pd . data_frame ( { string act_cum_transactions , string pred_cum_transactions } , index = index ) return df_cum_transactions	1
Q_155	get expect and actual repeat cumulative transaction	C_155_1	def double_smoothed_stochastic ( data , period ) catch_errors . check_for_period_error ( data , period ) lows = [ data [ var ] _ np . min ( data [ var + number _ period var + number ] ) for var in range ( period _ number , len ( data ) ) ] sm_lows = ema ( ema ( lows , period ) , period ) highs = [ np . max ( data [ var + number _ period var + number ] ) _ np . min ( data [ var + number _ period var + number ] ) for var in range ( period _ number , len ( data ) ) ] sm_highs = ema ( ema ( highs , period ) , period ) dss = ( sm_lows / sm_highs ) * number dss = fill_for_noncomputable_vals ( data , dss ) return dss	0
Q_155	get expect and actual repeat cumulative transaction	C_155_2	def _broadcast_item ( self , row_lookup , col_lookup , item , to_shape ) if isinstance ( item , ( pandas . series , pandas . data_frame , data_frame ) ) if not all ( var in item . index for var in row_lookup ) raise value_error ( string string ) if hasattr ( item , string ) if not all ( var in item . columns for var in col_lookup ) raise value_error ( string string ) item = item . reindex ( index = row_lookup , columns = col_lookup ) else item = item . reindex ( index = row_lookup ) try item = np . array ( item ) if np . prod ( to_shape ) = = np . prod ( item . shape ) return item . reshape ( to_shape ) else return np . broadcast_to ( item , to_shape ) except value_error from_shape = np . array ( item ) . shape raise value_error ( string string . format ( from_shape = from_shape , to_shape = to_shape ) )	0
Q_156	validate that all the documentation reference in a docstring be	C_156_0	def _validate_doc_refs_helper ( self , env , doc , loc , type_context = none ) for var in doc_ref_re . finditer ( doc ) tag = var . group ( string ) val = var . group ( string ) if tag = = string if string in val type_name , field_name = val . split ( string , number ) if type_name not in env raise invalid_spec ( string string ( field_name , quote ( type_name ) ) , * loc ) elif isinstance ( env [ type_name ] , api_routes_by_version ) raise invalid_spec ( string ( quote ( field_name ) , quote ( type_name ) ) , * loc ) if isinstance ( env [ type_name ] , environment ) namespace_name , type_name , field_name = val . split ( string , number ) data_type_to_check = env [ namespace_name ] [ type_name ] elif isinstance ( env [ type_name ] , alias ) data_type_to_check = env [ type_name ] . data_type else data_type_to_check = env [ type_name ] if not any ( field . name = = field_name for field in data_type_to_check . all_fields ) raise invalid_spec ( string quote ( val ) , * loc ) else assert type_context is not none if not any ( field . name = = val for field in type_context . all_fields ) raise invalid_spec ( string quote ( val ) , * loc ) elif tag = = string if not ( number < val . rfind ( string ) < len ( val ) _ number ) raise invalid_spec ( string string quote ( val ) , * loc ) elif tag = = string if string in val namespace_name , val = val . split ( string , number ) if namespace_name not in env raise invalid_spec ( string s string namespace_name , * loc ) env_to_check = env [ namespace_name ] else env_to_check = env route_name , version = parse_route_name_and_version ( val ) if route_name not in env_to_check raise invalid_spec ( string . format ( quote ( route_name ) ) , * loc ) if not isinstance ( env_to_check [ route_name ] , api_routes_by_version ) raise invalid_spec ( string . format ( quote ( route_name ) ) , * loc ) if version not in env_to_check [ route_name ] . at_version raise invalid_spec ( string . format ( quote ( route_name ) , version ) , * loc ) elif tag = = string if string in val namespace_name , val = val . split ( string , number ) if namespace_name not in env raise invalid_spec ( string s string namespace_name , * loc ) env_to_check = env [ namespace_name ] else env_to_check = env if val not in env_to_check raise invalid_spec ( string s string val , * loc ) elif not isinstance ( env_to_check [ val ] , ( struct , union ) ) raise invalid_spec ( string quote ( val ) , * loc ) elif tag = = string if not doc_ref_val_re . var ( val ) raise invalid_spec ( string quote ( val ) , * loc ) else raise invalid_spec ( string quote ( tag ) , * loc )	1
Q_156	validate that all the documentation reference in a docstring be	C_156_1	def liquid_jet_pump_ancillary ( rhop , rhos , kp , ks , d_nozzle = none , d_mixing = none , qp = none , qs = none , p1 = none , p2 = none ) r unknowns = sum ( var is none for var in ( d_nozzle , d_mixing , qs , qp , p1 , p2 ) ) if unknowns > number raise exception ( string ) elif unknowns < number raise exception ( string ) c = rhos / rhop if qp is not none and qs is not none m = qs / qp if d_nozzle is not none a_nozzle = pi / number * d_nozzle * d_nozzle if d_mixing is not none a_mixing = pi / number * d_mixing * d_mixing r = a_nozzle / a_mixing if p1 is none return rhop / number * ( qp / a_nozzle ) * * number * ( ( number + kp ) _ c * ( number + ks ) * ( ( m * r ) / ( number _ r ) ) * * number ) + p2 elif p2 is none return _ rhop / number * ( qp / a_nozzle ) * * number * ( ( number + kp ) _ c * ( number + ks ) * ( ( m * r ) / ( number _ r ) ) * * number ) + p1 elif qs is none try return ( ( _ number * a_nozzle * * number * p1 + number * a_nozzle * * number * p2 + kp * qp * * number * rhop + qp * * number * rhop ) / ( c * rhop * ( ks + number ) ) ) * * number . number * ( a_mixing _ a_nozzle ) / a_nozzle except value_error return _ number elif qp is none return a_nozzle * ( ( number * a_mixing * * number * p1 _ number * a_mixing * * number * p2 _ number * a_mixing * a_nozzle * p1 + number * a_mixing * a_nozzle * p2 + number * a_nozzle * * number * p1 _ number * a_nozzle * * number * p2 + c * ks * qs * * number * rhop + c * qs * * number * rhop ) / ( rhop * ( kp + number ) ) ) * * number . number / ( a_mixing _ a_nozzle ) elif d_nozzle is none def err ( d_nozzle ) return p1 _ liquid_jet_pump_ancillary ( rhop = rhop , rhos = rhos , kp = kp , ks = ks , d_nozzle = d_nozzle , d_mixing = d_mixing , qp = qp , qs = qs , p1 = none , p2 = p2 ) return brenth ( err , number e _ number , d_mixing * number ) elif d_mixing is none def err ( d_mixing ) return p1 _ liquid_jet_pump_ancillary ( rhop = rhop , rhos = rhos , kp = kp , ks = ks , d_nozzle = d_nozzle , d_mixing = d_mixing , qp = qp , qs = qs , p1 = none , p2 = p2 ) try return brenth ( err , number e _ number , d_nozzle * number ) except return newton ( err , d_nozzle * number )	0
Q_156	validate that all the documentation reference in a docstring be	C_156_2	def binary_operation_logical ( self , rule , left , right , * * kwargs ) return logical_bin_op_rule ( rule . operation , left , right )	0
Q_157	assign a view to current graph	C_157_0	def _set_view ( self ) view_class = reverse_view if self . inverse_y_axis else view self . view = view_class ( self . width _ self . margin_box . x , self . height _ self . margin_box . y , self . _box )	1
Q_157	assign a view to current graph	C_157_1	def concentrations ( self , complexes , concs , ordered = false , pairs = false , cutoff = number . number number , temp = number . number ) nstrands = len ( complexes [ number ] [ string ] ) try if len ( concs ) = nstrands raise value_error ( string ) except type_error concs = [ concs for var in range ( len ( complexes [ string ] ) ) ] cmd_args = [ string ] if ordered cmd_args . append ( string ) with open ( os . path . join ( self . _tempdir , string ) ) as f f . writelines ( concs ) header = [ string . format ( nstrands ) , string ] for var , strand in enumerate ( complexes [ string ] ) header . append ( string . format ( var + number , strand ) ) header . append ( string . format ( temp ) ) body = [ ] for var , cx in enumerate ( complexes ) permutation = string . join ( complexes [ string ] ) line = string . format ( var + number , permutation , complexes [ string ] ) body . append ( line ) if ordered cxfile = os . path . join ( self . _tempdir , string ) else cxfile = os . path . join ( self . _tempdir , string ) with open ( cxfile ) as f f . writelines ( header + body ) self . _run ( string , cmd_args , none ) eq_lines = self . _read_tempfile ( string ) . split ( string ) tsv_lines = [ l for l in eq_lines if not l . startswith ( string ) ] output = [ ] for var , line in enumerate ( tsv_lines ) data = line . split ( string ) cx = [ int ( c ) for c in data [ number nstrands ] ] eq = float ( data [ nstrands + number ] ) output [ var ] = { string cx , string eq } if pairs pairs = self . _read_tempfile ( string ) pairs_tsv = [ l for l in pairs . split ( string ) if not l . startswith ( string ) ] dim = int ( pairs_tsv . pop ( number ) ) pprob = [ [ int ( p [ number ] ) , int ( p [ number ] ) , float ( p [ number ] ) ] for p in pairs_tsv ] fpairs_mat = self . pairs_to_np ( pprob , dim ) for var , out in enumerate ( output ) output [ var ] [ string ] = fpairs_mat return output	0
Q_157	assign a view to current graph	C_157_2	def get_all_tep ( self ) teps = { } for var in self . get_enabled_plugins for e , v in var [ string ] . items ( ) tep = teps . get ( e , dict ( ) ) tep_hf = tep . get ( string , [ ] ) tep_hs = tep . get ( string , [ ] ) tep_hf + = [ s for f , s in v . items ( ) if f = = string ] tep_hs + = [ s for f , s in v . items ( ) if f = = string ] teps [ e ] = dict ( html_file = tep_hf , html_string = tep_hs ) return teps	0
Q_158	add the log file option to the argument group	C_158_0	def add_log_file_options ( self , argument_group ) argument_group . add_argument ( string , string , string , action = string , metavar = string , dest = string , type = str , default = string , help = ( string string { number s } _ yyyymmdd_thhmmss . log . gz string string string . gz string ) . format ( self . name ) )	1
Q_158	add the log file option to the argument group	C_158_1	def minify ( drop_semi = true ) layout_handlers = { open_block layout_handler_openbrace , close_block layout_handler_closebrace , end_statement layout_handler_semicolon , space layout_handler_space_minimum , optional_space layout_handler_space_minimum , required_space layout_handler_space_imply , ( space , open_block ) layout_handler_openbrace , ( space , end_statement ) layout_handler_semicolon , ( optional_space , end_statement ) layout_handler_semicolon , } if drop_semi layout_handlers . update ( { end_statement layout_handler_semicolon_optional , ( optional_space , end_statement ) layout_handler_semicolon_optional , ( end_statement , close_block ) layout_handler_closebrace , ( end_statement , dedent ) rule_handler_noop , ( ( optional_space , end_statement ) , close_block ) layout_handler_closebrace , } ) def minify_rule ( ) return { string layout_handlers , string { literal deferrable_handler_literal_continuation , } , } return minify_rule	0
Q_158	add the log file option to the argument group	C_158_2	def preprocess_xarray ( func ) functools . wraps ( func ) def wrapper ( * args , * * kwargs ) args = tuple ( var . metpy . unit_array if isinstance ( var , xr . data_array ) else var for var in args ) kwargs = { name ( v . metpy . unit_array if isinstance ( v , xr . data_array ) else v ) for name , v in kwargs . items ( ) } return func ( * args , * * kwargs ) return wrapper	0
Q_159	par a comment	C_159_0	def _parse_comment ( self , structure ) if structure [ number ] = = string self . _year , self . _month , self . _day_of_month , _ , _ , _ = structure . date_time elif structure [ number ] = = string self . _parse_fields_metadata ( structure )	1
Q_159	par a comment	C_159_1	def set_show_territory ( self , state ) string string string if state = = self . _show_territory return self . _show_territory = state self . set_dirty ( )	0
Q_159	par a comment	C_159_2	def _insert_event ( self , event , force_flush = false ) if event event_document = { string { string self . _index_name , string self . _document_type } } event_values = self . _get_sanitized_event_values ( event ) self . _event_documents . append ( event_document ) self . _event_documents . append ( event_values ) self . _number_of_buffered_events + = number if force_flush or self . _number_of_buffered_events > self . _flush_interval self . _flush_events ( )	0
Q_160	stop time cpu time	C_160_0	def stop_timing ( self , profile_name ) measurements = self . _profile_measurements . get ( profile_name ) if measurements measurements . sample_stop ( ) sample = string . format ( measurements . start_sample_time , profile_name , measurements . total_cpu_time ) self . _writes_string ( sample )	1
Q_160	stop time cpu time	C_160_1	def notimplemented ( self , tup_tree ) raise cimxml_parse_error ( _format ( string string , name ( tup_tree ) ) , conn_id = self . conn_id )	0
Q_160	stop time cpu time	C_160_2	def retinotopy_comparison ( arg1 , arg2 , arg3 = none , eccentricity_range = none , polar_angle_range = none , visual_area_mask = none , weight = ellipsis , weight_min = none , visual_area = ellipsis , method = string , distance = string , gold = none ) if arg3 is not none ( obj , dsets ) = ( arg1 , [ retinotopy_data ( arg1 , var ) for var in ( arg2 , arg3 ) ] ) else ( obj , dsets ) = ( none , [ arg1 , arg2 ] ) ( gi , gold ) = ( none , false ) if not gold else ( gold _ number , true ) result = { } vis = [ as_retinotopy ( ds , string ) for ds in dsets ] ps = ( vis [ number ] [ number ] , vis [ number ] [ number ] ) es = ( vis [ number ] [ number ] , vis [ number ] [ number ] ) rs = [ ds [ string ] if string in ds else none for ds in dsets ] for ii in ( number , number ) s = string ( ii + number ) ( p , e ) = ( ps [ ii ] , es [ ii ] ) result [ string + s ] = p result [ string + s ] = e if rs [ ii ] is not none result [ string + s ] = rs [ ii ] p = np . pi / number . number * ( number . number _ p ) ( x , y ) = ( e * np . cos ( p ) , e * np . sin ( p ) ) result [ string + s ] = x result [ string + s ] = y result [ string + s ] = x + number * y n = len ( ps [ number ] ) if pimms . is_vector ( weight ) and len ( weight ) = = number ws = [ ( none if w is none else ds [ w ] if pimms . is_str ( w ) and w in ds else geo . to_property ( obj , w ) ) for ( w , ds ) in zip ( weight , dsets ) ] weight = ellipsis else ws = [ next ( ( ds [ k ] for k in ( string , string ) if k in ds ) , none ) for ds in dsets ] if pimms . is_vector ( weight , string ) wgt = weight elif pimms . is_str ( weight ) if obj is none raise value_error ( string ) wgt = geo . to_property ( obj , weight ) elif weight is ellipsis if gold wgt = ws [ gi ] elif ws [ number ] is none and ws [ number ] is none wgt = none elif ws [ number ] is none wgt = ws [ number ] elif ws [ number ] is none wgt = ws [ number ] else wgt = ws [ number ] * ws [ number ] else raise value_error ( string ) if wgt is none wgt = np . ones ( n ) if ws [ number ] is not none result [ string ] = ws [ number ] if ws [ number ] is not none result [ string ] = ws [ number ] if is_tuple ( visual_area ) and len ( visual_area ) = = number ls = [ ( none if l is none else ds [ l ] if pimms . is_str ( l ) and l in ds else geo . to_property ( obj , l ) ) for ( l , ds ) in zip ( visual_area , dsets ) ] visual_area = ellipsis else ls = [ next ( ( ds [ k ] for k in ( string , string ) if k in ds ) , none ) for ds in dsets ] if pimms . is_vector ( visual_area ) lbl = visual_area elif pimms . is_str ( visual_area ) if obj is none raise value_error ( string ) lbl = geo . to_property ( obj , visual_area ) elif visual_area is none lbl = none elif visual_area is ellipsis if gold lbl = ls [ gi ] elif ls [ number ] is none and ls [ number ] is none lbl = none elif ls [ number ] is none lbl = ls [ number ] elif ls [ number ] is none lbl = ls [ number ] else lbl = l [ number ] * ( l [ number ] = = l [ number ] ) else raise value_error ( string ) if ls [ number ] is not none result [ string ] = ls [ number ] if ls [ number ] is not none result [ string ] = ls [ number ] wgt = np . array ( wgt ) if weight_min is not none wgt [ wgt < weight_min ] = number lbl = none if lbl is none else np . array ( lbl ) if lbl is not none and visual_area_mask is not none if pimms . is_int ( visual_area_mask ) visual_area_mask = [ visual_area_mask ] oomask = ( number = = np . sum ( [ lbl = = va for va in visual_area_mask ] , axis = number ) ) wgt [ oomask ] = number lbl [ oomask ] = number if lbl is not none result [ string ] = lbl if eccentricity_range is not none er = eccentricity_range if pimms . is_real ( er ) er = ( number , er ) if gold wgt [ ( es [ gi ] < er [ number ] ) ( es [ gi ] > er [ number ] ) ] = number else wgt [ ( es [ number ] < er [ number ] ) ( es [ number ] > er [ number ] ) ( es [ number ] < er [ number ] ) ( es [ number ] > er [ number ] ) ] = number if polar_angle_range is not none pr = polar_angle_range if pimms . is_str ( pr ) pr = pr . lower ( ) if pr in [ string , string ] pr = ( number , number ) elif pr in [ string , string ] pr = ( _ number , number ) else raise value_error ( string pr ) if gold wgt [ ( ps [ gi ] < pr [ number ] ) ( ps [ gi ] > pr [ number ] ) ] = number else wgt [ ( ps [ number ] < pr [ number ] ) ( ps [ number ] > pr [ number ] ) ( ps [ number ] < pr [ number ] ) ( ps [ number ] > pr [ number ] ) ] = number result [ string ] = wgt * zinv ( np . sum ( wgt ) ) gsecc = es [ gi ] if gold else np . mean ( es , axis = number ) gsang = ps [ gi ] if gold else np . mean ( ps , axis = number ) gsrad = rs [ gi ] if gold else rs [ number ] if rs [ number ] is none else rs [ number ] if rs [ number ] is none else np . mean ( rs , axis = number ) gsecc_inv = zinv ( gsecc ) gsrad_inv = none if gsrad is none else zinv ( gsrad ) for ( tag , resprop ) in [ ( string , string ) , ( string , string ) , ( string , string ) , ( string , string ) , ( string , string ) ] serr = result [ tag + string ] _ result [ tag + string ] aerr = np . abs ( serr ) result [ resprop + string ] = serr result [ resprop + string ] = aerr result [ resprop + string ] = aerr * gsecc_inv if gsrad_inv is not none result [ resprop + string ] = aerr * gsrad_inv return pimms . itable ( result )	0
Q_161	remove duration from the instance	C_161_0	def subtract ( self , years = number , months = number , weeks = number , days = number ) return self . add ( years = _ years , months = _ months , weeks = _ weeks , days = _ days )	1
Q_161	remove duration from the instance	C_161_1	def peek ( self , session , address , width ) if width = = number return self . peek_8 ( session , address ) elif width = = number return self . peek_16 ( session , address ) elif width = = number return self . peek_32 ( session , address ) elif width = = number return self . peek_64 ( session , address ) raise value_error ( string width )	0
Q_161	remove duration from the instance	C_161_2	def sys_names ( self ) names = [ ] for var in self . samples names . extend ( var . sys_names ( ) ) return list ( set ( names ) )	0
Q_162	see the rustdocs for scheduler_fork_context for more information	C_162_0	def with_fork_context ( self , func ) res = self . _native . lib . scheduler_fork_context ( self . _scheduler , function ( self . _to_key ( func ) ) ) return self . _raise_or_return ( res )	1
Q_162	see the rustdocs for scheduler_fork_context for more information	C_162_1	def load_suffixes ( self , filename ) with open ( filename , string ) as f for var in f if len ( var . split ( string ) ) = number continue self . suffixes [ var . strip ( ) . split ( string ) [ number ] ] = var . strip ( ) . split ( string ) [ number ]	0
Q_162	see the rustdocs for scheduler_fork_context for more information	C_162_2	def save_config ( self , lookup_key , config ) with self . _config_lock self . _configs [ lookup_key ] = config	0
Q_163	return the gopath for the give target	C_163_0	def get_gopath ( self , target ) return os . path . join ( self . workdir , target . id )	1
Q_163	return the gopath for the give target	C_163_1	def run ( self , peer_table = none ) self . running = true while self . running local_inv = atlas_get_zonefile_inventory ( ) t1 = time_now ( ) self . step ( peer_table = peer_table , local_inv = local_inv , path = self . atlasdb_path ) t2 = time_now ( ) if t2 _ t1 < peer_health_neighbor_work_interval deadline = time_now ( ) + peer_health_neighbor_work_interval _ ( t2 _ t1 ) while time_now ( ) < deadline and self . running time_sleep ( self . hostport , self . _class_ . _name_ , number . number ) if not self . running break	0
Q_163	return the gopath for the give target	C_163_2	def couchdb_admin_party ( * * kwargs ) couchdb_session = couch_db ( none , none , true , * * kwargs ) couchdb_session . connect ( ) yield couchdb_session couchdb_session . disconnect ( )	0
Q_164	cluster name be use for resource group security group virtual network and subnets and	C_164_0	def _check_valid_cluster_name ( self ) p = re . compile ( string ) if len ( self . cluster_name ) < number or len ( self . cluster_name ) > number or not p . match ( self . cluster_name ) raise runtime_error ( string string string string self . cluster_name )	1
Q_164	cluster name be use for resource group security group virtual network and subnets and	C_164_1	def _load_config ( config_path = none ) config_files = [ string , string , string , string ] if isinstance ( config_path , dict ) for var in config_files config . load_config ( filename = string . format ( var ) , config_dir = config_path [ var ] , copy_default_config = false ) else for var in config_files config . load_config ( filename = string . format ( var ) , config_dir = config_path ) config_dict = config . cfg . _sections for sec , subsecs in config_dict . items ( ) for subsec , val in subsecs . items ( ) try config_dict [ sec ] [ subsec ] = float ( val ) except pass config_dict [ string ] [ string ] = datetime . datetime . strptime ( config_dict [ string ] [ string ] , string ) config_dict [ string ] [ string ] = datetime . time ( config_dict [ string ] [ string ] . hour , config_dict [ string ] [ string ] . minute ) config_dict [ string ] [ string ] = datetime . datetime . strptime ( config_dict [ string ] [ string ] , string ) config_dict [ string ] [ string ] = datetime . time ( config_dict [ string ] [ string ] . hour , config_dict [ string ] [ string ] . minute ) return config_dict	0
Q_164	cluster name be use for resource group security group virtual network and subnets and	C_164_2	def split_role_content ( role_rawsource ) parts = { string false , string none , string none } if role_rawsource . startswith ( string ) parts [ string ] = true role_rawsource = role_rawsource . lstrip ( string ) match = role_display_pattern . match ( role_rawsource ) if match parts [ string ] = match . group ( string ) . strip ( ) parts [ string ] = match . group ( string ) . strip ( ) else parts [ string ] = none parts [ string ] = role_rawsource . strip ( ) return parts	0
Q_165	this method will return the cumulative number of byte occupy by the file	C_165_0	def get_dir_size_recursively ( dir_path ) return int ( subprocess . check_output ( [ string , string , dir_path ] , env = dict ( os . environ , blocksize = string ) ) . decode ( string ) . split ( ) [ number ] ) * number	1
Q_165	this method will return the cumulative number of byte occupy by the file	C_165_1	def config ( self ) if not hasattr ( self , string ) raw_config = configparser . raw_config_parser ( ) f = self . _open ( ) if f raw_config . readfp ( f ) f . close ( ) self . _config = raw_config return self . _config	0
Q_165	this method will return the cumulative number of byte occupy by the file	C_165_2	def user_id ( self , user ) headers = { string string , string string , string string + self . domain + string , string user_agent } req = self . session . get ( string + self . domain + string , headers = headers ) . content soup = beautiful_soup ( req ) for var in soup . find ( string , cellpadding = number ) . find_all ( string ) try if ( user = = var . find_all ( string ) [ number ] . text . encode ( string ) ) return var . find ( string ) [ string ] . split ( string ) [ number ] except continue return none	0
Q_166	access the daily	C_166_0	def daily ( self ) if self . _daily is none self . _daily = daily_list ( self . _version , account_sid = self . _solution [ string ] , ) return self . _daily	1
Q_166	access the daily	C_166_1	def _parse_network_settings ( opts , current ) opts = dict ( ( k . lower ( ) , v ) for ( k , v ) in six . iteritems ( opts ) ) current = dict ( ( k . lower ( ) , v ) for ( k , v ) in six . iteritems ( current ) ) retain_settings = opts . get ( string , false ) result = current if retain_settings else { } quote_type = string valid = _config_true + _config_false if string not in opts try opts [ string ] = current [ string ] quote_type = salt . utils . stringutils . is_quoted ( opts [ string ] ) _log_default_network ( string , current [ string ] ) except value_error _raise_error_network ( string , valid ) else opts [ string ] = opts [ string ] true_val = string . format ( quote_type ) false_val = string . format ( quote_type ) networking = salt . utils . stringutils . dequote ( opts [ string ] ) if networking in valid if networking in _config_true result [ string ] = true_val elif networking in _config_false result [ string ] = false_val else _raise_error_network ( string , valid ) if string not in opts try opts [ string ] = current [ string ] _log_default_network ( string , current [ string ] ) except exception _raise_error_network ( string , [ string ] ) if opts [ string ] result [ string ] = string . format ( salt . utils . stringutils . dequote ( opts [ string ] ) , quote_type ) else _raise_error_network ( string , [ string ] ) if string in opts nozeroconf = salt . utils . stringutils . dequote ( opts [ string ] ) if nozeroconf in valid if nozeroconf in _config_true result [ string ] = true_val elif nozeroconf in _config_false result [ string ] = false_val else _raise_error_network ( string , valid ) for var in opts if var not in [ string , string , string ] result [ var ] = string . format ( salt . utils . stringutils . dequote ( opts [ var ] ) , quote_type ) return result	0
Q_166	access the daily	C_166_2	def open_config ( self , type = string ) try output = self . dev . rpc ( string . format ( type ) ) except exception as err print err	0
Q_167	build an instance of deployment_instance	C_167_0	def get_instance ( self , payload ) return deployment_instance ( self . _version , payload , service_sid = self . _solution [ string ] , environment_sid = self . _solution [ string ] , )	1
Q_167	build an instance of deployment_instance	C_167_1	def show_message ( message , msg_type = none , text_color = none , back_color = none , scroll_speed = number . number ) text_color = text_color or [ number , number , number ] back_color = back_color or [ number , number , number ] color_by_type = { string [ number , number , number ] , string [ number , number , number ] , string [ number , number , number ] , string [ number , number , number ] } if msg_type in color_by_type text_color = color_by_type [ msg_type ] _sensehat . show_message ( message , scroll_speed , text_color , back_color ) return { string message }	0
Q_167	build an instance of deployment_instance	C_167_2	def stopwatch_now ( ) if six . py2 now = time . time ( ) else now = time . monotonic ( ) return now	0
Q_168	access the origination_urls	C_168_0	def origination_urls ( self ) if self . _origination_urls is none self . _origination_urls = origination_url_list ( self . _version , trunk_sid = self . _solution [ string ] , ) return self . _origination_urls	1
Q_168	access the origination_urls	C_168_1	def clear_data ( self ) self . _counts = np . zeros_like ( self . _bins ) self . histo . set_opts ( height = self . _counts )	0
Q_168	access the origination_urls	C_168_2	def get_mask ( self , tag = none , var = none ) if tag is not none and var is not none raise type_error ( string string var string tag string ) selected_fields = self . _select_by_field_or_tag ( tag , var ) mask = number for var , var in iteritems ( selected_fields ) if var . length is none or var . start_at is none raise value_error ( string { } string . format ( var ) ) mask = ( ( number < < var . length ) _ number ) < < var . start_at return mask	0
Q_169	generate an instance context for the instance the context be capable of	C_169_0	def _proxy ( self ) if self . _context is none self . _context = function_version_context ( self . _version , service_sid = self . _solution [ string ] , function_sid = self . _solution [ string ] , sid = self . _solution [ string ] , ) return self . _context	1
Q_169	generate an instance context for the instance the context be capable of	C_169_1	def dump ( raw_data , output_file ) data = encode_output ( raw_data , output_file ) output_file . write ( data )	0
Q_169	generate an instance context for the instance the context be capable of	C_169_2	def download_file_from_url ( self , url ) fullurl = self . base_url + url try urlobj = urllib2 . urlopen ( fullurl ) contents = urlobj . read ( ) except urllib2 . http_error , e self . printd ( string , e . code , url ) return none except urllib2 . url_error , e self . printd ( string , e . code , url ) return none self . printd ( string s string ( fullurl , len ( contents ) ) ) return contents	0
Q_170	run a command on the currently active container	C_170_0	def command ( self , command ) return self . _conn . command ( string { } string . format ( self . id , command ) )	1
Q_170	run a command on the currently active container	C_170_1	def get_power ( self ) value = self . box . homeautoswitch ( string , self . actor_id ) return int ( value ) if value . isdigit ( ) else none	0
Q_170	run a command on the currently active container	C_170_2	def _add_handler_factory ( self , svc_ref ) with self . _handlers_lock handler_id = svc_ref . get_property ( handlers_const . prop_handler_id ) if handler_id in self . _handlers _logger . warning ( string , handler_id ) else self . _handlers_refs . add ( svc_ref ) self . _handlers [ handler_id ] = self . _context . get_service ( svc_ref ) succeeded = set ( ) for ( var , ( context , instance ) , ) in self . _waiting_handlers . items ( ) if self . _try_instantiate ( context , instance ) succeeded . add ( var ) for var in succeeded del self . _waiting_handlers [ var ]	0
Q_171	send a post request and parse the json response potentially contain	C_171_0	def multipart_parse_json ( api_url , data ) headers = { string string } response_text = requests . post ( api_url , data = data , headers = headers ) . text . encode ( string , errors = string ) return json . loads ( response_text . decode ( ) )	1
Q_171	send a post request and parse the json response potentially contain	C_171_1	"def create_doc ( self ) doc_node = uri_ref ( "" http / / www . spdx . org / tools self . graph . add ( ( doc_node , rdf . type , self . spdx_namespace . spdx_document ) ) vers_literal = literal ( str ( self . document . version ) ) self . graph . add ( ( doc_node , self . spdx_namespace . spec_version , vers_literal ) ) data_lics = uri_ref ( self . document . data_license . url ) self . graph . add ( ( doc_node , self . spdx_namespace . data_license , data_lics ) ) doc_name = uri_ref ( self . document . name ) self . graph . add ( ( doc_node , self . spdx_namespace . name , doc_name ) ) return doc_node"	0
Q_171	send a post request and parse the json response potentially contain	C_171_2	def _load_scpd ( self , service_type , timeout ) if service_type not in self . _device_service_definitions . keys ( ) raise value_error ( string + service_type ) if string not in self . _device_service_definitions [ service_type ] . keys ( ) raise value_error ( string + service_type ) self . _device_scpd . pop ( service_type , none ) uri = self . _device_service_definitions [ service_type ] [ string ] proxies = { } if self . _https_proxy proxies = { string self . _https_proxy } if self . _http_proxy proxies = { string self . _http_proxy } auth = none if self . _password auth = http_digest_auth ( self . _username , self . _password ) location = self . _protocol + string + self . _hostname + string + str ( self . port ) + uri headers = { string string } request = requests . get ( location , auth = auth , proxies = proxies , headers = headers , timeout = timeout , verify = self . _verify ) if request . status_code = number error_str = device_tr64 . _extract_error_string ( request ) raise value_error ( string string string + location + string + str ( request . status_code ) + string + request . reason + string + error_str ) data = request . text . encode ( string ) if len ( data ) = = number return try root = et . fromstring ( data ) except exception as e raise value_error ( string string string string string + str ( e ) ) actions = { } variable_types = { } variable_parameter_dict = { } for var in root . getchildren ( ) tag_name = var . tag . lower ( ) if tag_name . endswith ( string ) self . _parse_scpd_actions ( var , actions , variable_parameter_dict ) elif tag_name . endswith ( string ) self . _parse_scpd_variable_types ( var , variable_types ) for name in variable_parameter_dict . keys ( ) if name not in variable_types . keys ( ) raise value_error ( string + name ) for argument in variable_parameter_dict [ name ] argument [ string ] = variable_types [ name ] [ string ] if string in variable_types [ name ] . keys ( ) argument [ string ] = variable_types [ name ] [ string ] self . _device_scpd [ service_type ] = actions	0
Q_172	a function which determine whether or not to retry	C_172_0	def _should_retry ( self , context ) if context . count > = self . max_attempts return false status = none if context . response and context . response . status status = context . response . status if status is none return true elif number < = status < number return true elif number < = status < number if status = = number and context . location_mode = = location_mode . secondary return true if status = = number return true return false elif status > = number if status = = number or status = = number return false return true else return true	1
Q_172	a function which determine whether or not to retry	C_172_1	def _tls_auth_encrypt ( self , s ) wcs = self . tls_session . wcs write_seq_num = struct . pack ( string , wcs . seq_num ) wcs . seq_num + = number return wcs . cipher . auth_encrypt ( s , b string , write_seq_num )	0
Q_172	a function which determine whether or not to retry	C_172_2	def report_idle_after ( seconds ) def decorator ( func ) def wrapper ( * args , * * kwargs ) def _handle_timeout ( signum , frame ) config = get_config ( ) if not config . ready config . load ( ) message = { string string , string idle_template . format ( app_id = config . get ( string ) , minutes_so_far = round ( seconds / number ) ) , } log ( string ) get_messenger ( config ) . send ( message ) signal . signal ( signal . sigalrm , _handle_timeout ) signal . alarm ( seconds ) try result = func ( * args , * * kwargs ) finally signal . alarm ( number ) return result return wraps ( func ) ( wrapper ) return decorator	0
Q_173	load and parse the configuration file use pyyaml	C_173_0	"def load ( self , config_path = none ) self . _reset_config ( ) if not config_path config_path = os . path . join ( appdirs . user_config_dir ( self . _app_name ) , self . _config_file ) try if hasattr ( config_path , string ) config = yaml . safe_load ( config_path ) else with open ( config_path , string ) as config_file config = yaml . safe_load ( config_file ) except environment_error raise exceptions . config_not_found_error ( string t read config at { config_path } string error parsing yaml string printer string printer string type string printer type string is invalid "" . format ( printer_name = self . _printer_name , ) ) self . _has_loaded = true"	1
Q_173	load and parse the configuration file use pyyaml	C_173_1	def parse_frequencies ( variant , transcripts ) frequencies = { } thousand_genomes_keys = [ string ] thousand_genomes_max_keys = [ string ] exac_keys = [ string ] exac_max_keys = [ string , string ] gnomad_keys = [ string , string ] gnomad_max_keys = [ string , string ] for var in thousand_genomes_keys thousand_g = parse_frequency ( variant , var ) if thousand_g frequencies [ string ] = thousand_g break for var in thousand_genomes_max_keys thousand_g_max = parse_frequency ( variant , var ) if thousand_g_max frequencies [ string ] = thousand_g_max break for var in exac_keys exac = parse_frequency ( variant , var ) if exac frequencies [ string ] = exac break for var in exac_max_keys exac_max = parse_frequency ( variant , var ) if exac_max frequencies [ string ] = exac_max break for var in gnomad_keys gnomad = parse_frequency ( variant , var ) if gnomad frequencies [ string ] = gnomad break for var in gnomad_max_keys gnomad_max = parse_frequency ( variant , var ) if gnomad_max frequencies [ string ] = gnomad_max break if not frequencies for transcript in transcripts exac = transcript . get ( string ) exac_max = transcript . get ( string ) thousand_g = transcript . get ( string ) thousandg_max = transcript . get ( string ) gnomad = transcript . get ( string ) gnomad_max = transcript . get ( string ) if exac frequencies [ string ] = exac if exac_max frequencies [ string ] = exac_max if thousand_g frequencies [ string ] = thousand_g if thousandg_max frequencies [ string ] = thousandg_max if gnomad frequencies [ string ] = gnomad if gnomad_max frequencies [ string ] = gnomad_max thousand_g_left = parse_frequency ( variant , string ) if thousand_g_left frequencies [ string ] = thousand_g_left thousand_g_right = parse_frequency ( variant , string ) if thousand_g_right frequencies [ string ] = thousand_g_right return frequencies	0
Q_173	load and parse the configuration file use pyyaml	C_173_2	def flatatt ( self , * * attr ) cs = string attr = self . _attr classes = self . _classes data = self . _data css = self . _css attr = attr . copy ( ) if attr else { } if classes cs = string . join ( classes ) attr [ string ] = cs if css attr [ string ] = string . join ( ( string ( var , var ) for var , var in css . items ( ) ) ) if data for var , var in data . items ( ) attr [ string var ] = dump_data_value ( var ) if attr return string . join ( attr_iter ( attr ) ) else return string	0
Q_174	return priority for a give key object	C_174_0	def _dict_key_priority ( s ) if isinstance ( s , hook ) return _priority ( s . _schema ) _ number . number if isinstance ( s , optional ) return _priority ( s . _schema ) + number . number return _priority ( s )	1
Q_174	return priority for a give key object	C_174_1	def enable_scanners_by_group ( self , group ) if group = = string self . logger . debug ( string ) return self . zap . ascan . enable_all_scanners ( ) try scanner_list = self . scanner_group_map [ group ] except key_error raise zap_error ( string { number } string . format ( group , string . join ( self . scanner_groups ) ) ) self . logger . debug ( string . format ( group ) ) return self . enable_scanners_by_ids ( scanner_list )	0
Q_174	return priority for a give key object	C_174_2	def _head_length ( self , port ) if not port return number . parent_state_v = self . get_parent_state_v ( ) if parent_state_v is port . parent return port . port_size [ number ] return max ( port . port_size [ number ] * number . number , self . _calc_line_width ( ) / number . number )	0
Q_175	return the log likelihood ratio marginalize over time	C_175_0	def _margtime_loglr ( self , mf_snr , opt_snr ) return special . logsumexp ( mf_snr , b = self . _deltat ) _ number . number * opt_snr	1
Q_175	return the log likelihood ratio marginalize over time	C_175_1	def find_notes ( freq_table , max_note = number ) res = [ number ] * number n = note ( ) for ( freq , ampl ) in freq_table if freq > number and ampl > number f = _find_log_index ( freq ) if f < max_note res [ f ] + = ampl else res [ number ] + = ampl return [ ( note ( ) . from_int ( x ) if x < number else none , n ) for ( x , n ) in enumerate ( res ) ]	0
Q_175	return the log likelihood ratio marginalize over time	C_175_2	def _set_fields ( self ) self . fields = [ ] self . required_input = [ ] for var , var in inspect . getmembers ( self . _class_ ) if inspect . isdatadescriptor ( var ) and not var . startswith ( string ) self . fields . append ( var ) if var . required_input self . required_input . append ( var )	0
Q_176	read channel from losc data	C_176_0	"def read_frame_losc ( channels , start_time , end_time ) from pycbc . frame import read_frame if not isinstance ( channels , list ) channels = [ channels ] ifos = [ var [ number number ] for var in channels ] urls = { } for ifo in ifos urls [ ifo ] = losc_frame_urls ( ifo , start_time , end_time ) if len ( urls [ ifo ] ) = = number raise value_error ( string string t produce a time series "" ifo ) fnames = { ifo [ ] for ifo in ifos } for ifo in ifos for url in urls [ ifo ] fname = download_file ( url , cache = true ) fnames [ ifo ] . append ( fname ) ts = [ read_frame ( fnames [ channel [ number number ] ] , channel , start_time = start_time , end_time = end_time ) for channel in channels ] if len ( ts ) = = number return ts [ number ] else return ts"	1
Q_176	read channel from losc data	C_176_1	def to_dict ( self ) raw_conditions = [ ] for var in self . conditions if isinstance ( var , ( condition , group ) ) raw_conditions . append ( var . to_dict ( ) ) else raw_conditions . append ( var ) return { self . operator raw_conditions }	0
Q_176	read channel from losc data	C_176_2	def copy ( self ) wcnf = wcnf ( ) wcnf . nv = self . nv wcnf . topw = self . topw wcnf . hard = copy . deepcopy ( self . hard ) wcnf . soft = copy . deepcopy ( self . soft ) wcnf . wght = copy . deepcopy ( self . wght ) wcnf . comments = copy . deepcopy ( self . comments ) return wcnf	0
Q_177	format the sql_storage_update_settings object remove argument that be empty	C_177_0	def format_sql_storage_update_settings ( result ) from collections import ordered_dict order_dict = ordered_dict ( ) if result . disk_count is not none order_dict [ string ] = result . disk_count if result . disk_configuration_type is not none order_dict [ string ] = result . disk_configuration_type return order_dict	1
Q_177	format the sql_storage_update_settings object remove argument that be empty	C_177_1	def destroy ( self ) if self . library return self . library . par_destroy ( ctypes . byref ( self . pointer ) )	0
Q_177	format the sql_storage_update_settings object remove argument that be empty	C_177_2	def decode_bytes ( f ) buf = f . read ( field_u16 . size ) if len ( buf ) < field_u16 . size raise underflow_decode_error ( ) ( num_bytes , ) = field_u16 . unpack_from ( buf ) num_bytes_consumed = field_u16 . size + num_bytes buf = f . read ( num_bytes ) if len ( buf ) < num_bytes raise underflow_decode_error ( ) return num_bytes_consumed , buf	0
Q_178	synchronously refresh table metadata	C_178_0	def refresh_table_metadata ( self , keyspace , table , max_schema_agreement_wait = none ) if not self . control_connection . refresh_schema ( target_type = schema_target_type . table , keyspace = keyspace , table = table , schema_agreement_wait = max_schema_agreement_wait , force = true ) raise driver_exception ( string )	1
Q_178	synchronously refresh table metadata	C_178_1	def get_client ( self ) if self . _ssh is none self . _connect ( ) return self . _ssh else try chan = self . _ssh . get_transport ( ) . open_session ( ) except ( socket . error , paramiko . ssh_exception ) logger . warning ( string ) self . _ssh . close ( ) self . _connect ( ) else chan . close ( ) return self . _ssh	0
Q_178	synchronously refresh table metadata	C_178_2	def scalar_leaf_only ( operator ) def decorator ( f ) wraps ( f ) def wrapper ( filter_operation_info , context , parameters , * args , * * kwargs ) if string in kwargs current_operator = kwargs [ string ] else current_operator = operator if not is_leaf_type ( filter_operation_info . field_type ) raise graph_ql_compilation_error ( u string { } string u string . format ( current_operator , filter_operation_info ) ) return f ( filter_operation_info , context , parameters , * args , * * kwargs ) return wrapper return decorator	0
Q_179	when user log in update the access_log relate to the user	C_179_0	def user_logged_in ( self , sender , request axes_http_request , user , * * kwargs ) username = user . get_username ( ) credentials = get_credentials ( username ) client_str = get_client_str ( username , request . axes_ip_address , request . axes_user_agent , request . axes_path_info ) log . info ( string , client_str ) if settings . axes_reset_on_success cache_key = get_client_cache_key ( request , credentials ) failures_since_start = self . cache . get ( cache_key , default = number ) self . cache . delete ( cache_key ) log . info ( string , failures_since_start , client_str )	1
Q_179	when user log in update the access_log relate to the user	C_179_1	def autogen_argparse_block ( extra_args = [ ] ) grouped_args = [ ] for var in _registered_args_ argstr_list , type_ , default , help_ = var argstr_set = set ( argstr_list ) found = false for index , ( keyset , vals ) in enumerate ( grouped_args ) if len ( keyset . intersection ( argstr_set ) ) > number keyset . update ( argstr_set ) vals . append ( var ) found = true break if not found new_keyset = argstr_set new_vals = [ var ] grouped_args . append ( ( new_keyset , new_vals ) ) multi_groups = [ ] for keyset , vals in grouped_args if len ( vals ) > number multi_groups . append ( vals ) if len ( multi_groups ) > number import utool as ut print ( string ) print ( ut . repr4 ( multi_groups , newlines = number ) )	0
Q_179	when user log in update the access_log relate to the user	C_179_2	def show_system_monitor_output_switch_status_switch_state_reason ( self , * * kwargs ) config = et . element ( string ) show_system_monitor = et . element ( string ) config = show_system_monitor output = et . sub_element ( show_system_monitor , string ) switch_status = et . sub_element ( output , string ) switch_state_reason = et . sub_element ( switch_status , string ) switch_state_reason . text = kwargs . pop ( string ) callback = kwargs . pop ( string , self . _callback ) return callback ( config )	0
Q_180	"check whether we "" re enable"	C_180_0	def enabled ( self ) if self . _enabled is none if self . parent is not none and self . parent . enabled ( ) self . _enabled = true else self . _enabled = getattr ( self . options , self . dest , true ) return self . _enabled	1
Q_180	"check whether we "" re enable"	C_180_1	def determine_length ( length ) integer = ord ( length ) if integer < number return number elif integer < number return number elif integer < number return number elif integer < number return number else raise connection_error ( string . format ( length ) )	0
Q_180	"check whether we "" re enable"	C_180_2	def detect_terminal ( _environ = os . environ ) if _environ . get ( string ) return string elif subdict_by_key_prefix ( _environ , string ) return string elif _environ . get ( string ) . startswith ( string ) return _environ [ string ] elif _environ . get ( string ) return _environ [ string ] else return _environ . get ( string )	0
Q_181	check available plugins and attempt to import them	C_181_0	def load ( ) plugins = [ ] for var in os . listdir ( plugins_path ) if not var . endswith ( string ) or var . startswith ( string ) continue if not os . path . isfile ( os . path . join ( plugins_path , var ) ) continue plugin = var [ _ number ] if plugin in failed_plugins continue try _import_ ( plugins . _name_ , { } , { } , [ plugin ] ) plugins . append ( plugin ) log . debug ( string . format ( plugin ) ) except ( import_error , syntax_error ) as error message = string . format ( plugin , error ) if config ( ) . sections ( kind = plugin ) log . warn ( message ) else log . debug ( message ) failed_plugins . append ( plugin ) return plugins	1
Q_181	check available plugins and attempt to import them	C_181_1	def grow_synapses ( self , segments , active_inputs_by_source , initial_permanence ) for var , var in self . connections_by_source . iteritems ( ) var . grow_synapses ( segments , active_inputs_by_source [ var ] , initial_permanence )	0
Q_181	check available plugins and attempt to import them	C_181_2	def objective ( self , objective ) if objective is not none and len ( objective ) > number raise value_error ( string ) if objective is not none and len ( objective ) < number raise value_error ( string ) self . _objective = objective	0
Q_182	overcome bug in gnu readline in relation to calculation of prompt length in presence of ansi escape code	C_182_0	def rl_make_safe_prompt ( prompt str ) _ > str if rl_type = = rl_type . gnu start = string end = string escaped = false result = string for var in prompt if var = = string and not escaped result + = start + var escaped = true elif var . isalpha ( ) and escaped result + = var + end escaped = false else result + = var return result else return prompt	1
Q_182	overcome bug in gnu readline in relation to calculation of prompt length in presence of ansi escape code	C_182_1	def nearest ( self , idx ) hi = self . after ( idx ) lo = self . before ( idx ) if hi is none return lo if lo is none return hi if abs ( hi _ idx ) < abs ( lo _ idx ) return hi return lo	0
Q_182	overcome bug in gnu readline in relation to calculation of prompt length in presence of ansi escape code	C_182_2	def on_patch ( self , req , resp , handler = none , * * kwargs ) self . handle ( handler or self . create_bulk , req , resp , * * kwargs ) resp . status = falcon . http_created	0
Q_183	grab filename and enable it to be read	C_183_0	def read_file ( self ) with open ( self . filename , mode = string , encoding = string ) as text_file self . raw_file = text_file . read ( ) self . file_lines = [ var . rstrip ( ) for var in self . raw_file . splitlines ( ) ]	1
Q_183	grab filename and enable it to be read	C_183_1	def get_pwm_frequency ( self , device = default_device_id , message = true ) return self . _get_pwm_frequency ( device , message )	0
Q_183	grab filename and enable it to be read	C_183_2	def run_experiment ( methods , data , n_classes , true_labels , n_runs = number , use_purity = true , use_nmi = false , use_ari = false , use_nne = false , consensus = false ) results = [ ] names = [ ] clusterings = { } other_results = { } other_results [ string ] = { } other_results [ string ] = { } if use_purity purity_method = purity elif use_nmi purity_method = nmi elif use_ari purity_method = ari elif use_nne purity_method = nne for var in range ( n_runs ) print ( string . format ( var ) ) purities = [ ] r = number method_index = number for preproc , cluster in methods t0 = time . time ( ) if isinstance ( preproc , preprocess ) preprocessed , ll = preproc . run ( data ) output_names = preproc . output_names else p1 = data output_names = [ string ] for p in preproc p1 , ll = p . run ( p1 ) p1 = p1 [ number ] if output_names [ number ] = string output_names [ number ] = output_names [ number ] + string + p . output_names [ number ] else output_names [ number ] = p . output_names [ number ] preprocessed = [ p1 ] t1 = time . time ( ) _ t0 for name , pre in zip ( output_names , preprocessed ) starting_index = method_index if isinstance ( cluster , cluster ) t0 = time . time ( ) labels = cluster . run ( pre ) t2 = t1 + time . time ( ) _ t0 if use_nne purities . append ( purity_method ( pre , true_labels ) ) else purities . append ( purity_method ( labels , true_labels ) ) if var = = number names . append ( name + string + cluster . name ) clusterings [ names [ _ number ] ] = [ ] other_results [ string ] [ names [ _ number ] ] = [ ] print ( names [ r ] ) clusterings [ names [ r ] ] . append ( labels ) print ( string + str ( t2 ) ) other_results [ string ] [ names [ r ] ] . append ( t2 ) print ( purities [ _ number ] ) r + = number method_index + = number elif type ( cluster ) = = list for c in cluster if isinstance ( c , list ) t2 = t1 name2 = name sub_data = pre . copy ( ) for subproc in c [ _ number ] t0 = time . time ( ) subproc_out , ll = subproc . run ( sub_data ) sub_data = subproc_out [ number ] name2 = name2 + string + subproc . output_names [ number ] t2 + = time . time ( ) _ t0 t0 = time . time ( ) labels = c [ _ number ] . run ( sub_data ) t2 + = time . time ( ) _ t0 if use_nne purities . append ( purity_method ( sub_data , true_labels ) ) else purities . append ( purity_method ( labels , true_labels ) ) if var = = number names . append ( name2 + string + c [ _ number ] . name ) clusterings [ names [ _ number ] ] = [ ] other_results [ string ] [ names [ _ number ] ] = [ ] print ( names [ r ] ) clusterings [ names [ r ] ] . append ( labels ) other_results [ string ] [ names [ r ] ] . append ( t2 ) print ( string + str ( t2 ) ) print ( purities [ _ number ] ) r + = number method_index + = number else try t0 = time . time ( ) labels = c . run ( pre ) t2 = t1 + time . time ( ) _ t0 if var = = number names . append ( name + string + c . name ) clusterings [ names [ _ number ] ] = [ ] other_results [ string ] [ names [ _ number ] ] = [ ] if use_nne purities . append ( purity_method ( pre , true_labels ) ) else purities . append ( purity_method ( labels , true_labels ) ) print ( names [ r ] ) clusterings [ names [ r ] ] . append ( labels ) other_results [ string ] [ names [ r ] ] . append ( t2 ) print ( string + str ( t2 ) ) print ( purities [ _ number ] ) r + = number method_index + = number except print ( string ) num_clustering_results = method_index _ starting_index clustering_results = purities [ _ num_clustering_results ] if var > number and len ( clustering_results ) > number old_clustering_results = results [ _ number ] [ starting_index method_index ] if max ( old_clustering_results ) < max ( clustering_results ) other_results [ string ] [ name ] = pre else other_results [ string ] [ name ] = pre print ( string . join ( names ) ) print ( string + string . join ( map ( str , purities ) ) ) results . append ( purities ) consensus_purities = [ ] if consensus other_results [ string ] = { } k = len ( np . unique ( true_labels ) ) for name , clusts in clusterings . items ( ) print ( name ) clusts = np . vstack ( clusts ) consensus_clust = ce . cluster_ensembles ( clusts , verbose = false , n_clusters_max = k ) other_results [ string ] [ name ] = consensus_clust if use_purity consensus_purity = purity ( consensus_clust . flatten ( ) , true_labels ) print ( string + str ( consensus_purity ) ) consensus_purities . append ( consensus_purity ) if use_nmi consensus_nmi = nmi ( true_labels , consensus_clust ) print ( string + str ( consensus_nmi ) ) consensus_purities . append ( consensus_nmi ) if use_ari consensus_ari = ari ( true_labels , consensus_clust ) print ( string + str ( consensus_ari ) ) consensus_purities . append ( consensus_ari ) print ( string + string . join ( map ( str , consensus_purities ) ) ) other_results [ string ] = clusterings return results , names , other_results	0
Q_184	assert that lst contain list data and be not structure	C_184_0	def _assert_data_is_list ( key , lst ) if not isinstance ( lst , list ) and not isinstance ( lst , tuple ) raise not_a_list_error ( string key ) for var in lst if not isinstance ( var , str ) raise element_not_a_string_error ( string , ( var , lst ) )	1
Q_184	assert that lst contain list data and be not structure	C_184_1	def archive_history_item ( item , destination , no_color ) log_src , description = split_history_item ( item . strip ( ) ) log_dest = os . path . sep . join ( log_src . rsplit ( os . path . sep , number ) [ number ] ) results_src = log_src . rsplit ( string , number ) [ number ] + string results_dest = log_dest . rsplit ( string , number ) [ number ] + string destination_path = os . path . join ( destination , log_dest ) log_dir = os . path . dirname ( destination_path ) try if not os . path . isdir ( log_dir ) os . makedirs ( log_dir ) shutil . copyfile ( log_src , destination_path ) shutil . copyfile ( results_src , os . path . join ( destination , results_dest ) ) except exception as error echo_style ( string error , no_color , fg = string ) sys . exit ( number ) else update_history_log ( os . path . join ( destination , string ) , description = description , test_log = log_dest )	0
Q_184	assert that lst contain list data and be not structure	C_184_2	def on_parent_exit ( signame ) signum = getattr ( signal , signame ) def set_parent_exit_signal ( ) result = cdll [ string ] . prctl ( pr_set_pdeathsig , signum ) if result = number raise pr_ctl_error ( string result ) return set_parent_exit_signal	0
Q_185	create an element from inp or from scratch	C_185_0	def element ( self , inp = none , order = none , * * kwargs ) if inp is none return self . element_type ( self , self . tspace . element ( order = order ) ) elif inp in self and order is none return inp elif inp in self . tspace and order is none return self . element_type ( self , inp ) elif callable ( inp ) vectorized = kwargs . pop ( string , true ) inp_elem = self . fspace . element ( inp , vectorized = vectorized ) sampled = self . sampling ( inp_elem , * * kwargs ) return self . element_type ( self , self . tspace . element ( sampled , order = order ) ) else return self . element_type ( self , self . tspace . element ( inp , order = order ) )	1
Q_185	create an element from inp or from scratch	C_185_1	def get_batch ( self , batch_size , next_states = false ) if batch_size > len ( self . observations ) raise tensor_force_error ( string ) states = { var np . zeros ( ( batch_size , ) + tuple ( var [ string ] ) , dtype = util . np_dtype ( var [ string ] ) ) for var , var in self . states_spec . items ( ) } internals = [ np . zeros ( ( batch_size , ) + shape , dtype ) for shape , dtype in self . internals_spec ] actions = { var np . zeros ( ( batch_size , ) + tuple ( action [ string ] ) , dtype = util . np_dtype ( action [ string ] ) ) for var , action in self . actions_spec . items ( ) } terminal = np . zeros ( ( batch_size , ) , dtype = util . np_dtype ( string ) ) reward = np . zeros ( ( batch_size , ) , dtype = util . np_dtype ( string ) ) if next_states next_states = { var np . zeros ( ( batch_size , ) + tuple ( var [ string ] ) , dtype = util . np_dtype ( var [ string ] ) ) for var , var in self . states_spec . items ( ) } next_internals = [ np . zeros ( ( batch_size , ) + shape , dtype ) for shape , dtype in self . internals_spec ] unseen_indices = list ( xrange ( self . none_priority_index + self . observations . _capacity _ number , len ( self . observations ) + self . observations . _capacity _ number ) ) self . batch_indices = unseen_indices [ batch_size ] remaining = batch_size _ len ( self . batch_indices ) if remaining samples = self . observations . sample_minibatch ( remaining ) sample_indices = [ i for i , o in samples ] self . batch_indices + = sample_indices np . random . shuffle ( self . batch_indices ) for n , index in enumerate ( self . batch_indices ) observation , _ = self . observations . _memory [ index ] for var , var in states . items ( ) var [ n ] = observation [ number ] [ var ] for k , internal in enumerate ( internals ) internal [ n ] = observation [ number ] [ k ] for var , action in actions . items ( ) action [ n ] = observation [ number ] [ var ] terminal [ n ] = observation [ number ] reward [ n ] = observation [ number ] if next_states for var , next_state in next_states . items ( ) next_state [ n ] = observation [ number ] [ var ] for k , next_internal in enumerate ( next_internals ) next_internal [ n ] = observation [ number ] [ k ] if next_states return dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next_states = next_states , next_internals = next_internals ) else return dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )	0
Q_185	create an element from inp or from scratch	C_185_2	def export_records ( self , records = none , fields = none , forms = none , events = none , raw_or_label = string , event_name = string , format = string , export_survey_fields = false , export_data_access_groups = false , df_kwargs = none , export_checkbox_labels = false , filter_logic = none ) ret_format = format if format = = string from pandas import read_csv ret_format = string pl = self . _basepl ( string , format = ret_format ) fields = self . backfill_fields ( fields , forms ) keys_to_add = ( records , fields , forms , events , raw_or_label , event_name , export_survey_fields , export_data_access_groups , export_checkbox_labels ) str_keys = ( string , string , string , string , string , string , string , string , string ) for var , var in zip ( str_keys , keys_to_add ) if var if var in ( string , string , string , string ) pl [ var ] = string . join ( var ) else pl [ var ] = var if filter_logic pl [ string ] = filter_logic response , _ = self . _call_api ( pl , string ) if format in ( string , string , string ) return response elif format = = string if not df_kwargs if self . is_longitudinal ( ) df_kwargs = { string [ self . def_field , string ] } else df_kwargs = { string self . def_field } buf = string_io ( response ) df = read_csv ( buf , * * df_kwargs ) buf . close ( ) return df	0
Q_186	get an annotate list from a tree branch	C_186_0	def get_annotated_list ( cls , parent = none , max_depth = none ) result , info = [ ] , { } start_depth , prev_depth = ( none , none ) qs = cls . get_tree ( parent ) if max_depth qs = qs . filter ( depth_lte = max_depth ) return cls . get_annotated_list_qs ( qs )	1
Q_186	get an annotate list from a tree branch	C_186_1	def _clean ( self , seed ) seed = deepcopy ( seed ) self . _inherit ( * array ( get ( seed , string , [ ] ) ) ) if string in seed self . arguments = merge ( self . arguments , seed . pop ( string ) ) elif string in seed self . arguments = seed . pop ( string ) self . seeds . append ( seed )	0
Q_186	get an annotate list from a tree branch	C_186_2	def outer_product ( vec ) r return ( np . dot ( vec [ , np . newaxis ] , vec [ np . newaxis , ] ) if len ( vec . shape ) = = number else np . dot ( vec , vec . t ) )	0
Q_187	put the host in maintenance mode	C_187_0	def enter_maintenance_mode ( self ) cmd = self . _cmd ( string ) if cmd . success self . _update ( get_host ( self . _get_resource_root ( ) , self . host_id ) ) return cmd	1
Q_187	put the host in maintenance mode	C_187_1	def _create_examples ( self , lines , set_type ) examples = [ ] for ( i , line ) in enumerate ( lines ) if i = = number continue guid = string ( set_type , i ) text_a = line [ number ] text_b = line [ number ] label = line [ number ] examples . append ( input_example ( guid = guid , text_a = text_a , text_b = text_b , label = label ) ) return examples	0
Q_187	put the host in maintenance mode	C_187_2	def fetch_url_as_json ( fetch_url , default_value = none ) if default_value is none default_value = dict ( ) log . debug ( string , fetch_url ) ret = default_value start = time . time ( ) http_response = yield tornado . httpclient . async_http_client ( ) . fetch ( fetch_url ) if http_response . error log . error ( string , fetch_url , http_response . error ) raise tornado . gen . return ( ret ) response = json . loads ( http_response . body ) if not string in response log . error ( string , fetch_url ) raise tornado . gen . return ( ret ) ret = response [ string ] execution = number * response [ string ] end = time . time ( ) duration = number * ( end _ start ) log . debug ( string , execution , fetch_url ) log . debug ( string , duration , fetch_url ) raise tornado . gen . return ( ret )	0
Q_188	display the clipboard content	C_188_0	def show_buffer ( pymux , variables ) text = get_app ( ) . clipboard . get_data ( ) . text pymux . get_client_state ( ) . layout_manager . display_popup ( string , text )	1
Q_188	display the clipboard content	C_188_1	def current_item_changed ( self , _current_index = none , _previous_index = none ) self . editor . clear ( ) self . editor . set_text_color ( qcolor_regular ) reg_item = self . get_current_reg_item ( ) if reg_item is none return if self . _import_on_select and reg_item . successfully_imported is none self . import_reg_item ( reg_item ) if reg_item . successfully_imported is none self . editor . set_text_color ( qcolor_not_imported ) self . editor . set_plain_text ( string ) elif reg_item . successfully_imported is false self . editor . set_text_color ( qcolor_error ) self . editor . set_plain_text ( str ( reg_item . exception ) ) elif reg_item . description_html self . editor . set_html ( reg_item . description_html ) else self . editor . set_plain_text ( reg_item . doc_string )	0
Q_188	display the clipboard content	C_188_2	def shutdown ( self ) logger . debug ( string ) start_time = time . time ( ) self . _stop . set ( ) self . _worker . join ( ) logger . debug ( string , time . time ( ) _ start_time )	0
Q_189	set the docstring	C_189_0	def set_docstring ( self , loc , tokens ) internal_assert ( len ( tokens ) = = number , string , tokens ) self . docstring = self . reformat ( tokens [ number ] ) + string return tokens [ number ]	1
Q_189	set the docstring	C_189_1	def potential_from_grid ( self , grid ) eta = ( number . number / self . scale_radius ) * self . grid_to_grid_radii ( grid ) + number return np . real ( number . number * self . scale_radius * self . kappa_s * self . potential_func_sph ( eta ) )	0
Q_189	set the docstring	C_189_2	def to_xml ( self ) if not len ( self . lines ) raise group_error ( string ) doc = document ( ) root = doc . create_element ( string ) super ( group , self ) . to_xml ( root ) self . _create_text_node ( root , string , self . name , true ) self . _create_text_node ( root , string , self . description , true ) lines = doc . create_element ( string ) root . append_child ( lines ) for var in self . _lines if not issubclass ( var . _class_ , line ) raise group_error ( string string ( var . _class_ . _name_ , line . _name_ ) ) lines . append_child ( var . to_xml ( ) ) return root	0
Q_190	fetch all transfer entity	C_190_0	def all ( self , data = { } , * * kwargs ) if string in data url = string . format ( data [ string ] ) del data [ string ] return self . get_url ( url , data , * * kwargs ) return super ( transfer , self ) . all ( data , * * kwargs )	1
Q_190	fetch all transfer entity	C_190_1	def pop ( self , count = number ) if count < number return self . popleft ( _ count ) new_right_list , new_left_list = p_deque . _pop_lists ( self . _right_list , self . _left_list , count ) return p_deque ( new_left_list , new_right_list , max ( self . _length _ count , number ) , self . _maxlen )	0
Q_190	fetch all transfer entity	C_190_2	def load_res_prm ( lst_func , lst_fls_msk = none ) lst_prm = [ ] for var , var in enumerate ( lst_func ) ary_fnc = load_nii ( var ) [ number ] . astype ( np . float32 ) if ary_fnc . ndim = = number lst_prm . append ( ary_fnc ) elif ary_fnc . ndim = = number for ind_ax in range ( ary_fnc . shape [ _ number ] ) lst_prm . append ( ary_fnc [ . , ind_ax ] ) if lst_fls_msk is not none lst_msk = [ none ] * len ( lst_fls_msk ) for var , var in enumerate ( lst_fls_msk ) ary_msk = load_nii ( var ) [ number ] . astype ( np . bool ) lst_msk [ var ] = ary_msk else print ( string ) if lst_fls_msk is none lst_prm_ary = [ ary . flatten ( ) for ary in lst_prm ] else lst_prm_ary = [ none ] * len ( lst_fls_msk ) for ind_lst , ary_msk in enumerate ( lst_msk ) ary_prm_sel = np . empty ( ( np . sum ( ary_msk ) , len ( lst_prm ) ) , dtype = np . float32 ) for ind_ary , ary_prm in enumerate ( lst_prm ) ary_prm_sel [ , ind_ary ] = ary_prm [ ary_msk , . ] lst_prm_ary [ ind_lst ] = ary_prm_sel obj_hdr , ary_aff = load_nii ( lst_func [ number ] ) [ number ] return lst_prm_ary , obj_hdr , ary_aff	0
Q_191	convenience method for test	C_191_0	def query_cache_by_object ( self , zenpy_object ) object_type = get_object_type ( zenpy_object ) cache_key = self . _cache_key_attribute ( object_type ) return self . get ( object_type , getattr ( zenpy_object , cache_key ) )	1
Q_191	convenience method for test	C_191_1	def create_client ( self , name ) client = { string { string name , } } response = self . post_request ( string , client , follow = true ) if response return client ( self , response [ string ] )	0
Q_191	convenience method for test	C_191_2	def update_all_view ( self , request ) log . info ( string ) total_count = errors = number for var in self . model . objects . all ( ) total_count + = number try var . pull ( ) except log . exception ( string var ) errors + = number msg = string . format ( total_count , errors ) self . message_user ( request , msg , level = messages . success ) return redirect ( string )	0
Q_192	search through all host network interface for an interface with	C_192_0	def find_host_network_interface_by_name ( self , name ) if not isinstance ( name , basestring ) raise type_error ( string ) network_interface = self . _call ( string , in_p = [ name ] ) network_interface = i_host_network_interface ( network_interface ) return network_interface	1
Q_192	search through all host network interface for an interface with	C_192_1	def rigid_linear_interpolate ( x_axis , y_axis , x_new_axis ) f = interp1d ( x_axis , y_axis ) return f ( x_new_axis )	0
Q_192	search through all host network interface for an interface with	C_192_2	def delete_nodes ( self , nodes iterable [ str ] ) for var in nodes if self . has_node ( var ) self . remove_node ( var )	0
Q_193	set the acceleration that a give axis will move	C_193_0	def set_acceleration ( self , settings ) self . _acceleration . update ( settings ) values = [ string . format ( var . upper ( ) , var ) for var , var in sorted ( settings . items ( ) ) ] command = string . format ( gcodes [ string ] , string . join ( values ) ) log . debug ( string . format ( command ) ) self . _send_command ( command )	1
Q_193	set the acceleration that a give axis will move	C_193_1	def _candidate_log_files ( ) relpath = string prevpath = none while true abspath = os . path . abspath ( relpath ) if abspath = = prevpath break prevpath = abspath relpath = string + relpath yield abspath yield os . path . expanduser ( string )	0
Q_193	set the acceleration that a give axis will move	C_193_2	def run ( self ) description_py_module = helpers . load_experiment_description_script_from_dir ( self . _experiment_dir ) exp_iface = helpers . get_experiment_description_interface_from_module ( description_py_module ) exp_iface . normalize_stream_sources ( ) model_description = exp_iface . get_model_description ( ) self . _model_control = exp_iface . get_model_control ( ) stream_def = self . _model_control [ string ] from nupic . data . stream_reader import stream_reader read_timeout = number self . _input_source = stream_reader ( stream_def , is_blocking = false , max_timeout = read_timeout ) field_stats = self . _get_field_stats ( ) self . _model = model_factory . create ( model_description ) self . _model . set_field_statistics ( field_stats ) self . _model . enable_learning ( ) self . _model . enable_inference ( self . _model_control . get ( string , none ) ) self . _metric_mgr = metrics_manager ( self . _model_control . get ( string , none ) , self . _model . get_field_info ( ) , self . _model . get_inference_type ( ) ) self . _logged_metric_patterns = self . _model_control . get ( string , [ ] ) self . _optimized_metric_label = self . _get_optimized_metric_label ( ) self . _report_metric_labels = match_patterns ( self . _report_key_patterns , self . _get_metric_labels ( ) ) self . _periodic = self . _init_periodic_activities ( ) num_iters = self . _model_control . get ( string , _ number ) learning_off_at = none iteration_count_infer_only = self . _model_control . get ( string , number ) if iteration_count_infer_only = = _ number self . _model . disable_learning ( ) elif iteration_count_infer_only > number assert num_iters > iteration_count_infer_only , string string string learning_off_at = num_iters _ iteration_count_infer_only self . _run_task_main_loop ( num_iters , learning_off_at = learning_off_at ) self . _finalize ( ) return ( self . _cmp_reason , none )	0
Q_194	wait for packet to send to the client	C_194_0	async def poll ( self ) try packets = [ await asyncio . wait_for ( self . queue . get ( ) , self . server . ping_timeout ) ] self . queue . task_done ( ) except ( asyncio . timeout_error , asyncio . cancelled_error ) raise exceptions . queue_empty ( ) if packets = = [ none ] return [ ] try packets . append ( self . queue . get_nowait ( ) ) self . queue . task_done ( ) except asyncio . queue_empty pass return packets	1
Q_194	wait for packet to send to the client	C_194_1	def square ( m , eff_max , c , m0 , sigma , m1 = number ) return ( eff_max _ c * ( m _ number ) * * number ) / ( number + numpy . exp ( ( m _ m0 ) / sigma ) )	0
Q_194	wait for packet to send to the client	C_194_2	def feeds ( ctx , assets , pricethreshold , maxage ) import builtins witnesses = witnesses ( bitshares_instance = ctx . bitshares ) def test_price ( p , ref ) if math . fabs ( float ( p / ref ) _ number . number ) > pricethreshold / number . number return click . style ( str ( p ) , fg = string ) elif math . fabs ( float ( p / ref ) _ number . number ) > pricethreshold / number . number / number . number return click . style ( str ( p ) , fg = string ) else return click . style ( str ( p ) , fg = string ) def price_diff ( p , ref ) d = ( float ( p ) _ float ( ref ) ) / float ( ref ) * number if math . fabs ( d ) > = number color = string elif math . fabs ( d ) > = number . number color = string else color = string return click . style ( string . format ( d ) , fg = color ) def test_date ( d ) t = d . replace ( tzinfo = none ) now = datetime . utcnow ( ) if now < t + timedelta ( minutes = maxage ) return click . style ( str ( t ) , fg = string ) if now < t + timedelta ( minutes = maxage / number . number ) return click . style ( str ( t ) , fg = string ) else return click . style ( str ( t ) , fg = string ) output = string for var in tqdm ( assets ) t = pretty_table ( [ string , string , string , string , string , string , string , string , string , ] ) t . align = string t . align [ string ] = string var = asset ( var , full = true , bitshares_instance = ctx . bitshares ) current_feed = var . feed feeds = var . feeds producingwitnesses = builtins . set ( ) witness_accounts = [ x [ string ] for x in witnesses ] for feed in tqdm ( feeds ) producingwitnesses . add ( feed [ string ] [ string ] ) t . add_row ( [ var [ string ] , feed [ string ] [ string ] , click . style ( string if feed [ string ] [ string ] in witness_accounts else string , bold = true , ) , test_date ( feed [ string ] ) , test_price ( feed [ string ] , current_feed [ string ] ) , test_price ( feed [ string ] , current_feed [ string ] ) , feed [ string ] / number , feed [ string ] / number , price_diff ( feed [ string ] , current_feed [ string ] ) , ] ) for missing in builtins . set ( witness_accounts ) . difference ( producingwitnesses ) witness = witness ( missing ) t . add_row ( [ click . style ( var [ string ] , bg = string ) , click . style ( witness . account [ string ] , bg = string ) , click . style ( string if feed [ string ] [ string ] in witness_accounts else string , bold = true , ) , click . style ( str ( datetime ( number , number , number ) ) ) , click . style ( string , bg = string ) , click . style ( string , bg = string ) , click . style ( string , bg = string ) , click . style ( string , bg = string ) , click . style ( string , bg = string ) , ] ) output + = t . get_string ( sortby = string , reversesort = true ) output + = string click . echo ( output )	0
Q_195	return a list of path between start and end function	C_195_0	def simple_paths_by_address ( self , start_address , end_address ) cfg_start = self . find_function_by_address ( start_address ) cfg_end = self . find_function_by_address ( end_address ) if not cfg_start or not cfg_end raise exception ( string ) start_address = cfg_start . start_address end_address = cfg_end . start_address paths = networkx . all_simple_paths ( self . _graph , source = start_address , target = end_address ) return ( [ self . _cfg_by_addr [ var ] for var in path ] for path in paths )	1
Q_195	return a list of path between start and end function	C_195_1	def enable ( cls , auto_colors = false , reset_atexit = false ) if not is_windows return false kernel32 , stderr , stdout = init_kernel32 ( ) if stderr = = invalid_handle_value and stdout = = invalid_handle_value return false bg_color , native_ansi = bg_color_native_ansi ( kernel32 , stderr , stdout ) if auto_colors if bg_color in ( number , number , number , number , number , number , number ) ansi_code_mapping . set_light_background ( ) else ansi_code_mapping . set_dark_background ( ) if native_ansi return false if reset_atexit atexit . register ( cls . disable ) if stderr = invalid_handle_value sys . stderr . flush ( ) sys . stderr = windows_stream ( kernel32 , stderr , sys . stderr ) if stdout = invalid_handle_value sys . stdout . flush ( ) sys . stdout = windows_stream ( kernel32 , stdout , sys . stdout ) return true	0
Q_195	return a list of path between start and end function	C_195_2	def _get_frdata ( stream , num , name , var = none ) ctypes = ( var , ) if var else ( string , string , string ) for var in ctypes _reader = getattr ( stream , string . format ( var . title ( ) ) ) try return _reader ( num , name ) except index_error as exc if frerr_no_channel_of_type . match ( str ( exc ) ) continue raise raise value_error ( string string . format ( name ) )	0
Q_196	go into command mode	C_196_0	def enter_command_mode ( self ) self . application . layout . focus ( self . command_buffer ) self . application . vi_state . input_mode = input_mode . insert self . previewer . save ( )	1
Q_196	go into command mode	C_196_1	def session_list ( consul_url = none , token = none , return_list = false , * * kwargs ) ret = { } if not consul_url consul_url = _get_config ( ) if not consul_url log . error ( string ) ret [ string ] = string ret [ string ] = false return ret query_params = { } if string in kwargs query_params [ string ] = kwargs [ string ] function = string ret = _query ( consul_url = consul_url , function = function , token = token , query_params = query_params ) if return_list _list = [ ] for var in ret [ string ] _list . append ( var [ string ] ) return _list return ret	0
Q_196	go into command mode	C_196_2	def get_link ( self , peer ) assert isinstance ( peer , peer ) for var in self . _protocols try return var . get_link ( peer ) except value_error pass return none	0
Q_197	return the current lyapunov characteristic number	C_197_0	def calculate_lyapunov ( self ) if self . _calculate_megno = = number raise runtime_error ( string ) clibrebound . reb_tools_calculate_lyapunov . restype = c_double return clibrebound . reb_tools_calculate_lyapunov ( byref ( self ) )	1
Q_197	return the current lyapunov characteristic number	C_197_1	def monitor ( ) log = logging . get_logger ( _name_ ) loop = asyncio . get_event_loop ( ) asyncio . ensure_future ( console ( loop , log ) ) loop . run_forever ( )	0
Q_197	return the current lyapunov characteristic number	C_197_2	def _inject_function_into_js ( context , name , func ) context . export_function ( name , func ) context . evaljs ( string string { name } string string . format ( name = name ) )	0
Q_198	return the index of the sample to be query and label and	C_198_0	def make_query ( self , return_score = false ) dataset = self . dataset self . model . train ( dataset ) unlabeled_entry_ids , x_pool = zip ( * dataset . get_unlabeled_entries ( ) ) if isinstance ( self . model , probabilistic_model ) dvalue = self . model . predict_proba ( x_pool ) elif isinstance ( self . model , continuous_model ) dvalue = self . model . predict_real ( x_pool ) if self . method = = string score = _ np . max ( dvalue , axis = number ) elif self . method = = string if np . shape ( dvalue ) [ number ] > number dvalue = _ ( np . partition ( _ dvalue , number , axis = number ) [ , number ] ) score = _ np . abs ( dvalue [ , number ] _ dvalue [ , number ] ) elif self . method = = string score = np . sum ( _ dvalue * np . log ( dvalue ) , axis = number ) ask_id = np . argmax ( score ) if return_score return unlabeled_entry_ids [ ask_id ] , list ( zip ( unlabeled_entry_ids , score ) ) else return unlabeled_entry_ids [ ask_id ]	1
Q_198	return the index of the sample to be query and label and	C_198_1	def _require_staff_for_shared_settings ( request , view , obj = none ) if obj is none return if obj . settings . shared and not request . user . is_staff raise permission_denied ( _ ( string ) )	0
Q_198	return the index of the sample to be query and label and	C_198_2	def float2int ( x ) if not pd . isnull ( x ) if is_numeric ( x ) x = int ( x ) return x	0
Q_199	prog agp siirt_female_pistachio_23_may2017_table	C_199_0	def agp ( args ) p = option_parser ( agp . _doc_ ) opts , args = p . parse_args ( args ) if len ( args ) = number sys . exit ( not p . print_help ( ) ) tablefile , = args fp = open ( tablefile ) for var in fp atoms = var . split ( ) hr = atoms [ number ] scaf = atoms [ number ] scaf_start = int ( atoms [ number ] ) + number scaf_end = int ( atoms [ number ] ) strand = atoms [ number ] hr_start = int ( atoms [ number ] ) + number hr_end = int ( atoms [ number ] ) print ( string . join ( str ( x ) for x in ( hr , hr_start , hr_end , number , string , scaf , scaf_start , scaf_end , strand ) ) )	1
Q_199	prog agp siirt_female_pistachio_23_may2017_table	C_199_1	def text ( self , path , wholetext = false , line_sep = none ) self . _set_opts ( wholetext = wholetext , line_sep = line_sep ) if isinstance ( path , basestring ) return self . _df ( self . _jreader . text ( path ) ) else raise type_error ( string )	0
Q_199	prog agp siirt_female_pistachio_23_may2017_table	C_199_2	def labels_at ( y_true , y_score , proportion , normalize = false ) indexes = np . argsort ( y_score ) [ _ number ] y_true_sorted = y_true [ indexes ] cutoff_index = max ( int ( len ( y_true_sorted ) * proportion ) _ number , number ) y_true_top = y_true_sorted [ cutoff_index + number ] values = int ( ( np . isnan ( y_true_top ) ) . sum ( ) ) if normalize values = float ( values ) / ( np . isnan ( y_true ) ) . sum ( ) return values	0
Q_200	prog count fastafile jf	C_200_0	def count ( args ) from bitarray import bitarray p = option_parser ( count . _doc_ ) opts , args = p . parse_args ( args ) if len ( args ) = number sys . exit ( not p . print_help ( ) ) fastafile , jfdb = args k = get_k ( jfdb ) cmd = string string . format ( jfdb ) t = must_open ( string , string ) proc = popen ( cmd , stdin = pipe , stdout = t ) t . flush ( ) f = fasta ( fastafile , lazy = true ) for var , var in f . iteritems_ordered ( ) kmers = list ( make_kmers ( var . seq , k ) ) print ( string . join ( kmers ) , file = proc . stdin ) proc . stdin . close ( ) logging . debug ( cmd ) proc . wait ( ) a = bitarray ( ) binfile = string . join ( ( fastafile , jfdb , string ) ) fw = open ( binfile , string ) t . seek ( number ) for row in t c = row . strip ( ) a . append ( int ( c ) ) a . tofile ( fw ) logging . debug ( string . format ( len ( a ) , binfile ) ) fw . close ( ) sh ( string . format ( t . var ) ) logging . debug ( string . format ( k , fastafile , jfdb , binfile ) ) cntfile = string . join ( ( fastafile , jfdb , string ) ) bincount ( [ fastafile , binfile , string , cntfile , string . format ( k ) ] ) logging . debug ( string . format ( cntfile ) )	1
Q_200	prog count fastafile jf	C_200_1	def solve ( self , x0 , params = ( ) , internal_x0 = none , solver = none , attached_solver = none , * * kwargs ) if not isinstance ( solver , ( tuple , list ) ) solver = [ solver ] if not isinstance ( attached_solver , ( tuple , list ) ) attached_solver = [ attached_solver ] + [ none ] * ( len ( solver ) _ number ) _x0 , self . internal_params = self . pre_process ( x0 , params ) for var , var in zip ( solver , attached_solver ) if internal_x0 is not none _x0 = internal_x0 elif self . internal_x0_cb is not none _x0 = self . internal_x0_cb ( x0 , params ) nfo = self . _get_solver_cb ( var , var ) ( _x0 , * * kwargs ) _x0 = nfo [ string ] . copy ( ) self . internal_x = _x0 x0 = self . post_process ( self . internal_x , self . internal_params ) [ number ] return x0 , nfo	0
Q_200	prog count fastafile jf	C_200_2	def run_migrations_online ( ) engine = create_engine ( get_url ( ) ) connection = engine . connect ( ) context . configure ( connection = connection , target_metadata = target_metadata , version_table = string , transaction_per_migration = true , ) try with context . begin_transaction ( ) context . run_migrations ( ) finally connection . close ( )	0
Q_201	prog summary txtfile fastafile	C_201_0	def summary ( args ) from jcvi . utils . cbook import thousands from jcvi . utils . table import tabulate p = option_parser ( summary . _doc_ ) p . add_option ( string , help = string ) p . add_option ( string , help = string ) opts , args = p . parse_args ( args ) if len ( args ) = number sys . exit ( not p . print_help ( ) ) txtfile , fastafile = args bedfw = open ( opts . bed , string ) if opts . bed else none fp = open ( txtfile ) header = fp . next ( ) . split ( ) snps = defaultdict ( list ) combinations = defaultdict ( int ) intra_sn_ps = inter_sn_ps = number distinct_set = set ( ) ref , alt = header [ number number ] snpcounts , goodsnpcounts = defaultdict ( int ) , defaultdict ( int ) for var in fp atoms = var . split ( ) assert len ( atoms ) = = number , string locus , intra , inter = atoms ctg , pos = locus . rsplit ( string , number ) pos = int ( pos ) snps [ ctg ] . append ( pos ) snpcounts [ ctg ] + = number if intra = = string intra_sn_ps + = number if inter in ( string , string ) inter_sn_ps + = number if intra = = string and inter = = string distinct_set . add ( ctg ) goodsnpcounts [ ctg ] + = number intra = ref + string + intra inter = alt + string + inter combinations [ ( intra , inter ) ] + = number if bedfw print ( string . join ( str ( x ) for x in ( ctg , pos _ number , pos , locus ) ) , file = bedfw ) if bedfw logging . debug ( string . format ( opts . bed ) ) bedfw . close ( ) nsites = sum ( len ( x ) for x in snps . values ( ) ) sizes = sizes ( fastafile ) bpsize = sizes . totalsize snprate = lambda a a * number . / bpsize m = string . format ( fastafile , len ( sizes ) , thousands ( bpsize ) ) m + = string . format ( nsites , len ( snps ) , thousands ( sum ( sizes . mapping [ x ] for x in snps . keys ( ) ) ) ) m + = string . format ( snprate ( nsites ) ) m + = string . format ( intra_sn_ps , snprate ( intra_sn_ps ) , inter_sn_ps , snprate ( inter_sn_ps ) ) print ( m , file = sys . stderr ) print ( tabulate ( combinations ) , file = sys . stderr ) leg = string print ( leg , file = sys . stderr ) tag = ( ref + string , alt + string ) distinct_sn_ps = combinations [ tag ] tag = str ( tag ) . replace ( string string string a total of { number } disparate { number } sn_ps in { number } contigs . string w string t string contig string print ( header , file = fw ) assert sum ( snpcounts . values ( ) ) = = nsites assert sum ( goodsnpcounts . values ( ) ) = = distinct_sn_ps for ctg in sorted ( snps . keys ( ) ) snpcount = snpcounts [ ctg ] goodsnpcount = goodsnpcounts [ ctg ] print ( string . join ( str ( x ) for x in ( ctg , snpcount , goodsnpcount ) ) , file = fw ) fw . close ( ) logging . debug ( string . format ( snpcountsfile ) )	1
Q_201	prog summary txtfile fastafile	C_201_1	def setup ( self , helper = none , * * create_kwargs ) if self . created return self . set_helper ( helper ) self . create ( * * create_kwargs ) return self	0
Q_201	prog summary txtfile fastafile	C_201_2	def ismonitor ( self ) if self . cache_mode is not none return self . cache_mode try res = ( self . mode ( ) = = string ) self . cache_mode = res return res except scapy_exception return false	0
Q_202	create and set as current warc file	C_202_0	def _start_new_warc_file ( self , meta = false ) if self . _params . max_size and not meta and self . _params . appending while true self . _warc_filename = self . _generate_warc_filename ( ) if os . path . exists ( self . _warc_filename ) _logger . debug ( string , self . _warc_filename ) self . _sequence_num + = number else break else self . _warc_filename = self . _generate_warc_filename ( meta = meta ) _logger . debug ( string , self . _warc_filename ) if not self . _params . appending wpull . util . truncate_file ( self . _warc_filename ) self . _warcinfo_record = warc_record ( ) self . _populate_warcinfo ( self . _params . extra_fields ) self . write_record ( self . _warcinfo_record )	1
Q_202	create and set as current warc file	C_202_1	def get_price ( self ) for var in self . conditions for p in var . parameters if p . name = = string return p . value	0
Q_202	create and set as current warc file	C_202_2	def list ( self ) return self . _post ( request = api_actions . list . value , uri = api_uri . hooks . value , ) . get ( string )	0
Q_203	get http header or the http http request	C_203_0	def headers ( self ) action = self . method . soap . action result = { string string , string action } result . update ( self . options . headers ) log . debug ( string , result ) return result	1
Q_203	get http header or the http http request	C_203_1	def delete_bond ( self , n , m ) self . remove_edge ( n , m ) self . flush_cache ( )	0
Q_203	get http header or the http http request	C_203_2	def _get_device_id ( self , bus ) _dbus = bus . get ( service_bus , path ) devices = _dbus . devices ( ) if self . device is none and self . device_id is none and len ( devices ) = = number return devices [ number ] for var in devices self . _dev = bus . get ( service_bus , device_path + string var ) if self . device = = self . _dev . name return var return none	0
Q_204	generate a schema and convert it directly to serialize json	C_204_0	def to_json ( self , * args , * * kwargs ) return json . dumps ( self . to_schema ( ) , * args , * * kwargs )	1
Q_204	generate a schema and convert it directly to serialize json	C_204_1	def write_to_socket ( self , frame_data ) self . _wr_lock . acquire ( ) try total_bytes_written = number bytes_to_send = len ( frame_data ) while total_bytes_written < bytes_to_send try if not self . socket raise socket . error ( string ) bytes_written = ( self . socket . send ( frame_data [ total_bytes_written ] ) ) if bytes_written = = number raise socket . error ( string ) total_bytes_written + = bytes_written except socket . timeout pass except socket . error as why if why . args [ number ] in ( ewouldblock , eagain ) continue self . _exceptions . append ( amqp_connection_error ( why ) ) return finally self . _wr_lock . release ( )	0
Q_204	generate a schema and convert it directly to serialize json	C_204_2	"def parse_reports ( self ) self . infer_exp = dict ( ) regexes = { string r string number + + , number _ , number + _ , number _ + string , string r string number + _ , number _ + , number + + , number _ string , string r string + + , _ string , string r string ( d . d + ) string failed string fraction of reads failed to determine ( d . d + ) string rseqc / infer_experiment string var string s_name string duplicate sample name found overwriting { } string s_name string infer_experiment string s_name string multiqc_rseqc_infer_experiment string pe string se string sense string name string sense string antisense string name string antisense string failed string name string undetermined string id string rseqc_infer_experiment_plot string title string r_se_qc infer experiment string ylab string tags string ymin string ymax string tt_percentages string ylab_format string { value } string cpswitch string infer experiment string rseqc _ infer_experiment string < a href = string counts the percentage of reads and read pairs that match the strandedness of overlapping transcripts . string it can be used to infer whether rna _ seq library preps are stranded ( sense or antisense ) . "" , plot = bargraph . plot ( pdata , keys , pconfig ) ) return len ( self . infer_exp )"	0
Q_205	get the work_result as a dict	C_205_0	def as_dict ( self ) string return { string self . output , string self . test_outcome , string self . worker_outcome , string self . diff , }	1
Q_205	get the work_result as a dict	C_205_1	def init_app ( self , app ) self . init_config ( app . config ) state = _invenio_migrator_state ( app ) app . extensions [ string ] = state app . cli . add_command ( dumps ) return state	0
Q_205	get the work_result as a dict	C_205_2	def process_elements_surface ( elem , mconf , colorval , idx , force_tsl , update_delta , delta , reset_names ) if idx < number lock . acquire ( ) idx = counter . value counter . value + = number lock . release ( ) if update_delta elem . delta = delta elem . evaluate ( ) if reset_names elem . name = string if elem . name = = string and idx > = number elem . name = elem . name + string + str ( idx ) color = select_color ( colorval [ number ] , colorval [ number ] , idx = idx ) rl = [ ] if mconf [ string ] = = string ret = dict ( ptsarr = elem . ctrlpts , name = ( elem . name , string ) , color = color [ number ] , plot_type = string , idx = idx ) rl . append ( ret ) if mconf [ string ] = = string qtsl = tessellate . quad_tessellate ( ) qtsl . tessellate ( elem . ctrlpts , size_u = elem . ctrlpts_size_u , size_v = elem . ctrlpts_size_v ) ret = dict ( ptsarr = [ qtsl . vertices , qtsl . faces ] , name = ( elem . name , string ) , color = color [ number ] , plot_type = string , idx = idx ) rl . append ( ret ) if mconf [ string ] = = string ret = dict ( ptsarr = elem . evalpts , name = ( elem . name , idx ) , color = color [ number ] , plot_type = string , idx = idx ) rl . append ( ret ) if mconf [ string ] = = string qtsl = tessellate . quad_tessellate ( ) qtsl . tessellate ( elem . evalpts , size_u = elem . sample_size_u , size_v = elem . sample_size_v ) ret = dict ( ptsarr = [ qtsl . vertices , qtsl . faces ] , name = elem . name , color = color [ number ] , plot_type = string , idx = idx ) rl . append ( ret ) if mconf [ string ] = = string elem . tessellate ( force = force_tsl ) ret = dict ( ptsarr = [ elem . tessellator . vertices , elem . tessellator . faces ] , name = elem . name , color = color [ number ] , plot_type = string , idx = idx ) rl . append ( ret ) for var , var in enumerate ( elem . trims ) ret = dict ( ptsarr = elem . evaluate_list ( var . evalpts ) , name = ( string , var ) , color = colorval [ number ] , plot_type = string , idx = idx ) rl . append ( ret ) return rl	0
Q_206	create a new keypair from a batchfile	C_206_0	def create_key ( batchfile ) key = gpg . gen_key ( batchfile ) fingerprint = key . fingerprint if not fingerprint log . error ( string key . status ) return none , none return key , fingerprint	1
Q_206	create a new keypair from a batchfile	C_206_1	def get_text_alignment ( pos ) pos = pos . lower ( ) va , ha = none , none if pos = = string va , ha = string , string elif pos = = string va , ha = string , string elif pos = = string va , ha = string , string elif pos = = string va , ha = string , string else raise value_error ( string pos string ( str ( pos ) ) ) return { string va , string ha }	0
Q_206	create a new keypair from a batchfile	C_206_2	def example_panel ( self , ax , feature ) txt = string ( feature . chrom , feature . start , feature . stop ) ax . text ( number . number , number . number , txt , transform = ax . trans_axes ) return feature	0
Q_207	disambiguate similarly_named weather condition and return the icon	C_207_0	def get_color_data ( self , condition ) if condition not in self . color_icons condition_lc = condition . lower ( ) if string in condition_lc or string in condition_lc if string in condition_lc condition = string else condition = string elif condition_lc = = string condition = string elif string in condition_lc or string in condition_lc condition = string elif string in condition_lc condition = string elif string in condition_lc or string in condition_lc condition = string elif string in condition_lc condition = string elif string in condition_lc or string in condition_lc condition = string elif string in condition_lc condition = string return self . color_icons [ string ] if condition not in self . color_icons else self . color_icons [ condition ]	1
Q_207	disambiguate similarly_named weather condition and return the icon	C_207_1	def _get_authorization_headers ( self , context ) headers = { } self . _credentials . before_request ( self . _request , context . method_name , context . service_url , headers ) return list ( six . iteritems ( headers ) )	0
Q_207	disambiguate similarly_named weather condition and return the icon	C_207_2	def handle_err ( * args ) if debug print_err ( traceback . format_exc ( ) , color = false ) else print_err ( * args , newline = true )	0
Q_208	switch label to vertical if they be long	C_208_0	def _vertical_x ( axis , ticks = none , max_width = number ) if ticks is none ticks = axis . get_xticks ( ) if ( np . array ( ticks ) = = np . rint ( ticks ) ) . all ( ) ticks = np . rint ( ticks ) . astype ( np . int ) if max ( [ len ( str ( var ) ) for var in ticks ] ) > max_width axis . set_xticklabels ( ticks , rotation = string )	1
Q_208	switch label to vertical if they be long	C_208_1	def get_option_names ( self ) return [ var for var in self . option_definitions . keys_breadth_first ( ) if isinstance ( self . option_definitions [ var ] , option ) ]	0
Q_208	switch label to vertical if they be long	C_208_2	def xfrange ( start , stop = none , step = number ) if stop is none stop = start start = number . number start = float ( start ) while start < stop yield start start + = step	0
Q_209	this greenlet kill job in other greenlets if they timeout	C_209_0	def greenlet_timeouts ( self ) while true now = datetime . datetime . utcnow ( ) for var in list ( self . gevent_pool ) job = get_current_job ( id ( var ) ) if job and job . timeout and job . datestarted expires = job . datestarted + datetime . timedelta ( seconds = job . timeout ) if now > expires job . kill ( block = false , reason = string ) time . sleep ( number )	1
Q_209	this greenlet kill job in other greenlets if they timeout	C_209_1	def get_operations ( self , indices sequence [ logical_index ] , qubits sequence [ ops . qid ] ) _ > ops . op_tree string string string	0
Q_209	this greenlet kill job in other greenlets if they timeout	C_209_2	def status ( self , message , rofi_args = none , * * kwargs ) rofi_args = rofi_args or [ ] args = [ string , string , message ] args . extend ( self . _common_args ( allow_fullscreen = false , * * kwargs ) ) args . extend ( rofi_args ) self . _run_nonblocking ( args )	0
Q_210	retrieve a directory	C_210_0	def _get_directory ( self ) if self . entry_type = definitions . file_entry_type_directory return none return tsk_partition_directory ( self . _file_system , self . path_spec )	1
Q_210	retrieve a directory	C_210_1	def set_proxy ( self , host , port , user , password ) if user and password proxy_string = string . format ( user , password , host , port ) else proxy_string = string . format ( host , port ) self . proxies = { string string . format ( proxy_string ) , string string . format ( proxy_string ) }	0
Q_210	retrieve a directory	C_210_2	def make_google_blueprint ( client_id = none , client_secret = none , scope = none , offline = false , reprompt_consent = false , reprompt_select_account = false , redirect_url = none , redirect_to = none , login_url = none , authorized_url = none , session_class = none , storage = none , hosted_domain = none , ) scope = scope or [ string ] authorization_url_params = { } prompt_params = [ ] auto_refresh_url = none if offline authorization_url_params [ string ] = string auto_refresh_url = string if reprompt_consent prompt_params . append ( string ) if reprompt_select_account prompt_params . append ( string ) if prompt_params prompt_params = string . join ( prompt_params ) authorization_url_params [ string ] = prompt_params if hosted_domain authorization_url_params [ string ] = hosted_domain google_bp = o_auth2_consumer_blueprint ( string , _name_ , client_id = client_id , client_secret = client_secret , scope = scope , base_url = string , authorization_url = string , token_url = string , auto_refresh_url = auto_refresh_url , redirect_url = redirect_url , redirect_to = redirect_to , login_url = login_url , authorized_url = authorized_url , authorization_url_params = authorization_url_params , session_class = session_class , storage = storage , ) google_bp . from_config [ string ] = string google_bp . from_config [ string ] = string google_bp . before_app_request def set_applocal_session ( ) ctx = stack . top ctx . google_oauth = google_bp . session return google_bp	0
Q_211	determine base on the data type if the data be an integer	C_211_0	def data_is_integer ( self ) return self . data_type in ( definitions . reg_dword , definitions . reg_dword_big_endian , definitions . reg_qword )	1
Q_211	determine base on the data type if the data be an integer	C_211_1	def tmpdir ( prefix = string , delete = true ) path = tempfile . mkdtemp ( prefix = prefix ) if not os . path . isdir ( path ) raise value_error ( string ) if delete atexit . register ( shutil . rmtree , path ) return path	0
Q_211	determine base on the data type if the data be an integer	C_211_2	def credits ( self , * * kwargs ) path = self . _get_series_id_season_number_episode_number_path ( string ) response = self . _get ( path , kwargs ) self . _set_attrs_to_values ( response ) return response	0
Q_212	aggregate node use lloyd clustering	C_212_0	def lloyd_aggregation ( c , ratio = number . number number , distance = string , maxiter = number ) if ratio < = number or ratio > number raise value_error ( string ) if not ( isspmatrix_csr ( c ) or isspmatrix_csc ( c ) ) raise type_error ( string ) if distance = = string data = np . ones_like ( c . data ) . astype ( float ) elif distance = = string data = abs ( c . data ) elif distance = = string data = number . number / abs ( c . data ) elif distance is string data = c . data elif distance is string data = c . data _ c . data . min ( ) else raise value_error ( string distance ) if c . dtype = = complex data = np . real ( data ) assert ( data . min ( ) > = number ) g = c . _class_ ( ( data , c . indices , c . indptr ) , shape = c . shape ) num_seeds = int ( min ( max ( ratio * g . shape [ number ] , number ) , g . shape [ number ] ) ) distances , clusters , seeds = lloyd_cluster ( g , num_seeds , maxiter = maxiter ) row = ( clusters > = number ) . nonzero ( ) [ number ] col = clusters [ row ] data = np . ones ( len ( row ) , dtype = string ) agg_op = coo_matrix ( ( data , ( row , col ) ) , shape = ( g . shape [ number ] , num_seeds ) ) . tocsr ( ) return agg_op , seeds	1
Q_212	aggregate node use lloyd clustering	C_212_1	def midi ( self ) result = int ( round ( number * log2 ( self . frequency / number ) + number ) ) if number < = result < number return result raise value_error ( string self . frequency )	0
Q_212	aggregate node use lloyd clustering	C_212_2	def on_each_ ( * validation_functions_collection ) validation_function_funcs = tuple ( _process_validation_function_s ( var ) for var in validation_functions_collection ) def on_each_val ( x ) if len ( validation_function_funcs ) = len ( x ) raise failure ( string ) else idx = _ number for elt , validation_function_func in zip ( x , validation_function_funcs ) idx + = number try res = validation_function_func ( elt ) except exception as e raise invalid_item_in_sequence ( wrong_value = elt , wrapped_func = validation_function_func , validation_outcome = e ) if not result_is_success ( res ) raise invalid_item_in_sequence ( wrong_value = elt , wrapped_func = validation_function_func , validation_outcome = res ) return true on_each_val . _name_ = string string . format ( string + string . join ( [ get_callable_name ( f ) for f in validation_function_funcs ] ) + string ) return on_each_val	0
Q_213	start trace mode in the give thread	C_213_0	def start_tracing ( self , tid ) if not self . is_tracing ( tid ) thread = self . system . get_thread ( tid ) self . _start_tracing ( thread )	1
Q_213	start trace mode in the give thread	C_213_1	def load_data ( self ) images = list ( ) labels = list ( ) emotion_index_map = dict ( ) label_directories = [ var for var in os . listdir ( self . datapath ) if not var . startswith ( string ) ] for label_directory in label_directories if self . target_emotion_map if label_directory not in self . target_emotion_map . keys ( ) continue self . _add_new_label_to_map ( label_directory , emotion_index_map ) label_directory_path = self . datapath + string + label_directory if self . time_delay self . _load_series_for_single_emotion_directory ( images , label_directory , label_directory_path , labels ) else image_files = [ image_file for image_file in os . listdir ( label_directory_path ) if not image_file . startswith ( string ) ] self . _load_images_from_directory_to_array ( image_files , images , label_directory , label_directory_path , labels ) vectorized_labels = self . _vectorize_labels ( emotion_index_map , labels ) self . _check_data_not_empty ( images ) return self . _load_dataset ( np . array ( images ) , np . array ( vectorized_labels ) , emotion_index_map )	0
Q_213	start trace mode in the give thread	C_213_2	def update ( self , method = string , dims = { } , fmt = { } , replot = false , auto_update = false , draw = none , force = false , todefault = false , enable_post = none , * * kwargs ) dims = dict ( dims ) fmt = dict ( fmt ) vars_and_coords = set ( chain ( self . dims , self . coords , [ string , string , string , string , string ] ) ) furtherdims , furtherfmt = utils . sort_kwargs ( kwargs , vars_and_coords ) dims . update ( furtherdims ) fmt . update ( furtherfmt ) self . _register_update ( method = method , replot = replot , dims = dims , fmt = fmt , force = force , todefault = todefault ) if enable_post is not none for var in self . with_plotter var . psy . plotter . enable_post = enable_post if not self . no_auto_update or auto_update self . start_update ( draw )	0
Q_214	param disconnect_request request	C_214_0	def on_disconnect_request ( self , py_db , request ) self . api . set_enable_thread_notifications ( py_db , false ) self . api . remove_all_breakpoints ( py_db , filename = string ) self . api . remove_all_exception_breakpoints ( py_db ) self . api . request_resume_thread ( thread_id = string ) response = pydevd_base_schema . build_response ( request ) return net_command ( cmd_return , number , response , is_json = true )	1
Q_214	param disconnect_request request	C_214_1	def execute ( self , controller_id , command , * args , * * kwargs ) controller_instance = self . controllers [ controller_id ] controller_instance . last_command_at = self . last_command_at ret_val = getattr ( controller_instance , command ) ( * args , * * kwargs ) self . last_command_at = controller_instance . last_command_at return ret_val	0
Q_214	param disconnect_request request	C_214_2	def load_functional_channels ( self , groups iterable [ group ] ) self . functional_channels = [ ] for var in self . _raw_json_data [ string ] . values ( ) fc = self . _parse_functional_channel ( var , groups ) self . functional_channels . append ( fc ) self . functional_channel_count = counter ( x . functional_channel_type for x in self . functional_channels )	0
Q_215	check parameter be in a specify range	C_215_0	def check_between ( v_min , v_max , * * params ) for var in params if params [ var ] < v_min or params [ var ] > v_max raise value_error ( string string . format ( var , v_min , v_max , params [ var ] ) )	1
Q_215	check parameter be in a specify range	C_215_1	def make_bag ( bag_dir , bag_info = none , processes = number , checksum = none ) bag_dir = os . path . abspath ( bag_dir ) logger . info ( string , bag_dir ) if not checksum checksum = [ string ] if not os . path . isdir ( bag_dir ) logger . error ( string , bag_dir ) raise runtime_error ( string bag_dir ) old_dir = os . path . abspath ( os . path . curdir ) os . chdir ( bag_dir ) try unbaggable = _can_bag ( os . curdir ) if unbaggable logger . error ( string , unbaggable ) raise bag_error ( string ) unreadable_dirs , unreadable_files = _can_read ( os . curdir ) if unreadable_dirs or unreadable_files if unreadable_dirs logger . error ( string , unreadable_dirs ) if unreadable_files logger . error ( string , unreadable_files ) raise bag_error ( string ) else logger . info ( string ) cwd = os . getcwd ( ) temp_data = tempfile . mkdtemp ( dir = cwd ) for var in os . listdir ( string ) if os . path . abspath ( var ) = = temp_data continue new_f = os . path . join ( temp_data , var ) logger . info ( string , var , new_f ) os . rename ( var , new_f ) logger . info ( string , temp_data , string ) os . rename ( temp_data , string ) os . chmod ( string , os . stat ( cwd ) . st_mode ) for c in checksum logger . info ( string , c ) oxum = _make_manifest ( string c , string , processes , c ) logger . info ( string ) txt = with open ( string , string ) as bagit_file bagit_file . write ( txt ) logger . info ( string ) if bag_info is none bag_info = { } if string not in bag_info bag_info [ string ] = date . strftime ( date . today ( ) , string ) if string not in bag_info bag_info [ string ] = string bag_info [ string ] = oxum _make_tag_file ( string , bag_info ) for c in checksum _make_tagmanifest_file ( c , bag_dir ) except exception logger . exception ( string ) raise finally os . chdir ( old_dir ) return bag ( bag_dir )	0
Q_215	check parameter be in a specify range	C_215_2	def decoherence_noise_with_asymmetric_ro ( gates sequence [ gate ] , p00 = number . number , p11 = number . number ) noise_model = _decoherence_noise_model ( gates ) aprobs = np . array ( [ [ p00 , number _ p00 ] , [ number _ p11 , p11 ] ] ) aprobs = { var aprobs for var in noise_model . assignment_probs . keys ( ) } return noise_model ( noise_model . gates , aprobs )	0
Q_216	compute gradient in fourier domain	C_216_0	def eval_grad ( self ) self . ryf [ ] = self . eval_rf ( self . yf ) ry = sl . irfftn ( self . ryf , self . cri . nv , self . cri . axis_n ) self . w_ry [ ] = ( self . w * * number ) * ry w_ryf = sl . rfftn ( self . w_ry , self . cri . nv , self . cri . axis_n ) gradf = np . conj ( self . df ) * w_ryf if self . cri . cd > number gradf = np . sum ( gradf , axis = self . cri . axis_c , keepdims = true ) return gradf	1
Q_216	compute gradient in fourier domain	C_216_1	def add_trial ( self , trial ) trial . set_verbose ( self . _verbose ) self . _trials . append ( trial ) with warn_if_slow ( string ) self . _scheduler_alg . on_trial_add ( self , trial ) self . trial_executor . try_checkpoint_metadata ( trial )	0
Q_216	compute gradient in fourier domain	C_216_2	def make_tmp_name ( name ) path , base = os . path . split ( name ) tmp_base = string ( base , uuid4 ( ) . hex ) tmp_name = os . path . join ( path , tmp_base ) try yield tmp_name finally safe_remove ( tmp_name )	0
Q_217	compute component of regularisation function as well as total	C_217_0	def eval_objfn ( self ) g0v = self . obfn_g0 ( self . obfn_g0var ( ) ) g1v = self . obfn_g1 ( self . obfn_g1var ( ) ) rgr = sl . rfl2norm2 ( np . sqrt ( self . gh_gf * np . conj ( self . xf ) * self . xf ) , self . cri . nv , self . cri . axis_n ) / number . number obj = g0v + self . lmbda * g1v + self . mu * rgr return ( obj , g0v , g1v , rgr )	1
Q_217	compute component of regularisation function as well as total	C_217_1	def eval_set ( self , evals , iteration = number , feval = none ) if feval is none for var in evals if not isinstance ( var [ number ] , d_matrix ) raise type_error ( string . format ( type ( var [ number ] ) . _name_ ) ) if not isinstance ( var [ number ] , string_types ) raise type_error ( string . format ( type ( var [ number ] ) . _name_ ) ) self . _validate_features ( var [ number ] ) dmats = c_array ( ctypes . c_void_p , [ var [ number ] . handle for var in evals ] ) evnames = c_array ( ctypes . c_char_p , [ c_str ( var [ number ] ) for var in evals ] ) msg = ctypes . c_char_p ( ) _check_call ( _lib . xg_booster_eval_one_iter ( self . handle , iteration , dmats , evnames , len ( evals ) , ctypes . byref ( msg ) ) ) return msg . value else res = string iteration for dmat , evname in evals name , val = feval ( self . predict ( dmat ) , dmat ) res + = string ( evname , name , val ) return res	0
Q_217	compute component of regularisation function as well as total	C_217_2	def node_is_noop ( node ast . ast ) _ > bool return isinstance ( node . value , ast . str ) if isinstance ( node , ast . expr ) else isinstance ( node , ast . pass )	0
Q_218	almost equal base on the math a _ b value	C_218_0	def almost_eq ( a , b , max_diff = number e _ number , ignore_type = true , pad = number . ) if not ( ignore_type or type ( a ) = = type ( b ) ) return false is_it_a = isinstance ( a , iterable ) is_it_b = isinstance ( b , iterable ) if is_it_a = is_it_b return false if is_it_a return all ( almost_eq . diff ( var , var , max_diff , ignore_type ) for var , var in xzip_longest ( a , b , fillvalue = pad ) ) return abs ( a _ b ) < = max_diff	1
Q_218	almost equal base on the math a _ b value	C_218_1	def is_simple ( polygon ) n = len ( polygon ) order = list ( range ( n ) ) order . sort ( key = lambda i polygon [ i ] ) rank_to_y = list ( set ( var [ number ] for var in polygon ) ) rank_to_y . sort ( ) y_to_rank = { y rank for rank , y in enumerate ( rank_to_y ) } s = range_min_query ( [ number ] * len ( rank_to_y ) ) last_y = none for i in order x , y = polygon [ i ] rank = y_to_rank [ y ] right_x = max ( polygon [ i _ number ] [ number ] , polygon [ ( i + number ) n ] [ number ] ) left = x < right_x below_y = min ( polygon [ i _ number ] [ number ] , polygon [ ( i + number ) n ] [ number ] ) high = y > below_y if left if s [ rank ] return false s [ rank ] = _ number else s [ rank ] = number if high lo = y_to_rank [ below_y ] if ( below_y = last_y or last_y = = y or rank _ lo > = number and s . range_min ( lo + number , rank ) ) return false last_y = y return true	0
Q_218	almost equal base on the math a _ b value	C_218_2	def to_header ( self ) if self . date is not none return http_date ( self . date ) if self . etag is not none return quote_etag ( self . etag ) return string	0
Q_219	rotate the image clockwise around it center	C_219_0	def rotate ( self , deg , * * kwargs ) opts = image . _normalize_options ( kwargs ) if deg = = string if self . _orig_format = = string try exif = self . img . _getexif ( ) or dict ( ) deg = _orientation_to_rotation . get ( exif . get ( number , number ) , number ) except exception logger . warn ( string ) deg = number else deg = number deg = number _ ( int ( deg ) number ) if deg number = = number if deg = = number self . img = self . img . transpose ( pil . image . rotate_90 ) elif deg = = number self . img = self . img . transpose ( pil . image . rotate_180 ) elif deg = = number self . img = self . img . transpose ( pil . image . rotate_270 ) else self . img = self . img . rotate ( deg , expand = bool ( int ( opts [ string ] ) ) ) return self	1
Q_219	rotate the image clockwise around it center	C_219_1	def package_info ( package , image = none ) cmd = [ string , string , string . format ( image ) if image else string , string ] if string in package cmd . append ( string . format ( package ) ) else cmd . append ( string . format ( package ) ) out = _salt_ [ string ] ( cmd ) if out [ string ] = = number ret = dict ( ) for var in six . text_type ( out [ string ] ) . splitlines ( ) if string in var info = var . split ( string ) if len ( info ) < number continue ret [ info [ number ] ] = info [ number ] else ret = out return ret	0
Q_219	rotate the image clockwise around it center	C_219_2	def _use_color ( msg , ansi_fmt , output_stream ) if configs . _on_rtd or ( not configs . always_colorize and not output_stream . isatty ( ) ) log = msg else log = colorize ( msg , ansi_fmt ) return log	0
Q_220	write the image to the open file object	C_220_0	def write ( self , file ) w = writer ( * * self . info ) w . write ( file , self . rows )	1
Q_220	write the image to the open file object	C_220_1	def remove_node_from_cluster ( self , node , callback = none ) if not self . _conf . dynamic_membership_change raise exception ( string ) if not isinstance ( node , node ) node = self . _node_class ( node ) self . _apply_command ( pickle . dumps ( [ string , node . id , node ] ) , callback , _command_type . membership )	0
Q_220	write the image to the open file object	C_220_2	def local_fehdist ( feh ) fehdist = number . number / number . number * np . exp ( _ number . number * ( feh _ number . number number ) * * number . / number . number * * number . ) + number . number / number . number * np . exp ( _ number . number * ( feh + number . number ) * * number . / number . number * * number . ) return fehdist	0
Q_221	return a temporary file	C_221_0	def tmpfile ( dir = none , prefix = string , suffix = none ) return tempfile . mkstemp ( suffix = suffix , prefix = prefix , dir = dir ) [ number ]	1
Q_221	return a temporary file	C_221_1	def assuan_serialize ( data ) for var in [ b string , b string , b string ] escaped = string . format ( ord ( var ) ) . encode ( string ) data = data . replace ( var , escaped ) return data	0
Q_221	return a temporary file	C_221_2	def commit ( self ) tc = self . qte_widget . text_cursor ( ) if self . cursor_pos0 is none self . cursor_pos0 = tc . position ( ) self . sel_text = tc . selection ( ) . to_html ( ) self . sel_start = tc . selection_start ( ) self . sel_end = tc . selection_end ( ) else tc . set_position ( self . cursor_pos0 , qt_gui . q_text_cursor . move_anchor ) tc . set_position ( self . sel_start , qt_gui . q_text_cursor . move_anchor ) tc . set_position ( self . sel_end , qt_gui . q_text_cursor . keep_anchor ) tc . remove_selected_text ( ) if len ( self . sel_text ) > number pos = self . sel_start else pos = tc . position ( ) tc . set_position ( pos ) self . cursor_pos1 = tc . position ( ) tc . insert_text ( self . text ) self . cursor_pos2 = tc . position ( ) self . qte_widget . set_text_cursor ( tc )	0
Q_222	return the struct format and list of the size and value	C_222_0	def render ( self ) size_format = self . size_primitive . fmt if self . value is none return size_format , [ _ number ] value = self . render_value ( self . value ) size = len ( value ) fmt = string ( size_format , size ) return fmt , [ size , value ]	1
Q_222	return the struct format and list of the size and value	C_222_1	def percentile_of_series ( request_context , series_list , n , interpolate = false ) if n < = number raise value_error ( string ) if not series_list return [ ] name = string ( series_list [ number ] . path_expression , n ) start , end , step = normalize ( [ series_list ] ) [ number ] values = [ _get_percentile ( var , n , interpolate ) for var in zip_longest ( * series_list ) ] result_series = time_series ( name , start , end , step , values ) result_series . path_expression = name return [ result_series ]	0
Q_222	return the struct format and list of the size and value	C_222_2	def replace_webhook ( self , scaling_group , policy , webhook , name , metadata = none ) return self . _manager . replace_webhook ( scaling_group , policy , webhook , name , metadata = metadata )	0
Q_223	set user of a segment	C_223_0	def set_user ( self , user ) super ( segment , self ) . _check_ended ( ) self . user = user	1
Q_223	set user of a segment	C_223_1	def _set_lsp_secpath_auto_bandwidth ( self , v , load = false ) if hasattr ( v , string ) v = v . _utype ( v ) try t = yang_dyn_class ( v , base = lsp_secpath_auto_bandwidth . lsp_secpath_auto_bandwidth , is_container = string , presence = true , yang_name = string , rest_name = string , parent = self , path_helper = self . _path_helper , extmethods = self . _extmethods , register_paths = true , extensions = { u string { u string u string , u string u string , u string none , u string none , u string u string , u string u string , u string u string } } , namespace = string , defining_module = string , yang_type = string , is_config = true ) except ( type_error , value_error ) raise value_error ( { string string string string , string string , string string string container string lsp _ secpath _ auto _ bandwidth string autobw string tailf _ common string info string enable auto _ bandwidth on primary path string callpoint string mpls_lsp_sec_path_auto_bandwidth string cli _ full _ command string cli _ add _ mode string hidden string full string alt _ name string autobw string cli _ mode _ name string config _ router _ mpls _ lsp _ ( . / . / lsp _ name ) _ secpath _ ( . / secpath _ name ) _ autobw string urn brocade . com mgmt brocade _ mpls string brocade _ mpls string container string string , } ) self . _lsp_secpath_auto_bandwidth = t if hasattr ( self , string ) self . _set ( )	0
Q_223	set user of a segment	C_223_2	def update_fw_db_result ( self , tenant_id , os_status = none , dcnm_status = none , dev_status = none ) serv_obj = self . get_service_obj ( tenant_id ) serv_obj . update_fw_local_result ( os_status , dcnm_status , dev_status ) serv_obj . commit_fw_db_result ( )	0
Q_224	run a calculation bypass the database layer	C_224_0	def run ( job_ini , slowest = false , hc = none , param = string , concurrent_tasks = none , exports = string , loglevel = string , pdb = none ) dbserver . ensure_on ( ) if param params = oqvalidation . oq_param . check ( dict ( var . split ( string , number ) for var in param . split ( string ) ) ) else params = { } if slowest prof = c_profile . profile ( ) stmt = ( string string ) prof . runctx ( stmt , globals ( ) , locals ( ) ) pstat = calc_path + string prof . dump_stats ( pstat ) print ( string pstat ) print ( get_pstats ( pstat , slowest ) ) else _run ( job_ini , concurrent_tasks , pdb , loglevel , hc , exports , params )	1
Q_224	run a calculation bypass the database layer	C_224_1	def complex_dtype ( dtype , default = none ) dtype , dtype_in = np . dtype ( dtype ) , dtype if is_complex_floating_dtype ( dtype ) return dtype try complex_base_dtype = type_map_r2_c [ dtype . base ] except key_error if default is not none return default else raise value_error ( string string . format ( dtype_repr ( dtype_in ) ) ) else return np . dtype ( ( complex_base_dtype , dtype . shape ) )	0
Q_224	run a calculation bypass the database layer	C_224_2	def update_namelist_file ( self , rapid_namelist_file , new_namelist_file = none ) if os . path . exists ( rapid_namelist_file ) and rapid_namelist_file log ( string , string ) with open ( rapid_namelist_file , string ) as old_file for var in old_file var = var . strip ( ) if not var [ number ] . isalpha ( ) or not var continue line_split = var . split ( string ) attr = line_split [ number ] . strip ( ) value = none if len ( line_split ) > number value = line_split [ number ] . strip ( ) . replace ( string string string string , string ) try value = int ( value ) except value_error pass if attr . startswith ( string ) value = value . replace ( string , string ) if attr in dir ( self ) and not attr . startswith ( string ) if not getattr ( self , attr ) setattr ( self , attr , value ) else log ( string . format ( attr ) , string ) if new_namelist_file is none new_namelist_file = rapid_namelist_file self . generate_namelist_file ( new_namelist_file ) else log ( string , string )	0
Q_225	return an instance of a linestring node to class	C_225_0	def linestring_node_to_line ( node , with_depth = false ) assert string in node . tag crds = [ float ( var ) for var in node . nodes [ number ] . text . split ( ) ] if with_depth return line ( [ point ( crds [ iloc ] , crds [ iloc + number ] , crds [ iloc + number ] ) for iloc in range ( number , len ( crds ) , number ) ] ) else return line ( [ point ( crds [ iloc ] , crds [ iloc + number ] ) for iloc in range ( number , len ( crds ) , number ) ] )	1
Q_225	return an instance of a linestring node to class	C_225_1	def time_slice ( self , timerange , surf = false ) string string string if isinstance ( timerange [ number ] , str ) trange = cnes_convert ( timerange ) [ number ] else trange = timerange return self . slice ( string , trange , surf = surf )	0
Q_225	return an instance of a linestring node to class	C_225_2	def imagetransformer_b12l_4h_b128_h512_uncond_dr01_im ( ) hparams = imagetransformer_b12l_4h_b256_uncond_dr03_tpu ( ) update_hparams_for_tpu ( hparams ) hparams . batch_size = number hparams . optimizer = string hparams . learning_rate_schedule = string hparams . learning_rate_warmup_steps = number hparams . layer_prepostprocess_dropout = number . number return hparams	0
Q_226	calculate and return the rupture length and width	C_226_0	def _get_rupture_dimensions ( src , mag , nodal_plane ) area = src . magnitude_scaling_relationship . get_median_area ( mag , nodal_plane . rake ) rup_length = math . sqrt ( area * src . rupture_aspect_ratio ) rup_width = area / rup_length seismogenic_layer_width = ( src . lower_seismogenic_depth _ src . upper_seismogenic_depth ) max_width = ( seismogenic_layer_width / math . sin ( math . radians ( nodal_plane . dip ) ) ) if rup_width > max_width rup_width = max_width rup_length = area / rup_width return rup_length , rup_width	1
Q_226	calculate and return the rupture length and width	C_226_1	def post_build ( self , p , pay ) p + = pay if self . auxdlen = number print ( string ) print ( string ) return p	0
Q_226	calculate and return the rupture length and width	C_226_2	def plt_goids ( gosubdag , fout_img , goids , * * kws_plt ) gosubdag_plt = go_sub_dag ( goids , gosubdag . go2obj , rcntobj = gosubdag . rcntobj , * * kws_plt ) godagplot = go_sub_dag_plot ( gosubdag_plt , * * kws_plt ) godagplot . plt_dag ( fout_img ) return godagplot	0
Q_227	return the time of the catalogue as a decimal	C_227_0	def get_decimal_time ( self ) return decimal_time ( self . data [ string ] , self . data [ string ] , self . data [ string ] , self . data [ string ] , self . data [ string ] , self . data [ string ] )	1
Q_227	return the time of the catalogue as a decimal	C_227_1	def request ( self ) self . _request . url = string . format ( self . tcex . default_args . tc_api_path , self . _request_uri ) self . _apply_filters ( ) self . tcex . log . debug ( u string . format ( self . _request . url ) ) response = self . _request . send ( stream = self . _stream ) data , status = self . _request_process ( response ) return { string data , string response , string status }	0
Q_227	return the time of the catalogue as a decimal	C_227_2	def _reconstruct_object ( typ , obj , axes , dtype ) try typ = typ . type except attribute_error pass res_t = np . result_type ( obj . dtype , dtype ) if ( not isinstance ( typ , partial ) and issubclass ( typ , pd . core . generic . pandas_object ) ) return typ ( obj , dtype = res_t , * * axes ) if hasattr ( res_t , string ) and typ = = np . bool_ and res_t = np . bool_ ret_value = res_t . type ( obj ) else ret_value = typ ( obj ) . astype ( res_t ) if len ( obj . shape ) = = number and len ( obj ) = = number if not isinstance ( ret_value , np . ndarray ) ret_value = np . array ( [ ret_value ] ) . astype ( res_t ) return ret_value	0
Q_228	in the case of the base model the basin depth term be switch off	C_228_0	def _get_basin_depth_term ( self , c , sites , period ) return np . zeros ( len ( sites . vs30 ) , dtype = float )	1
Q_228	in the case of the base model the basin depth term be switch off	C_228_1	def client_new ( ) form = client_form ( request . form ) if form . validate_on_submit ( ) c = client ( user_id = current_user . get_id ( ) ) c . gen_salt ( ) form . populate_obj ( c ) db . session . add ( c ) db . session . commit ( ) return redirect ( url_for ( string , client_id = c . client_id ) ) return render_template ( string , form = form , )	0
Q_228	in the case of the base model the basin depth term be switch off	C_228_2	def pvpc_url_dia ( dt_day ) if type ( dt_day ) is str return server + string + string + dt_day else return server + string + string + dt_day . date ( ) . isoformat ( )	0
Q_229	start all the synchonization loop	C_229_0	def start_sync ( self ) if self . _syncing return [ var . start ( ) for var in self . _controllers ] [ var . wait_to_start ( ) for var in self . _controllers ] self . _primitive_manager . start ( ) self . _primitive_manager . _running . wait ( ) self . _syncing = true logger . info ( string )	1
Q_229	start all the synchonization loop	C_229_1	def _compile_update_from ( self , query ) if not query . joins return string froms = [ ] for var in query . joins froms . append ( self . wrap_table ( var . table ) ) if len ( froms ) return string string . var ( froms ) return string	0
Q_229	start all the synchonization loop	C_229_2	def get_stdout ( self , os_path = none , skip_sub_command = false ) sub_command = none if skip_sub_command else self . stdout_sub_command out , path = self . _get_out_and_path ( self . stdout , self . stdout_root , sub_command , os_path ) if hasattr ( out , string ) return out . stdin return out	0
Q_230	read an item from an rlp string lazily	C_230_0	def consume_item_lazy ( rlp , start ) p , t , l , s = consume_length_prefix ( rlp , start ) if t is bytes item , _ , end = consume_payload ( rlp , p , s , bytes , l ) return item , end else assert t is list return lazy_list ( rlp , s , s + l ) , s + l	1
Q_230	read an item from an rlp string lazily	C_230_1	def walk_recursive ( f , data ) results = { } if isinstance ( data , list ) return [ walk_recursive ( f , var ) for var in data ] elif isinstance ( data , dict ) results = funcy . walk_keys ( f , data ) for k , v in data . iteritems ( ) if isinstance ( v , dict ) results [ f ( k ) ] = walk_recursive ( f , v ) elif isinstance ( v , list ) results [ f ( k ) ] = [ walk_recursive ( f , var ) for var in v ] else return f ( data ) return results	0
Q_230	read an item from an rlp string lazily	C_230_2	def print_stats ( self , stream = none ) if not stream stream = sys . stdout self . metadata . sort ( key = lambda x _ x . size ) stream . write ( string ( string , string , string , string ) ) for var in self . metadata stream . write ( string ( var . id , var . size , trunc ( var . type , number ) , trunc ( var . str , number ) ) ) stream . write ( string ( self . count , self . num_in_cycles , pp ( self . total_size ) ) )	0
Q_231	ensure the authorization header have a valid access token	C_231_0	def _headers ( self ) if not self . access_token or not self . access_token_expires self . _basic_login ( ) elif datetime . now ( ) > self . access_token_expires _ timedelta ( seconds = number ) self . _basic_login ( ) return { string header_accept , string string + self . access_token }	1
Q_231	ensure the authorization header have a valid access token	C_231_1	def _plot_connectivity_helper ( self , ii , ji , mat_datai , data , lims = [ number , number ] ) from matplotlib . pyplot import quiver , colorbar , clim , matshow i = np . isnan ( mat_datai ) ( ji = _ number ) ( mat_datai > = number ) mat_data = mat_datai [ i ] j = ji [ i ] i = ii [ i ] x = i . astype ( float ) data . shape [ number ] y = i . astype ( float ) / / data . shape [ number ] x1 = ( j . astype ( float ) data . shape [ number ] ) . ravel ( ) y1 = ( j . astype ( float ) / / data . shape [ number ] ) . ravel ( ) nx = ( x1 _ x ) ny = ( y1 _ y ) matshow ( data , cmap = string ) colorbar ( ) clim ( lims ) quiver ( x , y , nx , ny , mat_data . ravel ( ) , angles = string , scale_units = string , scale = number , cmap = string ) colorbar ( ) clim ( [ number , number ] )	0
Q_231	ensure the authorization header have a valid access token	C_231_2	def build_nj_phylip ( alignment , outfile , outgroup , work_dir = string ) phy_file = op . join ( work_dir , string , string ) try align_io . write ( alignment , file ( phy_file , string ) , string ) except value_error print ( string , file = sys . stderr ) return none seqboot_out = phy_file . rsplit ( string , number ) [ number ] + string seqboot_cl = f_seq_boot_commandline ( fphylip_bin ( string ) , sequence = phy_file , outfile = seqboot_out , seqtype = string , reps = number , seed = number ) stdout , stderr = seqboot_cl ( ) logging . debug ( string seqboot_cl ) dnadist_out = phy_file . rsplit ( string , number ) [ number ] + string dnadist_cl = fdna_dist_commandline ( fphylip_bin ( string ) , sequence = seqboot_out , outfile = dnadist_out , method = string ) stdout , stderr = dnadist_cl ( ) logging . debug ( string dnadist_cl ) neighbor_out = phy_file . rsplit ( string , number ) [ number ] + string e = phy_file . rsplit ( string , number ) [ number ] + string neighbor_cl = f_neighbor_commandline ( fphylip_bin ( string ) , datafile = dnadist_out , outfile = e , outtreefile = neighbor_out ) stdout , stderr = neighbor_cl ( ) logging . debug ( string neighbor_cl ) consense_out = phy_file . rsplit ( string , number ) [ number ] + string e = phy_file . rsplit ( string , number ) [ number ] + string consense_cl = f_consense_commandline ( fphylip_bin ( string ) , intreefile = neighbor_out , outfile = e , outtreefile = consense_out ) stdout , stderr = consense_cl ( ) logging . debug ( string consense_cl ) dnadist_out0 = phy_file . rsplit ( string , number ) [ number ] + string dnadist_cl0 = fdna_dist_commandline ( fphylip_bin ( string ) , sequence = phy_file , outfile = dnadist_out0 , method = string ) stdout , stderr = dnadist_cl0 ( ) logging . debug ( string dnadist_cl0 ) consensustree1 = phy_file . rsplit ( string , number ) [ number ] + string run_ffitch ( distfile = dnadist_out0 , outtreefile = consensustree1 , intreefile = consense_out ) ct_s = tree ( consense_out ) if outgroup t1 = consensustree1 + string t2 = smart_reroot ( consensustree1 , outgroup , t1 ) if t2 = = t1 outfile = outfile . replace ( string , string ) ct_b = tree ( t2 ) else ct_b = tree ( consensustree1 ) nodesupport = { } for var in ct_s . traverse ( string ) node_children = tuple ( sorted ( [ f . name for f in var ] ) ) if len ( node_children ) > number nodesupport [ node_children ] = var . dist / number . for k , v in nodesupport . items ( ) ct_b . get_common_ancestor ( * k ) . support = v print ( ct_b ) ct_b . write ( format = number , outfile = outfile ) try s = op . getsize ( outfile ) except os_error s = number if s logging . debug ( string outfile ) return outfile , phy_file else logging . debug ( string ) return none	0
Q_232	dispatch to the appropriate ngettext method to handle text object	C_232_0	def ungettext ( self ) if six . py2 return self . _translations . ungettext else return self . _translations . ngettext	1
Q_232	dispatch to the appropriate ngettext method to handle text object	C_232_1	def name ( self ) self . open ( ) name = lvm_pv_get_name ( self . handle ) self . close ( ) return name	0
Q_232	dispatch to the appropriate ngettext method to handle text object	C_232_2	def put_attributes ( self , item_name , attributes , replace = true , expected_value = none ) return self . connection . put_attributes ( self , item_name , attributes , replace , expected_value )	0
Q_233	"return alpha view of a give tagint _bit color q_image_ "" s memory"	C_233_0	def alpha_view ( qimage ) bytes = byte_view ( qimage , byteorder = none ) if bytes . shape [ number ] = number raise value_error ( string ) return bytes [ . , _bgra [ number ] ]	1
Q_233	"return alpha view of a give tagint _bit color q_image_ "" s memory"	C_233_1	def _get_shard ( self , shard ) wraps ( api_wrapper . _get_shard ) def get_shard ( * arg , * * kwargs ) string string { } string string . format ( shard ) return self . get_shards ( shard ( shard , * arg , * * kwargs ) ) return get_shard	0
Q_233	"return alpha view of a give tagint _bit color q_image_ "" s memory"	C_233_2	def _normalize ( value ) if hasattr ( value , string ) value = value . value if value is not none value = long ( value ) return value	0
Q_234	get the request header entry by keyword name	C_234_0	def get ( self , item , default_value = none ) found , name = self . _contains_and_name ( item ) if found return self . _record_map [ name ] [ string ] else return default_value	1
Q_234	get the request header entry by keyword name	C_234_1	def get_command ( self , ctx , name ) try mod = importlib . import_module ( string . format ( name = name ) ) return mod . cli except ( import_error , attribute_error ) return	0
Q_234	get the request header entry by keyword name	C_234_2	def delete_contacts ( self , ids list [ int ] ) contacts = [ ] for var in ids try input_user = self . resolve_peer ( var ) except peer_id_invalid continue else if isinstance ( input_user , types . input_peer_user ) contacts . append ( input_user ) return self . send ( functions . contacts . delete_contacts ( id = contacts ) )	0
Q_235	send out the correspondence	C_235_0	def _correspond ( self , ticket_id , text = string , action = string , cc = string , bcc = string , content_type = string , files = [ ] ) post_data = { string string string string . format ( str ( ticket_id ) , action , re . sub ( r string , r string , text ) , cc , bcc , content_type ) } for var in files post_data [ string ] + = string . format ( var [ number ] , ) msg = self . _request ( string . format ( str ( ticket_id ) , ) , post_data = post_data , files = files ) return self . _get_status_code ( msg ) = = number	1
Q_235	send out the correspondence	C_235_1	def setup_restart ( ) exit_code = os . environ . get ( string , none ) if exit_code is none try atexit . unregister ( _on_exit ) except attribute_error atexit . _exithandlers = filter ( lambda exit_hnd exit_hnd [ number ] = _on_exit , atexit . _exithandlers ) _start_restart_loop ( none , in_atexit = false )	0
Q_235	send out the correspondence	C_235_2	def init_original_response ( self ) if string in self . request self . request [ string ] = json . dumps ( self . request . pop ( string ) ) . encode ( self . encoding ) r1 = self . req . request ( retry = self . retry , timeout = self . timeout , * * self . request ) resp = r1 . x assert resp , value_error ( string self . request ) self . encoding = self . encoding or resp . encoding self . original_response = self . ensure_response ( r1 ) return self . original_response	0
Q_236	print system and cluster info	C_236_0	def execute ( self ) if not self . cmd . is_conn_available ( ) return if self . cmd . connection . lowest_server_version > = sysinfo_min_version success , rows = self . _sys_info ( ) self . cmd . exit_code = self . cmd . exit_code or int ( not success ) if success for var in rows self . cmd . pprint ( var . rows , var . cols ) self . cmd . logger . info ( string ) else tmpl = string sysinfo string self . cmd . logger . warn ( tmpl . format ( version = self . cmd . connection . lowest_server_version ) )	1
Q_236	print system and cluster info	C_236_1	def email_user ( video , error = none ) html = render_to_string ( string , { string video . author , string error , string video , string frog_site_url , } ) subject , from_email , to = string . format ( error or string ) , string , video . author . email text_content = string html_content = html send_mail ( subject , text_content , from_email , [ to ] , html_message = html_content )	0
Q_236	print system and cluster info	C_236_2	def lstm_hidden_bias ( tensor torch . tensor ) _ > none tensor . data . zero_ ( ) hidden_size = tensor . shape [ number ] / / number tensor . data [ hidden_size ( number * hidden_size ) ] = number . number	0
Q_237	return the width of a single bar when width of the page be give	C_237_0	def _get_width ( maxwidth ) width = maxwidth / number if maxwidth < = number width = maxwidth elif number < maxwidth < = number width = maxwidth / number return width	1
Q_237	return the width of a single bar when width of the page be give	C_237_1	def flatten_trees ( root , node_selector callable [ [ l_node ] , bool ] ) for var in root . children if var . children flatten_trees ( var , node_selector ) reducible_children = set ( ) for var in root . children if node_selector ( var ) reducible_children . add ( var ) while reducible_children _tree_root = reducible_children . pop ( ) reducible_children . add ( _tree_root ) tree_root = search_root_of_tree ( reducible_children , _tree_root ) reduced_nodes , input_edges = collect_nodes_in_tree ( tree_root , reducible_children ) if len ( reduced_nodes ) > number new_name = reduced_nodes [ number ] . name new_node = root . add_node ( new_name ) o = new_node . add_port ( string , port_type . output , port_side . east ) o_edges = tree_root . east [ number ] . outgoing_edges for outputedge in list ( o_edges ) dsts = list ( outputedge . dsts ) assert len ( dsts ) > number outputedge . remove ( ) root . add_hyper_edge ( [ o , ] , dsts , origin_obj = outputedge . origin_obj ) for i , ( i_n , i_p , i_e ) in enumerate ( input_edges ) name = none index = len ( input_edges ) _ i _ number if hasattr ( i_e . origin_obj , string ) w = i_e . origin_obj . _dtype . bit_length ( ) if w > number name = string ( ( index + number ) * w , index * w ) else name = none if name is none name = string ( index ) inp = new_node . add_port ( name , port_type . input , port_side . west ) i_e . remove_target ( i_p ) i_e . add_target ( inp ) for n in reduced_nodes root . children . remove ( n ) reducible_children . remove ( n ) else reducible_children . remove ( reduced_nodes [ number ] )	0
Q_237	return the width of a single bar when width of the page be give	C_237_2	def ranges ( self ) steps = [ ] for var in self . _parameters if var [ string ] = = string steps . append ( var [ string ] ) else if var [ string ] > number start = var [ string ] stop = var [ string ] if start > stop step = var [ string ] * _ number else step = var [ string ] nsteps = self . n_steps_for_param ( var ) step_tmp = np . linspace ( start , start + step * ( nsteps _ number ) , nsteps _ number ) step_tmp = np . append ( step_tmp , stop ) steps . append ( np . around ( step_tmp , number ) ) else assert var [ string ] = = var [ string ] steps . append ( [ var [ string ] ] ) return steps	0
Q_238	give a tuple represent the position of the	C_238_0	def _destination_position ( pdf , destination ) pagewidth = pdf . get_page ( pdf . get_destination_page_number ( destination ) ) . crop_box . lower_right [ number ] if not destination . left or not destination . top raise incomplete_coordinates_error ( destination ) column = ( number * destination . left ) / / pagewidth return ( pdf . get_destination_page_number ( destination ) , column , _ destination . top , destination . left )	1
Q_238	give a tuple represent the position of the	C_238_1	def get_method_names ( obj ) method_names = [ ] for var in dir ( obj ) method = getattr ( obj , var ) if method_reflector . _is_method ( method , var ) method_names . append ( var ) return method_names	0
Q_238	give a tuple represent the position of the	C_238_2	def parse_noise ( raw_array ) nn_genes = [ ] for var in range ( number , ( raw_array . shape [ number ] ) ) count0 = np . asarray ( np . where ( raw_array [ , var ] = = number ) ) count1 = np . asarray ( np . where ( raw_array [ , var ] = = number ) ) if ( ( count1 . shape [ number ] + count0 . shape [ number ] ) < raw_array . shape [ number ] ) nn_genes = np . append ( nn_genes , var ) return nn_genes . astype ( int )	0
Q_239	set wikidata entity from query claim	C_239_0	def _marshal_claims ( self , query_claims ) claims = reduce_claims ( query_claims ) self . data [ string ] = claims entities = set ( ) for var in claims if self . user_labels if var in self . user_labels or var = = string entities . add ( var ) else continue else entities . add ( var ) for val in claims [ var ] if utils . is_text ( val ) and re . match ( r string , val ) entities . add ( val ) self . data [ string ] = list ( entities )	1
Q_239	set wikidata entity from query claim	C_239_1	def plot_decision_boundary ( model , x , y , step = number . number , figsize = ( number , number ) , alpha = number . number , size = number ) x_min , x_max = x [ , number ] . min ( ) _ number , x [ , number ] . max ( ) + number y_min , y_max = x [ , number ] . min ( ) _ number , x [ , number ] . max ( ) + number xx , yy = np . meshgrid ( np . arange ( x_min , x_max , step ) , np . arange ( y_min , y_max , step ) ) f , ax = plt . subplots ( figsize = figsize ) z = model . predict ( np . c_ [ xx . ravel ( ) , yy . ravel ( ) ] ) z = z . reshape ( xx . shape ) ax . contourf ( xx , yy , z , alpha = alpha ) ax . scatter ( x [ , number ] , x [ , number ] , c = y , s = size , edgecolor = string ) plt . show ( )	0
Q_239	set wikidata entity from query claim	C_239_2	def set_register ( self , motors ) if not motors return ids = [ var . id for var in motors ] values = ( var . _dict_ [ self . varname ] for var in motors ) getattr ( self . io , string . format ( self . regname ) ) ( dict ( zip ( ids , values ) ) ) for var in motors var . _write_synced [ self . varname ] . done ( )	0
Q_240	when use ah all tagstr field must be tagstr before calculate	C_240_0	def zero_mutable_fields ( pkt , sending = false ) if pkt . haslayer ( ah ) pkt [ ah ] . icv = chr ( number ) * len ( pkt [ ah ] . icv ) else raise type_error ( string ) if pkt . version = = number pkt . tos = number pkt . flags = number pkt . ttl = number pkt . chksum = number immutable_opts = [ ] for var in pkt . options if var . option in immutable_ipv4_options immutable_opts . append ( var ) else immutable_opts . append ( raw ( chr ( number ) * len ( var ) ) ) pkt . options = immutable_opts else pkt . tc = number pkt . fl = number pkt . hlim = number next_hdr = pkt . payload while isinstance ( next_hdr , ( i_pv6_ext_hdr_hop_by_hop , i_pv6_ext_hdr_routing , i_pv6_ext_hdr_dest_opt ) ) if isinstance ( next_hdr , ( i_pv6_ext_hdr_hop_by_hop , i_pv6_ext_hdr_dest_opt ) ) for var in next_hdr . options if var . otype number var . optdata = chr ( number ) * var . optlen elif isinstance ( next_hdr , i_pv6_ext_hdr_routing ) and sending next_hdr . segleft = number if next_hdr . addresses final = next_hdr . addresses . pop ( ) next_hdr . addresses . insert ( number , pkt . dst ) pkt . dst = final else break next_hdr = next_hdr . payload return pkt	1
Q_240	when use ah all tagstr field must be tagstr before calculate	C_240_1	def report ( self ) _ > none while self . _stack self . stop ( self . _stack [ _ number ] ) now = get_now_utc_pendulum ( ) grand_total = datetime . timedelta ( ) overall_duration = now _ self . _overallstart for var , var in self . _totaldurations . items ( ) grand_total + = var log . info ( string ) summaries = [ ] for var , var in self . _totaldurations . items ( ) n = self . _count [ var ] total_sec = var . total_seconds ( ) mean = total_sec / n if n > number else none summaries . append ( { string total_sec , string ( string . format ( var , total_sec , ( number * total_sec / grand_total . total_seconds ( ) ) , n , mean ) ) , } ) summaries . sort ( key = lambda x x [ string ] , reverse = true ) for s in summaries log . info ( s [ string ] ) if not self . _totaldurations log . info ( string ) unmetered = overall_duration _ grand_total log . info ( string , unmetered . total_seconds ( ) , number * unmetered . total_seconds ( ) / overall_duration . total_seconds ( ) ) log . info ( string , grand_total . total_seconds ( ) )	0
Q_240	when use ah all tagstr field must be tagstr before calculate	C_240_2	def monitor ( self , name , handler , request = none , * * kws ) chan = self . _channel ( name ) return subscription ( context = self , nt = self . _nt , channel = chan , handler = mon_handler ( handler ) , pv_request = wrap_request ( request ) , * * kws )	0
Q_241	channel mode modify acknowledge section tagint	C_241_0	def channel_mode_modify_acknowledge ( ) a = tp_pd ( pd = number ) b = message_type ( mes_type = number ) c = channel_description2 ( ) d = channel_mode ( ) packet = a / b / c / d return packet	1
Q_241	channel mode modify acknowledge section tagint	C_241_1	def _as_in_context ( data , ctx ) if isinstance ( data , nd . nd_array ) return data . as_in_context ( ctx ) elif isinstance ( data , ( list , tuple ) ) return [ _as_in_context ( var , ctx ) for var in data ] return data	0
Q_241	channel mode modify acknowledge section tagint	C_241_2	def stop_receive ( self , fd ) self . _rfds . pop ( fd , none ) self . _update ( fd )	0
Q_242	retrieve acknowledge section tagint	C_242_0	def retrieve_acknowledge ( ) a = tp_pd ( pd = number ) b = message_type ( mes_type = number ) packet = a / b return packet	1
Q_242	retrieve acknowledge section tagint	C_242_1	def can_write ( variable ) if variable not in variables_enum raise value_error ( string ) return lifepo4wered_so . access_lifepo4wered ( variable . value , defines . access_write )	0
Q_242	retrieve acknowledge section tagint	C_242_2	def get_spatial_anchor_descriptor ( self , un_handle , pch_descriptor_out ) fn = self . function_table . get_spatial_anchor_descriptor pun_descriptor_buffer_len_in_out = c_uint32 ( ) result = fn ( un_handle , pch_descriptor_out , byref ( pun_descriptor_buffer_len_in_out ) ) return result , pun_descriptor_buffer_len_in_out . value	0
Q_243	set the pin value to tagint or tagint	C_243_0	def set ( pin , value ) string string string if value is low value = number value = int ( bool ( value ) ) log . debug ( string . format ( pin , value ) ) f = _open [ pin ] . value _write ( f , value )	1
Q_243	set the pin value to tagint or tagint	C_243_1	def _outer_distance_mod_n ( ref , est , modulus = number ) ref_mod_n = np . mod ( ref , modulus ) est_mod_n = np . mod ( est , modulus ) abs_diff = np . abs ( np . subtract . outer ( ref_mod_n , est_mod_n ) ) return np . minimum ( abs_diff , modulus _ abs_diff )	0
Q_243	set the pin value to tagint or tagint	C_243_2	def remove_context ( self , name ) self . _context ( name ) del self . contexts [ name ] self . _flush_tools ( )	0
Q_244	handle api request response transport	C_244_0	def do_request ( self , method , url , data = none , headers = none , params = none ) response_content = none if data if headers headers . update ( { string string } ) else headers = { string string } if self . _debug print md . parse_string ( data ) . toprettyxml ( ) response = self . _session . request ( method , url , headers = headers , params = params , data = data ) if string in response . headers if response . headers [ string ] . find ( string ) = _ number response_content = xmloperations . xml_to_dict ( et . fromstring ( response . content ) ) elif response . headers [ string ] . find ( string ) = _ number response_content = json . loads ( response . content ) else response_content = response . content response_odict = ordered_dict ( [ ( string , response . status_code ) , ( string , response_content ) , ( string , none ) , ( string , none ) , ( string , none ) ] ) if string in response . headers response_odict [ string ] = response . headers [ string ] response_odict [ string ] = response . headers [ string ] . split ( string ) [ _ number ] if string in response . headers response_odict [ string ] = response . headers [ string ] if response . status_code not in [ number , number , number , number ] if self . fail_mode = = string sys . exit ( string . format ( response . status_code , response_content ) ) elif self . fail_mode = = string raise nsx_error ( response . status_code , response_content ) elif self . fail_mode = = string pass return response_odict	1
Q_244	handle api request response transport	C_244_1	def _close_event_stream ( self ) self . _subscribed = false del self . _events [ ] self . _event_handle . clear ( )	0
Q_244	handle api request response transport	C_244_2	def temp_file ( content = none , suffix = string , prefix = string , parent_dir = none ) binary = isinstance ( content , ( bytes , bytearray ) ) parent_dir = parent_dir if parent_dir is none else str ( parent_dir ) fd , abs_path = tempfile . mkstemp ( suffix , prefix , parent_dir , text = false ) path = pathlib . path ( abs_path ) try try if content os . write ( fd , content if binary else content . encode ( ) ) finally os . close ( fd ) yield path . resolve ( ) finally with temporary . util . allow_missing_file ( ) path . unlink ( )	0
Q_245	parse a config from a magic cell body for select config key	C_245_0	def parse_config_for_selected_keys ( content , keys ) config_items = { var none for var in keys } if not content return config_items , content stripped = content . strip ( ) if len ( stripped ) = = number return { } , none elif stripped [ number ] = = string config = json . loads ( content ) else config = yaml . load ( content ) if not isinstance ( config , dict ) raise value_error ( string ) for var in keys config_items [ var ] = config . pop ( var , none ) if not config return config_items , none if stripped [ number ] = = string content_out = json . dumps ( config , indent = number ) else content_out = yaml . dump ( config , default_flow_style = false ) return config_items , content_out	1
Q_245	parse a config from a magic cell body for select config key	C_245_1	def finalize ( self , * * kwargs ) self . ax . set_ylim ( _ number , len ( self . indexed_words_ ) ) self . ax . set_title ( string ) self . ax . set_xlabel ( string ) self . ax . grid ( false ) if not all ( self . classes_ = = np . array ( [ self . null_class ] ) ) box = self . ax . get_position ( ) self . ax . set_position ( [ box . x0 , box . y0 , box . width * number . number , box . height ] ) self . ax . legend ( loc = string , bbox_to_anchor = ( number , number . number ) )	0
Q_245	parse a config from a magic cell body for select config key	C_245_2	def sanitize_op ( self , op_data ) op_data = super ( blockstack_db , self ) . sanitize_op ( op_data ) to_remove = get_state_invariant_tags ( ) for var in to_remove if var in op_data . keys ( ) del op_data [ var ] opcode_family = op_get_opcode_name ( op_data [ string ] ) mutate_fields = op_get_mutate_fields ( opcode_family ) for mf in mutate_fields if not op_data . has_key ( mf ) log . debug ( string s . s string ( opcode_family , mf ) ) op_data [ mf ] = none for extra_field in [ string ] if extra_field in op_data del op_data [ extra_field ] return op_data	0
Q_246	issue a request to retrieve information about a dataset	C_246_0	def datasets_get ( self , dataset_name ) url = api . _endpoint + ( api . _datasets_path dataset_name ) return datalab . utils . http . request ( url , credentials = self . _credentials )	1
Q_246	issue a request to retrieve information about a dataset	C_246_1	def evaluate ( self , dataset , metric = string , verbose = true , batch_size = number ) import os , json , math if ( batch_size < number ) raise value_error ( string batch_size string ) extracted_features = self . _extract_features ( dataset , verbose = verbose , batch_size = batch_size ) extracted_features [ self . target ] = dataset [ self . target ] metrics = self . classifier . evaluate ( extracted_features , metric = metric , with_predictions = true ) predictions = metrics [ string ] [ string ] state = self . _proxy_ . get_state ( ) labels = state [ string ] def entropy ( probs ) return _reduce ( lambda x , y x + ( y * math . log ( number / y , number ) if y > number else number ) , probs , number ) / math . log ( len ( probs ) , number ) def confidence ( probs ) return max ( probs ) def relative_confidence ( probs ) lp = len ( probs ) return probs [ lp _ number ] _ probs [ lp _ number ] def get_confusion_matrix ( extended_test , labels ) sf_confusion_matrix = { string [ ] , string [ ] , string [ ] } for var in labels for predicted_l in labels sf_confusion_matrix [ string ] . append ( var ) sf_confusion_matrix [ string ] . append ( predicted_l ) sf_confusion_matrix [ string ] . append ( number ) sf_confusion_matrix = _tc . s_frame ( sf_confusion_matrix ) sf_confusion_matrix = sf_confusion_matrix . join ( extended_test . groupby ( [ string , string ] , { string _tc . aggregate . count } ) , how = string , on = [ string , string ] ) sf_confusion_matrix = sf_confusion_matrix . fillna ( string , number ) label_column = _tc . s_frame ( { string extended_test [ string ] } ) predictions = extended_test [ string ] for i in range ( number , len ( labels ) ) new_test_data = label_column . add_columns ( [ predictions . apply ( lambda probs probs [ i ] ) , predictions . apply ( lambda probs labels [ i ] ) ] , [ string , string ] ) if ( i = = number ) test_longer_form = new_test_data else test_longer_form = test_longer_form . append ( new_test_data ) if len ( extended_test ) is number sf_confusion_matrix = sf_confusion_matrix . rename ( { string string , string string } ) else sf_confusion_matrix = sf_confusion_matrix . join ( test_longer_form . groupby ( [ string , string ] , { string _tc . aggregate . sum ( string ) } ) , how = string , on = [ string , string ] ) sf_confusion_matrix = sf_confusion_matrix . rename ( { string string } ) . fillna ( string , number ) def wo_divide_by_zero ( a , b ) if b = = number return none else return a * number . number / b sf_confusion_matrix [ string ] = sf_confusion_matrix . join ( sf_confusion_matrix . groupby ( string , { string _tc . aggregate . sum ( string ) } ) , how = string ) . apply ( lambda x wo_divide_by_zero ( x [ string ] , x [ string ] ) ) return sf_confusion_matrix . fillna ( string , number ) def hcluster_sort ( vectors , dist_fn ) distances = [ ] vecs = list ( vectors ) [ ] for i in range ( number , len ( vecs ) ) for j in range ( i + number , len ( vecs ) ) distances . append ( { string vecs [ i ] , string vecs [ j ] , string dist_fn ( vecs [ i ] , vecs [ j ] ) } ) distances = sorted ( distances , key = lambda d d [ string ] ) excluding_names = [ ] while ( len ( distances ) > number ) min_dist = distances [ number ] new_vec = { string str ( min_dist [ string ] [ string ] ) + string + str ( min_dist [ string ] [ string ] ) , string min_dist [ string ] . get ( string , [ min_dist [ string ] ] ) + min_dist [ string ] . get ( string , [ min_dist [ string ] ] ) } excluding_names = [ min_dist [ string ] [ string ] , min_dist [ string ] [ string ] ] vecs = filter ( lambda v v [ string ] not in excluding_names , vecs ) distances = filter ( lambda dist ( dist [ string ] [ string ] not in excluding_names ) and ( dist [ string ] [ string ] not in excluding_names ) , distances ) for v in vecs total = number for vi in v . get ( string , [ v ] ) for vj in new_vec [ string ] total + = dist_fn ( vi , vj ) distances . append ( { string v , string new_vec , string total / len ( v . get ( string , [ v ] ) ) / len ( new_vec [ string ] ) } ) vecs . append ( new_vec ) distances = sorted ( distances , key = lambda d d [ string ] ) return vecs def l2_dist ( v1 , v2 ) dist = number for i in range ( number , len ( v1 [ string ] ) ) dist + = math . pow ( v1 [ string ] [ i ] _ v2 [ string ] [ i ] , number ) return math . pow ( dist , number . number ) evaluation_result = { k metrics [ k ] for k in [ string , string , string , string , string , string ] } evaluation_result [ string ] = len ( dataset ) for k in [ string , string , string , string , string , string , string , string ] evaluation_result [ k ] = getattr ( self , k ) extended_test = dataset . add_column ( predictions , string ) extended_test [ string ] = dataset [ self . target ] extended_test = extended_test . add_columns ( [ extended_test . apply ( lambda d labels [ d [ string ] . index ( confidence ( d [ string ] ) ) ] ) , extended_test . apply ( lambda d entropy ( d [ string ] ) ) , extended_test . apply ( lambda d confidence ( d [ string ] ) ) , extended_test . apply ( lambda d relative_confidence ( d [ string ] ) ) ] , [ string , string , string , string ] ) extended_test = extended_test . add_column ( extended_test . apply ( lambda d d [ string ] = = d [ string ] ) , string ) sf_conf_mat = get_confusion_matrix ( extended_test , labels ) confidence_threshold = number . number hesitant_threshold = number . number evaluation_result [ string ] = confidence_threshold evaluation_result [ string ] = hesitant_threshold evaluation_result [ string ] = string sf_hesitant_conf_mat = get_confusion_matrix ( extended_test [ extended_test [ evaluation_result [ string ] ] < hesitant_threshold ] , labels ) sf_confidently_wrong_conf_mat = get_confusion_matrix ( extended_test [ ( extended_test [ evaluation_result [ string ] ] > confidence_threshold ) ( extended_test [ string ] = = true ) ] , labels ) evaluation_result [ string ] = list ( sf_conf_mat ) evaluation_result [ string ] = list ( sf_hesitant_conf_mat ) evaluation_result [ string ] = list ( sf_confidently_wrong_conf_mat ) vectors = map ( lambda l { string l , string list ( sf_conf_mat [ sf_conf_mat [ string ] = = l ] . sort ( string ) [ string ] ) } , labels ) evaluation_result [ string ] = hcluster_sort ( vectors , l2_dist ) [ number ] [ string ] . split ( string ) per_l = extended_test . groupby ( [ string ] , { string _tc . aggregate . count , string _tc . aggregate . sum ( string ) } ) per_l [ string ] = per_l . apply ( lambda l l [ string ] * number . number / l [ string ] ) per_pl = extended_test . groupby ( [ string ] , { string _tc . aggregate . count , string _tc . aggregate . sum ( string ) } ) per_pl [ string ] = per_pl . apply ( lambda l l [ string ] * number . number / l [ string ] ) per_pl = per_pl . rename ( { string string } ) evaluation_result [ string ] = list ( per_l . join ( per_pl , on = string , how = string ) . select_columns ( [ string , string , string , string , string , string ] ) ) evaluation_result [ string ] = labels extended_test = extended_test . add_row_number ( string ) . rename ( { string string } ) evaluation_result [ string ] = extended_test evaluation_result [ string ] = self . feature return _evaluation ( evaluation_result )	0
Q_246	issue a request to retrieve information about a dataset	C_246_2	def create_network ( data_source ) with open ( _params_path , string ) as f model_params = yaml . safe_load ( f ) [ string ] network = network ( ) network . add_region ( string , string , string ) sensor_region = network . regions [ string ] . get_self ( ) sensor_region . encoder = create_encoder ( model_params [ string ] [ string ] ) sensor_region . data_source = data_source model_params [ string ] [ string ] = sensor_region . encoder . get_width ( ) network . add_region ( string , string , json . dumps ( model_params [ string ] ) ) network . add_region ( string , string , json . dumps ( model_params [ string ] ) ) cl_name = string model_params [ string ] . pop ( string ) network . add_region ( string , cl_name , json . dumps ( model_params [ string ] ) ) create_sensor_to_classifier_links ( network , string , string ) create_data_out_link ( network , string , string ) create_feed_forward_link ( network , string , string ) create_feed_forward_link ( network , string , string ) create_reset_link ( network , string , string ) create_reset_link ( network , string , string ) network . initialize ( ) return network	0
Q_247	provide additional validation of the argument collect by argparse	C_247_0	def get_arguments ( parser ) string args = parser . parse_args ( ) if args . example args . example_image , args . example_header = load ( args . example ) dimensions = args . example_image . ndim else dimensions = len ( args . shape ) if isinstance ( args . spacing , int ) args . spacing = [ args . spacing ] * dimensions elif len ( args . spacing ) = dimensions raise argparse . argument_type_error ( string . format ( string . join ( map ( str , args . spacing ) ) , dimensions ) ) if args . offset and len ( args . offset ) = dimensions raise argparse . argument_type_error ( string . format ( string . join ( map ( str , args . offset ) ) , dimensions ) ) if args . pixelspacing and len ( args . pixelspacing ) = dimensions raise argparse . argument_type_error ( string . format ( string . join ( map ( str , args . pixelspacing ) ) , dimensions ) ) return args	1
Q_247	provide additional validation of the argument collect by argparse	C_247_1	def update ( self , device_json = none , info_json = none , settings_json = none , avatar_json = none ) if device_json utils . update ( self . _device_json , device_json ) if avatar_json utils . update ( self . _avatar_json , avatar_json ) if info_json utils . update ( self . _info_json , info_json ) if settings_json utils . update ( self . _settings_json , settings_json )	0
Q_247	provide additional validation of the argument collect by argparse	C_247_2	def set_select ( cls , authors ) cls . select_el . html = string if not authors cls . select_el . disabled = true cls . select_el < = html . option ( string ) return cls . select_el . disabled = false for var in authors name = var . get ( string ) code = var . get ( string ) alt_name = var . get ( string , name ) if not ( name and code ) continue cls . code_to_data [ code ] = var cls . select_el < = html . option ( alt_name , value = code )	0
Q_248	represent heading as sql field list	C_248_0	def as_sql ( self ) return string . join ( string var if self . attributes [ var ] . sql_expression is none else string ( self . attributes [ var ] . sql_expression , var ) for var in self . names )	1
Q_248	represent heading as sql field list	C_248_1	"def generate_categories ( ) code_points_ranges = [ ] iso_15924_aliases = [ ] categories = [ ] match = re . compile ( r string ftp / / ftp . unicode . org / public / unidata / scripts . txt string iso_15924_aliases string categories string code_points_ranges string categories . json "" , categories_data )"	0
Q_248	represent heading as sql field list	C_248_2	def consultar_tipos_liquidacion ( self , sep = string ) string ret = self . client . consultar_tipos_liquidacion ( auth = { string self . token , string self . sign , string self . cuit , } , ) [ string ] self . _analizar_errores ( ret ) array = ret . get ( string , [ ] ) if sep is none return dict ( [ ( var [ string ] , var [ string ] ) for var in array ] ) else return [ ( string ( sep , sep , sep ) ) ( var [ string ] , var [ string ] ) for var in array ]	0
Q_249	load graph and weight data	C_249_0	def load ( self , data_dir , tf_input_var = none , tf_predict_var = none ) try latest_ckpt_fn = max ( filter ( lambda x os . path . splitext ( x ) [ _ number ] . startswith ( string ) or os . path . splitext ( x ) [ _ number ] . startswith ( string ) , glob . glob ( os . path . join ( data_dir , string ) ) ) , key = os . path . getctime ) latest_ckpt_time = str ( datetime . fromtimestamp ( os . path . getmtime ( latest_ckpt_fn ) ) ) fileext_div = latest_ckpt_fn . rfind ( string ) additional_ext = latest_ckpt_fn . rfind ( string , fileext_div + number ) if additional_ext < number latest_ckpt = latest_ckpt_fn else latest_ckpt = latest_ckpt_fn [ additional_ext ] except value_error raise file_not_found_error ( string string . format ( data_dir ) ) try latest_meta = max ( glob . iglob ( os . path . join ( data_dir , string ) ) , key = os . path . getctime ) except value_error raise file_not_found_error ( string string . format ( data_dir ) ) self . _sess = tf . session ( ) self . _sess . as_default ( ) self . _saver = tf . train . import_meta_graph ( latest_meta ) self . _saver . restore ( self . _sess , latest_ckpt ) self . _tf_input_var = self . _sess . graph . get_tensor_by_name ( tf_input_var ) self . _tf_predict_var = self . _sess . graph . get_tensor_by_name ( tf_predict_var ) self . _model_name = type ( self ) . _name_ self . _latest_ckpt_name = latest_ckpt_fn self . _latest_ckpt_time = latest_ckpt_time	1
Q_249	load graph and weight data	C_249_1	def solve ( solver , mzn , * dzn_files , data = none , include = none , stdlib_dir = none , globals_dir = none , allow_multiple_assignments = false , output_mode = string , timeout = none , two_pass = none , pre_passes = none , output_objective = false , non_unique = false , all_solutions = false , num_solutions = none , free_search = false , parallel = none , seed = none , * * kwargs ) args = _solve_args ( solver , timeout = timeout , two_pass = two_pass , pre_passes = pre_passes , output_objective = output_objective , non_unique = non_unique , all_solutions = all_solutions , num_solutions = num_solutions , free_search = free_search , parallel = parallel , seed = seed , * * kwargs ) args + = _flattening_args ( mzn , * dzn_files , data = data , stdlib_dir = stdlib_dir , globals_dir = globals_dir , output_mode = output_mode , include = include , allow_multiple_assignments = allow_multiple_assignments ) input = mzn if args [ _ number ] = = string else none t0 = _time ( ) try proc = _run_minizinc_proc ( * args , input = input ) except runtime_error as err raise mini_zinc_error ( mzn_file , args ) from err solve_time = _time ( ) _ t0 logger . info ( string . format ( solve_time ) ) return proc	0
Q_249	load graph and weight data	C_249_2	def space_list ( line str ) _ > list [ int ] spaces = [ ] for var , var in enumerate ( list ( line ) ) if var = = string spaces . append ( var ) return spaces	0
Q_250	connect to the device and start the read thread	C_250_0	def connect ( self ) string string string self . serial = serial . serial ( port = self . port , baudrate = self . baudrate , timeout = self . timeout ) self . alive = true self . rx_thread = threading . thread ( target = self . _read_loop ) self . rx_thread . daemon = true self . rx_thread . start ( )	1
Q_250	connect to the device and start the read thread	C_250_1	def mouse_press_event ( self , event ) self . set_pixmap ( self . _active_pixmap ) self . _menu and self . _menu . exec_ ( q_cursor . pos ( ) ) self . pressed . emit ( )	0
Q_250	connect to the device and start the read thread	C_250_2	def current ( self , value ) current = min ( max ( self . _min , value ) , self . _max ) self . _current = current if current > self . _stop self . _stop = current self . _start = current _ self . _width elif current < self . _start self . _start = current self . _stop = current + self . _width if abs ( self . _start _ self . _min ) < = self . _sticky_lenght self . _start = self . _min if abs ( self . _stop _ self . _max ) < = self . _sticky_lenght self . _stop = self . _max	0
Q_251	return inline cs from cs key value	C_251_0	def _to_inline_css ( self , style ) return string . join ( [ string . format ( convert_style_key ( var ) , var ) for var , var in style . items ( ) ] )	1
Q_251	return inline cs from cs key value	C_251_1	def block_sep0 ( self , y ) r return y [ ( slice ( none ) , ) * self . blkaxis + ( slice ( number , self . blkidx ) , ) ]	0
Q_251	return inline cs from cs key value	C_251_2	def fwriter ( filename , gz = false , bz = false ) if filename . endswith ( string ) gz = true elif filename . endswith ( string ) bz = true if gz if not filename . endswith ( string ) filename + = string return gzip . open ( filename , string ) elif bz if not filename . endswith ( string ) filename + = string return bz2 . bz2_file ( filename , string ) else return open ( filename , string )	0
Q_252	validate filter condition	C_252_0	def validate ( self ) val = self . _condition . lower ( ) if val and val [ number ] in string val = val [ number ] matcher = pattern_filter if not val or val . startswith ( string ) and val . endswith ( string ) pass elif val . replace ( string , string ) . isdigit ( ) matcher = float_filter elif self . _condition in ( true false ) matcher = bool_filter elif any ( val . endswith ( var ) for var in byte_size_filter . units ) and val [ _ number ] . isdigit ( ) matcher = byte_size_filter elif time_filter . timedelta_re . match ( val ) matcher = time_filter self . _inner = matcher ( self . _name , self . _condition )	1
Q_252	validate filter condition	C_252_1	def _ref_check ( self , case ) refs = [ var . _i for var in case . buses if var . type = = reference ] if len ( refs ) = = number return true , refs else logger . error ( string ) return false , refs	0
Q_252	validate filter condition	C_252_2	def _prepare_namespaces ( self ) self . namespaces = dict ( text = string , draw = string , table = string , office = string , xlink = string , svg = string , manifest = string , ) for var in self . tree_roots self . namespaces . update ( var . nsmap ) self . namespaces . pop ( none , none ) self . namespaces [ string ] = genshi_uri self . namespaces [ string ] = py3_o_uri	0
Q_253	transform the colorscale so that the minimal and maximal value	C_253_0	def scale ( self , vmin = number . , vmax = number . ) return step_colormap ( self . colors , index = [ vmin + ( vmax _ vmin ) * ( var _ self . vmin ) * number . / ( self . vmax _ self . vmin ) for var in self . index ] , vmin = vmin , vmax = vmax , )	1
Q_253	transform the colorscale so that the minimal and maximal value	C_253_1	def off ( self ) for var in self if isinstance ( var , ( output_device , composite_output_device ) ) var . off ( )	0
Q_253	transform the colorscale so that the minimal and maximal value	C_253_2	def text_till ( self , strings , keep_index = false ) if isinstance ( strings , str ) strings = [ strings ] original_index = self . index text = string matched_string = string while self . more test_against = self . characters ( len ( max ( strings , key = len ) ) ) for var in strings if var . startswith ( string ) if test_against [ number ] in ( string , string , string , string , string ) and test_against [ number ] . startswith ( var [ number ] ) matched_string = var break if test_against . startswith ( var ) matched_string = var break if matched_string break text + = self . pop ( ) self + = number if keep_index self . index = original_index return ( text , matched_string )	0
Q_254	"r tagstr "" calculate the expansibility factor for a nozzle or venturi nozzle"	C_254_0	def nozzle_expansibility ( d , do , p1 , p2 , k , beta = none ) r if beta is none beta = do / d beta2 = beta * beta beta4 = beta2 * beta2 tau = p2 / p1 term1 = k * tau * * ( number . number / k ) / ( k _ number . number ) term2 = ( number . number _ beta4 ) / ( number . number _ beta4 * tau * * ( number . number / k ) ) try term3 = ( number . number _ tau * * ( ( k _ number . number ) / k ) ) / ( number . number _ tau ) except zero_division_error term3 = ( k _ number . number ) / k return ( term1 * term2 * term3 ) * * number . number	1
Q_254	"r tagstr "" calculate the expansibility factor for a nozzle or venturi nozzle"	C_254_1	"def _look_for_interface ( self , network_backend ) result = yield from self . _execute ( string , [ self . _vmname , string ] ) interface = _ number for var in result . splitlines ( ) if string in var name , value = var . split ( string , number ) if name . startswith ( string ) and value . strip ( string "" ) = = network_backend try interface = int ( name [ number ] ) break except value_error continue return interface"	0
Q_254	"r tagstr "" calculate the expansibility factor for a nozzle or venturi nozzle"	C_254_2	def center_of_mass ( self , scalars_weight = false ) comfilter = vtk . vtk_center_of_mass ( ) comfilter . set_input_data ( self ) comfilter . set_use_scalars_as_weights ( scalars_weight ) comfilter . update ( ) return np . array ( comfilter . get_center ( ) )	0
Q_255	"r tagstr "" calculate the diameter ratio beta use to characterize a wedge"	C_255_0	def diameter_ratio_wedge_meter ( d , h ) r h_d = h / d t0 = number . number _ number . number * h_d t1 = acos ( t0 ) t2 = number . number * ( t0 ) t3 = ( h_d _ h_d * h_d ) * * number . number t4 = t1 _ t2 * t3 return ( number . / pi * t4 ) * * number . number	1
Q_255	"r tagstr "" calculate the diameter ratio beta use to characterize a wedge"	C_255_1	def generate_single_kcorrection_listing ( log , path_to_output_directory , path_to_spectral_database , model , rest_frame_filter , redshift , temporal_resolution = number . number ) import re import os import glob import yaml import pysynphot as syn file_name = path_to_output_directory + string stream = file ( file_name , string ) generated_l_cs = yaml . load ( stream ) filter_data = generated_l_cs [ model ] peak_mag = generated_l_cs [ model ] [ rest_frame_filter ] [ string ] peak_time = generated_l_cs [ model ] [ rest_frame_filter ] [ string ] stream . close ( ) pwd = os . getcwd ( ) path = path_to_spectral_database + string + model + string title = string ( model , ) os . chdir ( path ) spectrum_files = [ ] for var in glob . glob ( string ) var = path + var spectrum_files . append ( var ) os . chdir ( pwd ) re_time = re . compile ( r string ) filters = [ string , string , string , string ] for this_filter in filters str_red = string ( redshift , ) try log . debug ( string ) data_dir = path_to_output_directory + string ( model , this_filter ) os . makedirs ( data_dir ) except exception as e log . debug ( string ( str ( e ) , ) ) try log . debug ( string ) file_name = data_dir + string + str ( str_red ) . replace ( string , string ) + string stream = file ( file_name , string ) stream . close ( ) except exception as e log . critical ( string ( str ( e ) , ) ) return _ number times_list = [ ] file_dictionary = { } for var in spectrum_files this_time = float ( re_time . search ( var ) . group ( number ) ) file_dictionary [ this_time ] = var import collections ofile_dictionary = collections . ordered_dict ( sorted ( file_dictionary . items ( ) ) ) next_time = _ number . number for this_time , var in ofile_dictionary . iteritems ( ) log . debug ( string locals ( ) ) if this_time < next_time log . debug ( string locals ( ) ) continue else next_time = this_time + temporal_resolution this_time _ = peak_time wavelength_array , flux_array = extract_spectra_from_file ( log , var ) sp_rest = syn . array_spectrum ( wave = wavelength_array , flux = flux_array , waveunits = string , fluxunits = string ) try log . debug ( string ( rest_frame_filter , ) ) if rest_frame_filter in [ string , string , string , string ] obsmode = string ( rest_frame_filter , ) else obsmode = rest_frame_filter g_rest = calcphot ( log , wavelength_array = wavelength_array , flux_array = flux_array , obsmode = obsmode ) except exception as e if string in str ( e ) log . warning ( string ( rest_frame_filter , model , this_time , var , str ( e ) , ) ) continue elif string in str ( e ) log . warning ( string ( rest_frame_filter , model , this_time , var , str ( e ) , ) ) continue else log . warning ( string ( rest_frame_filter , model , this_time , var , str ( e ) , ) ) pass for this_filter in filters str_red = string ( redshift , ) sp_rest . convert ( string ) sp_obs = sp_rest . redshift ( redshift ) sp_obs . convert ( string ) sp_rest . convert ( string ) data_dir = path_to_output_directory + string ( model , this_filter ) try log . debug ( string ) file_name = data_dir + string + str ( str_red ) . replace ( string , string ) + string stream = file ( file_name , string ) except exception as e log . critical ( string ( str ( e ) , ) ) return _ number try log . debug ( string ( str_red , this_filter , model ) ) if this_filter in [ string , string , string , string ] obsmode = string ( this_filter , ) else obsmode = this_filter filter_obs = calcphot ( log , wavelength_array = sp_obs . wave , flux_array = sp_obs . flux , obsmode = obsmode ) except exception as e if string in str ( e ) log . warning ( string ( str_red , this_filter , model , str ( e ) , ) ) continue elif string in str ( e ) log . warning ( string ( str_red , this_filter , model , str ( e ) , ) ) continue elif string in str ( e ) log . warning ( string ( str_red , this_filter , model , str ( e ) , ) ) continue else log . warning ( string ( str_red , this_filter , model , str ( e ) , ) ) pass k_cor = g_rest _ filter_obs kc_name = string ( rest_frame_filter , this_filter , ) this_kcor = { } this_kcor [ string ] = this_time this_kcor [ kc_name ] = k_cor yaml_list = [ this_kcor ] yaml . dump ( yaml_list , stream , default_flow_style = false ) stream . close ( ) return	0
Q_255	"r tagstr "" calculate the diameter ratio beta use to characterize a wedge"	C_255_2	def get_peb_address ( self ) try return self . _peb_ptr except attribute_error h_process = self . get_handle ( win32 . process_query_information ) pbi = win32 . nt_query_information_process ( h_process , win32 . process_basic_information ) address = pbi . peb_base_address self . _peb_ptr = address return address	0
Q_256	helper proc to list file in a directory	C_256_0	def ls ( self , directory , note = none , loglevel = logging . debug ) shutit_global . shutit_global_object . yield_to_draw ( ) shutit_pexpect_session = self . get_current_shutit_pexpect_session ( ) return shutit_pexpect_session . is_shutit_installed ( directory , note = note , loglevel = loglevel )	1
Q_256	helper proc to list file in a directory	C_256_1	def set_animation ( self , animation , transition = none , force = false ) if self . model is none raise runtime_error ( string ) self . model . set_animation ( self , animation , transition , force )	0
Q_256	helper proc to list file in a directory	C_256_2	def squeeze ( self , trits , offset = number , length = none ) pad = ( ( len ( trits ) trit_hash_length ) or trit_hash_length ) trits + = [ number ] * ( trit_hash_length _ pad ) if length is none length = len ( trits ) or trit_hash_length if length < number raise with_context ( exc = value_error ( string ) , context = { string trits , string offset , string length , } , ) while offset < length unsigned_hash = self . k . digest ( ) if py2 unsigned_hash = map ( ord , unsigned_hash ) signed_hash = [ conv . convert_sign ( var ) for var in unsigned_hash ] trits_from_hash = conv . convert_to_trits ( signed_hash ) trits_from_hash [ trit_hash_length _ number ] = number stop = min ( trit_hash_length , length _ offset ) trits [ offset offset + stop ] = trits_from_hash [ number stop ] flipped_bytes = bytearray ( conv . convert_sign ( var ) for var in unsigned_hash ) self . reset ( ) self . k . update ( flipped_bytes ) offset + = trit_hash_length	0
Q_257	get and editor command from environment variable	C_257_0	def get_editor_cmd_from_environment ( ) result = os . getenv ( env_visual ) if ( not result ) result = os . getenv ( env_editor ) return result	1
Q_257	get and editor command from environment variable	C_257_1	def from_binary_string ( value ) bit_no = number byte = number r = [ ] for var in value if bit_no bit_no _ = number else bit_no = number r . append ( byte ) byte = number if var in ( string , string ) var = int ( var ) else raise error . py_asn1_error ( string ( var , ) ) byte = var < < bit_no r . append ( byte ) return octets . ints2octs ( r )	0
Q_257	get and editor command from environment variable	C_257_2	async def get_data ( self ) try await self . get_session_data ( ) await self . get_home_data ( ) await self . get_users ( ) await self . get_user_data ( ) except ( asyncio . timeout_error , aiohttp . client_error , socket . gaierror ) msg = string logger ( msg , number )	0
Q_258	assert the mock have be call with the specify call	C_258_0	def assert_has_calls ( self , calls , any_order = false ) expected = [ self . _call_matcher ( var ) for var in calls ] cause = expected if isinstance ( expected , exception ) else none all_calls = _call_list ( self . _call_matcher ( var ) for var in self . mock_calls ) if not any_order if expected not in all_calls six . raise_from ( assertion_error ( string string ( _call_list ( calls ) , self . mock_calls ) ) , cause ) return all_calls = list ( all_calls ) not_found = [ ] for kall in expected try all_calls . remove ( kall ) except value_error not_found . append ( kall ) if not_found six . raise_from ( assertion_error ( string . format ( tuple ( not_found ) ) ) , cause )	1
Q_258	assert the mock have be call with the specify call	C_258_1	def get_task ( self , id , client = none ) client = self . _require_client ( client ) task = task ( taskqueue = self , id = id ) try response = client . connection . api_request ( method = string , path = task . path , _target_object = task ) task . _set_properties ( response ) return task except not_found return none	0
Q_258	assert the mock have be call with the specify call	C_258_2	"def query ( self , * args , * * kwargs ) qitem = th . merge_dicts ( * args ) query_type = qitem . get ( string ) _query_type = kwargs . get ( string ) if query_type is none and _query_type is none raise attribute_error ( string ) if query_type is none query_type = _query_type if query_type = = string try th . price_query_filter ( qitem ) except key_error raise attribute_error ( string t find the required variables . must have exchange ( to select what exchange the data is coming from ) and period ( to select the time period of the data ) string pagination string page_size string page_num string _id string _id "" ] yield var"	0
Q_259	start an http server for expose the metric if the	C_259_0	def start_http_server ( self , port , host = string , endpoint = none ) if self . should_start_http_server ( ) pc_start_http_server ( port , host , registry = self . registry )	1
Q_259	start an http server for expose the metric if the	C_259_1	def remove ( self , cls , original_member_name_list , member_name , class_naming_convention ) if member_name not in original_member_name_list delattr ( cls , member_name )	0
Q_259	start an http server for expose the metric if the	C_259_2	def main ( arguments = none ) su = tools ( arguments = arguments , doc_string = _doc_ , log_level = string , options_first = false , project_name = string , default_settings_file = true ) arguments , settings , log , db_conn = su . setup ( ) for var , var in arguments . iteritems ( ) if var [ number ] = = string varname = var . replace ( string , string ) + string else varname = var . replace ( string , string ) . replace ( string , string ) if isinstance ( var , str ) or isinstance ( var , unicode ) exec ( varname + string s string ( var , ) ) else exec ( varname + string ( var , ) ) if var = = string db_conn = var log . debug ( string ( varname , var , ) ) start_time = times . get_now_sql_datetime ( ) log . info ( string ( start_time , ) ) if init from os . path import expanduser home = expanduser ( string ) filepath = home + string cmd = string string string locals ( ) p = popen ( cmd , stdout = pipe , stderr = pipe , shell = true ) try cmd = string string string locals ( ) p = popen ( cmd , stdout = pipe , stderr = pipe , shell = true ) except pass try cmd = string string string locals ( ) p = popen ( cmd , stdout = pipe , stderr = pipe , shell = true ) except pass return if where and orbfit_flag from rockfinder import orbfit_ephemeris eph = orbfit_ephemeris ( log = log , object_id = object_id , mjd = mjd , obscode = obscode , settings = settings , verbose = extra_flag ) else from rockfinder import jpl_horizons_ephemeris eph = jpl_horizons_ephemeris ( log = log , object_id = object_id , mjd = mjd , obscode = obscode , verbose = extra_flag ) data_set = list_of_dictionaries ( log = log , list_of_dictionaries = eph ) output = data_set . table ( filepath = none ) if csv output = data_set . csv ( filepath = none ) elif json output = data_set . json ( filepath = none ) elif yaml output = data_set . yaml ( filepath = none ) elif md output = data_set . markdown ( filepath = none ) elif rst output = data_set . re_st ( filepath = none ) print output if string in locals ( ) and db_conn db_conn . commit ( ) db_conn . close ( ) end_time = times . get_now_sql_datetime ( ) running_time = times . calculate_time_difference ( start_time , end_time ) log . info ( string ( end_time , running_time , ) ) return	0
Q_260	give a description stub out a task dict	C_260_0	def _stub_task ( self , description , tags = none , * * kw ) task = { string description . strip ( ) } if string in kw and tags is none task [ string ] = tags del ( kw [ string ] ) if tags is not none task [ string ] = tags task . update ( kw ) if self . _marshal return task . from_stub ( task , udas = self . config . get_udas ( ) ) return task	1
Q_260	give a description stub out a task dict	C_260_1	def _get_user_info ( self , access_token ) info_response = self . _call ( string , self . info_url , params = { string access_token } ) user_info = info_response . get ( string ) return user_info	0
Q_260	give a description stub out a task dict	C_260_2	def p_namedblock ( self , p ) string p [ number ] = block ( p [ number ] , p [ number ] , lineno = p . lineno ( number ) ) p . set_lineno ( number , p . lineno ( number ) )	0
Q_261	add a cache to the current config	C_261_0	def add ( self , alias str , config dict ) _ > none self . _config [ alias ] = config	1
Q_261	add a cache to the current config	C_261_1	def delete_pod ( name , namespace = string , * * kwargs ) cfg = _setup_conn ( * * kwargs ) body = kubernetes . client . v1_delete_options ( orphan_dependents = true ) try api_instance = kubernetes . client . core_v1_api ( ) api_response = api_instance . delete_namespaced_pod ( name = name , namespace = namespace , body = body ) return api_response . to_dict ( ) except ( api_exception , http_error ) as exc if isinstance ( exc , api_exception ) and exc . status = = number return none else log . exception ( string string ) raise command_execution_error ( exc ) finally _cleanup ( * * cfg )	0
Q_261	add a cache to the current config	C_261_2	def get_link_domain ( link , dist ) domain = np . array ( [ _ np . inf , _ number , number , number , np . inf ] ) domain = domain [ np . isnan ( link . link ( domain , dist ) ) ] return [ domain [ number ] , domain [ _ number ] ]	0
Q_262	buy a domain	C_262_0	def create ( gandi , resource , domain , duration , owner , admin , tech , bill , nameserver , extra_parameter , background ) if domain gandi . echo ( string string ) gandi . echo ( string gandi domain create s string domain ) if ( domain and resource ) and ( domain = resource ) gandi . echo ( string string ( domain , resource ) ) return _domain = domain or resource if not _domain _domain = click . prompt ( string ) result = gandi . domain . create ( _domain , duration , owner , admin , tech , bill , nameserver , extra_parameter , background ) if background gandi . pretty_echo ( result ) return result	1
Q_262	buy a domain	C_262_1	def _view_or_add ( self , uid ) postinfo = m_post . get_by_uid ( uid ) if postinfo self . viewinfo ( postinfo ) elif self . userinfo self . _to_add ( uid = uid ) else self . show404 ( )	0
Q_262	buy a domain	C_262_2	def get_box ( box , pagesize ) box = str ( box ) . split ( ) if len ( box ) = number raise exception ( string ) x , y , w , h = [ get_size ( var ) for var in box ] return get_coords ( x , y , w , h , pagesize )	0
Q_263	parse command line fill fuse_args attribute	C_263_0	def parse ( self , * args , * * kw ) ev = string in kw and kw . pop ( string ) if ev and not isinstance ( ev , int ) raise type_error ( string ) try self . cmdline = self . parser . parse_args ( * args , * * kw ) except opt_parse_error if ev sys . exit ( ev ) raise return self . fuse_args	1
Q_263	parse command line fill fuse_args attribute	C_263_1	def kitchen_get ( backend , kitchen_name , recipe ) found_kitchen = dk_kitchen_disk . find_kitchen_name ( ) if found_kitchen is not none and len ( found_kitchen ) > number raise click . click_exception ( string ) if len ( recipe ) > number click . secho ( string s string ( get_datetime ( ) , kitchen_name , str ( recipe ) ) , fg = string ) else click . secho ( string s string ( get_datetime ( ) , kitchen_name ) , fg = string ) check_and_print ( dk_cloud_command_runner . get_kitchen ( backend . dki , kitchen_name , os . getcwd ( ) , recipe ) )	0
Q_263	parse command line fill fuse_args attribute	C_263_2	def exists ( self , relpath , rsc = none , use_filepath = none ) string string string path = self . find ( relpath , rsc , use_filepath ) if path . startswith ( string ) return qt_core . q_resource ( path ) . is_valid ( ) else return os . path . exists ( path )	0
Q_264	daemonize and exec main	C_264_0	def _exec ( self , detach = true ) kwargs = { string self . pidfile , string self . home_dir , } if not detach kwargs . update ( { string false , string [ number , number , number ] , string sys . stdout , string sys . stderr , } ) ctx = daemon . daemon_context ( * * kwargs ) with ctx self . _main ( )	1
Q_264	daemonize and exec main	C_264_1	def concat ( * seqs ) _ > i_seq allseqs = lseq . sequence ( itertools . chain ( * filter ( none , map ( to_seq , seqs ) ) ) ) if allseqs is none return lseq . empty return allseqs	0
Q_264	daemonize and exec main	C_264_2	def register ( self , signal , description ) return self . _app . signals . register ( signal , self . _plugin , description )	0
Q_265	same as parameterized	C_265_0	def script_repr ( self , imports = [ ] , prefix = string ) return self . pprint ( imports , prefix , unknown_value = string , qualify = true , separator = string )	1
Q_265	same as parameterized	C_265_1	def main ( ) for var in b6_evalue_filter ( args . b6 , args . e_value ) args . output . write ( var . write ( ) )	0
Q_265	same as parameterized	C_265_2	def dict_from_hdf5 ( dict_like , h5group ) for var , var in h5group . attrs . items ( ) dict_like [ var ] = var	0
Q_266	merge table together join around a designate key column	C_266_0	def merge ( self , data_type_target_for_network_collection = none , data_type_target_for_network_list = none , merge_type = none , source_merge_columns = none , source_merge_key = none , source_table = none , target_key_network_collection = none , target_merge_key = none , target_network_collection = none , target_network_list = none , unassigned_table = none , where_merge_table = none , verbose = none ) params = set_param ( [ string , string , string , string , string , string , string , string , string , string , string , string ] , [ data_type_target_for_network_collection , data_type_target_for_network_list , merge_type , source_merge_columns , source_merge_key , source_table , target_key_network_collection , target_merge_key , target_network_collection , target_network_list , unassigned_table , where_merge_table ] ) response = api ( url = self . _url + string , params = params , method = string , verbose = verbose ) return response	1
Q_266	merge table together join around a designate key column	C_266_1	def _unsorted_set ( df , label , * * kwargs ) out = string . format ( label ) labels = df . index . values return out + argiope . utils . list_to_string ( labels , * * kwargs )	0
Q_266	merge table together join around a designate key column	C_266_2	def get_or_create_annotation ( self , url str ) _ > namespace result = self . get_namespace_by_url ( url ) if result is not none return result t = time . time ( ) bel_resource = get_bel_resource ( url ) result = namespace ( url = url , is_annotation = true , * * _get_annotation_insert_values ( bel_resource ) ) result . entries = [ namespace_entry ( var = var , identifier = var ) for var , var in bel_resource [ string ] . items ( ) if var ] self . session . add ( result ) self . session . commit ( ) log . info ( string , url , len ( bel_resource [ string ] ) , time . time ( ) _ t ) return result	0
Q_267	dispatch the event be represent by the event object	C_267_0	def dispatch_event ( event ) try if event . http_verb = = enums . http_verbs . get requests . get ( event . url , params = event . params , timeout = request_timeout ) . raise_for_status ( ) elif event . http_verb = = enums . http_verbs . post requests . post ( event . url , data = json . dumps ( event . params ) , headers = event . headers , timeout = request_timeout ) . raise_for_status ( ) except request_exception . request_exception as error logging . error ( string str ( error ) )	1
Q_267	dispatch the event be represent by the event object	C_267_1	def findall ( self , title = none ) if title is none return list ( self . _sheets ) if title not in self . _titles return [ ] return list ( self . _titles [ title ] )	0
Q_267	dispatch the event be represent by the event object	C_267_2	def cli ( env , volume_id ) file_manager = soft_layer . file_storage_manager ( env . client ) snapshot_schedules = file_manager . list_volume_schedules ( volume_id ) table = formatting . table ( [ string , string , string , string , string , string , string , string , string , string , string , string , string ] ) for var in snapshot_schedules if string in var [ string ] [ string ] replication = string else replication = formatting . blank ( ) file_schedule_type = var [ string ] [ string ] . replace ( string , string ) file_schedule_type = file_schedule_type . replace ( string , string ) property_list = [ string , string , string , string , string , string , string , string ] schedule_properties = [ ] for prop_key in property_list item = formatting . blank ( ) for schedule_property in var . get ( string , [ ] ) if schedule_property [ string ] [ string ] = = prop_key if schedule_property [ string ] = = string item = string else item = schedule_property [ string ] break schedule_properties . append ( item ) table_row = [ var [ string ] , string if var . get ( string , string ) else string , file_schedule_type , replication , var . get ( string , string ) ] table_row . extend ( schedule_properties ) table . add_row ( table_row ) env . fout ( table )	0
Q_268	get openstack release	C_268_0	def _get_openstack_release ( self ) for var , var in enumerate ( openstack_releases_pairs ) setattr ( self , var , var ) releases = { ( string , none ) self . trusty_icehouse , ( string , string ) self . trusty_kilo , ( string , string ) self . trusty_liberty , ( string , string ) self . trusty_mitaka , ( string , none ) self . xenial_mitaka , ( string , string ) self . xenial_newton , ( string , string ) self . xenial_ocata , ( string , string ) self . xenial_pike , ( string , string ) self . xenial_queens , ( string , none ) self . yakkety_newton , ( string , none ) self . zesty_ocata , ( string , none ) self . artful_pike , ( string , none ) self . bionic_queens , ( string , string ) self . bionic_rocky , ( string , string ) self . bionic_stein , ( string , none ) self . cosmic_rocky , ( string , none ) self . disco_stein , } return releases [ ( self . series , self . openstack ) ]	1
Q_268	get openstack release	C_268_1	def df ( self , y , f , n ) r y , f , n = np . broadcast_arrays ( y , f , n ) return y _ expit ( f ) * n	0
Q_268	get openstack release	C_268_2	def center_str ( txt , font_name , font_size , offset = number ) return _ ( text_width ( txt , font_name , font_size ) / number . number ) + offset	0
Q_269	return all cache set	C_269_0	def get_bcache_fs ( ) cachesetroot = string . format ( sysfs ) try dirs = os . listdir ( cachesetroot ) except os_error log ( string ) return [ ] cacheset = set ( [ bcache ( string . format ( cachesetroot , var ) ) for var in dirs if not var . startswith ( string ) ] ) return cacheset	1
Q_269	return all cache set	C_269_1	def prompt_force ( ) logger_doi_main . info ( string ) count = number print ( string ) while true force = input ( string ) try if count = = number return true elif force . lower ( ) in ( string , string ) return true elif force . lower ( ) in ( string , string ) return false else print ( string ) except attribute_error as e print ( string ) logger_doi_main . warn ( string . format ( force , e ) ) count + = number logger_doi_main . info ( string . format ( force ) ) logger_doi_main . info ( string ) return true	0
Q_269	return all cache set	C_269_2	def writefile ( filename , content ) with open ( path_expand ( filename ) , string ) as outfile outfile . write ( content )	0
Q_270	the export_image operation be perform on an image service resource	C_270_0	def export_image ( self , bbox , image_sr , bbox_sr , size = [ number , number ] , time = none , format = string , pixel_type = string , no_data = none , no_data_interpretation = string , interpolation = none , compression = none , compression_quality = number , band_ids = none , moasiac_rule = none , rendering_rule = string , f = string , save_folder = none , save_file = none ) params = { string bbox , string image_sr , string bbox_sr , string string ( size [ number ] , size [ number ] ) , string pixel_type , string compression_quality , } url = self . _url + string _allowed_format = [ string , string , string , string , string , string , string , string , string ] _allowed_pixel_types = [ string , string , string , string , string , string , string , string , string , string , string , string , string , string ] _allowedno_data_int = [ string , string ] _allowed_interpolation = [ string , string , string , string ] _allowed_compression = [ string , string ] if isinstance ( moasiac_rule , mosaic_rule_object ) params [ string ] = moasiac_rule . value if format in _allowed_format params [ string ] = format if isinstance ( time , datetime . datetime ) params [ string ] = local_time_to_online ( time ) if interpolation is not none and interpolation in _allowed_interpolation and isinstance ( interpolation , str ) params [ string ] = interpolation if pixel_type is not none and pixel_type in _allowed_pixel_types params [ string ] = pixel_type if no_data_interpretation in _allowed_interpolation params [ string ] = no_data_interpretation if no_data is not none params [ string ] = no_data if compression is not none and compression in _allowed_compression params [ string ] = compression if band_ids is not none and isinstance ( band_ids , list ) params [ string ] = string . join ( band_ids ) if rendering_rule is not none params [ string ] = rendering_rule params [ string ] = f if f = = string return self . _get ( url = url , param_dict = params , security_handler = self . _security_handler , proxy_port = self . _proxy_port , proxy_url = self . _proxy_url ) elif f = = string result = self . _get ( url = url , param_dict = params , security_handler = self . _security_handler , proxy_url = self . _proxy_url , proxy_port = self . _proxy_port , out_folder = save_folder , file_name = save_file ) return result elif f = = string return self . _get ( url = url , param_dict = params , security_handler = self . _security_handler , proxy_url = self . _proxy_url , proxy_port = self . _proxy_port , out_folder = save_folder , file_name = save_file )	1
Q_270	the export_image operation be perform on an image service resource	C_270_1	def import_event_based_gateway_to_graph ( diagram_graph , process_id , process_attributes , element ) element_id = element . get_attribute ( consts . consts . id ) bpmn_diagram_graph_import . import_gateway_to_graph ( diagram_graph , process_id , process_attributes , element ) diagram_graph . node [ element_id ] [ consts . consts . instantiate ] = element . get_attribute ( consts . consts . instantiate ) if element . has_attribute ( consts . consts . instantiate ) else string diagram_graph . node [ element_id ] [ consts . consts . event_gateway_type ] = element . get_attribute ( consts . consts . event_gateway_type ) if element . has_attribute ( consts . consts . event_gateway_type ) else string	0
Q_270	the export_image operation be perform on an image service resource	C_270_2	def dispatch ( self , request , * args , * * kwargs ) self . request = request self . args = args self . kwargs = kwargs self . cache_middleware = none response = none if self . should_cache ( ) prefix = string ( self . get_cache_version ( ) , self . get_cache_prefix ( ) ) self . set_cache_middleware ( self . cache_time , prefix ) response = self . cache_middleware . process_request ( self . request ) else self . set_do_not_cache ( ) if not response response = super ( cache_view , self ) . dispatch ( self . request , * args , * * kwargs ) return self . _finalize_cached_response ( request , response )	0
Q_271	private mode be only applicable to address_balance unspent_outputs and	C_271_0	def _do_private_mode ( fetcher_class , services , kwargs , random_wait_seconds , timeout , verbose ) addresses = kwargs . pop ( string ) results = { } with futures . thread_pool_executor ( max_workers = len ( addresses ) ) as executor fetches = { } for var in addresses k = kwargs k [ string ] = var random . shuffle ( services ) srv = fetcher_class ( services = services , verbose = verbose , timeout = timeout or number . number , random_wait_seconds = random_wait_seconds ) fetches [ executor . submit ( srv . action , * * k ) ] = ( srv , var ) to_iterate = futures . as_completed ( fetches ) for future in to_iterate service , var = fetches [ future ] results [ var ] = future . result ( ) return results	1
Q_271	private mode be only applicable to address_balance unspent_outputs and	C_271_1	def from_robot ( cls , robot , * * kwargs ) m = cls ( len ( robot . m_feats ) , len ( robot . s_feats ) , * * kwargs ) return m	0
Q_271	private mode be only applicable to address_balance unspent_outputs and	C_271_2	def exists ( self ) if string not in self or self [ string ] is none return false resp = self . r_session . head ( self . document_url ) if resp . status_code not in [ number , number ] resp . raise_for_status ( ) return resp . status_code = = number	0
Q_272	write log data to a file with one json object per line	C_272_0	def write_output ( self , data , filename = none , args = none ) if args if not args . linejson return number if not filename filename = args . linejson entrylist = [ ] for var in data [ string ] entrystring = json . dumps ( var , sort_keys = true ) entrylist . append ( entrystring ) with open ( str ( filename ) , string ) as output_file output_file . write ( string . join ( entrylist ) )	1
Q_272	write log data to a file with one json object per line	C_272_1	def _apply_rewrites ( date_classes , rules ) for var in rules date_classes = var . execute ( date_classes ) return date_classes	0
Q_272	write log data to a file with one json object per line	C_272_2	def normalized_mean_square_error ( output , target , name = string ) if output . get_shape ( ) . ndims = = number nmse_a = tf . sqrt ( tf . reduce_sum ( tf . squared_difference ( output , target ) , axis = number ) ) nmse_b = tf . sqrt ( tf . reduce_sum ( tf . square ( target ) , axis = number ) ) elif output . get_shape ( ) . ndims = = number nmse_a = tf . sqrt ( tf . reduce_sum ( tf . squared_difference ( output , target ) , axis = [ number , number ] ) ) nmse_b = tf . sqrt ( tf . reduce_sum ( tf . square ( target ) , axis = [ number , number ] ) ) elif output . get_shape ( ) . ndims = = number nmse_a = tf . sqrt ( tf . reduce_sum ( tf . squared_difference ( output , target ) , axis = [ number , number , number ] ) ) nmse_b = tf . sqrt ( tf . reduce_sum ( tf . square ( target ) , axis = [ number , number , number ] ) ) nmse = tf . reduce_mean ( nmse_a / nmse_b , name = name ) return nmse	0
Q_273	get render model to draw behind this overlay	C_273_0	def get_overlay_render_model ( self , ul_overlay_handle , pch_value , un_buffer_size ) fn = self . function_table . get_overlay_render_model p_color = hmd_color_t ( ) p_error = evr_overlay_error ( ) result = fn ( ul_overlay_handle , pch_value , un_buffer_size , byref ( p_color ) , byref ( p_error ) ) return result , p_color , p_error	1
Q_273	get render model to draw behind this overlay	C_273_1	def binary_shader_for_rules ( self , output_jar , jar , rules , jvm_options = none ) with self . temporary_rules_file ( rules ) as rules_file logger . debug ( string . format ( string . join ( var . render ( ) for var in rules ) ) ) yield self . _executor . runner ( classpath = self . _jarjar_classpath , main = string , jvm_options = jvm_options , args = [ string , rules_file , jar , output_jar ] )	0
Q_273	get render model to draw behind this overlay	C_273_2	def wire_names ( self , with_initial_value = true ) qubit_labels = self . _get_qubit_labels ( ) clbit_labels = self . _get_clbit_labels ( ) if with_initial_value qubit_labels = [ string var for var in qubit_labels ] clbit_labels = [ string clbit for clbit in clbit_labels ] else qubit_labels = [ string var for var in qubit_labels ] clbit_labels = [ string clbit for clbit in clbit_labels ] return qubit_labels + clbit_labels	0
Q_274	get the button mask for all button associate with this component	C_274_0	def get_component_button_mask ( self , pch_render_model_name , pch_component_name ) fn = self . function_table . get_component_button_mask result = fn ( pch_render_model_name , pch_component_name ) return result	1
Q_274	get the button mask for all button associate with this component	C_274_1	def fcoe_get_login_output_fcoe_login_total_logins ( self , * * kwargs ) config = et . element ( string ) fcoe_get_login = et . element ( string ) config = fcoe_get_login output = et . sub_element ( fcoe_get_login , string ) fcoe_login_total_logins = et . sub_element ( output , string ) fcoe_login_total_logins . text = kwargs . pop ( string ) callback = kwargs . pop ( string , self . _callback ) return callback ( config )	0
Q_274	get the button mask for all button associate with this component	C_274_2	def open_for_io ( self , writable , password ) if not isinstance ( writable , bool ) raise type_error ( string ) if not isinstance ( password , basestring ) raise type_error ( string ) medium_io = self . _call ( string , in_p = [ writable , password ] ) medium_io = i_medium_io ( medium_io ) return medium_io	0
Q_275	create an engine connect to a oracle database use cx_oracle	C_275_0	def create_oracle_cx_oracle ( username , password , host , port , database , * * kwargs ) return create_engine ( _create_oracle_cx_oracle ( username , password , host , port , database ) , * * kwargs )	1
Q_275	create an engine connect to a oracle database use cx_oracle	C_275_1	def get_messages ( self , soft_size_limit = none ) with self . _lock ret = rdf_flows . message_list ( ) ret_size = number for var in self . _generate ( ) self . _total_size _ = len ( var ) ret . job . append ( rdf_flows . grr_message . from_serialized_string ( var ) ) ret_size + = len ( var ) if soft_size_limit is not none and ret_size > soft_size_limit break return ret	0
Q_275	create an engine connect to a oracle database use cx_oracle	C_275_2	def start_action_tool_tip ( self , action ) string string string self . _tool_tip_timer . stop ( ) q_tool_tip . hide_text ( ) if not action . tool_tip ( ) return self . _tool_tip_action = action self . _tool_tip_timer . start ( )	0
Q_276	create the primary index on the bucket	C_276_0	def n1ql_index_create_primary ( self , defer = false , ignore_exists = false ) return self . n1ql_index_create ( string , defer = defer , primary = true , ignore_exists = ignore_exists )	1
Q_276	create the primary index on the bucket	C_276_1	def reshape ( self , dims ) adims = numpy . array ( dims , ndmin = number , dtype = string ) self . _fits . reshape_image ( self . _ext + number , adims )	0
Q_276	create the primary index on the bucket	C_276_2	def run_airbnb_demo ( data_dir ) meta_file = os . path . join ( data_dir , string ) transformer_list = [ string , string , string ] ht = hyper_transformer ( meta_file ) transformed = ht . fit_transform ( transformer_list = transformer_list ) result = ht . reverse_transform ( tables = transformed ) assert result . keys ( ) = = ht . table_dict . keys ( ) for var , var in result . items ( ) assert not result [ var ] . isnull ( ) . all ( ) . all ( )	0
Q_277	get the vc migration manager api client	C_277_0	def migratable_vc_domains ( self ) if not self . _migratable_vc_domains self . _migratable_vc_domains = migratable_vc_domains ( self . _connection ) return self . _migratable_vc_domains	1
Q_277	get the vc migration manager api client	C_277_1	def check ( self , options = none ) self . check_values ( options ) self . check_attributes ( options ) self . check_values ( options ) return self	0
Q_277	get the vc migration manager api client	C_277_2	def format_for_prodigy ( self , x , meta , placename , return_feature_subset = false ) all_tasks = [ ] sorted_x , sorted_meta = self . ranker ( x , meta ) sorted_meta = sorted_meta [ number ] sorted_x = sorted_x [ number ] for var , var in enumerate ( sorted_meta ) feature_code = var [ string ] try fc = self . _code_to_text [ feature_code ] except key_error fc = string text = string . join ( [ string string place_name string string , string , fc , string , var [ string ] , string , var [ string ] ] ) d = { string var + number , string text } all_tasks . append ( d ) if return_feature_subset return ( all_tasks , sorted_meta , sorted_x ) else return all_tasks	0
Q_278	parse a general name from user input	C_278_0	def parse_general_name ( name ) name = force_text ( name ) typ = none match = general_name_re . match ( name ) if match is not none typ , name = match . groups ( ) typ = typ . lower ( ) if typ is none if re . match ( string , name ) try return x509 . uniform_resource_identifier ( name ) except exception pass if string in name try return x509 . rfc822_name ( validate_email ( name ) ) except exception pass if name . strip ( ) . startswith ( string ) return x509 . directory_name ( x509_name ( name ) ) try return x509 . ip_address ( ip_address ( name ) ) except value_error pass try return x509 . ip_address ( ip_network ( name ) ) except value_error pass if name . startswith ( string ) idna . encode ( name [ number ] ) elif name . startswith ( string ) idna . encode ( name [ number ] ) else idna . encode ( name ) return x509 . dns_name ( name ) if typ = = string return x509 . uniform_resource_identifier ( name ) elif typ = = string return x509 . rfc822_name ( validate_email ( name ) ) elif typ = = string try return x509 . ip_address ( ip_address ( name ) ) except value_error pass try return x509 . ip_address ( ip_network ( name ) ) except value_error pass raise value_error ( string ) elif typ = = string return x509 . registered_id ( x509 . object_identifier ( name ) ) elif typ = = string regex = string if re . match ( regex , name ) is not none oid , asn_typ , val = re . match ( regex , name ) . groups ( ) oid = x509 . object_identifier ( oid ) if asn_typ = = string val = val . encode ( string ) elif asn_typ = = string val = bytes ( bytearray . fromhex ( val ) ) val = octet_string ( val ) . dump ( ) else raise value_error ( string asn_typ ) val = force_bytes ( val ) return x509 . other_name ( oid , val ) else raise value_error ( string name ) elif typ = = string return x509 . directory_name ( x509_name ( name ) ) else if name . startswith ( string ) idna . encode ( name [ number ] ) elif name . startswith ( string ) idna . encode ( name [ number ] ) else idna . encode ( name ) return x509 . dns_name ( name )	1
Q_278	parse a general name from user input	C_278_1	def dim_axis_label ( dimensions , separator = string ) if not isinstance ( dimensions , list ) dimensions = [ dimensions ] return separator . join ( [ var . pprint_label for var in dimensions ] )	0
Q_278	parse a general name from user input	C_278_2	def update_hslice ( self , blob ) nmove , nreflect = blob [ string ] , blob [ string ] ncontract = blob . get ( string , number ) fmove = ( number . * nmove ) / ( nmove + nreflect + ncontract + number ) norm = max ( self . fmove , number . _ self . fmove ) self . scale * = math . exp ( ( fmove _ self . fmove ) / norm )	0
Q_279	get a specific service base on a host_name and service_description	C_279_0	def find_srv_by_name_and_hostname ( self , host_name , sdescr ) key = ( host_name , sdescr ) return self . name_to_item . get ( key , none )	1
Q_279	get a specific service base on a host_name and service_description	C_279_1	def _check_args ( * argl , * * argd ) args = { string [ ] } for var in argl args [ string ] . append ( _tracker_str ( var ) ) for key , var in argd . items ( ) args [ key ] = _tracker_str ( var ) return args	0
Q_279	get a specific service base on a host_name and service_description	C_279_2	def iterate ( self , image , feature_extractor , feature_vector ) for var , var in self . scales ( image ) feature_extractor . prepare ( image , var ) for bb in self . sample_scaled ( var ) feature_extractor . extract_indexed ( bb , feature_vector ) yield bb . var ( number . / var )	0
Q_280	create the share queue that will be use by alignak daemon	C_280_0	def create_queues ( self , manager = none ) self . clear_queues ( manager ) if not manager self . from_q = queue ( ) self . to_q = queue ( ) else self . from_q = manager . queue ( ) self . to_q = manager . queue ( )	1
Q_280	create the share queue that will be use by alignak daemon	C_280_1	def decryption_statem ( self , ciphertext_in , key_in , reset ) if len ( key_in ) = len ( ciphertext_in ) raise pyrtl . pyrtl_error ( string ) cipher_text , key = ( pyrtl . register ( len ( ciphertext_in ) ) for var in range ( number ) ) key_exp_in , add_round_in = ( pyrtl . wire_vector ( len ( ciphertext_in ) ) for var in range ( number ) ) reversed_key_list = reversed ( self . _key_gen ( key_exp_in ) ) counter = pyrtl . register ( number , string ) round = pyrtl . wire_vector ( number ) counter . next < < = round inv_shift = self . _inv_shift_rows ( cipher_text ) inv_sub = self . _sub_bytes ( inv_shift , true ) key_out = pyrtl . mux ( round , * reversed_key_list , default = number ) add_round_out = self . _add_round_key ( add_round_in , key_out ) inv_mix_out = self . _mix_columns ( add_round_out , true ) with pyrtl . conditional_assignment with reset = = number round = number key . next = key_in key_exp_in = key_in cipher_text . next = add_round_out add_round_in = ciphertext_in with counter = = number round = counter cipher_text . next = cipher_text with pyrtl . otherwise round = counter + number key . next = key key_exp_in = key add_round_in = inv_sub with counter = = number cipher_text . next = add_round_out with pyrtl . otherwise cipher_text . next = inv_mix_out ready = ( counter = = number ) return ready , cipher_text	0
Q_280	create the share queue that will be use by alignak daemon	C_280_2	def _update_video_volume_cell_attributes ( self , key ) try video_cell_panel = self . grid_renderer . video_cells [ key ] except key_error return old_video_volume = self . code_array . cell_attributes [ key ] [ string ] new_video_volume = video_cell_panel . volume if old_video_volume = = new_video_volume return selection = selection ( [ ] , [ ] , [ ] , [ ] , [ key ] ) self . actions . set_attr ( string , new_video_volume , selection )	0
Q_281	check a group via provision api	C_281_0	def group_exists ( self , group_name ) res = self . _make_ocs_request ( string , self . ocs_service_cloud , string + group_name ) if res . status_code = = number tree = et . fromstring ( res . content ) for var in tree . findall ( string ) if var is not none and var . text = = group_name return true return false raise http_response_error ( res )	1
Q_281	check a group via provision api	C_281_1	def populate_tabs ( self ) self . delete_tabs ( ) layer_purpose = self . metadata . get ( string ) if not layer_purpose message = tr ( string ) . format ( layer_name = self . layer . name ( ) ) raise keyword_not_found_error ( message ) if layer_purpose = = layer_purpose_exposure [ string ] layer_subcategory = self . metadata . get ( string ) elif layer_purpose = = layer_purpose_hazard [ string ] layer_subcategory = self . metadata . get ( string ) else layer_subcategory = none field_groups = get_field_groups ( layer_purpose , layer_subcategory ) for var in field_groups tab = field_mapping_tab ( var , self , self . iface ) tab . set_layer ( self . layer , self . metadata ) self . add_tab ( tab , var [ string ] ) self . tabs . append ( tab )	0
Q_281	check a group via provision api	C_281_2	def add_coreference ( self , coreference ) if self . coreference_layer is none self . coreference_layer = ccoreferences ( type = self . type ) self . root . append ( self . coreference_layer . get_node ( ) ) self . coreference_layer . add_coreference ( coreference )	0
Q_282	an internal method to write out the directory record from a particular	C_282_0	def _write_directory_records ( self , vd , outfp , progress ) log_block_size = vd . logical_block_size ( ) le_ptr_offset = number be_ptr_offset = number dirs = collections . deque ( [ vd . root_directory_record ( ) ] ) while dirs curr = dirs . popleft ( ) curr_dirrecord_offset = number if curr . is_dir ( ) if curr . ptr is none raise pycdlibexception . py_cdlib_internal_error ( string ) outfp . seek ( vd . path_table_location_le * log_block_size + le_ptr_offset ) ret = curr . ptr . record_little_endian ( ) self . _outfp_write_with_check ( outfp , ret ) le_ptr_offset + = len ( ret ) outfp . seek ( vd . path_table_location_be * log_block_size + be_ptr_offset ) ret = curr . ptr . record_big_endian ( ) self . _outfp_write_with_check ( outfp , ret ) be_ptr_offset + = len ( ret ) progress . call ( curr . get_data_length ( ) ) dir_extent = curr . extent_location ( ) for var in curr . children recstr = var . record ( ) if ( curr_dirrecord_offset + len ( recstr ) ) > log_block_size dir_extent + = number curr_dirrecord_offset = number outfp . seek ( dir_extent * log_block_size + curr_dirrecord_offset ) self . _outfp_write_with_check ( outfp , recstr ) curr_dirrecord_offset + = len ( recstr ) if var . rock_ridge is not none if var . rock_ridge . dr_entries . ce_record is not none ce_rec = var . rock_ridge . dr_entries . ce_record outfp . seek ( ce_rec . bl_cont_area * self . pvd . logical_block_size ( ) + ce_rec . offset_cont_area ) rec = var . rock_ridge . record_ce_entries ( ) self . _outfp_write_with_check ( outfp , rec ) progress . call ( len ( rec ) ) if var . rock_ridge . child_link_record_exists ( ) continue if var . is_dir ( ) if not var . is_dot ( ) and not var . is_dotdot ( ) dirs . append ( var )	1
Q_282	an internal method to write out the directory record from a particular	C_282_1	"def shap_values ( self , x , * * kwargs ) phi = none if self . mimic_model_type = = string if not str ( type ( x ) ) . endswith ( string > "" ) x = xgboost . d_matrix ( x ) phi = self . trees . predict ( x , pred_contribs = true ) if phi is not none if len ( phi . shape ) = = number return [ phi [ , var , ] for var in range ( phi . shape [ number ] ) ] else return phi"	0
Q_282	an internal method to write out the directory record from a particular	C_282_2	def format_epilog ( self , ctx , formatter ) if self . epilog formatter . write_paragraph ( ) with formatter . indentation ( ) formatter . write_text ( self . epilog )	0
Q_283	add site_name to context	C_283_0	def extra_context ( self ) from django . conf import settings return { string ( lambda r settings . leonardo_site_name if getattr ( settings , string , string ) = string else settings . site_name ) , string lambda r settings . template_debug }	1
Q_283	add site_name to context	C_283_1	def _select_word_cursor ( self ) cursor = text_helper ( self . editor ) . word_under_mouse_cursor ( ) if ( self . _previous_cursor_start = cursor . selection_start ( ) and self . _previous_cursor_end = cursor . selection_end ( ) ) self . _remove_decoration ( ) self . _add_decoration ( cursor ) self . _previous_cursor_start = cursor . selection_start ( ) self . _previous_cursor_end = cursor . selection_end ( )	0
Q_283	add site_name to context	C_283_2	def volumes ( self , assets , dt ) market_open , prev_dt , dt_value , entries = self . _prelude ( dt , string ) volumes = [ ] session_label = self . _trading_calendar . minute_to_session_label ( dt ) for var in assets if not var . is_alive_for_session ( session_label ) volumes . append ( number ) continue if prev_dt is none val = self . _minute_reader . get_value ( var , dt , string ) entries [ var ] = ( dt_value , val ) volumes . append ( val ) continue else try last_visited_dt , last_total = entries [ var ] if last_visited_dt = = dt_value volumes . append ( last_total ) continue elif last_visited_dt = = prev_dt val = self . _minute_reader . get_value ( var , dt , string ) val + = last_total entries [ var ] = ( dt_value , val ) volumes . append ( val ) continue else after_last = pd . timestamp ( last_visited_dt + self . _one_min , tz = string ) window = self . _minute_reader . load_raw_arrays ( [ string ] , after_last , dt , [ var ] , ) [ number ] val = np . nansum ( window ) + last_total entries [ var ] = ( dt_value , val ) volumes . append ( val ) continue except key_error window = self . _minute_reader . load_raw_arrays ( [ string ] , market_open , dt , [ var ] , ) [ number ] val = np . nansum ( window ) entries [ var ] = ( dt_value , val ) volumes . append ( val ) continue return np . array ( volumes )	0
Q_284	helper prevent copy code	C_284_0	def _add_to_index_operations ( self , which , reconstrained , what , warning ) if warning and reconstrained . size > number logging . get_logger ( self . name ) . warning ( string . format ( self . hierarchy_name ( ) or self . name ) ) index = self . _raveled_index ( ) which . add ( what , index ) return index	1
Q_284	helper prevent copy code	C_284_1	def reload ( self ) if self . filename is not none with open ( self . filename , self . _read_flags ) as f self . data = f . read ( ) elif self . url is not none try import urllib2 response = urllib2 . urlopen ( self . url ) self . data = response . read ( ) encoding = none for var in response . headers [ string ] . split ( string ) var = var . strip ( ) if var . startswith ( string ) encoding = var . split ( string ) [ _ number ] . strip ( ) break if encoding self . data = self . data . decode ( encoding , string ) except self . data = none	0
Q_284	helper prevent copy code	C_284_2	def get_items ( self ) _ > iterator [ story_item ] yield from ( story_item ( self . _context , var , self . owner_profile ) for var in reversed ( self . _node [ string ] ) )	0
Q_285	estimate the pericenter of the orbit by identify local minimum in	C_285_0	def pericenter ( self , return_times = false , func = np . mean , interp_kwargs = none , minimize_kwargs = none , approximate = false ) if return_times and func is not none raise value_error ( string string string string ) if func is none reduce = false func = lambda x x else reduce = true if self . t [ _ number ] < self . t [ number ] self = self [ _ number ] vals = [ ] times = [ ] for var in self . orbit_gen ( ) v , t = var . _max_helper ( _ var . physicsspherical . r , interp_kwargs = interp_kwargs , minimize_kwargs = minimize_kwargs , approximate = approximate ) vals . append ( func ( _ v ) ) times . append ( t ) return self . _max_return_helper ( vals , times , return_times , reduce )	1
Q_285	estimate the pericenter of the orbit by identify local minimum in	C_285_1	def _set_initial_contents ( self , contents ) contents = self . _encode_contents ( contents ) changed = self . _byte_contents = contents st_size = len ( contents ) if self . _byte_contents self . size = number current_size = self . st_size or number self . filesystem . change_disk_usage ( st_size _ current_size , self . name , self . st_dev ) self . _byte_contents = contents self . st_size = st_size self . epoch + = number return changed	0
Q_285	estimate the pericenter of the orbit by identify local minimum in	C_285_2	def panels ( self ) return [ var for var in self . find_children ( x_view_panel ) if var . view_widget ( ) = = self ]	0
Q_286	parse the response of an rpc call into a dictionary with integer and buffer result	C_286_0	def _create_old_return_value ( payload , num_ints , buff ) parsed = { string payload [ num_ints ] , string none , string string , string false , string number } if buff parsed [ string ] = bytearray ( payload [ _ number ] ) return parsed	1
Q_286	parse the response of an rpc call into a dictionary with integer and buffer result	C_286_1	def read ( self , length , timeout ) self . _read_messages_until_true ( lambda self . _buffer_size and self . _buffer_size > = length , timeout ) with self . _read_buffer_lock data , push_back = string . join ( self . _read_buffer ) , string if length data , push_back = data [ length ] , data [ length ] self . _read_buffer . clear ( ) self . _buffer_size = len ( push_back ) if push_back self . _read_buffer . appendleft ( push_back ) return data	0
Q_286	parse the response of an rpc call into a dictionary with integer and buffer result	C_286_2	def default_permission_factory ( query_name , params ) from invenio_stats import current_stats if current_stats . queries [ query_name ] . permission_factory is none return allow_all_permission else return current_stats . queries [ query_name ] . permission_factory ( query_name , params )	0
Q_287	create a new list from tagstr by concatenate the tagstr and	C_287_0	def _concat_ixes ( prefix , list , suffix , env ) result = [ ] prefix = str ( env . subst ( prefix , s_cons . subst . subst_raw ) ) suffix = str ( env . subst ( suffix , s_cons . subst . subst_raw ) ) for var in list if isinstance ( var , s_cons . node . fs . file ) result . append ( var ) continue var = str ( var ) if var if prefix if prefix [ _ number ] = = string result . append ( prefix [ _ number ] ) elif var [ len ( prefix ) ] = prefix var = prefix + var result . append ( var ) if suffix if suffix [ number ] = = string result . append ( suffix [ number ] ) elif var [ _ len ( suffix ) ] = suffix result [ _ number ] = result [ _ number ] + suffix return result	1
Q_287	create a new list from tagstr by concatenate the tagstr and	C_287_1	def _set_traffic_class_exp_state ( self , v , load = false ) if hasattr ( v , string ) v = v . _utype ( v ) try t = yang_dyn_class ( v , base = traffic_class_exp_state . traffic_class_exp_state , is_container = string , presence = false , yang_name = string , rest_name = string , parent = self , path_helper = self . _path_helper , extmethods = self . _extmethods , register_paths = true , extensions = { u string { u string u string , u string none } } , namespace = string , defining_module = string , yang_type = string , is_config = true ) except ( type_error , value_error ) raise value_error ( { string string string string , string string , string string string container string traffic _ class _ exp _ state string traffic _ class _ exp _ state string tailf _ common string callpoint string qos _ traffic _ class _ exp string cli _ suppress _ show _ path string urn brocade . com mgmt brocade _ qos _ operational string brocade _ qos _ operational string container string string , } ) self . _traffic_class_exp_state = t if hasattr ( self , string ) self . _set ( )	0
Q_287	create a new list from tagstr by concatenate the tagstr and	C_287_2	def set_figure ( self , figure , handle = none ) self . figure = figure self . bkimage = none self . _push_handle = handle wd = figure . plot_width ht = figure . plot_height self . configure_window ( wd , ht ) doc = curdoc ( ) doc . add_periodic_callback ( self . timer_cb , number ) self . logger . info ( string )	0
Q_288	add remove update gem package	C_288_0	def packages ( state , host , packages = none , present = true , latest = false ) yield ensure_packages ( packages , host . fact . gem_packages , present , install_command = string , uninstall_command = string , upgrade_command = string , version_join = string , latest = latest , )	1
Q_288	add remove update gem package	C_288_1	def set_source_variable ( self , source_id , variable , value ) source_id = int ( source_id ) return self . _send_cmd ( string s string ( source_id , variable , value ) )	0
Q_288	add remove update gem package	C_288_2	def build_listen ( self , listen_node ) proxy_name = listen_node . listen_header . proxy_name . text service_address_node = listen_node . listen_header . service_address config_block_lines = self . _build_config_block ( listen_node . config_block ) host , port = string , string if isinstance ( service_address_node , pegnode . service_address ) host = service_address_node . host . text port = service_address_node . port . text else for var in config_block_lines if isinstance ( var , config . bind ) host , port = var . host , var . port break else raise exception ( string ) return config . listen ( name = proxy_name , host = host , port = port , config_block = config_block_lines )	0
Q_289	dump a postgre_sql database into a	C_289_0	def dump ( state , host , remote_filename , database = none , postgresql_user = none , postgresql_password = none , postgresql_host = none , postgresql_port = none , ) yield string . format ( make_psql_command ( executable = string , database = database , user = postgresql_user , password = postgresql_password , host = postgresql_host , port = postgresql_port , ) , remote_filename )	1
Q_289	dump a postgre_sql database into a	C_289_1	def get_collections ( self ) collections = self . request . matchdict [ string ] . split ( string ) [ number ] collections = [ var . strip ( ) for var in collections . split ( string ) ] return set ( collections )	0
Q_289	dump a postgre_sql database into a	C_289_2	def _find_newest_ckpt ( ckpt_dir ) full_paths = [ os . path . join ( ckpt_dir , var ) for var in os . listdir ( ckpt_dir ) if var . startswith ( string ) and var . endswith ( string ) ] return max ( full_paths )	0
Q_290	print dataset information in tabular form	C_290_0	def print_data ( data_sources ) if not data_sources return headers = [ string , string , string , string ] data_list = [ ] for var in data_sources data_list . append ( [ var . name , var . created_pretty , var . state , var . size ] ) floyd_logger . info ( tabulate ( data_list , headers = headers ) )	1
Q_290	print dataset information in tabular form	C_290_1	def get_node ( api_url = none , node_name = none , verify = false , cert = list ( ) ) return utils . _make_api_request ( api_url , string . format ( node_name ) , verify , cert )	0
Q_290	print dataset information in tabular form	C_290_2	def peek ( self ) info = self . _registry . get ( self ) obj = info and info . weakref ( ) if obj is not none return ( obj , info . func , info . args , info . kwargs or { } )	0
Q_291	create list of channel	C_291_0	def add_channels_to_list ( self , l , add_ref = false ) l . clear ( ) l . set_selection_mode ( q_abstract_item_view . extended_selection ) for var in self . chan_name item = q_list_widget_item ( var ) l . add_item ( item ) if add_ref item = q_list_widget_item ( string ) l . add_item ( item )	1
Q_291	create list of channel	C_291_1	def _convert_to_indexer ( self , obj , axis = none , is_setter = false ) if axis is none axis = self . axis or number if isinstance ( obj , slice ) return self . _convert_slice_indexer ( obj , axis ) elif is_float ( obj ) return self . _convert_scalar_indexer ( obj , axis ) try self . _validate_key ( obj , axis ) return obj except value_error raise value_error ( string string . format ( types = self . _valid_types ) )	0
Q_291	create list of channel	C_291_2	def check_exists ( path , type = string ) if type = = string if not os . path . isfile ( path ) raise runtime_error ( string path ) else if not os . path . isdir ( path ) raise runtime_error ( string path ) return true	0
Q_292	message channel or user	C_292_0	async def message ( self , target , message ) hostmask = self . _format_user_mask ( self . nickname ) chunklen = protocol . message_length_limit _ len ( string . format ( hostmask = hostmask , target = target ) ) _ number for var in message . replace ( string , string ) . split ( string ) for chunk in chunkify ( var , chunklen ) await self . rawmsg ( string , target , chunk or string )	1
Q_292	message channel or user	C_292_1	def timestamp_with_timezone ( dt = none ) dt = dt or datetime . now ( ) if timezone is none return dt . strftime ( string ) if not dt . tzinfo tz = timezone . get_current_timezone ( ) if not tz tz = timezone . utc dt = dt . replace ( tzinfo = timezone . get_current_timezone ( ) ) return dt . strftime ( string )	0
Q_292	message channel or user	C_292_2	def send ( self , value , error = false ) if error resp = [ number , self . _request_id , value , none ] else resp = [ number , self . _request_id , none , value ] debug ( string , self . _request_id , resp ) self . _msgpack_stream . send ( resp )	0
Q_293	start a new empty machine and optionally a container or add a	C_293_0	async def add_machine ( self , spec = none , constraints = none , disks = none , series = none ) params = client . add_machine_params ( ) if spec if spec . startswith ( string ) placement , target , private_key_path = spec . split ( string ) user , host = target . split ( string ) ssh_provisioner = provisioner . ssh_provisioner ( host = host , user = user , private_key_path = private_key_path , ) params = ssh_provisioner . provision_machine ( ) else placement = parse_placement ( spec ) if placement params . placement = placement [ number ] params . jobs = [ string ] if constraints params . constraints = client . value . from_json ( constraints ) if disks params . disks = [ client . constraints . from_json ( var ) for var in disks ] if series params . series = series client_facade = client . client_facade . from_connection ( self . connection ( ) ) results = await client_facade . add_machines ( [ params ] ) error = results . machines [ number ] . error if error raise value_error ( string error . message ) machine_id = results . machines [ number ] . machine if spec if spec . startswith ( string ) await ssh_provisioner . install_agent ( self . connection ( ) , params . nonce , machine_id , ) log . debug ( string , machine_id ) return await self . _wait_for_new ( string , machine_id )	1
Q_293	start a new empty machine and optionally a container or add a	C_293_1	def is_number ( num , if_bool = false ) if isinstance ( num , bool ) return if_bool elif isinstance ( num , int ) return true try number = float ( num ) return not ( isnan ( number ) or isinf ( number ) ) except ( type_error , value_error ) return false	0
Q_293	start a new empty machine and optionally a container or add a	C_293_2	def _range_min ( self , p , start , span , i , k ) if start + span < = i or k < = start return self . inf if i < = start and start + span < = k return self . s [ p ] left = self . _range_min ( number * p , start , span / / number , i , k ) right = self . _range_min ( number * p + number , start + span / / number , span / / number , i , k ) return min ( left , right )	0
Q_294	initialize a local hardware dut	C_294_0	def init_generic_serial_dut ( contextlist , conf , index , args ) port = conf [ string ] baudrate = ( args . baudrate if args . baudrate else conf . get ( string , { } ) . get ( string , number ) ) serial_config = { } if args . serial_rtscts serial_config [ string ] = args . serial_rtscts elif args . serial_xonxoff serial_config [ string ] = args . serial_xonxoff if args . serial_timeout serial_config [ string ] = args . serial_timeout ch_mode_config = { } if args . serial_ch_size > number ch_mode_config [ string ] = true ch_mode_config [ string ] = args . serial_ch_size elif args . serial_ch_size is number ch_mode_config [ string ] = false if args . ch_mode_ch_delay ch_mode_config [ string ] = args . ch_mode_ch_delay dut = dut_serial ( name = string index , port = port , baudrate = baudrate , config = conf , ch_mode_config = ch_mode_config , serial_config = serial_config , params = args ) dut . index = index dut . platform = conf . get ( string , string ) msg = string contextlist . logger . info ( msg . format ( port , index ) ) contextlist . duts . append ( dut ) contextlist . dutinformations . append ( dut . get_info ( ) )	1
Q_294	initialize a local hardware dut	C_294_1	def add_output_variable ( self , var ) assert ( isinstance ( var , variable ) ) self . output_variable_list . append ( var )	0
Q_294	initialize a local hardware dut	C_294_2	def add ( self , * widgets ) for var in widgets if is_widget ( var ) if var not in self . widgets self . widgets . add ( var ) var . add_internal ( self ) elif is_container ( var ) if var not in self . containers self . containers . add ( var ) var . add_internal ( self ) else self . add ( * var )	0
Q_295	run something on command line	C_295_0	def xrun ( command , options , log = none , _log_container_as_started = false , logfile = none , timeout = _ number , kill_callback = none ) cmd = string . join ( [ command ] + list ( map ( str , options ) ) ) def _print_info ( msg ) if msg is none return if log log . info ( msg ) else print ( msg ) def _print_warn ( msg ) if msg is none return if log log . warn ( msg ) else print ( msg ) _print_info ( u string . format ( cmd ) ) sys . stdout . flush ( ) starttime = time . time ( ) process = p = none try foutname = os . path . join ( string , string . format ( hashlib . md5 ( cmd . encode ( string ) ) . hexdigest ( ) , starttime ) ) with open ( foutname , string ) as fout p = process = subprocess . popen ( cmd , stderr = fout , stdout = fout , shell = true ) def clock_killer ( p ) while process . poll ( ) is none and ( timeout > = number ) currenttime = time . time ( ) if ( currenttime _ starttime < timeout ) debug and _print_warn ( u string . format ( currenttime _ starttime , timeout ) ) else _print_warn ( u string { number s } string . format ( cmd ) ) ( kill_callback is not none ) and kill_callback ( ) time . sleep ( interrupt_time ) thread ( target = clock_killer , args = tuple ( [ p ] ) ) . start ( ) while ( process . poll ( ) is none ) currenttime = time . time ( ) debug and _print_info ( u string . format ( currenttime _ starttime ) ) time . sleep ( interrupt_time ) assert hasattr ( process , string ) , string with open ( foutname , string ) as fout _print_info ( fout . read ( ) ) finally if ( process is not none ) and process . returncode raise stimela_cab_runtime_error ( string ( command , process . returncode ) )	1
Q_295	run something on command line	C_295_1	def intranges_from_list ( list_ ) sorted_list = sorted ( list_ ) ranges = [ ] last_write = _ number for var in range ( len ( sorted_list ) ) if var + number < len ( sorted_list ) if sorted_list [ var ] = = sorted_list [ var + number ] _ number continue current_range = sorted_list [ last_write + number var + number ] ranges . append ( _encode_range ( current_range [ number ] , current_range [ _ number ] + number ) ) last_write = var return tuple ( ranges )	0
Q_295	run something on command line	C_295_2	def build_hit_qualifications ( self , approve_requirement , restrict_to_usa , blacklist ) quals = [ { string percentage_approved_requirement_id , string string , string [ approve_requirement ] , string true , } ] if restrict_to_usa quals . append ( { string locale_requirement_id , string string , string [ { string string } ] , string true , } ) if blacklist is not none for var in blacklist qtype = self . get_qualification_type_by_name ( var ) if qtype quals . append ( { string qtype [ string ] , string string , string true , } ) return quals	0
Q_296	run a series of calculation in parallel use some	C_296_0	def parallel_calculate ( cls , syslst , properties = [ string ] , system_changes = all_changes ) print ( string , end = string ) sys . stdout . flush ( ) for var , var in enumerate ( syslst ) try var . calc . block = false var . calc . calculate ( atoms = var , properties = properties , system_changes = system_changes ) except calc_not_ready_error var . calc . block = true print ( var + number , end = string ) sys . stdout . flush ( ) print ( ) print ( string , end = string ) sys . stdout . flush ( ) for var , var in enumerate ( syslst ) var . calc . read_results ( ) print ( var + number , end = string ) sys . stdout . flush ( ) print ( ) return syslst	1
Q_296	run a series of calculation in parallel use some	C_296_1	def to_er7 ( self , encoding_chars = none , trailing_children = false ) if encoding_chars is none encoding_chars = self . encoding_chars if self . is_named ( string ) try return self . msh_1_1 . children [ number ] . value . value except index_error return self . msh_1_1 . children [ number ] . value elif self . is_named ( string ) try return self . msh_2_1 . children [ number ] . value . value except index_error return self . msh_2_1 . children [ number ] . value return super ( field , self ) . to_er7 ( encoding_chars , trailing_children )	0
Q_296	run a series of calculation in parallel use some	C_296_2	def average_hash ( image_path , hash_size = number ) with open ( image_path , string ) as f image = image . open ( f ) . resize ( ( hash_size , hash_size ) , image . antialias ) . convert ( string ) pixels = list ( image . getdata ( ) ) avg = sum ( pixels ) / len ( pixels ) bits = string . join ( map ( lambda pixel string if pixel > avg else string , pixels ) ) hashformat = string . format ( hashlength = hash_size * * number / / number ) return int ( bits , number ) . _format_ ( hashformat )	0
Q_297	return a list of inasafe field the current layer	C_297_0	def inasafe_fields_for_the_layer ( self ) if ( self . parent . get_layer_geometry_key ( ) = = layer_geometry_raster [ string ] ) return [ ] layer_purpose_key = self . parent . step_kw_purpose . selected_purpose ( ) [ string ] if layer_purpose_key = layer_purpose_aggregation [ string ] subcategory_key = self . parent . step_kw_subcategory . selected_subcategory ( ) [ string ] else subcategory_key = none inasafe_fields = get_fields ( layer_purpose_key , subcategory_key , replace_null = false , in_group = false ) try inasafe_fields . remove ( get_compulsory_fields ( layer_purpose_key , subcategory_key ) ) except value_error pass return inasafe_fields	1
Q_297	return a list of inasafe field the current layer	C_297_1	def timestamp_to_datetime ( string ) string = pad_timestamp ( string , pad_6_up ) def clamp ( val , min_ , max_ ) try val = int ( val ) val = max ( min_ , min ( val , max_ ) ) return val except return max_ def extract ( string , start , end , min_ , max_ ) if len ( string ) > = end return clamp ( string [ start end ] , min_ , max_ ) else return max_ year = extract ( string , number , number , number , number ) month = extract ( string , number , number , number , number ) day = extract ( string , number , number , number , calendar . monthrange ( year , month ) [ number ] ) hour = extract ( string , number , number , number , number ) minute = extract ( string , number , number , number , number ) second = extract ( string , number , number , number , number ) return datetime . datetime ( year = year , month = month , day = day , hour = hour , minute = minute , second = second )	0
Q_297	return a list of inasafe field the current layer	C_297_2	def weakref_proxy ( obj ) if type ( obj ) in weakref . proxy_types return obj else return weakref . proxy ( obj )	0
Q_298	preprocessor to create contour from an earthquake	C_298_0	def earthquake_contour_preprocessor ( impact_function ) contour_path = create_smooth_contour ( impact_function . hazard ) if os . path . exists ( contour_path ) from safe . gis . tools import load_layer return load_layer ( contour_path , tr ( string ) , string ) [ number ]	1
Q_298	preprocessor to create contour from an earthquake	C_298_1	def _create_content_body ( self , body ) frames = int ( math . ceil ( len ( body ) / float ( self . _max_frame_size ) ) ) for var in compatibility . range ( number , frames ) start_frame = self . _max_frame_size * var end_frame = start_frame + self . _max_frame_size body_len = len ( body ) if end_frame > body_len end_frame = body_len yield pamqp_body . content_body ( body [ start_frame end_frame ] )	0
Q_298	preprocessor to create contour from an earthquake	C_298_2	def create_group ( self , trigger ) data = self . _serialize_object ( trigger ) return trigger ( self . _post ( self . _service_url ( [ string , string ] ) , data ) )	0
Q_299	add intrusion set data to batch object	C_299_0	def intrusion_set ( self , name , * * kwargs ) group_obj = intrusion_set ( name , * * kwargs ) return self . _group ( group_obj )	1
Q_299	add intrusion set data to batch object	C_299_1	def create_target_group ( name , protocol , port , vpc_id , region = none , key = none , keyid = none , profile = none , health_check_protocol = string , health_check_port = string , health_check_path = string , health_check_interval_seconds = number , health_check_timeout_seconds = number , healthy_threshold_count = number , unhealthy_threshold_count = number ) conn = _get_conn ( region = region , key = key , keyid = keyid , profile = profile ) if target_group_exists ( name , region , key , keyid , profile ) return true try alb = conn . create_target_group ( name = name , protocol = protocol , port = port , vpc_id = vpc_id , health_check_protocol = health_check_protocol , health_check_port = health_check_port , health_check_path = health_check_path , health_check_interval_seconds = health_check_interval_seconds , health_check_timeout_seconds = health_check_timeout_seconds , healthy_threshold_count = healthy_threshold_count , unhealthy_threshold_count = unhealthy_threshold_count ) if alb log . info ( string , name , alb [ string ] [ number ] [ string ] ) return true else log . error ( string , name ) return false except client_error as error log . error ( string , name , error . response [ string ] [ string ] , error . response [ string ] [ string ] , exc_info_on_loglevel = logging . debug )	0
Q_299	add intrusion set data to batch object	C_299_2	def _rescale_to_unit_diagonals ( mat ) d = np . sqrt ( np . diag ( mat ) ) mat / = d mat / = d [ , np . newaxis ] return mat	0
Q_300	get a security label from a attribute	C_300_0	def attribute_label ( self , attribute_id , label , action = string , params = none ) if params is none params = { } if not self . can_update ( ) self . _tcex . handle_error ( number , [ self . type ] ) if action = = string return self . tc_requests . get_attribute_label ( self . api_type , self . api_sub_type , self . unique_id , attribute_id , label , owner = self . owner , params = params , ) if action = = string return self . tc_requests . delete_attribute_label ( self . api_type , self . api_sub_type , self . unique_id , attribute_id , label , owner = self . owner , ) self . _tcex . handle_error ( number , [ string , string , string , string , action ] ) return none	1
Q_300	get a security label from a attribute	C_300_1	def main ( ) parser = argparse . argument_parser ( description = string string ) parser . add_argument ( string ) parser . add_argument ( string , nargs = number ) parser . add_argument ( string , type = str ) var = parser . add_argument_group ( string ) var . add_argument ( string , nargs = string , help = string string string ) var . add_argument ( string , type = str , nargs = string , help = string string ) parser . add_argument ( string , type = str , help = string ) parser . add_argument ( string , type = int , help = string string ) parser . add_argument ( string , type = int , help = string string ) parser . add_argument ( string , type = int , help = string string ) parser . add_argument ( string , type = float , help = string ) group = parser . add_mutually_exclusive_group ( required = false ) group . add_argument ( string , action = string , help = string string string ) group . add_argument ( string , action = string , help = string string string ) group . add_argument ( string , action = string ) group . add_argument ( string , action = string , help = string ) group . add_argument ( string , type = str , help = string ) nspace = parser . parse_args ( ) if nspace . file [ _ number ] = = string data = pd . read_csv ( nspace . file ) elif nspace . file [ _ number ] = = string import sav_reader_writer as spss raw_data = spss . sav_reader ( nspace . file , return_header = true ) raw_data_list = list ( raw_data ) data = pd . data_frame ( raw_data_list ) data = data . rename ( columns = data . loc [ number ] ) . iloc [ number ] else print ( string ) exit ( number ) config = { } if nspace . max_depth config [ string ] = nspace . max_depth if nspace . alpha_merge config [ string ] = nspace . alpha_merge if nspace . min_parent_node_size config [ string ] = nspace . min_parent_node_size if nspace . min_child_node_size config [ string ] = nspace . min_child_node_size if nspace . weights config [ string ] = nspace . weights if nspace . dependent_variable_type config [ string ] = nspace . dependent_variable_type ordinal = nspace . ordinal_variables or [ ] nominal = nspace . nominal_variables or [ ] independent_variables = nominal + ordinal types = dict ( zip ( nominal + ordinal , [ string ] * len ( nominal ) + [ string ] * len ( ordinal ) ) ) if len ( independent_variables ) = = number print ( string ) exit ( number ) tree = tree . from_pandas_df ( data , types , nspace . dependent_variable [ number ] , * * config ) if nspace . export or nspace . export_path tree . render ( nspace . export_path , true ) if nspace . classify predictions = pd . series ( tree . node_predictions ( ) ) predictions . name = string data = pd . concat ( [ data , predictions ] , axis = number ) print ( data . to_csv ( ) ) elif nspace . predict predictions = pd . series ( tree . model_predictions ( ) ) predictions . name = string data = pd . concat ( [ data , predictions ] , axis = number ) print ( data . to_csv ( ) ) elif nspace . rules print ( string . join ( str ( var ) for var in tree . classification_rules ( ) ) ) else tree . print_tree ( ) print ( string , tree . accuracy ( ) )	0
Q_300	get a security label from a attribute	C_300_2	def update_broyden_j ( self ) clog . debug ( string ) delta_vals = self . param_vals _ self . _last_vals delta_residuals = self . calc_residuals ( ) _ self . _last_residuals nrm = np . sqrt ( np . dot ( delta_vals , delta_vals ) ) direction = delta_vals / nrm vals = delta_residuals / nrm self . _rank_1_j_update ( direction , vals ) self . jtj = np . dot ( self . j , self . j . t )	0
Q_301	parse args and return default args	C_301_0	def default_args ( self ) if self . _default_args is none self . _default_args , unknown = self . parser . parse_known_args ( ) self . tcex . _logger ( ) if self . _default_args . tc_aot_enabled params = self . tcex . playbook . aot_blpop ( ) self . inject_params ( params ) elif self . _default_args . tc_secure_params params = self . _load_secure_params ( ) self . inject_params ( params ) return self . _default_args	1
Q_301	parse args and return default args	C_301_1	def get_cur_batch ( items ) batches = [ ] for var in items batch = tz . get_in ( [ string , string ] , var , [ ] ) batches . append ( set ( batch ) if isinstance ( batch , ( list , tuple ) ) else set ( [ batch ] ) ) combo_batches = reduce ( lambda b1 , b2 b1 . intersection ( b2 ) , batches ) if len ( combo_batches ) = = number return combo_batches . pop ( ) elif len ( combo_batches ) = = number return none else raise value_error ( string ( combo_batches , batches ) )	0
Q_301	parse args and return default args	C_301_2	def delete_network ( context , id ) log . info ( string ( id , context . tenant_id ) ) with context . session . begin ( ) net = db_api . network_find ( context = context , limit = none , sorts = [ string ] , marker = none , page_reverse = false , id = id , scope = db_api . one ) if not net raise n_exc . network_not_found ( net_id = id ) if not context . is_admin if strategy . is_provider_network ( net . id ) raise n_exc . not_authorized ( net_id = id ) if net . ports raise n_exc . network_in_use ( net_id = id ) net_driver = registry . driver_registry . get_driver ( net [ string ] ) net_driver . delete_network ( context , id ) for var in net [ string ] subnets . _delete_subnet ( context , var ) db_api . network_delete ( context , net )	0
Q_302	create a specific dashboard noqa e501	C_302_0	def create_dashboard ( self , * * kwargs ) kwargs [ string ] = true if kwargs . get ( string ) return self . create_dashboard_with_http_info ( * * kwargs ) else ( data ) = self . create_dashboard_with_http_info ( * * kwargs ) return data	1
Q_302	create a specific dashboard noqa e501	C_302_1	def lat_from_inc ( inc , a95 = none ) rad = old_div ( np . pi , number . ) paleo_lat = old_div ( np . arctan ( number . number * np . tan ( inc * rad ) ) , rad ) if a95 is not none paleo_lat_max = old_div ( np . arctan ( number . number * np . tan ( ( inc + a95 ) * rad ) ) , rad ) paleo_lat_min = old_div ( np . arctan ( number . number * np . tan ( ( inc _ a95 ) * rad ) ) , rad ) return paleo_lat , paleo_lat_max , paleo_lat_min else return paleo_lat	0
Q_302	create a specific dashboard noqa e501	C_302_2	def differential_pressure_meter_d_p ( d , d2 , p1 , p2 , c = none , meter_type = iso_5167_orifice ) r if meter_type = = iso_5167_orifice d_p = d_p_orifice ( d = d , do = d2 , p1 = p1 , p2 = p2 , c = c ) elif meter_type = = long_radius_nozzle d_p = d_p_orifice ( d = d , do = d2 , p1 = p1 , p2 = p2 , c = c ) elif meter_type = = isa_1932_nozzle d_p = d_p_orifice ( d = d , do = d2 , p1 = p1 , p2 = p2 , c = c ) elif meter_type = = venturi_nozzle raise exception ( not_implemented ) elif meter_type = = as_cast_venturi_tube d_p = d_p_venturi_tube ( d = d , do = d2 , p1 = p1 , p2 = p2 ) elif meter_type = = machined_convergent_venturi_tube d_p = d_p_venturi_tube ( d = d , do = d2 , p1 = p1 , p2 = p2 ) elif meter_type = = rough_welded_convergent_venturi_tube d_p = d_p_venturi_tube ( d = d , do = d2 , p1 = p1 , p2 = p2 ) elif meter_type = = cone_meter d_p = d_p_cone_meter ( d = d , dc = d2 , p1 = p1 , p2 = p2 ) elif meter_type = = wedge_meter d_p = d_p_wedge_meter ( d = d , h = d2 , p1 = p1 , p2 = p2 ) return d_p	0
Q_303	get_storage_path return path to storage directory for download content	C_303_0	def get_storage_path ( filename ) directory = os . path . join ( storage_directory , filename [ number ] , filename [ number ] ) if not os . path . exists ( directory ) os . makedirs ( directory ) return os . path . join ( directory , filename )	1
Q_303	get_storage_path return path to storage directory for download content	C_303_1	def best_match ( self , matches , default = none ) if matches best_quality = _ number result = default for var , var in self for server_item in matches if var < = best_quality break if self . _value_matches ( server_item , var ) best_quality = var result = server_item return result else return self . best	0
Q_303	get_storage_path return path to storage directory for download content	C_303_2	def matrix_asformat ( lvl , name , format , blocksize = none ) desired_matrix = name + format m = getattr ( lvl , name ) if format = = string desired_matrix + = str ( blocksize [ number ] ) + str ( blocksize [ number ] ) if hasattr ( lvl , desired_matrix ) pass elif m . format = = format and format = string setattr ( lvl , desired_matrix , m ) elif m . format = = format and format = = string setattr ( lvl , desired_matrix , m . tobsr ( blocksize = blocksize ) ) else new_m = getattr ( m , string + format ) ( ) setattr ( lvl , desired_matrix , new_m ) return getattr ( lvl , desired_matrix )	0
Q_304	dict_list all gene that have at least one experimental structure	C_304_0	def genes_with_experimental_structures ( self ) return dict_list ( var for var in self . genes_with_structures if var . protein . num_structures_experimental > number )	1
Q_304	dict_list all gene that have at least one experimental structure	C_304_1	def sanity_updates_after_move ( self , oldpath , newpath ) if ( self . node_cls . get_database_vendor ( string ) = = string and len ( oldpath ) = len ( newpath ) ) self . stmts . append ( self . get_mysql_update_depth_in_branch ( newpath ) ) oldparentpath = self . node_cls . _get_parent_path_from_path ( oldpath ) newparentpath = self . node_cls . _get_parent_path_from_path ( newpath ) if ( ( not oldparentpath and newparentpath ) or ( oldparentpath and not newparentpath ) or ( oldparentpath = newparentpath ) ) if oldparentpath self . stmts . append ( self . get_sql_update_numchild ( oldparentpath , string ) ) if newparentpath self . stmts . append ( self . get_sql_update_numchild ( newparentpath , string ) )	0
Q_304	dict_list all gene that have at least one experimental structure	C_304_2	def _get_text ( self ) if self . _data . get ( string ) = = playing color = self . py3 . color_playing or self . py3 . color_good state_symbol = self . state_play elif self . _data . get ( string ) = = paused color = self . py3 . color_paused or self . py3 . color_degraded state_symbol = self . state_pause else color = self . py3 . color_stopped or self . py3 . color_bad state_symbol = self . state_stop if self . _data . get ( string ) color = self . py3 . color_bad try ptime_ms = self . _player . position ptime = _get_time_str ( ptime_ms ) except exception ptime = none if ( self . py3 . format_contains ( self . format , string ) and self . _data . get ( string ) = = playing ) update = time ( ) + number . number else update = self . py3 . cache_forever placeholders = { string self . _data . get ( string ) , string state_symbol , string self . _data . get ( string ) , string self . _data . get ( string ) , string self . _data . get ( string ) , string ptime , string self . _data . get ( string ) or string , string self . _player_details . get ( string ) , } return ( placeholders , color , update )	0
Q_305	run a dql command	C_305_0	def _run_cmd ( self , command ) if self . throttle tables = self . engine . describe_all ( false ) limiter = self . throttle . get_limiter ( tables ) else limiter = none self . engine . rate_limit = limiter results = self . engine . execute ( command ) if results is none pass elif isinstance ( results , basestring ) print ( results ) else with self . display ( ) as ostream formatter = formatters [ self . conf [ string ] ] ( results , ostream , pagesize = self . conf [ string ] , width = self . conf [ string ] , ) formatter . display ( ) print_count = number total = none for ( cmd_fragment , capacity ) in self . engine . consumed_capacities total + = capacity print ( cmd_fragment ) print ( indent ( str ( capacity ) ) ) print_count + = number if print_count > number print ( string ) print ( indent ( str ( total ) ) )	1
Q_305	run a dql command	C_305_1	def check_ensembl_api_version ( self ) self . attempt = number headers = { string string } ext = string r = self . ensembl_request ( ext , headers ) response = json . loads ( r ) self . cache . set_ensembl_api_version ( response [ string ] )	0
Q_305	run a dql command	C_305_2	def to_strings ( self , use_colors = true ) edit_script = self . to_primitive ( ) colors = collections . defaultdict ( str ) if use_colors colors [ string ] = string colors [ string ] = string colors [ string ] = string colors [ string ] = string src_txt = string dst_txt = string for var in edit_script if var [ string ] = = string width = max ( len ( var [ string ] ) , len ( var [ string ] ) ) if var [ string ] = = var [ string ] src_txt + = u string . format ( * * _combine_dicts ( colors , var , { string width } ) ) dst_txt + = u string . format ( * * _combine_dicts ( colors , var , { string width } ) ) else src_txt + = u string . format ( * * _combine_dicts ( colors , var , { string width } ) ) dst_txt + = u string . format ( * * _combine_dicts ( colors , var , { string width } ) ) elif var [ string ] = = string space = string * len ( var [ string ] ) src_txt + = u string . format ( space = space , * * _combine_dicts ( colors , var ) ) dst_txt + = u string . format ( * * _combine_dicts ( colors , var ) ) elif var [ string ] = = string space = string * len ( var [ string ] ) src_txt + = u string . format ( * * _combine_dicts ( colors , var ) ) dst_txt + = u string . format ( space = space , * * _combine_dicts ( colors , var ) ) elif var [ string ] = = string continue src_txt + = string dst_txt + = string return src_txt , dst_txt	0
Q_306	check whether all the element of the iterable x	C_306_0	def all ( pred callable , xs iterable ) for var in xs if not pred ( var ) return false return true	1
Q_306	check whether all the element of the iterable x	C_306_1	def do_setup_for_pip_local ( self , repo ) effective_repo_name = self . get_effective_repo_name ( repo ) self . abort_on_nonexisting_repo ( effective_repo_name , string ) self . network . setup_for_pip_local ( effective_repo_name )	0
Q_306	check whether all the element of the iterable x	C_306_2	def is_serializable ( obj ) if inspect . isclass ( obj ) return serializable . is_serializable_type ( obj ) return isinstance ( obj , serializable ) or hasattr ( obj , string )	0
Q_307	callback for read the config file	C_307_0	def configuration_callback ( cmd_name , option_name , config_file_name , saved_callback , provider , implicit , ctx , param , value ) ctx . default_map = ctx . default_map or { } cmd_name = cmd_name or ctx . info_name if implicit default_value = os . path . join ( click . get_app_dir ( cmd_name ) , config_file_name ) param . default = default_value value = value or default_value if value try config = provider ( value , cmd_name ) except exception as e raise click . bad_option_usage ( option_name , string . format ( e ) , ctx ) ctx . default_map . update ( config ) return saved_callback ( ctx , param , value ) if saved_callback else value	1
Q_307	callback for read the config file	C_307_1	def cloudpickle_dumps ( obj , dumper = cloudpickle . dumps ) return dumper ( obj , protocol = serialization . pickle_protocol )	0
Q_307	callback for read the config file	C_307_2	def queryset_iterator ( queryset , chunksize = number ) pk = number last_pk = queryset . order_by ( string ) . values_list ( string , flat = true ) . first ( ) if last_pk is not none queryset = queryset . order_by ( string ) while pk < last_pk for var in queryset . filter ( pk_gt = pk ) [ chunksize ] pk = var . pk yield var gc . collect ( )	0
Q_308	add textual information pass as dictionary	C_308_0	def set_text ( self , text = none , * * kwargs ) if text is none text = { } text . update ( popdict ( kwargs , _registered_kw ) ) if string in text and not isinstance ( text [ string ] , ( basestring , bytes ) ) text [ string ] = datetime . datetime ( * ( check_time ( text [ string ] ) [ number ] ) ) . isoformat ( ) self . text = text	1
Q_308	add textual information pass as dictionary	C_308_1	def is_valid ( self , wordid ) _ > bool return self . finished ( ) or wordid = self . eos_id or ( self . num_needed ( ) = = number and self . eos_id in self . allowed ( ) )	0
Q_308	add textual information pass as dictionary	C_308_2	def get_filtered_devices ( self , model_name , device_types = string , timeout = number ) upnp_devices = self . discover_upnp_devices ( st = device_types ) filtered_devices = collections . defaultdict ( dict ) for var in upnp_devices . values ( ) try r = requests . get ( var . location , timeout = timeout ) if r . status_code = = requests . codes . ok root = et . fromstring ( r . text ) ns = { string string } device = root . find ( string , ns ) if model_name in device . find ( string , ns ) . text udn = device . find ( string , ns ) . text url_base = root . find ( string , ns ) if url_base is not none filtered_devices [ udn ] [ string ] = url_base . text for attr in ( string , string , string , string , string , string , string ) el = device . find ( string attr , ns ) if el is not none filtered_devices [ udn ] [ attr ] = el . text . strip ( ) except et . parse_error pass except requests . exceptions . connect_timeout print ( string s string var . location ) return filtered_devices	0
Q_309	"check a variable "" s flag_meanings attribute for compliance under cf"	C_309_0	"def _check_flag_meanings ( self , ds , name ) variable = ds . variables [ name ] flag_meanings = getattr ( variable , string , none ) valid_meanings = test_ctx ( base_check . high , self . section_titles [ string ] ) valid_meanings . assert_true ( flag_meanings is not none , string s flag_meanings attribute is required for flag variables string { } string . format ( name ) ) if not isinstance ( flag_meanings , basestring ) return valid_meanings . to_result ( ) valid_meanings . assert_true ( len ( flag_meanings ) > number , string s flag_meanings can string . format ( name ) ) flag_regx = regex . compile ( r string ) meanings = flag_meanings . split ( ) for var in meanings if flag_regx . match ( var ) is none valid_meanings . assert_true ( false , string s flag_meanings attribute defined an illegal flag var string { } "" . format ( var ) ) return valid_meanings . to_result ( )"	1
Q_309	"check a variable "" s flag_meanings attribute for compliance under cf"	C_309_1	def authorize_security_group_egress ( dry_run = none , group_id = none , source_security_group_name = none , source_security_group_owner_id = none , ip_protocol = none , from_port = none , to_port = none , cidr_ip = none , ip_permissions = none ) pass	0
Q_309	"check a variable "" s flag_meanings attribute for compliance under cf"	C_309_2	def compute_bundle ( self , name , tdb , tdb2 = number . number ) input_was_scalar = getattr ( tdb , string , ( ) ) = = ( ) if input_was_scalar tdb = np . array ( ( tdb , ) ) coefficient_sets = self . load ( name ) number_of_sets , axis_count , coefficient_count = coefficient_sets . shape jalpha , jomega = self . jalpha , self . jomega days_per_set = ( jomega _ jalpha ) / number_of_sets index , offset = divmod ( ( tdb _ jalpha ) + tdb2 , days_per_set ) index = index . astype ( int ) if ( index < number ) . any ( ) or ( number_of_sets < index ) . any ( ) raise date_error ( string ( self . name , jalpha , jomega ) ) omegas = ( index = = number_of_sets ) index [ omegas ] _ = number offset [ omegas ] + = days_per_set coefficients = np . rollaxis ( coefficient_sets [ index ] , number ) t = np . empty ( ( coefficient_count , len ( index ) ) ) t [ number ] = number . number t [ number ] = t1 = number . number * offset / days_per_set _ number . number twot1 = t1 + t1 for var in range ( number , coefficient_count ) t [ var ] = twot1 * t [ var _ number ] _ t [ var _ number ] bundle = coefficients , days_per_set , t , twot1 return bundle	0
Q_310	read all lago variable from the environment	C_310_0	def get_env_dict ( root_section ) env_lago = defaultdict ( dict ) decider = re . compile ( ( r string r string r string ) . format ( root_section . upper ( ) ) ) for var , var in os . environ . iteritems ( ) match = decider . match ( var ) if not match continue if not match . group ( string ) or not var warn ( string string . format ( var ) ) else section = match . group ( string ) or root_section env_lago [ section . lower ( ) ] [ match . group ( string ) . lower ( ) ] = var return dict ( env_lago )	1
Q_310	read all lago variable from the environment	C_310_1	def _generate_branching_index ( self ) branch_count = np . array ( [ len ( self . slip ) , len ( self . msr ) , len ( self . shear_modulus ) , len ( self . disp_length_ratio ) , len ( self . msr_sigma ) , len ( self . config ) ] ) n_levels = len ( branch_count ) number_branches = np . prod ( branch_count ) branch_index = np . zeros ( [ number_branches , n_levels ] , dtype = int ) cumval = number dstep = number e _ number for var in range ( number , n_levels ) idx = np . linspace ( number . , float ( branch_count [ var ] ) _ dstep , number_branches / / cumval ) branch_index [ , var ] = np . reshape ( np . tile ( idx , [ cumval , number ] ) , number_branches ) cumval * = branch_count [ var ] return branch_index . tolist ( ) , number_branches	0
Q_310	read all lago variable from the environment	C_310_2	def add_host ( zone , name , ttl , ip , nameserver = string , replace = true , timeout = number , port = number , * * kwargs ) res = update ( zone , name , ttl , string , ip , nameserver , timeout , replace , port , * * kwargs ) if res is false return false fqdn = string . format ( name , zone ) parts = ip . split ( string ) [ _ number ] popped = [ ] while len ( parts ) > number p = parts . pop ( number ) popped . append ( p ) zone = string . format ( string . join ( parts ) , string ) name = string . join ( popped ) ptr = update ( zone , name , ttl , string , fqdn , nameserver , timeout , replace , port , * * kwargs ) if ptr return true return res	0
Q_311	overwrite dockerfile with specify content	C_311_0	"def content ( self , content ) if self . cache_content self . cached_content = b2u ( content ) try with self . _open_dockerfile ( string ) as dockerfile dockerfile . write ( u2b ( content ) ) except ( io_error , os_error ) as ex logger . error ( string t write content to dockerfile r "" , ex ) raise"	1
Q_311	overwrite dockerfile with specify content	C_311_1	def copy_from_date_time_string ( self , time_string ) date_time_values = self . _copy_date_time_from_string ( time_string ) year = date_time_values . get ( string , number ) month = date_time_values . get ( string , number ) day_of_month = date_time_values . get ( string , number ) hours = date_time_values . get ( string , number ) minutes = date_time_values . get ( string , number ) seconds = date_time_values . get ( string , number ) microseconds = date_time_values . get ( string , none ) if year > number raise value_error ( string . format ( year ) ) timestamp = self . _get_number_of_seconds_from_elements ( year , month , day_of_month , hours , minutes , seconds ) timestamp = float ( timestamp ) / definitions . seconds_per_day timestamp + = self . _delphi_to_posix_base if microseconds is not none timestamp + = float ( microseconds ) / definitions . microseconds_per_day self . _normalized_timestamp = none self . _timestamp = timestamp self . is_local_time = false	0
Q_311	overwrite dockerfile with specify content	C_311_2	def remove_sid2uid ( self , sid , uid ) self . remove ( string , uid , sid ) self . remove ( string , sid , uid )	0
Q_312	custom page not find handler	C_312_0	def page_not_found ( request , template_name = string ) rendered_page = get_response_page ( request , http . http_response_not_found , string , abstract_models . response_http404 ) if rendered_page is none return defaults . page_not_found ( request , template_name ) return rendered_page	1
Q_312	custom page not find handler	C_312_1	def _get_languages ( cls ) _ > set if not cls . _server_is_alive ( ) cls . _start_server_on_free_port ( ) url = urllib . parse . urljoin ( cls . _url , string ) languages = set ( ) for var in cls . _get_root ( url , num_tries = number ) languages . add ( var . get ( string ) ) languages . add ( var . get ( string ) ) return languages	0
Q_312	custom page not find handler	C_312_2	def ra_indices_for_traj ( self , traj ) assert not self . uniform_stride , string if traj in self . traj_keys return self . ra_indices_for_traj_dict [ traj ] else return np . array ( [ ] )	0
Q_313	drop entire query	C_313_0	async def drop ( self , tube ) cmd = tube . cmd ( string ) args = ( ) res = await self . tnt . call ( cmd , args ) return bool ( res . return_code = = number )	1
Q_313	drop entire query	C_313_1	def insert_reference_set ( self , reference_set ) try models . referenceset . create ( id = reference_set . get_id ( ) , name = reference_set . get_local_id ( ) , description = reference_set . get_description ( ) , assemblyid = reference_set . get_assembly_id ( ) , isderived = reference_set . get_is_derived ( ) , species = json . dumps ( reference_set . get_species ( ) ) , md5checksum = reference_set . get_md5_checksum ( ) , sourceaccessions = json . dumps ( reference_set . get_source_accessions ( ) ) , sourceuri = reference_set . get_source_uri ( ) , dataurl = reference_set . get_data_url ( ) ) for var in reference_set . get_references ( ) self . insert_reference ( var ) except exception raise exceptions . duplicate_name_exception ( reference_set . get_local_id ( ) )	0
Q_313	drop entire query	C_313_2	def _generate_dom_attrs ( attrs , allow_no_value = true ) for var in iterate_items ( attrs ) if isinstance ( var , basestring ) var = ( var , true ) key , value = var if value is none continue if value is true and not allow_no_value value = key if value is true yield true else yield string s string ( key , value . replace ( string string string ) )	0
Q_314	read pdf and decrypt if encrypt	C_314_0	def _reader ( path , password , prompt ) pdf = pdf_file_reader ( path ) if not isinstance ( path , pdf_file_reader ) else path if pdf . is_encrypted if not password pdf . decrypt ( string ) if pdf . is_encrypted and prompt print ( string , path ) password = input ( string ) else return false pdf . decrypt ( password ) return pdf	1
Q_314	read pdf and decrypt if encrypt	C_314_1	def get_nameserver_detail_output_show_nameserver_nameserver_index ( self , * * kwargs ) config = et . element ( string ) get_nameserver_detail = et . element ( string ) config = get_nameserver_detail output = et . sub_element ( get_nameserver_detail , string ) show_nameserver = et . sub_element ( output , string ) nameserver_portid_key = et . sub_element ( show_nameserver , string ) nameserver_portid_key . text = kwargs . pop ( string ) nameserver_index = et . sub_element ( show_nameserver , string ) nameserver_index . text = kwargs . pop ( string ) callback = kwargs . pop ( string , self . _callback ) return callback ( config )	0
Q_314	read pdf and decrypt if encrypt	C_314_2	def consume_token ( self , tokens , index , tokens_len ) if _is_really_comment ( tokens , index ) self . last_line_with_comment = tokens [ index ] . line finished = false if ( not _is_in_comment_type ( tokens [ index ] . type ) and self . last_line_with_comment = tokens [ index ] . line ) finished = true end = index elif index = = ( tokens_len _ number ) finished = true end = index + number if finished return _paste_tokens_line_by_line ( tokens , token_type . rst , self . begin , end )	0
Q_315	connect containerizer class to command line args and stdin	C_315_0	def stdio ( containerizer , * args ) try name = args [ number ] method , proto = { string ( containerizer . launch , launch ) , string ( containerizer . update , update ) , string ( containerizer . usage , usage ) , string ( containerizer . wait , wait ) , string ( containerizer . destroy , destroy ) , string ( containerizer . containers , none ) , string ( containerizer . recover , none ) , string ( containerizer . observe , none ) } [ name ] except index_error raise err ( string ) except key_error raise err ( string name ) log . debug ( string , ( method , proto ) ) if proto is not none return method ( recordio . read ( proto ) , * args [ number ] ) else return method ( * args [ number ] )	1
Q_315	connect containerizer class to command line args and stdin	C_315_1	def send_kwargs ( self ) _ > dict display_page = self . display_page page_num = f string content = self . pages [ display_page ] + page_num return { string content }	0
Q_315	connect containerizer class to command line args and stdin	C_315_2	def get_primary_key ( self , table ) for var in self . get_schema ( table ) if len ( var ) > number and string in var [ number ] . lower ( ) return var [ number ]	0
Q_316	restart fail task of a job	C_316_0	def retry ( self ) logger . info ( string . format ( self . name ) ) self . initialize_snapshot ( ) failed_task_names = [ ] for var , var in self . run_log [ string ] . items ( ) if var . get ( string , true ) = = false failed_task_names . append ( var ) if len ( failed_task_names ) = = number raise dagobah_error ( string ) self . _set_status ( string ) self . run_log [ string ] = datetime . utcnow ( ) logger . debug ( string . format ( self . name ) ) for var in failed_task_names self . _put_task_in_run_log ( var ) self . tasks [ var ] . start ( ) self . _commit_run_log ( )	1
Q_316	restart fail task of a job	C_316_1	def is_uri_to_be_filtered ( uri , filter_list ) match = false if filter_list for var in filter_list if re . search ( var , uri , flags = re . ignorecase ) match = true return match	0
Q_316	restart fail task of a job	C_316_2	def needs ( self , reglist ) if isinstance ( reglist , str ) reglist = [ reglist ] reglist = single_registers ( reglist ) return len ( [ var for var in self . requires if var in reglist ] ) > number	0
Q_317	transform value by script return all result as list	C_317_0	def apply ( script , value = none , vars = { } , url = none , opener = default_opener , library_paths = [ ] ) return all ( script , value , vars , url , opener , library_paths )	1
Q_317	transform value by script return all result as list	C_317_1	def touch_model ( self , model , * * data ) instance , created = model . objects . get_or_create ( * * data ) if not created if instance . updated < self . import_start_datetime instance . save ( ) return ( instance , created )	0
Q_317	transform value by script return all result as list	C_317_2	def position_target_local_ned_encode ( self , time_boot_ms , coordinate_frame , type_mask , x , y , z , vx , vy , vz , afx , afy , afz , yaw , yaw_rate ) return mav_link_position_target_local_ned_message ( time_boot_ms , coordinate_frame , type_mask , x , y , z , vx , vy , vz , afx , afy , afz , yaw , yaw_rate )	0
Q_318	permission check decorator for classbased generic view	C_318_0	def permission_required ( perm , queryset = none , login_url = none , raise_exception = false ) def wrapper ( cls ) def view_wrapper ( view_func ) wraps ( view_func , assigned = available_attrs ( view_func ) ) def inner ( self , request , * args , * * kwargs ) obj = get_object_from_classbased_instance ( self , queryset , request , * args , * * kwargs ) if not request . user . has_perm ( perm , obj = obj ) if raise_exception raise permission_denied else return redirect_to_login ( request , login_url ) return view_func ( self , request , * args , * * kwargs ) return inner cls . dispatch = view_wrapper ( cls . dispatch ) return cls return wrapper	1
Q_318	permission check decorator for classbased generic view	C_318_1	def ref_align ( self , found_seqs , sequence seq = none , locus str = none , annotation annotation = none , partial_ann annotation = none , run int = number , cutoff float = . number ) _ > annotation if annotation and isinstance ( annotation , annotation ) if number in annotation . mapping and not isinstance ( annotation . mapping [ number ] , int ) ft = annotation . mapping [ number ] start_order = self . refdata . structures [ locus ] [ ft ] else start_order = number exon_only = true if hasattr ( annotation , string ) and annotation . annotation for var in annotation . annotation if re . search ( string , var ) or re . search ( string , var ) exon_only = false elif ( len ( sequence . seq ) > number ) exon_only = false annoated = [ ] if hasattr ( annotation , string ) and annotation . annotation annoated = list ( annotation . annotation . keys ( ) ) tmp_missing = [ ] missing_blocks = annotation . blocks for b in sorted ( annotation . blocks ) start = b [ number ] _ number if b [ number ] = number else number seq_feat = seq_feature ( feature_location ( exact_position ( start ) , exact_position ( b [ len ( b ) _ number ] ) , strand = number ) , type = string ) feat = seq_feat . extract ( annotation . seq ) combosrecs , exons , fullrec = self . _refseqs ( locus , start , annotation , feat , b ) if self . verbose and self . verbosity > number for combseqr in combosrecs self . logger . info ( self . logname + string + combseqr . id ) ic = number for combseqr in combosrecs if self . verbose self . logger . info ( self . logname + string + combseqr . id ) an , ins , dels = align_seqs ( combseqr , feat , locus , start , annotation . missing , len ( annoated ) , cutoff = cutoff , verbose = self . align_verbose , verbosity = self . align_verbosity ) mapped_feat = list ( an . annotation . keys ( ) ) if len ( mapped_feat ) > = number for var in an . annotation f_order = self . refdata . structures [ locus ] [ var ] if var in annotation . missing and f_order > = start_order length , lengthsd = number , number length = float ( self . refdata . feature_lengths [ locus ] [ var ] [ number ] ) lengthsd = float ( self . refdata . feature_lengths [ locus ] [ var ] [ number ] ) incr = number if not is_class_ii ( locus ) else number max_length = length + ( lengthsd * incr ) + ins min_length = length _ ( lengthsd * incr ) _ dels if var = = string and not is_class_ii ( var ) max_length = number if exon_only f_order = self . refdata . structures [ locus ] [ var ] endp = an . features [ var ] . location . end + number if an . features [ var ] . location . start = = number and var = string and not isexon ( var ) del an . features [ var ] continue if endp in annotation . mapping and not isinstance ( annotation . mapping [ endp ] , int ) mf = annotation . mapping [ endp ] expected_order = f_order + number expected_order2 = f_order + number if expected_order = self . refdata . structures [ locus ] [ mf ] and expected_order2 = self . refdata . structures [ locus ] [ mf ] self . logger . info ( self . logname + string + mf + string + var ) del an . features [ var ] continue startp = an . features [ var ] . location . start _ number if startp in annotation . mapping and not isinstance ( annotation . mapping [ startp ] , int ) mf = annotation . mapping [ startp ] expected_order = f_order _ number expected_order2 = f_order _ number if expected_order = self . refdata . structures [ locus ] [ mf ] and expected_order2 = self . refdata . structures [ locus ] [ mf ] self . logger . info ( self . logname + string + mf + string + var ) del an . features [ var ] continue else if an . features [ var ] . location . start = = number and var = string and string in annotation . annotation del an . features [ var ] continue f_order = self . refdata . structures [ locus ] [ var ] endp = an . features [ var ] . location . end + number if endp in annotation . mapping and not isinstance ( annotation . mapping [ endp ] , int ) mf = annotation . mapping [ endp ] expected_order = f_order + number if expected_order = self . refdata . structures [ locus ] [ mf ] self . logger . info ( self . logname + string + mf + string + var ) del an . features [ var ] continue startp = an . features [ var ] . location . start _ number if startp in annotation . mapping and not isinstance ( annotation . mapping [ startp ] , int ) mf = annotation . mapping [ startp ] expected_order = f_order _ number if expected_order = self . refdata . structures [ locus ] [ mf ] self . logger . info ( self . logname + string + mf + string + var ) del an . features [ var ] continue if self . verbose and self . verbosity > number sl = str ( len ( an . annotation [ var ] ) ) self . logger . info ( self . logname + string + locus + string + var + string + sl + string + str ( max_length ) + string + str ( min_length ) ) if len ( an . annotation [ var ] ) < = max_length self . logger . info ( self . logname + string + locus + string + var + string + sl + string + str ( max_length ) ) else self . logger . info ( self . logname + string + locus + string + var + string + sl + string + str ( max_length ) ) if len ( an . annotation [ var ] ) > = min_length self . logger . info ( self . logname + string + locus + string + var + string + sl + string + str ( min_length ) ) else self . logger . info ( self . logname + string + locus + string + var + string + sl + string + str ( min_length ) ) if ( len ( an . annotation [ var ] ) < = max_length and len ( an . annotation [ var ] ) > = min_length ) if self . verbose and self . verbosity > number self . logger . info ( self . logname + string + var + string + combseqr . id ) self . logger . info ( self . logname + string + var + str ( an . features [ var ] . location . start ) + string + str ( an . features [ var ] . location . end ) ) if annotation . annotation annotation . annotation . update ( { var an . annotation [ var ] } ) annotation . features . update ( { var an . features [ var ] } ) else annotation . annotation = { } annotation . annotation . update ( { var an . annotation [ var ] } ) annotation . features . update ( { var an . features [ var ] } ) if var in annotation . refmissing i = annotation . refmissing . index ( var ) del annotation . refmissing [ i ] if var in annotation . missing del annotation . missing [ var ] else self . logger . info ( self . logname + string ) else self . logger . info ( self . logname + string ) coordinates = dict ( map ( lambda x [ x , number ] , [ i for i in range ( number , len ( sequence . seq ) + number ) ] ) ) for var in annotation . features s = annotation . features [ var ] . location . start e = annotation . features [ var ] . location . end if s = number s + = number e + = number else e + = number for i in range ( s , e ) annotation . mapping [ i ] = var if i in coordinates del coordinates [ i ] blocks = getblocks ( coordinates ) annotation . blocks = blocks annotation . check_annotation ( ) if annotation . complete_annotation if self . verbose self . logger . info ( self . logname + string + string ) return annotation else if an . features tmpann = self . seqsearch . search_seqs ( found_seqs , sequence , locus , partial_ann = annotation , run = run ) if tmpann . complete_annotation for var in tmpann . annotation if var not in annotation . annotation annotation . annotation [ var ] = tmpann . annotation [ var ] if self . verbose self . logger . info ( self . logname + string + string ) return tmpann annotation = tmpann ic + = number exons_n = number for var in annotation . missing if re . search ( string , var ) or re . search ( string , var ) exons_n + = number if len ( exons . seq ) > = number and exons_n > number exonan , ins , dels = align_seqs ( exons , feat , locus , start , annotation . missing , len ( annoated ) , cutoff = cutoff , verbose = self . align_verbose , verbosity = self . align_verbosity ) mapped_exons = list ( exonan . annotation . keys ( ) ) if len ( mapped_exons ) > = number if self . verbose self . logger . info ( self . logname + string ) for var in exonan . annotation if self . verbose and self . verbosity > number self . logger . info ( self . logname + string + var + string + str ( len ( exonan . annotation [ var ] ) ) ) annotation . annotation . update ( { var exonan . annotation [ var ] } ) annotation . features . update ( { var exonan . features [ var ] } ) coordinates = dict ( map ( lambda x [ x , number ] , [ i for i in range ( number , len ( sequence . seq ) + number ) ] ) ) for var in annotation . features s = annotation . features [ var ] . location . start e = annotation . features [ var ] . location . end if s = number s + = number e + = number else e + = number for i in range ( s , e ) annotation . mapping [ i ] = var if i in coordinates del coordinates [ i ] blocks = getblocks ( coordinates ) annotation . blocks = blocks annotation . check_annotation ( ) if annotation . complete_annotation if self . verbose self . logger . info ( self . logname + string ) return annotation return annotation elif partial_ann annoated = [ ] if hasattr ( partial_ann , string ) and partial_ann . annotation annoated = list ( partial_ann . annotation . keys ( ) ) if number in partial_ann . mapping and not isinstance ( partial_ann . mapping [ number ] , int ) ft = partial_ann . mapping [ number ] start_order = self . refdata . structures [ locus ] [ ft ] else start_order = number tmp_missing = [ ] missing_blocks = partial_ann . blocks for b in sorted ( partial_ann . blocks ) start = b [ number ] _ number if b [ number ] = number else number seq_feat = seq_feature ( feature_location ( exact_position ( start ) , exact_position ( b [ len ( b ) _ number ] ) , strand = number ) , type = string ) feat = seq_feat . extract ( partial_ann . seq ) combosrecs , exons , fullrec = self . _refseqs ( locus , start , partial_ann , feat , b ) if len ( fullrec . seq ) > = number fullref , ins , dels = align_seqs ( fullrec , feat , locus , start , partial_ann . missing , len ( annoated ) , cutoff = cutoff , verbose = self . align_verbose , verbosity = self . align_verbosity ) if hasattr ( fullref , string ) and fullref . features mapped_full = list ( fullref . annotation . keys ( ) ) if len ( mapped_full ) > = number if self . verbose self . logger . info ( self . logname + string + string ) del missing_blocks [ missing_blocks . index ( b ) ] for var in fullref . annotation if self . verbose and self . verbosity > number self . logger . info ( self . logname + string + var + string + str ( len ( fullref . annotation [ var ] ) ) ) partial_ann . annotation . update ( { var fullref . annotation [ var ] } ) if b in missing_blocks del missing_blocks [ missing_blocks . index ( b ) ] else for bm in tmp_missing if bm in missing_blocks del missing_blocks [ missing_blocks . index ( bm ) ] for var in fullref . features f_order = self . refdata . structures [ locus ] [ var ] if f_order > = start_order and var not in partial_ann . features and var in partial_ann . annotation partial_ann . features [ var ] = fullref . features [ var ] coordinates = dict ( map ( lambda x [ x , number ] , [ i for i in range ( number , len ( sequence . seq ) + number ) ] ) ) for var in partial_ann . features s = partial_ann . features [ var ] . location . start e = partial_ann . features [ var ] . location . end if s = number s + = number e + = number else e + = number for i in range ( s , e ) partial_ann . mapping [ i ] = var if i in coordinates del coordinates [ i ] blocks = getblocks ( coordinates ) partial_ann . check_annotation ( ) if partial_ann . complete_annotation if self . verbose self . logger . info ( self . logname + string ) return partial_ann if self . verbose self . logger . info ( self . logname + string ) return string	0
Q_318	permission check decorator for classbased generic view	C_318_2	def j9url_generator ( name_dict = false ) start = string end = string if name_dict urls = { string start + string + end } for var in string . ascii_uppercase urls [ var ] = start + var + end else urls = [ start + string + end ] for var in string . ascii_uppercase urls . append ( start + var + end ) return urls	0
Q_319	the actual execution of the actor	C_319_0	def do_execute ( self ) if isinstance ( self . input . payload , instances ) inst = none data = self . input . payload else inst = self . input . payload data = inst . dataset retrain = false if ( self . _header is none ) or ( self . _header . equal_headers ( data ) is not none ) or ( inst is none ) retrain = true self . _header = instances . template_instances ( data , number ) if retrain or ( self . _model is none ) cls = self . resolve_option ( string ) if isinstance ( cls , classifier ) self . _model = classifier . make_copy ( cls ) elif isinstance ( cls , clusterer ) self . _model = clusterer . make_copy ( cls ) elif isinstance ( cls , associator ) self . _model = associator . make_copy ( cls ) else return string + classes . get_classname ( cls ) if retrain if inst is not none data = instances . template_instances ( data , number ) data . add_instance ( inst ) if isinstance ( self . _model , classifier ) self . _model . build_classifier ( data ) elif isinstance ( self . _model , clusterer ) self . _model . build_clusterer ( data ) elif isinstance ( self . _model , associator ) self . _model . build_associations ( data ) else if isinstance ( self . _model , classifier ) self . _model . update_classifier ( inst ) elif isinstance ( self . _model , clusterer ) self . _model . update_clusterer ( inst ) else return string + classes . get_classname ( self . _model ) cont = model_container ( model = self . _model , header = self . _header ) self . _output . append ( token ( cont ) ) return none	1
Q_319	the actual execution of the actor	C_319_1	def _get_pkg_resources ( package_name , filepath ) requirement = pkg_resources . requirement . parse ( package_name ) try return pkg_resources . resource_filename ( requirement , filepath ) except pkg_resources . distribution_not_found pkg_resources . working_set = pkg_resources . working_set ( ) try return pkg_resources . resource_filename ( requirement , filepath ) except pkg_resources . distribution_not_found logging . error ( string , package_name ) return none	0
Q_319	the actual execution of the actor	C_319_2	def date_time_rect ( self , date_time ) string string string data = self . _date_time_grid . get ( date_time . to_time_t ( ) ) if ( data ) return q_rect_f ( data [ number ] ) return q_rect_f ( )	0
Q_320	parse a table block and build table	C_320_0	def run ( self , parent , blocks ) string string string block = blocks . pop ( number ) . split ( string ) header = block [ number ] . strip ( ) seperator = block [ number ] . strip ( ) rows = block [ number ] border = false if header . startswith ( string ) border = true align = [ ] for var in self . _split_row ( seperator , border ) if var . startswith ( string ) and var . endswith ( string ) align . append ( string ) elif var . startswith ( string ) align . append ( string ) elif var . endswith ( string ) align . append ( string ) else align . append ( none ) table = etree . sub_element ( parent , string ) table . set ( string , string ) thead = etree . sub_element ( table , string ) self . _build_row ( header , thead , align , border ) tbody = etree . sub_element ( table , string ) for row in rows self . _build_row ( row . strip ( ) , tbody , align , border )	1
Q_320	parse a table block and build table	C_320_1	def windowed_hudson_fst ( pos , ac1 , ac2 , size = none , start = none , stop = none , step = none , windows = none , fill = np . nan ) num , den = hudson_fst ( ac1 , ac2 ) def average_fst ( wn , wd ) return np . nansum ( wn ) / np . nansum ( wd ) fst , windows , counts = windowed_statistic ( pos , values = ( num , den ) , statistic = average_fst , size = size , start = start , stop = stop , step = step , windows = windows , fill = fill ) return fst , windows , counts	0
Q_320	parse a table block and build table	C_320_2	def random ( self ) utc_now = datetime . datetime . utcnow ( ) return self . value_from_string ( utc_now . strftime ( self . default_format ( ) ) )	0
Q_321	return a bit string generate from the give data string	C_321_0	def tape_gen ( self , data ) data = str ( data ) . encode ( ) hmac_obj = hmac . hmac ( self . key , digestmod = hashlib . sha256 ) hmac_obj . update ( data ) assert hmac_obj . digest_size = = number digest = hmac_obj . digest ( ) aes_algo = algorithms . aes ( digest ) aes_cipher = cipher ( aes_algo , mode = ctr ( b string * number ) , backend = default_backend ( ) ) encryptor = aes_cipher . encryptor ( ) while true encrypted_bytes = encryptor . update ( b string * number ) bits = util . str_to_bitstring ( encrypted_bytes ) for var in bits yield var	1
Q_321	return a bit string generate from the give data string	C_321_1	def add_oauth_header ( self ) oauth_token = self . _get_token ( ) if not oauth_token return self . headers [ string ] = string + oauth_token url = self . _api_url ( string ) raw_resp = self . requests . get ( url ) resp = raw_resp . json ( ) if resp . get ( string , none ) = self . user self . oops ( string + self . user ) self . headers . pop ( string ) return self . is_authenticated = true self . yeah ( string . format ( self . user ) )	0
Q_321	return a bit string generate from the give data string	C_321_2	def _get_metadata ( job_id ) job_id = unicode ( job_id ) results = engine . execute ( metadata_table . select ( ) . where ( metadata_table . c . job_id = = job_id ) ) . fetchall ( ) metadata = { } for var in results value = var [ string ] if var [ string ] = = string value = json . loads ( value ) metadata [ var [ string ] ] = value return metadata	0
Q_322	wait for the kindle cloud reader j module to initialize	C_322_0	def _wait_for_js ( self ) mod_mgr_script = ur string kindle_module_manager string mod_mgr_loaded = lambda br br . execute_script ( mod_mgr_script ) self . _wait ( number ) . until ( mod_mgr_loaded ) db_client_script = dedent ( ur string string kindle_module_manager string string ) db_client_loaded = lambda br br . execute_async_script ( db_client_script ) self . _wait ( number ) . until ( db_client_loaded )	1
Q_322	wait for the kindle cloud reader j module to initialize	C_322_1	def ensure_keys_are_setup ( self ) if not are_keys_setup ( self . name , self . keys_dir ) raise r_ex ( r_ex . reason . format ( self . name ) + self . keygen_script )	0
Q_322	wait for the kindle cloud reader j module to initialize	C_322_2	def write_sequences_to_tsv ( path , seqs ) import csv path = path ( path ) delimiter = { string string , string string } [ path . suffix ] with path . open ( string ) as file w = csv . writer ( file , delimiter = delimiter ) for var in seqs . items ( ) w . writerow ( var )	0
Q_323	return the python and pip version	C_323_0	def get_python ( cls ) python_version = sys . version_info [ number ] v_string = [ str ( var ) for var in python_version ] python_version_s = string . join ( v_string ) pip_version = shell . pip ( string ) . split ( ) [ number ] return python_version_s , pip_version	1
Q_323	return the python and pip version	C_323_1	def make_prediction_pipeline ( pipeline , args ) predicted_values , errors = ( pipeline string beam . io . read_from_text ( str ( args . predict_data ) , strip_trailing_newlines = true ) string beam . par_do ( emit_as_batch_do_fn ( args . batch_size ) ) string beam . par_do ( run_graph_do_fn ( args . trained_model_dir ) ) . with_outputs ( string , main = string ) ) ( ( predicted_values , errors ) string format_and_save ( args ) )	0
Q_323	return the python and pip version	C_323_2	def get_logger ( name ) log = logging . get_logger ( name = name ) for var in log . handlers if name = = var . name return log else return log_setup ( ) . default_logger ( name = name . split ( string ) [ number ] )	0
Q_324	update the user info by user_id	C_324_0	def update_info ( user_id , newemail , extinfo = none ) if extinfo is none extinfo = { } out_dic = { string false , string string } if not tools . check_email_valid ( newemail ) out_dic [ string ] = string return out_dic cur_info = m_user . get_by_uid ( user_id ) cur_extinfo = cur_info . extinfo for var in extinfo cur_extinfo [ var ] = extinfo [ var ] try entry = tab_member . update ( user_email = newemail , extinfo = cur_extinfo ) . where ( tab_member . uid = = user_id ) entry . execute ( ) out_dic [ string ] = true except out_dic [ string ] = string return out_dic	1
Q_324	update the user info by user_id	C_324_1	def radius_of_gyration ( coords , periodic ) gc = geometric_center ( coords , periodic ) return ( periodic_distance ( coords , gc , periodic ) * * number ) . sum ( ) / len ( coords )	0
Q_324	update the user info by user_id	C_324_2	def generate_bioprocess_mechanisms ( graph , key optional [ str ] = none ) _ > mapping [ biological_process , bel_graph ] return { var generate_mechanism ( graph , var , key = key ) for var in get_nodes_by_function ( graph , bioprocess ) }	0
Q_325	add the url as entity	C_325_0	def add_url ( self , post_data ) img_desc = post_data [ string ] img_path = post_data [ string ] cur_uid = tools . get_uudd ( number ) while m_entity . get_by_uid ( cur_uid ) cur_uid = tools . get_uudd ( number ) m_entity . create_entity ( cur_uid , img_path , img_desc , kind = post_data [ string ] if string in post_data else string ) kwd = { string post_data [ string ] if string in post_data else string , } self . render ( string , filename = img_path , cfg = config . cms_cfg , kwd = kwd , userinfo = self . userinfo )	1
Q_325	add the url as entity	C_325_1	def plot_series ( xres , varied_data , indices = none , info = none , fail_vline = none , plot_kwargs_cb = none , ls = ( string , string , string , string ) , c = ( string , string , string , string , string , string , string ) , labels = none , ax = none , names = none , latex_names = none ) import matplotlib . pyplot as plt if indices is none indices = range ( xres . shape [ number ] ) if fail_vline is none if info is none fail_vline = false else fail_vline = true if ax is none ax = plt . subplot ( number , number , number ) if labels is none labels = names if latex_names is none else [ string var . strip ( string ) for var in latex_names ] if plot_kwargs_cb is none def plot_kwargs_cb ( idx , labels = none ) kwargs = { string ls [ idx len ( ls ) ] , string c [ idx len ( c ) ] } if labels kwargs [ string ] = labels [ idx ] return kwargs else plot_kwargs_cb = plot_kwargs_cb or ( lambda idx { } ) for idx in indices ax . plot ( varied_data , xres [ , idx ] , * * plot_kwargs_cb ( idx , labels = labels ) ) if fail_vline for i , nfo in enumerate ( info ) if not nfo [ string ] ax . axvline ( varied_data [ i ] , c = string , ls = string ) return ax	0
Q_325	add the url as entity	C_325_2	def get_wave_set ( self ) wave1 = self . component1 . get_wave_set ( ) wave2 = self . component2 . get_wave_set ( ) return merge_wave_sets ( wave1 , wave2 )	0
Q_326	register a message consumer that execute the provide callback when	C_326_0	def consume ( self , callback , queue ) if queue in self . _consumers and self . _consumers [ queue ] . channel . is_open consumer = consumer ( tag = self . _consumers [ queue ] . tag , queue = queue , callback = callback , channel = self . _consumers [ queue ] . channel , ) self . _consumers [ queue ] = consumer defer . return_value ( consumer ) channel = yield self . _allocate_channel ( ) consumer = consumer ( tag = str ( uuid . uuid4 ( ) ) , queue = queue , callback = callback , channel = channel ) self . _consumers [ queue ] = consumer if not self . _running yield self . resume_producing ( ) defer . return_value ( consumer ) queue_object , _ = yield consumer . channel . basic_consume ( queue = consumer . queue , consumer_tag = consumer . tag ) deferred = self . _read ( queue_object , consumer ) deferred . add_errback ( lambda f _legacy_twisted_log . msg , string , c = consumer , log_level = logging . error , ) _legacy_twisted_log . msg ( string , c = consumer ) defer . return_value ( consumer )	1
Q_326	register a message consumer that execute the provide callback when	C_326_1	def tensor_vstack ( tensor_list , pad = number ) if len ( tensor_list ) = = number return tensor_list [ number ] [ np . newaxis , ] ndim = len ( tensor_list [ number ] . shape ) dimensions = [ len ( tensor_list ) ] for var in range ( ndim ) dimensions . append ( max ( [ tensor . shape [ var ] for tensor in tensor_list ] ) ) dtype = tensor_list [ number ] . dtype if pad = = number all_tensor = np . zeros ( tuple ( dimensions ) , dtype = dtype ) elif pad = = number all_tensor = np . ones ( tuple ( dimensions ) , dtype = dtype ) else all_tensor = np . full ( tuple ( dimensions ) , pad , dtype = dtype ) if ndim = = number for ind , tensor in enumerate ( tensor_list ) all_tensor [ ind , tensor . shape [ number ] ] = tensor elif ndim = = number for ind , tensor in enumerate ( tensor_list ) all_tensor [ ind , tensor . shape [ number ] , tensor . shape [ number ] ] = tensor elif ndim = = number for ind , tensor in enumerate ( tensor_list ) all_tensor [ ind , tensor . shape [ number ] , tensor . shape [ number ] , tensor . shape [ number ] ] = tensor else raise exception ( string ) return all_tensor	0
Q_326	register a message consumer that execute the provide callback when	C_326_2	def signed_session ( self , session = none ) session = session or requests . session ( ) session . headers [ string ] = string . format ( self . token ) return session	0
Q_327	"logic for any frameshift which isn "" t an insertion"	C_327_0	def cdna_codon_sequence_after_deletion_or_substitution_frameshift ( sequence_from_start_codon , cds_offset , trimmed_cdna_ref , trimmed_cdna_alt ) mutated_codon_index = cds_offset / / number sequence_after_mutated_codon = sequence_from_start_codon [ mutated_codon_index * number ] offset_into_mutated_codon = cds_offset number sequence_from_mutated_codon = substitute ( sequence = sequence_after_mutated_codon , offset = offset_into_mutated_codon , ref = trimmed_cdna_ref , alt = trimmed_cdna_alt ) return mutated_codon_index , sequence_from_mutated_codon	1
Q_327	"logic for any frameshift which isn "" t an insertion"	C_327_1	def should_update ( self ) last_update = self . get_last_update ( ) time_since = now ( ) _ last_update . last_update if time_since . seconds < self . update_interval return false return true	0
Q_327	"logic for any frameshift which isn "" t an insertion"	C_327_2	def reader_for_doc ( cur , url , encoding , options ) ret = libxml2mod . xml_reader_for_doc ( cur , url , encoding , options ) if ret is none raise tree_error ( string ) return xml_text_reader ( _obj = ret )	0
Q_328	initialize the qt_widgets	C_328_0	def init_q_application ( ) if string in sys . platform graphics_system = string os . environ . setdefault ( string , graphics_system ) logger . info ( string . format ( graphics_system ) ) app = qt_widgets . q_application ( sys . argv ) init_argos_application_settings ( app ) return app	1
Q_328	initialize the qt_widgets	C_328_1	def handle_error ( self , error , req , schema , error_status_code , error_headers ) status_code = error_status_code or self . default_validation_status response = exception_response ( status_code , detail = text_type ( error ) , headers = error_headers , content_type = string , ) body = json . dumps ( error . messages ) response . body = body . encode ( string ) if isinstance ( body , text_type ) else body raise response	0
Q_328	initialize the qt_widgets	C_328_2	def write_xml ( self , w , option , element_name = none ) if element_name = = none x = w . create_element ( self . prop_mo_meta . xml_attribute ) else x = w . create_element ( element_name ) for var in ucs_utils . get_ucs_property_meta_attribute_list ( self . class_id ) at_meta = ucs_utils . get_ucs_method_meta ( self . class_id , var ) if ( at_meta . io = = string ) continue if at_meta . is_complex_type if ( getattr ( self , var ) = none ) x . append_child ( self . _dict_ [ var ] . write_xml ( w , option , ucs_utils . word_l ( var ) ) ) elif ( getattr ( self , var ) = none ) x . set_attribute ( at_meta . xml_attribute , getattr ( self , var ) ) x_child = self . child_write_xml ( w , option ) for xc in x_child if ( xc = none ) x . append_child ( xc ) return x	0
Q_329	"r tagstr "" return the metadata for a station or station"	C_329_0	def metadata ( self , * * kwargs ) r self . _check_geo_param ( kwargs ) kwargs [ string ] = self . token return self . _get_response ( string , kwargs )	1
Q_329	"r tagstr "" return the metadata for a station or station"	C_329_1	def _create_controls ( self , can_kill ) debug_msg ( string , number , self ) self . set_tool_bitmap_size ( wx . size ( number , number ) ) self . add_simple_tool ( _ntb_x_pan_left , _load_bitmap ( string ) , string , string ) self . add_simple_tool ( _ntb_x_pan_right , _load_bitmap ( string ) , string , string ) self . add_simple_tool ( _ntb_x_zoomin , _load_bitmap ( string ) , string , string ) self . add_simple_tool ( _ntb_x_zoomout , _load_bitmap ( string ) , string , string ) self . add_separator ( ) self . add_simple_tool ( _ntb_y_pan_up , _load_bitmap ( string ) , string , string ) self . add_simple_tool ( _ntb_y_pan_down , _load_bitmap ( string ) , string , string ) self . add_simple_tool ( _ntb_y_zoomin , _load_bitmap ( string ) , string , string ) self . add_simple_tool ( _ntb_y_zoomout , _load_bitmap ( string ) , string , string ) self . add_separator ( ) self . add_simple_tool ( _ntb_save , _load_bitmap ( string ) , string , string ) self . add_separator ( ) bind ( self , wx . evt_tool , self . _on_left_scroll , id = _ntb_x_pan_left ) bind ( self , wx . evt_tool , self . _on_right_scroll , id = _ntb_x_pan_right ) bind ( self , wx . evt_tool , self . _on_x_zoom_in , id = _ntb_x_zoomin ) bind ( self , wx . evt_tool , self . _on_x_zoom_out , id = _ntb_x_zoomout ) bind ( self , wx . evt_tool , self . _on_up_scroll , id = _ntb_y_pan_up ) bind ( self , wx . evt_tool , self . _on_down_scroll , id = _ntb_y_pan_down ) bind ( self , wx . evt_tool , self . _on_y_zoom_in , id = _ntb_y_zoomin ) bind ( self , wx . evt_tool , self . _on_y_zoom_out , id = _ntb_y_zoomout ) bind ( self , wx . evt_tool , self . _on_save , id = _ntb_save ) bind ( self , wx . evt_tool_enter , self . _on_enter_tool , id = self . get_id ( ) ) if can_kill bind ( self , wx . evt_tool , self . _on_close , id = _ntb_close ) bind ( self , wx . evt_mousewheel , self . _on_mouse_wheel )	0
Q_329	"r tagstr "" return the metadata for a station or station"	C_329_2	def download_feature_values ( self , mapobject_type_name , plate_name = none , well_name = none , well_pos_y = none , well_pos_x = none , tpoint = none ) res = self . _download_feature_values ( mapobject_type_name , plate_name , well_name , well_pos_y , well_pos_x , tpoint ) logger . debug ( string ) file_obj = string_io ( res . content . decode ( string ) ) try return pd . read_csv ( file_obj ) except empty_data_error return pd . data_frame ( )	0
Q_330	run install process	C_330_0	def run ( self ) try self . linux . verify_system_status ( ) except install_skip_error log . info ( string ) return work_dir = tempfile . mkdtemp ( suffix = string ) log . info ( string { number } string . format ( work_dir ) ) with cmd . pushd ( work_dir ) self . rpm_py . download_and_install ( ) if not self . python . is_python_binding_installed ( ) message = ( string string ) raise install_error ( message ) if self . is_work_dir_removed shutil . rmtree ( work_dir ) log . info ( string { number } string . format ( work_dir ) ) else log . info ( string { number } string . format ( work_dir ) )	1
Q_330	run install process	C_330_1	def prune_to_ingroup ( self ) if not self . _ingroup_node_id _log . debug ( string ) self . _ingroup_node_id = self . root_node_id elif self . _ingroup_node_id = self . root_node_id self . _do_prune_to_ingroup ( ) self . root_node_id = self . _ingroup_node_id else _log . debug ( string ) return self . root_node_id	0
Q_330	run install process	C_330_2	def text_path ( self , text ) cairo . cairo_text_path ( self . _pointer , _encode_string ( text ) ) self . _check_status ( )	0
Q_331	return a build spec for this group spec	C_331_0	def get_build_spec ( self ) if base_specification . build not in self . _data return none return build_config . from_dict ( self . _data [ base_specification . build ] )	1
Q_331	return a build spec for this group spec	C_331_1	def epochs_distributed ( ts , variability = none , threshold = number . number , minlength = number . number , plot = true ) import distob if ts . ndim is number ts = ts [ , np . newaxis ] if variability is none dts = distob . scatter ( ts , axis = number ) vepochs = distob . vectorize ( epochs ) results = vepochs ( dts , none , threshold , minlength , plot = false ) else def f ( pair ) return epochs ( pair [ number ] , pair [ number ] , threshold , minlength , plot = false ) allpairs = [ ( ts [ , var ] , variability [ , var ] ) for var in range ( ts . shape [ number ] ) ] vf = distob . vectorize ( f ) results = vf ( allpairs ) vars , allchannels_epochs = zip ( * results ) variability = distob . hstack ( vars ) if plot _plot_variability ( ts , variability , threshold , allchannels_epochs ) return ( variability , allchannels_epochs )	0
Q_331	return a build spec for this group spec	C_331_2	def setup_plot ( self ) if self . tick_label_fontsize is not none self . x_tick_label_fontsize = self . tick_label_fontsize self . y_tick_label_fontsize = self . tick_label_fontsize xticks = np . arange ( float ( self . xlims [ number ] ) , float ( self . xlims [ number ] ) + float ( self . dx ) , float ( self . dx ) ) yticks = np . arange ( float ( self . ylims [ number ] ) , float ( self . ylims [ number ] ) + float ( self . dy ) , float ( self . dy ) ) xlim = [ xticks . min ( ) , xticks . max ( ) ] ylim = [ yticks . min ( ) , yticks . max ( ) ] if self . reverse_x_axis xticks = xticks [ _ number ] xlim = [ xticks . max ( ) , xticks . min ( ) ] if self . reverse_y_axis yticks = yticks [ _ number ] ylim = [ yticks . max ( ) , yticks . min ( ) ] self . axis . set_xlim ( xlim ) self . axis . set_ylim ( ylim ) if self . spacing = = string x_inds = np . arange ( len ( xticks ) ) y_inds = np . arange ( len ( yticks ) ) else x_inds = np . arange ( number , len ( xticks ) _ number ) y_inds = np . arange ( number , len ( yticks ) _ number ) self . axis . set_xticks ( xticks [ x_inds ] ) self . axis . set_yticks ( yticks [ y_inds ] ) if self . xscale = = string self . axis . set_xticklabels ( [ r string int ( var ) for var in xticks [ x_inds ] ] , fontsize = self . x_tick_label_fontsize ) else self . axis . set_xticklabels ( [ r string ( var ) for var in xticks [ x_inds ] ] , fontsize = self . x_tick_label_fontsize ) if self . yscale = = string self . axis . set_yticklabels ( [ r string int ( var ) for var in yticks [ y_inds ] ] , fontsize = self . y_tick_label_fontsize ) else self . axis . set_yticklabels ( [ r string ( var ) for var in yticks [ y_inds ] ] , fontsize = self . y_tick_label_fontsize ) if self . add_grid self . axis . grid ( true , linestyle = string , color = string ) if string in self . _dict_ . keys ( ) self . axis . set_title ( r string . format ( self . title ) , * * self . title_kwargs ) if string in self . _dict_ . keys ( ) self . axis . set_xlabel ( r string . format ( self . xlabel ) , * * self . xlabel_kwargs ) if string in self . _dict_ . keys ( ) self . axis . set_ylabel ( r string . format ( self . ylabel ) , * * self . ylabel_kwargs ) return	0
Q_332	remember what our original stream be so that we	C_332_0	def set_stream_color ( stream , disabled ) original_stdout = sys . stdout original_stderr = sys . stderr init ( strip = disabled ) if stream = original_stdout sys . stdout = original_stdout sys . stderr = binary_stream_wrapper ( stream , sys . stderr ) if stream = original_stderr sys . stderr = original_stderr sys . stdout = binary_stream_wrapper ( stream , sys . stdout )	1
Q_332	remember what our original stream be so that we	C_332_1	def validate ( self , val ) if self . validation self . type . validate ( val ) if self . custom_validator is not none self . custom_validator ( val ) return true	0
Q_332	remember what our original stream be so that we	C_332_2	"def _draw_header ( self ) n_rows , n_cols = self . term . stdscr . getmaxyx ( ) window = self . term . stdscr . derwin ( number , n_cols , self . _row , number ) window . erase ( ) window . bkgd ( str ( string ) , self . term . attr ( string ) ) sub_name = self . content . name sub_name = sub_name . replace ( string , string ) parts = sub_name . split ( string ) if len ( parts ) = = number pass elif string in sub_name _ , _ , user , _ , multi = parts sub_name = string . format ( multi , user ) elif parts [ number ] = = string noun = string if parts [ number ] = = string else parts [ number ] + string s string overview string overview string overview string submitted string submissions string comments string comments string saved string saved content string hidden string hidden content string upvoted string upvoted content string downvoted string downvoted content string { } { } string searching { number } { number } string / string _ string string / string display string inside_emacs string _ rtv { number } string utf _ number string x1b ] number { number } x07 string x1b ] number { number } x07 string ascii string hide_username string logged in "" else username = self . reddit . user . name s_col = ( n_cols _ width ( username ) _ number ) if ( s_col _ number ) > = width ( sub_name ) self . term . add_line ( window , username , number , s_col ) self . _row + = number"	0
Q_333	give a sentence and a list of	C_333_0	def build_trees_from_sentence ( sentence , syntactic_relations , layer = layer_vislcg3 , sentence_id = number , * * kwargs ) trees_of_sentence = [ ] nodes = [ _ number ] while ( len ( nodes ) > number ) node = nodes . pop ( number ) for var , var in enumerate ( syntactic_relations ) parents = [ o [ number ] for o in var [ parser_out ] ] parent = parents [ number ] if parent = = node labels = [ o [ number ] for o in var [ parser_out ] ] estnltk_token = sentence [ var ] tree1 = tree ( estnltk_token , var , sentence_id , labels , parser = layer ) if init_parser_out in var tree1 . parser_output = var [ init_parser_out ] tree1 . var = var if parent = = _ number trees_of_sentence . append ( tree1 ) elif parent = = var trees_of_sentence . append ( tree1 ) else for root_node in trees_of_sentence root_node . add_child_to_subtree ( parent , tree1 ) if parent = var nodes . append ( var ) return trees_of_sentence	1
Q_333	give a sentence and a list of	C_333_1	def haversine ( a0 , a1 ) a = [ np . asarray ( a0 , dtype = float ) , np . asarray ( a1 , dtype = float ) ] for var in range ( number ) if len ( a [ var ] . shape ) = = number a [ var ] = np . reshape ( a [ var ] , ( number , a [ var ] . shape [ number ] ) ) if a [ var ] . shape [ number ] = number logger . error ( string ) return a [ var ] = np . deg2rad ( a [ var ] ) distance_km = np . zeros ( ( a [ number ] . shape [ number ] , a [ number ] . shape [ number ] ) ) for var in range ( a [ number ] . shape [ number ] ) b = np . sin ( ( a [ number ] [ , number ] _ a [ number ] [ var , number ] ) / number . ) * * number + np . cos ( a [ number ] [ , number ] ) * np . cos ( a [ number ] [ var , number ] ) * np . sin ( ( a [ number ] [ , number ] _ a [ number ] [ var , number ] ) / number . ) * * number distance_km [ , var ] = number . number * number * np . arctan2 ( np . sqrt ( b ) , np . sqrt ( number _ b ) ) return distance_km	0
Q_333	give a sentence and a list of	C_333_2	def get_geocoder_for_service ( service ) try return service_to_geocoder [ service . lower ( ) ] except key_error raise geocoder_not_found ( string s string ( service , service_to_geocoder . keys ( ) ) )	0
Q_334	"raise buffer_underflow if there "" s not enough byte to satisfy"	C_334_0	def _check_underflow ( self , n ) if self . _pos + n > self . _end_pos raise self . buffer_underflow ( )	1
Q_334	"raise buffer_underflow if there "" s not enough byte to satisfy"	C_334_1	def dict ( self ) metadata = super ( impact_layer_metadata , self ) . dict metadata [ string ] = self . provenance metadata [ string ] = self . summary_data return metadata	0
Q_334	"raise buffer_underflow if there "" s not enough byte to satisfy"	C_334_2	def _get_b ( self ) orig = self . origin dest = self . destination ( ox , oy ) = orig . pos ( dx , dy ) = dest . pos denominator = dx _ ox x_numerator = ( dy _ oy ) * ox y_numerator = denominator * oy return ( ( y_numerator _ x_numerator ) , denominator )	0
Q_335	get the content for the soap i header node	C_335_0	def headercontent ( self , method ) content = [ ] wsse = self . options ( ) . wsse if wsse is not none content . append ( wsse . xml ( ) ) headers = self . options ( ) . soapheaders if not isinstance ( headers , ( tuple , list , dict ) ) headers = ( headers , ) elif not headers return content pts = self . headpart_types ( method ) if isinstance ( headers , ( tuple , list ) ) n = number for var in headers if isinstance ( var , element ) content . append ( deepcopy ( var ) ) continue if len ( pts ) = = n break h = self . mkheader ( method , pts [ n ] , var ) ns = pts [ n ] [ number ] . namespace ( string ) h . set_prefix ( ns [ number ] , ns [ number ] ) content . append ( h ) n + = number else for pt in pts var = headers . get ( pt [ number ] ) if var is none continue h = self . mkheader ( method , pt , var ) ns = pt [ number ] . namespace ( string ) h . set_prefix ( ns [ number ] , ns [ number ] ) content . append ( h ) return content	1
Q_335	get the content for the soap i header node	C_335_1	"def handle_privmsg ( self , params ) target , sep , msg = params . partition ( string ) if not msg raise irc_error . from_name ( string , string ) message = string ( self . client_ident ( ) , target , msg ) if target . startswith ( string nosuchnick string privmsg s string cannotsendtochan string s cannot send to channel string nosuchnick string privmsg s "" target ) client . send_queue . append ( message )"	0
Q_335	get the content for the soap i header node	C_335_2	def _parse_raw_data ( self ) if self . uuid = = _xmp_uuid txt = self . raw_data . decode ( string ) elt = et . fromstring ( txt ) self . data = et . element_tree ( elt ) elif self . uuid = = _geotiff_uuid self . data = tiff_header ( self . raw_data ) elif self . uuid = = _exif_uuid self . data = tiff_header ( self . raw_data [ number ] ) else self . data = self . raw_data	0
Q_336	get http header for a http http soap request	C_336_0	def _headers ( self ) action = self . method . soap . action if isinstance ( action , unicode ) action = action . encode ( string ) result = { string string , string action } result . update ( * * self . options . headers ) log . debug ( string , result ) return result	1
Q_336	get http header for a http http soap request	C_336_1	def switch_on ( self ) success = self . set_status ( const . status_open_int ) if success self . _json_state [ string ] = const . status_open return success	0
Q_336	get http header for a http http soap request	C_336_2	def _get_compressed_stream_types ( self , mediator , path_spec ) try type_indicators = analyzer . analyzer . get_compressed_stream_type_indicators ( path_spec , resolver_context = mediator . resolver_context ) except io_error as exception type_indicators = [ ] warning_message = ( string string ) . format ( exception ) mediator . produce_extraction_warning ( warning_message , path_spec = path_spec ) return type_indicators	0
Q_337	for a give message return the text processor that can handle it	C_337_0	def msg2processor ( msg , * * config ) for var in processors if var . handle_msg ( msg , * * config ) is not none return var else return processors [ _ number ]	1
Q_337	for a give message return the text processor that can handle it	C_337_1	def set_nearest_border ( self ) px , py = self . _point nw_x , nw_y , se_x , se_y = self . get_adjusted_border_positions ( ) if self . _port . side = = snapped_side . right _update ( px , se_x ) elif self . _port . side = = snapped_side . bottom _update ( py , se_y ) elif self . _port . side = = snapped_side . left _update ( px , nw_x ) elif self . _port . side = = snapped_side . top _update ( py , nw_y )	0
Q_337	for a give message return the text processor that can handle it	C_337_2	def set_group_icon ( cls , group_name , icon ) string string string if ( cls . _group_icons is none ) cls . _group_icons = { } cls . _group_icons [ nativestring ( group_name ) ] = icon	0
Q_338	trig the server_ready event	C_338_0	def connected ( self , * * kwargs ) self . bot . log . info ( string , self . bot . server_config ) self . bot . config [ string ] = kwargs [ string ] self . bot . recompile ( ) self . bot . notify ( string ) self . bot . detach_events ( * self . before_connect_events )	1
Q_338	trig the server_ready event	C_338_1	def get_response ( self , environ = none ) response = super ( same_content_exception , self ) . get_response ( environ = environ ) if self . etag is not none response . set_etag ( self . etag ) if self . last_modified is not none response . headers [ string ] = http_date ( self . last_modified ) return response	0
Q_338	trig the server_ready event	C_338_2	def _load_configuration ( self ) handler_to_enable = filter ( lambda h h . keys is not none , handlers ) for var in handler_to_enable if var . name in self . kwargs for k in var . keys self . kwargs . update ( { k true } ) self . config = self . configuration_class ( self . app . config , * * self . kwargs )	0
Q_339	a special set function to ensure	C_339_0	def _set_attachments ( self , value ) if value is none setattr ( self , string , [ ] ) elif isinstance ( value , list ) setattr ( self , string , value ) else raise type_error ( string )	1
Q_339	a special set function to ensure	C_339_1	def recommend_k_items_slow ( self , test , top_k = number , remove_seen = true ) if remove_seen raise value_error ( string ) self . get_user_affinity ( test ) . write . mode ( string ) . save_as_table ( self . f ( string ) ) query = self . f ( string string string , top_k = top_k , ) return self . spark . sql ( query )	0
Q_339	a special set function to ensure	C_339_2	def validate_host_parameters ( self , host_list , remote_user ) if host_list is none host_list = self . host_list if remote_user is none remote_user = self . remote_user if host_list is none or remote_user is none print string ( host_list , remote_user ) return ( none , none ) return ( host_list , remote_user )	0
Q_340	take a method which act on tagint or more regular expression object	C_340_0	def call_fsm ( method ) fsm_method = getattr ( fsm . fsm , method . _name_ ) def new_method ( * legos ) alphabet = set ( ) . union ( * [ var . alphabet ( ) for var in legos ] ) return from_fsm ( fsm_method ( * [ var . to_fsm ( alphabet ) for var in legos ] ) ) return new_method	1
Q_340	take a method which act on tagint or more regular expression object	C_340_1	def get_ceph_pools ( self , sentry_unit ) pools = { } cmd = string output , code = sentry_unit . run ( cmd ) if code = number msg = ( string string . format ( sentry_unit . info [ string ] , cmd , code , output ) ) amulet . raise_status ( amulet . fail , msg = msg ) output = output . replace ( string , string ) for var in str ( output ) . split ( string ) pool_id_name = var . split ( string ) if len ( pool_id_name ) = = number pool_id = pool_id_name [ number ] pool_name = pool_id_name [ number ] pools [ pool_name ] = int ( pool_id ) self . log . debug ( string . format ( sentry_unit . info [ string ] , pools ) ) return pools	0
Q_340	take a method which act on tagint or more regular expression object	C_340_2	def quick_ratio ( self ) if self . fullbcount is none self . fullbcount = fullbcount = { } for var in self . b fullbcount [ var ] = fullbcount . get ( var , number ) + number fullbcount = self . fullbcount avail = { } availhas , matches = avail . _contains_ , number for var in self . a if availhas ( var ) numb = avail [ var ] else numb = fullbcount . get ( var , number ) avail [ var ] = numb _ number if numb > number matches = matches + number return _calculate_ratio ( matches , len ( self . a ) + len ( self . b ) )	0
Q_341	return a list with key of this dawg that be prefix of the key	C_341_0	def prefixes ( self , key ) res = [ ] index = self . dct . root if not isinstance ( key , bytes ) key = key . encode ( string ) pos = number for var in key index = self . dct . follow_char ( int_from_byte ( var ) , index ) if not index break if self . _has_value ( index ) res . append ( key [ pos ] . decode ( string ) ) pos + = number return res	1
Q_341	return a list with key of this dawg that be prefix of the key	C_341_1	def load_or_create_config ( self , filename , config = none ) os . makedirs ( os . path . dirname ( os . path . expanduser ( filename ) ) , exist_ok = true ) if os . path . exists ( filename ) return self . load ( filename ) if ( config = = none ) config = self . random_config ( ) self . save ( filename , config ) return config	0
Q_341	return a list with key of this dawg that be prefix of the key	C_341_2	def _set_user ( self , v , load = false ) if hasattr ( v , string ) v = v . _utype ( v ) try t = yang_dyn_class ( v , base = yang_list_type ( string , user . user , yang_name = string , rest_name = string , parent = self , is_container = string , user_ordered = false , path_helper = self . _path_helper , yang_keys = string , extensions = { u string { u string u string , u string none , u string u string , u string none , u string none , u string u string , u string none , u string none , u string none } } ) , is_container = string , yang_name = string , rest_name = string , parent = self , path_helper = self . _path_helper , extmethods = self . _extmethods , register_paths = true , extensions = { u string { u string u string , u string none , u string u string , u string none , u string none , u string u string , u string none , u string none , u string none } } , namespace = string , defining_module = string , yang_type = string , is_config = true ) except ( type_error , value_error ) raise value_error ( { string string string string , string string , string string string username string user string user string list string username string tailf _ common string info string holds username , groupname ( admin user ) , auth and priv attributes associated with snmp username string cli _ suppress _ mode string sort _ priority string number string cli _ suppress _ show _ match string cli _ suppress _ list _ no string callpoint string snmpuser string cli _ compact _ syntax string cli _ suppress _ key _ abbreviation string cli _ full _ no string list string user string user string tailf _ common string info string holds username , groupname ( admin user ) , auth and priv attributes associated with snmp username string cli _ suppress _ mode string sort _ priority string number string cli _ suppress _ show _ match string cli _ suppress _ list _ no string callpoint string snmpuser string cli _ compact _ syntax string cli _ suppress _ key _ abbreviation string cli _ full _ no string urn brocade . com mgmt brocade _ snmp string brocade _ snmp string list string string , } ) self . _user = t if hasattr ( self , string ) self . _set ( )	0
Q_342	"callback that output libgphoto2 "" s log message via"	C_342_0	def _logging_callback ( level , domain , message , data ) domain = ffi . string ( domain ) . decode ( ) message = ffi . string ( message ) . decode ( ) logger = logger . get_child ( domain ) if level not in log_levels return logger . log ( log_levels [ level ] , message )	1
Q_342	"callback that output libgphoto2 "" s log message via"	C_342_1	def run ( self , handler ) import eventlet . patcher if not eventlet . patcher . is_monkey_patched ( os ) msg = ( string string self . _class_ . _name_ ) raise runtime_error ( msg ) wsgi_args = { } for var in ( string , string , string , string , string , string , string , string , string , string , string , string , string , string , string , string ) try wsgi_args [ var ] = self . options . pop ( var ) except key_error pass if string not in wsgi_args wsgi_args [ string ] = not self . quiet import eventlet . wsgi sock = self . options . pop ( string , none ) or self . get_socket ( ) eventlet . wsgi . server ( sock , handler , * * wsgi_args )	0
Q_342	"callback that output libgphoto2 "" s log message via"	C_342_2	def _fetch_route53_zone_tags ( self , zone_id ) route53 = self . session . client ( string ) try return { var [ string ] var [ string ] for var in route53 . list_tags_for_resource ( resource_type = string , resource_id = zone_id . split ( string ) [ _ number ] ) [ string ] [ string ] } finally del route53	0
Q_343	instantiate an ed from an xmrs	C_343_0	def from_xmrs ( cls , xmrs , predicate_modifiers = false , * * kwargs ) eps = xmrs . eps ( ) deps = _find_basic_dependencies ( xmrs , eps ) if predicate_modifiers is true func = non_argument_modifiers ( role = string , only_connecting = true ) addl_deps = func ( xmrs , deps ) elif predicate_modifiers is false or predicate_modifiers is none addl_deps = { } elif hasattr ( predicate_modifiers , string ) addl_deps = predicate_modifiers ( xmrs , deps ) else raise type_error ( string ) for var , var in addl_deps . items ( ) deps . setdefault ( var , [ ] ) . extend ( var ) ids = _unique_ids ( eps , deps ) root = _find_root ( xmrs ) if root is not none root = ids [ root ] nodes = [ node ( ids [ n . nodeid ] , * n [ number ] ) for n in make_nodes ( xmrs ) ] edges = [ ( ids [ a ] , rarg , ids [ b ] ) for a , var in deps . items ( ) for rarg , b in var ] return cls ( top = root , nodes = nodes , edges = edges )	1
Q_343	instantiate an ed from an xmrs	C_343_1	def gateway_by_type ( self , type = none , on_network = none ) gateways = route_level ( self , string ) if not type for var in gateways yield var else for node in gateways if type = = node . routing_node_element . typeof parent = node . _parent if parent . level = = string interface = parent network = none else network = parent interface = network . _parent if on_network is not none if network and network . ip = = on_network yield ( interface , network , node ) else yield ( interface , network , node )	0
Q_343	instantiate an ed from an xmrs	C_343_2	def grep_param_names ( self , regexp ) if not isinstance ( regexp , _pattern_type ) regexp = compile ( regexp ) found_params = [ ] def visit ( innerself , regexp ) if ( innerself is not self ) and regexp . match ( innerself . hierarchy_name ( ) . partition ( string ) [ number ] ) found_params . append ( innerself ) self . traverse ( visit , regexp ) return found_params	0
Q_344	call the clean method of the class and return the info in a structure	C_344_0	def build_api_struct ( self ) self . clean ( ) r = { string self . _type , string self . _requested , string self . _value } if self . _tags r [ string ] = self . _tags return r	1
Q_344	call the clean method of the class and return the info in a structure	C_344_1	def forward_search_history ( self , e ) u string string string log ( string ) self . _init_incremental_search ( self . _history . forward_search_history , e ) self . finalize ( )	0
Q_344	call the clean method of the class and return the info in a structure	C_344_2	def get_indexes ( self , schema , * * kwargs ) with self . connection ( * * kwargs ) as connection kwargs [ string ] = connection return self . _get_indexes ( schema , * * kwargs )	0
Q_345	reset the current configuration to the previous state	C_345_0	def unapply ( self ) for var , var in self . _old_config . items ( ) _config [ var ] = var	1
Q_345	reset the current configuration to the previous state	C_345_1	def rownumber ( self ) if self . _frame is not none and self . _pos is not none return self . _frame . offset + self . _pos return self . _pos	0
Q_345	reset the current configuration to the previous state	C_345_2	def collect ( self ) local_df = self . _schema_rdd . to_pandas ( ) correct_idx_df = _update_index_on_df ( local_df , self . _index_names ) return correct_idx_df	0
Q_346	helper function to get an entry by id or by object	C_346_0	def get_entry ( entry ) if hasattr ( entry , string ) return entry if isinstance ( entry , ( int , str ) ) return model . entry . get ( id = int ( entry ) ) raise value_error ( string . format ( type ( entry ) ) )	1
Q_346	helper function to get an entry by id or by object	C_346_1	def main ( ) try device = alarm_decoder ( serial_device ( interface = serial_device ) ) device . on_rfx_message + = handle_rfx with device . open ( baudrate = baudrate ) while true time . sleep ( number ) except exception as ex print ( string , ex )	0
Q_346	helper function to get an entry by id or by object	C_346_2	def _get_environs ( self , prefix str = none ) _ > dict if not prefix log . debug ( string ) prefix = default_environ_prefix return dict_from_environs ( prefix , self . name , list ( self . arguments . keys ( ) ) )	0
Q_347	read a magic_formatted tab_delimited file and return a dictionary of	C_347_0	def magic_read_dict ( path , data = none , sort_by_this_name = none , return_keys = false ) data = { } lines = open_file ( path ) if not lines if return_keys return { } , string , none else return { } , string first_line = lines . pop ( number ) if first_line [ number ] = = string or first_line [ number ] = = string delim = string elif first_line [ number ] = = string or first_line [ number ] = = string delim = string else print ( string , path ) if return_keys return { } , string , none else return { } , string file_type = first_line . strip ( string ) . strip ( string ) . split ( delim ) [ number ] item_type = file_type if sort_by_this_name pass elif item_type = = string sort_by_this_name = string else sort_by_this_name = item_type var = lines . pop ( number ) header = var . strip ( string ) . strip ( string ) . split ( delim ) counter = number for var in lines tmp_data = { } tmp_line = var . strip ( string ) . strip ( string ) . split ( delim ) for i in range ( len ( header ) ) if i < len ( tmp_line ) tmp_data [ header [ i ] ] = tmp_line [ i ] . strip ( ) else tmp_data [ header [ i ] ] = string if sort_by_this_name = = string data [ counter ] = tmp_data counter + = number else if tmp_data [ sort_by_this_name ] = string data [ tmp_data [ sort_by_this_name ] ] = tmp_data if return_keys return data , file_type , header else return data , file_type	1
Q_347	read a magic_formatted tab_delimited file and return a dictionary of	C_347_1	def _fix_valid_indices ( cls , valid_indices , insertion_index , dim ) indices = np . array ( sorted ( valid_indices [ dim ] ) ) slice_index = np . sum ( indices < = insertion_index ) indices [ slice_index ] + = number indices = np . insert ( indices , slice_index , insertion_index + number ) valid_indices [ dim ] = indices . tolist ( ) return valid_indices	0
Q_347	read a magic_formatted tab_delimited file and return a dictionary of	C_347_2	def excepthook ( exc_type , exc_value , tracebackobj ) separator = string * number log_file = os . path . join ( tempfile . gettempdir ( ) , string ) notice = string notice + = string string { } string string . format ( log_file ) time_string = time . strftime ( string ) tbinfofile = io . string_io ( ) traceback . print_tb ( tracebackobj , none , tbinfofile ) tbinfofile . seek ( number ) tbinfo = tbinfofile . read ( ) if python_version < number tbinfo = tbinfo . decode ( string ) else pass try if python_version < number exc_value_str = str ( exc_value ) . decode ( string ) else exc_value_str = str ( exc_value ) except unicode_encode_error exc_value_str = str ( exc_value ) errmsg = string . format ( exc_type , exc_value_str ) sections = [ string , separator , time_string , separator , errmsg , separator , tbinfo ] try msg = string . join ( sections ) except type_error sections = [ var for var in sections if type ( var ) = = str ] msg = string . join ( sections ) try f = codecs . open ( log_file , string , encoding = string ) f . write ( msg ) f . close ( ) except io_error msgbox ( string . format ( log_file ) , string ) try if not _is_q_app_running ( ) app = qt_gui . q_application ( [ ] ) _show_message_box ( str ( notice ) + str ( msg ) ) except exception msgbox ( str ( notice ) + str ( msg ) , string )	0
Q_348	invoke the container_xxxx list_folder api method	C_348_0	def container_list_folder ( object_id , input_params = { } , always_retry = true , * * kwargs ) return dxhttp_request ( string object_id , input_params , always_retry = always_retry , * * kwargs )	1
Q_348	invoke the container_xxxx list_folder api method	C_348_1	def get_var_data ( self , fmt = none ) safe_repr = safe_repr ( ) if fmt is not none safe_repr . convert_to_hex = fmt . get ( string , false ) safe_repr . raw_value = fmt . get ( string , false ) type_name , _type_qualifier , _is_exception_on_eval , resolver , value = get_variable_details ( self . value , to_string = safe_repr ) is_raw_string = type_name in ( string , string , string , string ) attributes = [ ] if is_raw_string attributes . append ( string ) name = self . name if self . _is_return_value attributes . append ( string ) name = string ( name , ) var_data = { string name , string value , string type_name , } if self . evaluate_name is not none var_data [ string ] = self . evaluate_name if resolver is not none var_data [ string ] = self . get_variable_reference ( ) if len ( attributes ) > number var_data [ string ] = { string attributes } return var_data	0
Q_348	invoke the container_xxxx list_folder api method	C_348_2	def remove_service ( self , zconf , typ , name ) _logger . debug ( string , typ , name ) service = self . services . pop ( name , none ) if not service _logger . debug ( string , typ , name ) return if self . remove_callback self . remove_callback ( name , service )	0
Q_349	create a set of directory so we could store the input file	C_349_0	def _create_dirs ( idir , dirs ) file_load_utils . ensure_dir ( idir ) for var in dirs file_load_utils . ensure_dir ( os . path . join ( idir , var ) )	1
Q_349	create a set of directory so we could store the input file	C_349_1	def successors_iter ( self , n , t = none ) try if t is none return iter ( self . _succ [ n ] ) else return iter ( [ var for var in self . _succ [ n ] if self . _presence_test ( n , var , t ) ] ) except key_error raise nx . network_x_error ( string ( n , ) )	0
Q_349	create a set of directory so we could store the input file	C_349_2	"def download ( self , id , directory ) url = ( self . _base + string ) . format ( id ) r = requests . get ( url , auth = self . _auth , stream = true ) file_name = r . headers [ string ] . split ( string string wb "" ) as file for var in r . iter_content ( chunk_size = number ) if var file . write ( var ) file . flush ( )"	0
Q_350	param length maximum number of byte to be read	C_350_0	def _read2 ( self , length = none , use_compression = none , project = none , * * kwargs ) if self . _file_length = = none desc = self . describe ( * * kwargs ) if desc [ string ] = string raise dx_file_error ( string ) self . _file_length = int ( desc [ string ] ) get_first_chunk_sequentially = ( self . _file_length > number * number and self . _pos = = number and dxpy . job_id ) if self . _pos = = self . _file_length return b string if length = = none or length > self . _file_length _ self . _pos length = self . _file_length _ self . _pos buf = self . _read_buf buf_remaining_bytes = dxpy . utils . string_buffer_length ( buf ) _ buf . tell ( ) if length < = buf_remaining_bytes self . _pos + = length return buf . read ( length ) else orig_buf_pos = buf . tell ( ) orig_file_pos = self . _pos buf . seek ( number , os . seek_end ) self . _pos + = buf_remaining_bytes while self . _pos < orig_file_pos + length remaining_len = orig_file_pos + length _ self . _pos if self . _response_iterator is none self . _request_iterator = self . _generate_read_requests ( start_pos = self . _pos , project = project , * * kwargs ) content = self . _next_response_content ( get_first_chunk_sequentially = get_first_chunk_sequentially ) if len ( content ) < remaining_len buf . write ( content ) self . _pos + = len ( content ) else buf . write ( content [ remaining_len ] ) self . _pos + = remaining_len self . _read_buf = bytes_io ( ) self . _read_buf . write ( content [ remaining_len ] ) self . _read_buf . seek ( number ) buf . seek ( orig_buf_pos ) return buf . read ( )	1
Q_350	param length maximum number of byte to be read	C_350_1	def search_string ( self , instring , max_matches = _max_int ) try return parse_results ( [ t for t , s , e in self . scan_string ( instring , max_matches ) ] ) except parse_base_exception as exc if parser_element . verbose_stacktrace raise else raise exc	0
Q_350	param length maximum number of byte to be read	C_350_2	def dev_get_chunk ( dev_name , state , pugrp = none , punit = none ) rprt = dev_get_rprt ( dev_name , pugrp , punit ) if not rprt return none return next ( ( var for var in rprt if var [ string ] = = state ) , none )	0
Q_351	validate user supply filter	C_351_0	def _validate_filters ( cls , filters , model_class ) if not filters return properties = model_class . properties ( ) for var in filters prop , _ , val = var if prop not in properties raise errors . bad_reader_params_error ( string , prop , model_class . kind ( ) ) try properties [ prop ] . validate ( val ) except db . bad_value_error , e raise errors . bad_reader_params_error ( e )	1
Q_351	validate user supply filter	C_351_1	def create_analytic_backend ( settings ) backend = settings . get ( string ) if isinstance ( backend , basestring ) backend = import_string ( backend ) elif backend backend = backend else raise key_error ( string ) return backend ( settings . get ( string , { } ) )	0
Q_351	validate user supply filter	C_351_2	def parse_version ( version ) global parse_version try if string in _import_ ( string ) . environ from pkg_resources import parse_version else from pex . third_party . pkg_resources import parse_version except import_error from distutils . version import loose_version as parse_version return parse_version ( version )	0
Q_352	convolution layer helper function	C_352_0	def conv ( input , kernel , biases , k_h , k_w , c_o , s_h , s_w , padding = string , group = number ) c_i = input . get_shape ( ) [ _ number ] assert c_i group = = number assert c_o group = = number convolve = lambda var , var tf . nn . conv2d ( var , var , [ number , s_h , s_w , number ] , padding = padding ) if group = = number conv = convolve ( input , kernel ) else input_groups = tf . split ( input , group , axis = number ) kernel_groups = tf . split ( kernel , group , axis = number ) output_groups = [ convolve ( var , var ) for var , var in zip ( input_groups , kernel_groups ) ] conv = tf . concat ( output_groups , axis = number ) return tf . reshape ( tf . nn . bias_add ( conv , biases ) , [ _ number ] + conv . get_shape ( ) . as_list ( ) [ number ] )	1
Q_352	convolution layer helper function	C_352_1	def update_milestone ( id , * * kwargs ) data = update_milestone_raw ( id , * * kwargs ) if data return utils . format_json ( data )	0
Q_352	convolution layer helper function	C_352_2	def p_funcvardecls ( self , p ) string p [ number ] = p [ number ] + ( p [ number ] , ) p . set_lineno ( number , p . lineno ( number ) )	0
Q_353	attach to an exist signal_flow computation	C_353_0	def attach ( self , handle , filters = none , resolution = none ) params = self . _get_params ( filters = filters , resolution = resolution ) c = computation . computation ( lambda since self . _transport . attach ( handle , params ) ) self . _computations . add ( c ) return c	1
Q_353	attach to an exist signal_flow computation	C_353_1	def recursive_copy ( source , dest ) for root , _ , files in salt . utils . path . os_walk ( source ) path_from_source = root . replace ( source , string ) . lstrip ( os . sep ) target_directory = os . path . join ( dest , path_from_source ) if not os . path . exists ( target_directory ) os . makedirs ( target_directory ) for var in files file_path_from_source = os . path . join ( source , path_from_source , var ) target_path = os . path . join ( target_directory , var ) shutil . copyfile ( file_path_from_source , target_path )	0
Q_353	attach to an exist signal_flow computation	C_353_2	def encode_params ( self , data = none , * * kwargs ) charset = kwargs . get ( string , self . charset ) collection_format = kwargs . get ( string , self . collection_format ) output_str = kwargs . get ( string , self . output_str ) if data is none return string , self . get_content_type ( charset ) elif isinstance ( data , ( str , bytes ) ) return data , self . get_content_type ( charset ) elif hasattr ( data , string ) return data , self . get_content_type ( charset ) elif collection_format = = string and hasattr ( data , string ) result = [ ] for var , var in to_key_val_list ( data ) if isinstance ( var , basestring ) or not hasattr ( var , string ) var = [ var ] for v in var result . append ( b string . join ( [ self . _encode ( var , charset ) , self . _encode ( v , charset , output_str ) ] ) ) return b string . join ( result ) , self . get_content_type ( charset ) elif collection_format = = string and hasattr ( data , string ) results = [ ] for var , var in to_key_val_dict ( data ) . items ( ) results . append ( b string . join ( [ self . _encode ( var , charset ) , self . _encode ( var , charset , output_str ) ] ) ) return b string . join ( results ) , self . get_content_type ( charset ) elif hasattr ( data , string ) results = [ ] for var , var in to_key_val_dict ( data ) . items ( ) if isinstance ( var , list ) v = self . collection_separators [ collection_format ] . join ( e for e in var ) key = var + string else v = var key = var results . append ( b string . join ( [ self . _encode ( key , charset ) , self . _encode ( v , charset , output_str ) ] ) ) return b string . join ( results ) , self . get_content_type ( charset ) else return str ( data ) . encode ( charset ) if charset else str ( data ) , self . get_content_type ( charset )	0
Q_354	this function have the same signature as the pre_v3 filter return a	C_354_0	def filter ( n node , query str ) _ > compat_node_iterator ctx = uast ( ) return compat_node_iterator ( node_iterator ( ctx . filter ( query , n . internal_node ) , ctx ) )	1
Q_354	this function have the same signature as the pre_v3 filter return a	C_354_1	def regret_met ( self , threshold = none ) if not threshold return self . regret ( ) < = self . stop_value elif self . regret ( ) < = threshold return true else return false	0
Q_354	this function have the same signature as the pre_v3 filter return a	C_354_2	def parse_content_type ( ct str ) _ > tuple [ str , str , mapping [ str , str ] ] pieces = ct . split ( string ) try main_type , sub_type = pieces [ number ] . split ( string ) except value_error msg = string { number } string . format ( pieces [ number ] ) raise value_error ( msg ) if len ( pieces ) > number options = { } for var in pieces [ number ] var = var . strip ( ) if not var continue try k , v = var . split ( string , number ) except value_error msg = string { number } string . format ( var ) raise value_error ( msg ) else options [ k . lower ( ) ] = v . lower ( ) else options = { } return main_type . lower ( ) , sub_type . lower ( ) , options	0
Q_355	"get our numerical description of how hottt a song "" s artist currently be"	C_355_0	def get_artist_hotttnesss ( self , cache = true ) if not ( cache and ( string in self . cache ) ) response = self . get_attribute ( string , bucket = string ) self . cache [ string ] = response [ string ] [ number ] [ string ] return self . cache [ string ]	1
Q_355	"get our numerical description of how hottt a song "" s artist currently be"	C_355_1	def get_templates ( fnames , blend = true ) if not blend newhdrs = blendheaders . get_single_template ( fnames [ number ] ) newtab = none else newhdrs , newtab = blendheaders . get_blended_headers ( inputs = fnames ) clean_templates ( newhdrs [ number ] , newhdrs [ number ] , newhdrs [ number ] ) return newhdrs , newtab	0
Q_355	"get our numerical description of how hottt a song "" s artist currently be"	C_355_2	def get_dataset ( self , dataset_key ) try return self . _datasets_api . get_dataset ( * ( parse_dataset_key ( dataset_key ) ) ) . to_dict ( ) except _swagger . rest . api_exception as e raise rest_api_error ( cause = e )	0
Q_356	convert a singularity recipe to a dockerfile	C_356_0	def singularity2docker ( self , runscript = string , force = false ) recipe = [ string self . from_header ] recipe + = self . comments recipe + = write_lines ( string , self . files ) recipe + = write_lines ( string , self . labels ) recipe + = write_lines ( string , self . environ ) recipe + = write_lines ( string , self . install ) runscript = self . _create_runscript ( runscript , force ) recipe . append ( string runscript ) if self . test is not none recipe . append ( write_lines ( string , self . test ) ) return string . join ( recipe ) . replace ( string , string )	1
Q_356	convert a singularity recipe to a dockerfile	C_356_1	def _bytes_to_str ( lines ) if len ( lines ) > = number and hasattr ( lines [ number ] , string ) return [ var . decode ( string ) for var in lines ] else return lines	0
Q_356	convert a singularity recipe to a dockerfile	C_356_2	def _init_metadata ( self , * * kwargs ) osid_objects . osid_object_form . _init_metadata ( self , * * kwargs ) self . _learning_objectives_default = self . _mdata [ string ] [ string ]	0
Q_357	launch the experiment	C_357_0	def launch ( ) try exp = experiment ( db . init_db ( drop_all = false ) ) except exception as ex return error_response ( error_text = string . format ( str ( ex ) ) , status = number , simple = true , ) try exp . log ( string , string ) except io_error as ex return error_response ( error_text = string . format ( str ( ex ) ) , status = number , simple = true , ) try recruitment_details = exp . recruiter . open_recruitment ( n = exp . initial_recruitment_size ) session . commit ( ) except exception as e return error_response ( error_text = string string . format ( str ( e ) ) , status = number , simple = true , ) for var in exp . background_tasks try gevent . spawn ( var ) except exception return error_response ( error_text = string . format ( var ) + string , status = number , simple = true , ) if _config ( ) . get ( string , false ) try var = replay_backend ( exp ) gevent . spawn ( var ) except exception return error_response ( error_text = string string , status = number , simple = true , ) if exp . channel is not none try from dallinger . experiment_server . sockets import chat_backend chat_backend . subscribe ( exp , exp . channel ) except exception return error_response ( error_text = string + string . format ( exp . channel ) + string , status = number , simple = true , ) message = string . join ( ( string . format ( string . join ( recruitment_details [ string ] ) ) , string . format ( recruitment_details [ string ] ) , ) ) return success_response ( recruitment_msg = message )	1
Q_357	launch the experiment	C_357_1	def require_ajax_logged_in ( func ) functools . wraps ( func ) def inner_func ( self , * pargs , * * kwargs ) if not self . _ajax_api . logged_in logger . info ( string ) if not self . has_credentials raise api_login_failure ( string ) self . _ajax_api . user_login ( name = self . _state [ string ] , password = self . _state [ string ] ) return func ( self , * pargs , * * kwargs ) return inner_func	0
Q_357	launch the experiment	C_357_2	def get_statement ( self , session_id int , statement_id int ) _ > statement response = self . _client . get ( f string ) return statement . from_json ( session_id , response )	0
Q_358	combine multiple file into one	C_358_0	"def merge_bams ( self , input_bams , merged_bam , in_sorted = string , tmp_dir = none ) if not len ( input_bams ) > number print ( string ) return number outdir , _ = os . path . split ( merged_bam ) if outdir and not os . path . exists ( outdir ) print ( string s folder string string true string false string input = string input = string _ xmx string _ jar string merge_sam_files string output = string assume_sorted = string create_index = true string validation_stringency = silent string tmp_dir = "" + tmp_dir return cmd"	1
Q_358	combine multiple file into one	C_358_1	"def pearson ( x , y ) mx = decimal ( mean ( x ) ) my = decimal ( mean ( y ) ) xm = [ decimal ( var ) _ mx for var in x ] ym = [ decimal ( j ) _ my for j in y ] sx = [ var * * number for var in xm ] sy = [ j * * number for j in ym ] num = sum ( [ a * b for a , b in zip ( xm , ym ) ] ) den = decimal ( sum ( sx ) * sum ( sy ) ) . sqrt ( ) if den = = number . number raise no_affinity_error ( string string scores is zero "" ) return float ( num / den )"	0
Q_358	combine multiple file into one	C_358_2	def delete_cache_settings ( self , service_id , version_number , name ) content = self . _fetch ( string ( service_id , version_number , name ) , method = string ) return self . _status ( content )	0
Q_359	get the appropriate delay value to use try in this order	C_359_0	def _get_delay ( self , userdelay , frameslist ) delay = userdelay or getattr ( frameslist , string , none ) delay = ( delay or self . default_delay ) _ self . nice_delay if delay < number delay = number return delay	1
Q_359	get the appropriate delay value to use try in this order	C_359_1	def next ( self ) try self . key , self . value = next ( self . current ) except stop_iteration if self . current = self . second self . current = self . second return self . next ( ) return false return true	0
Q_359	get the appropriate delay value to use try in this order	C_359_2	def get_stack ( ) _ > optional [ str ] stack = settings . stack if not stack require_test_mode_enabled ( ) raise run_error ( string ) return stack	0
Q_360	convert tagint d density into tagint d project density parameter	C_360_0	def rho2theta ( self , rho0 , gamma ) fac = np . sqrt ( np . pi ) * special . gamma ( number . / number * ( _ number + gamma ) ) / special . gamma ( gamma / number . ) * number / ( number _ gamma ) * rho0 theta_e = fac * * ( number . / ( gamma _ number ) ) return theta_e	1
Q_360	convert tagint d density into tagint d project density parameter	C_360_1	def get_distinfo_file ( self , path ) if path . find ( os . sep ) > = number distinfo_dirname , path = path . split ( os . sep ) [ _ number ] if distinfo_dirname = self . path . split ( os . sep ) [ _ number ] raise distlib_exception ( string string ( path , self . name , self . version ) ) if path not in dist_files raise distlib_exception ( string string ( path , self . path ) ) return os . path . join ( self . path , path )	0
Q_360	convert tagint d density into tagint d project density parameter	C_360_2	def iter_links ( operations , page ) for operation , ns , rule , func in operations yield link . for_ ( operation = operation , ns = ns , type = ns . subject_name , qs = page . to_items ( ) , )	0
Q_361	build nx_graph base on the lv grid model	C_361_0	def build_lv_graph_residential ( lvgd , selected_string_df ) houses_connected = ( selected_string_df [ string ] * selected_string_df [ string ] ) . sum ( ) average_load = lvgd . peak_load_residential / houses_connected average_consumption = lvgd . sector_consumption_residential / houses_connected hh_branch = number for var , var in selected_string_df . iterrows ( ) branch_count_sum = len ( list ( lvgd . lv_grid . _graph . neighbors ( lvgd . lv_grid . station ( ) ) ) ) for branch_no in range ( number , int ( var [ string ] ) + number ) hh_branch + = number for house_branch in range ( number , var [ string ] + number ) if house_branch number = = number variant = string else variant = string lv_cable_dist = lv_cable_distributor_ding0 ( grid = lvgd . lv_grid , string_id = var , branch_no = branch_no + branch_count_sum , load_no = house_branch ) lvgd . lv_grid . add_cable_dist ( lv_cable_dist ) lv_cable_dist_building = lv_cable_distributor_ding0 ( grid = lvgd . lv_grid , string_id = var , branch_no = branch_no + branch_count_sum , load_no = house_branch , in_building = true ) lvgd . lv_grid . add_cable_dist ( lv_cable_dist_building ) lv_load = lv_load_ding0 ( grid = lvgd . lv_grid , string_id = var , branch_no = branch_no + branch_count_sum , load_no = house_branch , peak_load = average_load , consumption = { string average_consumption } ) lvgd . lv_grid . add_load ( lv_load ) cable_name = var [ string ] + string . format ( var [ string ] ) cable_type = lvgd . lv_grid . network . static_data [ string ] . loc [ cable_name ] if house_branch = = number lvgd . lv_grid . _graph . add_edge ( lvgd . lv_grid . station ( ) , lv_cable_dist , branch = branch_ding0 ( length = var [ string ] , kind = string , type = cable_type , id_db = string . format ( branch = hh_branch , load = house_branch , sector = string ) ) ) else lvgd . lv_grid . _graph . add_edge ( lvgd . lv_grid . _cable_distributors [ _ number ] , lv_cable_dist , branch = branch_ding0 ( length = var [ string ] , kind = string , type = lvgd . lv_grid . network . static_data [ string ] . loc [ cable_name ] , id_db = string . format ( branch = hh_branch , load = house_branch , sector = string ) ) ) house_cable_name = var [ string . format ( variant ) ] + string . format ( var [ string . format ( variant ) ] ) lvgd . lv_grid . _graph . add_edge ( lv_cable_dist , lv_cable_dist_building , branch = branch_ding0 ( length = var [ string . format ( variant ) ] , kind = string , type = lvgd . lv_grid . network . static_data [ string ] . loc [ house_cable_name ] , id_db = string . format ( branch = hh_branch , load = house_branch , sector = string ) ) ) lvgd . lv_grid . _graph . add_edge ( lv_cable_dist_building , lv_load , branch = branch_ding0 ( length = number , kind = string , type = lvgd . lv_grid . network . static_data [ string ] . loc [ house_cable_name ] , id_db = string . format ( branch = hh_branch , load = house_branch , sector = string ) ) )	1
Q_361	build nx_graph base on the lv grid model	C_361_1	def run_parallel ( self , para_func ) if self . timer start_timer = time . time ( ) with mp . pool ( self . num_processors ) as pool print ( string . format ( self . num_processors , len ( self . args ) ) ) results = [ pool . apply_async ( para_func , var ) for var in self . args ] out = [ r . get ( ) for r in results ] out = { key np . concatenate ( [ out_i [ key ] for out_i in out ] ) for key in out [ number ] . keys ( ) } if self . timer print ( string , time . time ( ) _ start_timer ) return out	0
Q_361	build nx_graph base on the lv grid model	C_361_2	def is_subdict ( self , a , b ) return all ( ( var in b and b [ var ] = = var ) for var , var in a . iteritems ( ) )	0
Q_362	parse argument string as key value pair separate by comma	C_362_0	def args_kvp_nodup ( s ) if s is none return { } d = { } for var in [ e . strip ( ) for e in s . split ( string ) ] try key , value = var . split ( string , number ) except value_error msg = string { } string . format ( var ) if _argparse_is_dumb _alog . warn ( msg ) raise value_error ( msg ) if key in d msg = string { } string . format ( key ) if _argparse_is_dumb _alog . warn ( msg ) raise value_error ( msg ) d [ key ] = value return d	1
Q_362	parse argument string as key value pair separate by comma	C_362_1	def make_model_sources_image ( shape , model , source_table , oversample = number ) image = np . zeros ( shape , dtype = np . float64 ) y , x = np . indices ( shape ) params_to_set = [ ] for var in source_table . colnames if var in model . param_names params_to_set . append ( var ) init_params = { var getattr ( model , var ) for var in params_to_set } try for i , source in enumerate ( source_table ) for var in params_to_set setattr ( model , var , source [ var ] ) if oversample = = number image + = model ( x , y ) else image + = discretize_model ( model , ( number , shape [ number ] ) , ( number , shape [ number ] ) , mode = string , factor = oversample ) finally for var , value in init_params . items ( ) setattr ( model , var , value ) return image	0
Q_362	parse argument string as key value pair separate by comma	C_362_2	def initialize_parameters ( self , initializer mx . init . initializer , allow_missing_params bool ) self . module . init_params ( initializer = initializer , arg_params = self . params , aux_params = self . aux_params , allow_missing = allow_missing_params , force_init = false )	0
Q_363	add a host with a fixed_address and override it hostname with the give name	C_363_0	def add_host_supersede_name ( self , ip , mac , name ) msg = omapi_message . open ( b string ) msg . message . append ( ( b string , struct . pack ( string , number ) ) ) msg . message . append ( ( b string , struct . pack ( string , number ) ) ) msg . obj . append ( ( b string , pack_mac ( mac ) ) ) msg . obj . append ( ( b string , struct . pack ( string , number ) ) ) msg . obj . append ( ( b string , pack_ip ( ip ) ) ) msg . obj . append ( ( b string , name . encode ( string ) ) ) msg . obj . append ( ( b string , string { number } string . format ( name ) . encode ( string ) ) ) response = self . query_server ( msg ) if response . opcode = omapi_op_update raise omapi_error ( string )	1
Q_363	add a host with a fixed_address and override it hostname with the give name	C_363_1	def cli ( ctx , group , user ) return ctx . gi . users . remove_from_group ( group , user )	0
Q_363	add a host with a fixed_address and override it hostname with the give name	C_363_2	def do_capacity ( self , line ) string args = self . getargs ( line ) table = self . get_table ( args [ number ] ) read_units = int ( args [ number ] ) write_units = int ( args [ number ] ) desc = self . conn . describe_table ( table . name ) prov = desc [ string ] [ string ] current_read , current_write = prov [ string ] , prov [ string ] if read_units < current_read or write_units < current_write table . update_throughput ( read_units , write_units ) print string ( table . name , read_units , write_units ) print string self . do_refresh ( table . name ) else print string ( table . name , current_read , current_write ) while current_read < read_units or current_write < write_units if ( read_units _ current_read ) > current_read current_read * = number else current_read = read_units if ( write_units _ current_write ) > current_write current_write * = number else current_write = write_units print string ( table . name , current_read , current_write ) table . update_throughput ( current_read , current_write ) print string self . do_refresh ( table . name ) print string	0
Q_364	download file to the specify path or as temporary file	C_364_0	def content ( self , path = none , overwrite = true , encoding = string ) if path self . download ( wait = true , path = path , overwrite = overwrite ) with io . open ( path , string , encoding = encoding ) as fp return fp . read ( ) with tempfile . named_temporary_file ( ) as tmpfile self . download ( wait = true , path = tmpfile . name , overwrite = overwrite ) with io . open ( tmpfile . name , string , encoding = encoding ) as fp return fp . read ( )	1
Q_364	download file to the specify path or as temporary file	C_364_1	def _create_query_access_helper ( self ) h = check_access_helper ( string ) h . allow ( string , self . _is_home_dir ) h . allow ( string ) h . allow ( string ) h . allow ( string ) h . allow ( string ) h . allow ( string ) h . allow ( string ) h . allow ( self . client_urn_pattern ) h . allow ( self . client_urn_pattern + string ) h . allow ( string ) h . allow ( string ) h . allow ( string ) h . allow ( string ) h . allow ( string ) h . allow ( string + string * number ) h . allow ( string ) h . allow ( string ) h . allow ( string ) h . allow ( string ) h . allow ( string ) h . allow ( string ) return h	0
Q_364	download file to the specify path or as temporary file	C_364_2	def file_put ( blockchain_id , hostname , recipient_blockchain_ids , data_name , input_path , passphrase = none , config_path = config_path , wallet_keys = none ) fd , output_path = tempfile . mkstemp ( prefix = string ) os . fchmod ( fd , number number ) os . close ( fd ) config_dir = os . path . dirname ( config_path ) client_config_path = os . path . join ( config_dir , blockstack_client . config_filename ) all_recipients = [ ] my_hosts = file_list_hosts ( blockchain_id , wallet_keys = wallet_keys , config_path = config_path ) if string in my_hosts log . error ( string my_hosts [ string ] ) os . unlink ( output_path ) return { string string } if hostname in my_hosts my_hosts . remove ( hostname ) all_recipients + = [ ( blockchain_id , var ) for var in my_hosts [ string ] ] for recipient_blockchain_id in recipient_blockchain_ids their_hosts = file_list_hosts ( recipient_blockchain_id , wallet_keys = wallet_keys , config_path = config_path ) if string in their_hosts log . error ( string ( recipient_blockchain_id , their_hosts [ string ] ) ) os . unlink ( output_path ) return { string string } all_recipients + = [ ( recipient_blockchain_id , var ) for var in their_hosts [ string ] ] res = file_encrypt ( blockchain_id , hostname , all_recipients , input_path , output_path , passphrase = passphrase , config_path = config_path , wallet_keys = wallet_keys ) if string in res log . error ( string res [ string ] ) os . unlink ( output_path ) return { string string } with open ( output_path , string ) as f ciphertext = f . read ( ) message = { string ciphertext , string res [ string ] } fq_data_name = file_fq_data_name ( data_name ) proxy = blockstack_client . get_default_proxy ( config_path = client_config_path ) res = blockstack_client . data_put ( blockstack_client . make_mutable_data_url ( blockchain_id , fq_data_name , none ) , message , wallet_keys = wallet_keys , proxy = proxy ) if string in res log . error ( string res [ string ] ) os . unlink ( output_path ) return { string string } os . unlink ( output_path ) return { string true }	0
Q_365	create and configure a partition in this cpc	C_365_0	def create ( self , properties ) result = self . session . post ( self . cpc . uri + string , body = properties ) props = copy . deepcopy ( properties ) props . update ( result ) name = props . get ( self . _name_prop , none ) uri = props [ self . _uri_prop ] part = partition ( self , uri , name , props ) self . _name_uri_cache . update ( name , uri ) return part	1
Q_365	create and configure a partition in this cpc	C_365_1	def write_bytes ( self , data , n ) for var in xrange ( number , n ) self . payload [ self . var + var ] = data [ var ] self . var + = n	0
Q_365	create and configure a partition in this cpc	C_365_2	def setup_components_and_tf_funcs ( self , custom_getter = none ) self . network = network . from_spec ( spec = self . network_spec , kwargs = dict ( summary_labels = self . summary_labels ) ) assert len ( self . internals_spec ) = = number self . internals_spec = self . network . internals_spec ( ) for var in sorted ( self . internals_spec ) internal = self . internals_spec [ var ] self . internals_input [ var ] = tf . placeholder ( dtype = util . tf_dtype ( internal [ string ] ) , shape = ( none , ) + tuple ( internal [ string ] ) , var = ( string + var ) ) if internal [ string ] = = string self . internals_init [ var ] = np . zeros ( shape = internal [ string ] ) else raise tensor_force_error ( string ) custom_getter = super ( distribution_model , self ) . setup_components_and_tf_funcs ( custom_getter ) self . distributions = self . create_distributions ( ) self . fn_kl_divergence = tf . make_template ( name_ = string , func_ = self . tf_kl_divergence , custom_getter_ = custom_getter ) return custom_getter	0
Q_366	keep track of the number of pa1tagoer error warning and notice that	C_366_0	def track_pa11y_stats ( pa11y_results , spider ) num_err , num_warn , num_notice = pa11y_counts ( pa11y_results ) stats = spider . crawler . stats stats . inc_value ( string , count = num_err , spider = spider ) stats . inc_value ( string , count = num_warn , spider = spider ) stats . inc_value ( string , count = num_notice , spider = spider )	1
Q_366	keep track of the number of pa1tagoer error warning and notice that	C_366_1	def _handle_page_args ( self , rison_args ) page = rison_args . get ( api_page_index_ris_key , number ) page_size = rison_args . get ( api_page_size_ris_key , self . page_size ) return self . _sanitize_page_args ( page , page_size )	0
Q_366	keep track of the number of pa1tagoer error warning and notice that	C_366_2	def quarter_to_daterange ( quarter ) assert len ( quarter ) = = number year = int ( quarter [ number number ] ) quarter = quarter [ number ] month_day = { string ( ( number , number ) , ( number , number ) ) , string ( ( number , number ) , ( number , number ) ) , string ( ( number , number ) , ( number , number ) ) , string ( ( number , number ) , ( number , number ) ) } md = month_day [ quarter ] start_md , end_md = md return ( date ( year , * start_md ) , date ( year , * end_md ) )	0
Q_367	cuenta los campos obligatorios recomendados requeridos usados en	C_367_0	def _count_required_and_optional_fields ( catalog ) catalog = readers . read_catalog ( catalog ) catalog_fields_path = os . path . join ( catalog_fields_path , string ) with open ( catalog_fields_path ) as f catalog_fields = json . load ( f ) return _count_fields_recursive ( catalog , catalog_fields )	1
Q_367	cuenta los campos obligatorios recomendados requeridos usados en	C_367_1	def foreign_key_sets ( chain , node ) quack = false if node . attrname in manager_attrs or node . attrname . endswith ( string ) quack = true else if isinstance ( node . parent , attribute ) func_name = getattr ( node . parent , string , none ) if func_name in manager_attrs quack = true if quack children = list ( node . get_children ( ) ) for var in children try inferred_cls = var . inferred ( ) except inference_error pass else for cls in inferred_cls if ( node_is_subclass ( cls , string , string , string , string ) ) return chain ( )	0
Q_367	cuenta los campos obligatorios recomendados requeridos usados en	C_367_2	def plot3d ( points , color = ( number . number , number . number , number . number ) , tube_radius = number . number number , n_components = number , name = none ) points = np . asanyarray ( points ) mp = material_properties ( color = np . array ( color ) , k_a = number . number , k_d = number . number , k_s = number . number , alpha = number . number , smooth = true ) vec = np . array ( [ number , number ] ) * tube_radius angle = np . pi * number . number / n_components rotmat = np . array ( [ [ np . cos ( angle ) , _ np . sin ( angle ) ] , [ np . sin ( angle ) , np . cos ( angle ) ] ] ) perim = [ ] for var in range ( n_components ) perim . append ( vec ) vec = np . dot ( rotmat , vec ) poly = polygon ( perim ) mesh = trimesh . creation . sweep_polygon ( poly , points ) obj = scene_object ( mesh , material = mp ) if name is none name = str ( uuid . uuid4 ( ) ) visualizer3_d . _scene . add_object ( name , obj )	0
Q_368	generate the name with flag	C_368_0	def _name_with_flags ( self , include_restricted , title = none ) name = string if self . special else string name + = self . name if title name + = string . format ( title ) if include_restricted and self . restricted name + = string name + = string if self . both_blocks else string name + = string if self . administrative else string name + = string if self . sticky else string name + = string if self . deleted else string return name	1
Q_368	generate the name with flag	C_368_1	def resume ( self ) string if string not in vars ( self ) return type , exc = map ( pickle . loads , self . _saved ) six . reraise ( type , exc , self . _tb )	0
Q_368	generate the name with flag	C_368_2	def show_certificate ( cert , short = false ) for var in [ hashes . md5 , hashes . sha1 , hashes . sha256 , hashes . sha512 ] print ( string . format ( var . name , binascii . hexlify ( cert . fingerprint ( var ( ) ) ) . decode ( string ) ) ) print ( string . format ( get_name ( cert . issuer , short = short ) ) ) print ( string . format ( get_name ( cert . subject , short = short ) ) )	0
Q_369	determine if the site be be display in a web_view from a native application	C_369_0	def mobile_app ( request ) ctx = { } try ua = request . meta . get ( string , string ) if string in ua logger . debug ( string , request . user ) ctx [ string ] = true registered = string in ua ctx [ string ] = registered if request . user and request . user . is_authenticated import binascii import os from intranet . apps . notifications . models import notification_config from datetime import datetime ncfg , _ = notification_config . objects . get_or_create ( user = request . user ) if not ncfg . android_gcm_rand rand = binascii . b2a_hex ( os . urandom ( number ) ) ncfg . android_gcm_rand = rand else rand = ncfg . android_gcm_rand ncfg . android_gcm_time = datetime . now ( ) logger . debug ( string , rand ) ncfg . save ( ) ctx [ string ] = rand else ctx [ string ] = false ctx [ string ] = false except exception ctx [ string ] = false ctx [ string ] = false return ctx	1
Q_369	determine if the site be be display in a web_view from a native application	C_369_1	def get_sv_variants ( self , chromosome = none , end_chromosome = none , sv_type = none , pos = none , end = none ) query = { } if chromosome query [ string ] = chromosome if end_chromosome query [ string ] = end_chromosome if sv_type query [ string ] = sv_type if pos if not string in query query [ string ] = [ ] query [ string ] . append ( { string { string pos } } ) query [ string ] . append ( { string { string pos } } ) if end if not string in query query [ string ] = [ ] query [ string ] . append ( { string { string end } } ) query [ string ] . append ( { string { string end } } ) log . info ( string . format ( query ) ) return self . db . structural_variant . find ( query ) . sort ( [ ( string , ascending ) , ( string , ascending ) ] )	0
Q_369	determine if the site be be display in a web_view from a native application	C_369_2	def apply_scissor ( self , new_band_gap ) if self . is_metal ( ) max_index = _ number for var in range ( self . nb_bands ) below = false above = false for j in range ( len ( self . kpoints ) ) if self . bands [ spin . up ] [ var ] [ j ] < self . efermi below = true if self . bands [ spin . up ] [ var ] [ j ] > self . efermi above = true if above and below if var > max_index max_index = var if self . is_spin_polarized below = false above = false for j in range ( len ( self . kpoints ) ) if self . bands [ spin . down ] [ var ] [ j ] < self . efermi below = true if self . bands [ spin . down ] [ var ] [ j ] > self . efermi above = true if above and below if var > max_index max_index = var old_dict = self . as_dict ( ) shift = new_band_gap for spin in old_dict [ string ] for k in range ( len ( old_dict [ string ] [ spin ] ) ) for v in range ( len ( old_dict [ string ] [ spin ] [ k ] ) ) if k > = max_index old_dict [ string ] [ spin ] [ k ] [ v ] = old_dict [ string ] [ spin ] [ k ] [ v ] + shift else shift = new_band_gap _ self . get_band_gap ( ) [ string ] old_dict = self . as_dict ( ) for spin in old_dict [ string ] for k in range ( len ( old_dict [ string ] [ spin ] ) ) for v in range ( len ( old_dict [ string ] [ spin ] [ k ] ) ) if old_dict [ string ] [ spin ] [ k ] [ v ] > = old_dict [ string ] [ string ] old_dict [ string ] [ spin ] [ k ] [ v ] = old_dict [ string ] [ spin ] [ k ] [ v ] + shift old_dict [ string ] = old_dict [ string ] + shift return lobster_band_structure_symm_line . from_dict ( old_dict )	0
Q_370	create and update status record for a new py class user_task_mixin in a celery chord	C_370_0	def _create_chord_entry ( task_id , task_class , message_body , user_id ) args = message_body [ string ] kwargs = message_body [ string ] arguments_dict = task_class . arguments_as_dict ( * args , * * kwargs ) name = task_class . generate_name ( arguments_dict ) total_steps = task_class . calculate_total_steps ( arguments_dict ) parent_name = kwargs . get ( string , string ) chord_data = message_body [ string ] group_id = message_body [ string ] with transaction . atomic ( ) group , created = user_task_status . objects . get_or_create ( task_id = group_id , defaults = { string true , string parent_name , string string , string total_steps , string user_id } ) if created chord = user_task_status . objects . create ( is_container = true , name = parent_name , task_class = string , task_id = str ( uuid4 ( ) ) , total_steps = total_steps , user_id = user_id ) group . parent = chord group . save ( update_fields = { string , string } ) else chord = none group . increment_total_steps ( total_steps ) if parent_name and not group . name group . set_name ( parent_name ) user_task_status . objects . create ( name = name , parent = group , task_class = task_class , task_id = task_id , total_steps = total_steps , user_id = user_id ) if not created return task_id = chord_data [ string ] [ string ] body_task = chord_data [ string ] body_class = import_string ( body_task ) . _class_ if not issubclass ( body_class , user_task_mixin ) return args = chord_data [ string ] kwargs = chord_data [ string ] arguments_dict = body_class . arguments_as_dict ( * args , * * kwargs ) name = body_class . generate_name ( arguments_dict ) total_steps = body_class . calculate_total_steps ( arguments_dict ) user_task_status . objects . get_or_create ( task_id = task_id , defaults = { string name , string chord , string body_task , string total_steps , string user_id } ) chord . increment_total_steps ( total_steps )	1
Q_370	create and update status record for a new py class user_task_mixin in a celery chord	C_370_1	def _remove_os_link ( ) class no_value pass orig_val = getattr ( os , string , no_value ) try del os . link except exception pass try yield finally if orig_val is not no_value setattr ( os , string , orig_val )	0
Q_370	create and update status record for a new py class user_task_mixin in a celery chord	C_370_2	def secs_to_dhms ( time_in_secs ) days = int ( time_in_secs / secs_in_day ) remaining_secs = time_in_secs secs_in_day hours = int ( remaining_secs / secs_in_hour ) remaining_secs = remaining_secs secs_in_hour mins = int ( remaining_secs / secs_in_min ) secs = int ( remaining_secs secs_in_min ) return days , hours , mins , secs	0
Q_371	zip the receiver into an array and return it	C_371_0	def _zip ( self ) _ > array_value res = list ( self . before ) res . reverse ( ) res . append ( self . value ) res . extend ( list ( self . after ) ) return array_value ( res , self . timestamp )	1
Q_371	zip the receiver into an array and return it	C_371_1	def qualifiers ( self , qualifiers ) self . _qualifiers = nocase_dict ( ) if qualifiers try iterator = qualifiers . items ( ) except attribute_error iterator = qualifiers for var in iterator if isinstance ( var , cim_qualifier ) key = var . name value = var elif isinstance ( var , tuple ) key , value = var else raise type_error ( _format ( string string , var ) ) self . qualifiers [ key ] = _cim_qualifier ( key , value )	0
Q_371	zip the receiver into an array and return it	C_371_2	def _filter ( self , name ) name = name . replace ( string string string string , string ) name = name . replace ( string , string ) name = name . replace ( string , string ) name = name . replace ( string , string ) name = name . replace ( string , string ) return name . lower ( )	0
Q_372	add a callback function that be call when sensor value be update	C_372_0	def register_listener ( self , listener , reading = false ) listener_id = hashable_identity ( listener ) self . _listeners [ listener_id ] = ( listener , reading ) logger . debug ( string . format ( self . name ) )	1
Q_372	add a callback function that be call when sensor value be update	C_372_1	def add_property ( self , label , term_span , pid = none ) if self . features_layer is none self . features_layer = cfeatures ( type = self . type ) self . root . append ( self . features_layer . get_node ( ) ) self . features_layer . add_property ( pid , label , term_span )	0
Q_372	add a callback function that be call when sensor value be update	C_372_2	def fourier2dpad ( data , zero_pad = true ) if zero_pad ( n , m ) = data . shape order = np . int ( max ( number . , number * * np . ceil ( np . log ( number * max ( n , m ) ) / np . log ( number ) ) ) ) datapad = np . zeros ( ( order , order ) , dtype = float ) datapad [ data . shape [ number ] , data . shape [ number ] ] = data else datapad = data fft = np . fft . fftshift ( np . fft . fft2 ( datapad ) ) return fft	0
Q_373	wait for the sensor to satisfy a condition	C_373_0	def wait ( self , condition_or_value , timeout = none ) if ( isinstance ( condition_or_value , collections . sequence ) and not isinstance ( condition_or_value , basestring ) ) raise not_implemented_error ( string ) condition_test = ( condition_or_value if callable ( condition_or_value ) else lambda s s . value = = condition_or_value ) ioloop = tornado . ioloop . io_loop . current ( ) f = future ( ) if self . sampling_strategy = = ( string , ) raise katcp_sensor_error ( string ) def handle_update ( sensor , reading ) try assert sensor is self if condition_test ( reading ) self . unregister_listener ( handle_update ) if not f . done ( ) ioloop . add_callback ( f . set_result , true ) except exception f . set_exc_info ( sys . exc_info ( ) ) self . unregister_listener ( handle_update ) self . register_listener ( handle_update , reading = true ) ioloop . add_callback ( handle_update , self , self . _reading ) if timeout to = ioloop . time ( ) + timeout timeout_f = with_timeout ( to , f ) timeout_f . add_done_callback ( lambda f self . unregister_listener ( handle_update ) ) return timeout_f else return f	1
Q_373	wait for the sensor to satisfy a condition	C_373_1	def get_pages ( self , url , params = none , * * kwargs ) check_type ( url , basestring , may_be_none = false ) check_type ( params , dict ) erc = kwargs . pop ( string , expected_response_code [ string ] ) response = self . request ( string , url , erc , params = params , * * kwargs ) while true yield extract_and_parse_json ( response ) if response . links . get ( string ) next_url = response . links . get ( string ) . get ( string ) next_url = _fix_next_url ( next_url ) response = self . request ( string , next_url , erc , * * kwargs ) else break	0
Q_373	wait for the sensor to satisfy a condition	C_373_2	def _get_value_counts ( mapping ) return counter ( { var len ( var ) for var , var in viewitems ( mapping ) } )	0
Q_374	a authenticate use be try to access an application	C_374_0	def application_detail ( request , application_id , state = none , label = none ) application = base . get_application ( pk = application_id ) state_machine = base . get_state_machine ( application ) return state_machine . process ( request , application , state , label )	1
Q_374	a authenticate use be try to access an application	C_374_1	def assoc_ipv4 ( self , id_ip , id_equip , id_net ) if not is_valid_int_param ( id_ip ) raise invalid_parameter_error ( u string ) if not is_valid_int_param ( id_net ) raise invalid_parameter_error ( u string ) if not is_valid_int_param ( id_equip ) raise invalid_parameter_error ( u string ) ip_map = dict ( ) ip_map [ string ] = id_ip ip_map [ string ] = id_net ip_map [ string ] = id_equip url = string code , xml = self . submit ( { string ip_map } , string , url ) return self . response ( code , xml )	0
Q_374	a authenticate use be try to access an application	C_374_2	def is_ready ( self ) ready = len ( self . get ( self . name , [ ] ) ) > number if not ready hookenv . log ( string . format ( self . _class_ . _name_ ) , hookenv . debug ) return ready	0
Q_375	solve linear system ax b use numeric factorization n and symbolic factorization f	C_375_0	def solve ( self , a , f , n , b ) if self . sparselib = = string umfpack . solve ( a , n , b ) elif self . sparselib = = string klu . solve ( a , f , n , b )	1
Q_375	solve linear system ax b use numeric factorization n and symbolic factorization f	C_375_1	def _cli_helper_dedicated_host ( env , result , table ) dedicated_host_id = utils . lookup ( result , string , string ) if dedicated_host_id table . add_row ( [ string , dedicated_host_id ] ) try dedicated_host = env . client . call ( string , string , id = dedicated_host_id ) except soft_layer . soft_layer_api_error logger . error ( string , dedicated_host_id ) dedicated_host = { } table . add_row ( [ string , dedicated_host . get ( string ) or formatting . blank ( ) ] )	0
Q_375	solve linear system ax b use numeric factorization n and symbolic factorization f	C_375_2	def get_instance_health ( name , region = none , key = none , keyid = none , profile = none , instances = none ) conn = _get_conn ( region = region , key = key , keyid = keyid , profile = profile ) try instance_states = conn . describe_instance_health ( name , instances ) ret = [ ] for var in instance_states ret . append ( { string var . instance_id , string var . description , string var . state , string var . reason_code } ) return ret except boto . exception . boto_server_error as error log . debug ( error ) return [ ]	0
Q_376	apply distortion model to wcs include modify	C_376_0	def apply_model ( refwcs ) if string in refwcs . _dict_ and refwcs . ocx10 is not none linmat = np . array ( [ [ refwcs . ocx11 , refwcs . ocx10 ] , [ refwcs . ocy11 , refwcs . ocy10 ] ] ) / refwcs . idcscale refwcs . wcs . cd = np . dot ( refwcs . wcs . cd , linmat ) refwcs . wcs . set ( ) refwcs . set_orient ( ) refwcs . set_pscale ( )	1
Q_376	apply distortion model to wcs include modify	C_376_1	def ph_basename ( self , ph_type ) return { pp_placeholder . bitmap string , pp_placeholder . body string , pp_placeholder . center_title string , pp_placeholder . chart string , pp_placeholder . date string , pp_placeholder . footer string , pp_placeholder . header string , pp_placeholder . media_clip string , pp_placeholder . object string , pp_placeholder . org_chart string , pp_placeholder . picture string , pp_placeholder . slide_number string , pp_placeholder . subtitle string , pp_placeholder . table string , pp_placeholder . title string , } [ ph_type ]	0
Q_376	apply distortion model to wcs include modify	C_376_2	def convert_row ( self , keyed_row , schema , fallbacks ) for var , var in list ( keyed_row . items ( ) ) field = schema . get_field ( var ) if not field del keyed_row [ var ] if var in fallbacks var = _uncast_value ( var , field = field ) else var = field . cast_value ( var ) keyed_row [ var ] = var return keyed_row	0
Q_377	validate that segment offset offset length of	C_377_0	def validate_empty_bytes ( offset int , length int , bytecode bytes ) _ > none slot_length = offset + length slot = bytecode [ offset slot_length ] if slot = bytearray ( length ) raise validation_error ( f string f string )	1
Q_377	validate that segment offset offset length of	C_377_1	def exit ( msg , code = number ) print sys . stderr , msg sys . exit ( code )	0
Q_377	validate that segment offset offset length of	C_377_2	def _xfsdump_output ( data ) out = { } summary = [ ] summary_block = false for var in [ l . strip ( ) for l in data . split ( string ) if l . strip ( ) ] var = re . sub ( string , string , var ) if var . startswith ( string ) out [ string ] = var . split ( string ) [ _ number ] elif var . startswith ( string ) out [ string ] = re . sub ( string , string , var ) elif var . startswith ( string ) out [ string ] = re . sub ( r string , string , var ) elif var . startswith ( string ) out [ string ] = re . sub ( r string , string , var ) elif var . startswith ( string ) out [ string ] = re . sub ( r string , string , var ) elif var . startswith ( string ) summary_block = true continue if var . startswith ( string ) and summary_block summary . append ( var . strip ( ) ) elif not var . startswith ( string ) and summary_block summary_block = false if summary out [ string ] = string . join ( summary ) return out	0
Q_378	return an initial dict with the minimal requried field for a valid manifest	C_378_0	def init_manifest ( package_name str , version str , manifest_version optional [ str ] = string ) _ > dict [ str , any ] return { string package_name , string version , string manifest_version , }	1
Q_378	return an initial dict with the minimal requried field for a valid manifest	C_378_1	def numpyview ( arr , datatype , shape , raw = false ) if raw return n . frombuffer ( arr , dtype = n . dtype ( datatype ) ) . view ( n . dtype ( datatype ) ) . reshape ( shape ) else return n . frombuffer ( arr . get_obj ( ) , dtype = n . dtype ( datatype ) ) . view ( n . dtype ( datatype ) ) . reshape ( shape )	0
Q_378	return an initial dict with the minimal requried field for a valid manifest	C_378_2	async def traverse ( self , func ) async_executor = self if inspect . isasyncgenfunction ( func ) async for var in func ( * async_executor . args ) yield var else yield await func ( * async_executor . args )	0
Q_379	create a hpx object	C_379_0	def create_hpx ( cls , nside , nest , coordsys = string , order = _ number , ebins = none , region = none , conv = hpx_conv ( string ) , pixels = none ) return cls ( nside , nest , coordsys , order , ebins , region = region , conv = conv , pixels = pixels )	1
Q_379	create a hpx object	C_379_1	def _get_migrate_command ( ) tunnel = _salt_ [ string ] ( string ) if tunnel salt . utils . versions . warn_until ( string , string virt . tunnel string string virt tunnel string virt . tunnel string string ) else tunnel = _salt_ [ string ] ( string ) if tunnel return ( string string ) return string	0
Q_379	create a hpx object	C_379_2	def boottime ( ) global _boottime if _boottime is none up = uptime ( ) if up is none return none if _boottime is none _boottime_linux ( ) if datetime is none raise runtime_error ( string ) return datetime . fromtimestamp ( _boottime or time . time ( ) _ up )	0
Q_380	perform an energy_dependent convolution on a sequence of tagint _d spatial map	C_380_0	def convolve_map_hpx_gauss ( m , sigmas , imin = number , imax = none , wmap = none ) islice = slice ( imin , imax ) o = np . zeros ( m . data . shape ) nside = m . geom . nside nest = m . geom . nest for var , var in enumerate ( m . data [ islice , . ] ) sigma = sigmas [ islice ] [ var ] if nest var = hp . pixelfunc . reorder ( var , n2r = true ) o [ islice , . ] [ var ] = hp . sphtfunc . smoothing ( var , sigma = sigma ) if nest o [ islice , . ] [ var ] = hp . pixelfunc . reorder ( o [ islice , . ] [ var ] , r2n = true ) if wmap is not none o [ islice , . ] [ var ] * = wmap . data [ islice , . ] [ var ] return hpx_nd_map ( m . geom , o )	1
Q_380	perform an energy_dependent convolution on a sequence of tagint _d spatial map	C_380_1	def find_next_video_in_playlist ( self , playlist_id , cur_video_id ) url = string params = { string self . client_id , string playlist_id , string cur_video_id } r = requests . get ( url , params = params ) check_error ( r ) return r . json ( )	0
Q_380	perform an energy_dependent convolution on a sequence of tagint _d spatial map	C_380_2	def _bfs_subgraph ( self , start_id , forward = true ) if forward get_bfs = self . forw_bfs get_nbrs = self . out_nbrs else get_bfs = self . back_bfs get_nbrs = self . inc_nbrs g = graph ( ) bfs_list = get_bfs ( start_id ) for var in bfs_list g . add_node ( var ) for var in bfs_list for nbr_id in get_nbrs ( var ) g . add_edge ( var , nbr_id ) return g	0
Q_381	compute average wind direction for a slice of data	C_381_0	def winddir_average ( data , threshold , min_count , decay = number . number ) wind_filter = pywws . process . wind_filter ( ) count = number for var in data wind_filter . add ( var ) if var [ string ] is not none count + = number if count < min_count return none speed , direction = wind_filter . result ( ) if speed is none or speed < threshold return none return direction * number . number	1
Q_381	compute average wind direction for a slice of data	C_381_1	def get_upcoming_events_within_the_current_week ( self ) now = datetime . now ( tz = self . timezone ) start_time = datetime ( year = now . year , month = now . month , day = now . day , hour = now . hour , minute = now . minute , second = now . second , tzinfo = self . timezone ) end_time = start_time + timedelta ( days = number _ now . weekday ( ) ) end_time = datetime ( year = end_time . year , month = end_time . month , day = end_time . day , hour = number , minute = number , second = number , tzinfo = self . timezone ) assert ( end_time . weekday ( ) = = number ) start_time = start_time . isoformat ( ) end_time = end_time . isoformat ( ) return self . get_events ( start_time , end_time )	0
Q_381	compute average wind direction for a slice of data	C_381_2	def _kibiter_version ( self ) version = none es_url = self . conf [ string ] [ string ] config_url = string url = urijoin ( es_url , config_url ) version = none try res = self . grimoire_con . get ( url ) res . raise_for_status ( ) version = res . json ( ) [ string ] [ string ] [ number ] [ string ] logger . debug ( string , version ) except requests . exceptions . http_error logger . warning ( string ) return version	0
Q_382	a convenience method for obtain the header that be send from the	C_382_0	def get_response_headers ( self , * args , * * kwargs ) if self . response_headers return self . _unpack_headers ( self . response_headers )	1
Q_382	a convenience method for obtain the header that be send from the	C_382_1	def build ( self ) markdown_html = markdown . markdown ( self . markdown_text , extensions = [ toc_extension ( ) , string , string , string ] ) markdown_soup = beautiful_soup ( markdown_html , string ) if markdown_soup . find ( string , attrs = { string string } ) self . _add_mermaid_js ( ) for var in markdown_soup . find_all ( string , attrs = { string string } ) grap_svg = self . _text_to_graphiz ( var . string ) graph_soup = beautiful_soup ( grap_svg , string ) var . parent . replace_with ( graph_soup ) self . main_soup . body . append ( markdown_soup ) return self . main_soup . prettify ( )	0
Q_382	a convenience method for obtain the header that be send from the	C_382_2	def duration_to_text ( self , duration ) if isinstance ( duration , tuple ) start = ( duration [ number ] . strftime ( self . entry_duration_format ) if duration [ number ] is not none else string ) end = ( duration [ number ] . strftime ( self . entry_duration_format ) if duration [ number ] is not none else string ) duration = string ( start , end ) else duration = six . text_type ( duration ) return duration	0
Q_383	setter for longitude	C_383_0	def longitude ( self , longitude ) if not ( _ number < = longitude < = number ) raise value_error ( string . format ( longitude ) ) self . _longitude = longitude	1
Q_383	setter for longitude	C_383_1	def forum_topic_update ( self , topic_id , title = none , category = none ) params = { string title , string category } return self . _get ( string . format ( topic_id ) , params , method = string , auth = true )	0
Q_383	setter for longitude	C_383_2	def contains_cursor ( self , cursor ) start = self . cursor . selection_start ( ) end = self . cursor . selection_end ( ) if cursor . at_block_end ( ) end _ = number return start < = cursor . position ( ) < = end	0
Q_384	verify the validity of the give jwt use the give public key	C_384_0	def verify ( token , public_key , validate_nonce = none , algorithms = [ default_algorithm ] ) try token_data = jwt . decode ( token , public_key , algorithms = algorithms ) except jwt . invalid_token_error logger . debug ( string ) return false claimed_username = token_data . get ( string ) claimed_time = token_data . get ( string , number ) claimed_nonce = token_data . get ( string ) current_time = time . time ( ) min_time , max_time = ( current_time _ timestamp_tolerance , current_time + timestamp_tolerance ) if claimed_time < min_time or claimed_time > max_time logger . debug ( string ) return false if validate_nonce if not validate_nonce ( claimed_username , claimed_time , claimed_nonce ) logger . debug ( string ) return false else logger . warning ( string ) return token_data	1
Q_384	verify the validity of the give jwt use the give public key	C_384_1	def add_source ( self , item_source ) _indexer_input_source ( store = self . store , indexer = self , source = item_source ) item_source . add_reliable_listener ( self , style = iaxiom . remote )	0
Q_384	verify the validity of the give jwt use the give public key	C_384_2	def image_predict_proba ( self , x ) string string string self . _check_image ( x ) probabilities = self . pixel_classifier . image_predict_proba ( x ) patches , _ = self . _to_patches ( probabilities ) row_steps = self . _image_size [ number ] / / self . patch_size [ number ] col_steps = self . _image_size [ number ] / / self . patch_size [ number ] ps = self . patch_size [ number ] * self . patch_size [ number ] for i , j , k in itertools . product ( range ( row_steps ) , range ( col_steps ) , range ( self . _samples ) ) patches [ k , i , j , number ] = np . sum ( patches [ k , i , j , number ] ) / ps patches [ k , i , j , number ] = np . sum ( patches [ k , i , j , number ] ) / ps return probabilities	0
Q_385	update the jinja2 html context expose the version class instance to it	C_385_0	def html_page_context ( cls , app , pagename , templatename , context , doctree ) assert templatename or doctree cls . versions . context = context versions = cls . versions this_remote = versions [ cls . current_version ] banner_main_remote = versions [ cls . banner_main_version ] if cls . show_banner else none context [ string ] = cls . current_version context [ string ] = cls . current_version context [ string ] = cls . current_version context [ string ] = app . config . html_theme context [ string ] = cls . banner_greatest_tag context [ string ] = banner_main_remote [ string ] = = string if cls . show_banner else none context [ string ] = banner_main_remote [ string ] = = string if cls . show_banner else none context [ string ] = banner_main_remote [ string ] if cls . show_banner else none context [ string ] = cls . banner_recent_tag context [ string ] = this_remote [ string ] = = string context [ string ] = this_remote = = versions . greatest_tag_remote context [ string ] = this_remote = = versions . recent_branch_remote context [ string ] = this_remote = = versions . recent_remote context [ string ] = this_remote = = versions . recent_tag_remote context [ string ] = cls . is_root context [ string ] = this_remote [ string ] = = string context [ string ] = cls . show_banner context [ string ] = versions context [ string ] = versions . vhasdoc context [ string ] = versions . vpathto if cls . show_banner and string in context parsed = app . builder . templates . render ( string , context ) context [ string ] = parsed + context [ string ] css_files = context . setdefault ( string , list ( ) ) if string not in css_files css_files . append ( string ) if static_dir not in app . config . html_static_path app . config . html_static_path . append ( static_dir ) if app . config . html_last_updated_fmt is not none file_path = app . env . doc2path ( pagename ) if os . path . isfile ( file_path ) lufmt = app . config . html_last_updated_fmt or getattr ( locale , string ) ( string ) mtime = datetime . datetime . fromtimestamp ( os . path . getmtime ( file_path ) ) context [ string ] = format_date ( lufmt , mtime , language = app . config . language , warn = app . warn )	1
Q_385	update the jinja2 html context expose the version class instance to it	C_385_1	def get_diff_endpoints_from_commit_range ( repo , commit_range ) if not commit_range raise value_error ( string ) result = re_find ( commit_range_regex , commit_range ) if not result raise value_error ( string a . b string a . b string . format ( commit_range ) ) a , b = result [ string ] , result [ string ] a , b = repo . rev_parse ( a ) , repo . rev_parse ( b ) if result [ string ] a = one_or_raise ( repo . merge_base ( a , b ) ) return a , b	0
Q_385	update the jinja2 html context expose the version class instance to it	C_385_2	def subdivide ( self , nsub , subfilter = string , inplace = false ) subfilter = subfilter . lower ( ) if subfilter = = string sfilter = vtk . vtk_linear_subdivision_filter ( ) elif subfilter = = string sfilter = vtk . vtk_butterfly_subdivision_filter ( ) elif subfilter = = string sfilter = vtk . vtk_loop_subdivision_filter ( ) else raise exception ( string + string butterfly string loop string linear string ) sfilter . set_number_of_subdivisions ( nsub ) sfilter . set_input_data ( self ) sfilter . update ( ) submesh = _get_output ( sfilter ) if inplace self . overwrite ( submesh ) else return submesh	0
Q_386	use reserve switch bind get all switch ip	C_386_0	def get_all_switch_ips ( self ) switch_connections = [ ] try bindings = nxos_db . get_reserved_switch_binding ( ) except excep . nexus_port_binding_not_found log . error ( string ) bindings = [ ] for var in bindings switch_connections . append ( var . switch_ip ) return switch_connections	1
Q_386	use reserve switch bind get all switch ip	C_386_1	def get_latex_table ( self , parameters = none , transpose = false , caption = none , label = string , hlines = true , blank_fill = string ) if parameters is none parameters = self . parent . _all_parameters for var in parameters assert isinstance ( var , str ) , string num_parameters = len ( parameters ) num_chains = len ( self . parent . chains ) fit_values = self . get_summary ( squeeze = false ) if label is none label = string if caption is none caption = string end_text = string if transpose column_text = string * ( num_chains + number ) else column_text = string * ( num_parameters + number ) center_text = string hline_text = string if hlines center_text + = hline_text + string if transpose center_text + = string . join ( [ string ] + [ c . name for c in self . parent . chains ] ) + end_text if hlines center_text + = string + hline_text for var in parameters arr = [ string + var ] for chain_res in fit_values if var in chain_res arr . append ( self . get_parameter_text ( * chain_res [ var ] , wrap = true ) ) else arr . append ( blank_fill ) center_text + = string . join ( arr ) + end_text else center_text + = string . join ( [ string ] + parameters ) + end_text if hlines center_text + = string + hline_text for name , chain_res in zip ( [ c . name for c in self . parent . chains ] , fit_values ) arr = [ string + name ] for var in parameters if var in chain_res arr . append ( self . get_parameter_text ( * chain_res [ var ] , wrap = true ) ) else arr . append ( blank_fill ) center_text + = string . join ( arr ) + end_text if hlines center_text + = string + hline_text final_text = get_latex_table_frame ( caption , label ) ( column_text , center_text ) return final_text	0
Q_386	use reserve switch bind get all switch ip	C_386_2	def getpackage ( filename ) src_file = src ( filename ) if not src_file . endswith ( string ) and not ispackage ( src_file ) return none base , ext = os . path . splitext ( os . path . basename ( src_file ) ) if base = = string mod_parts = [ ] else mod_parts = [ base ] path , part = os . path . split ( os . path . split ( src_file ) [ number ] ) while part if ispackage ( os . path . join ( path , part ) ) mod_parts . append ( part ) else break path , part = os . path . split ( path ) mod_parts . reverse ( ) return string . join ( mod_parts )	0
Q_387	configure the nat rule for an internal network	C_387_0	def _nat_rules_for_internet_access ( self , acl_no , network , netmask , inner_itfc , outer_itfc , vrf_name ) acl_present = self . _check_acl ( acl_no , network , netmask ) if not acl_present conf_str = snippets . create_acl ( acl_no , network , netmask ) self . _edit_running_config ( conf_str , string ) pool_name = string vrf_name conf_str = asr1k_snippets . set_dyn_src_trl_pool ( acl_no , pool_name , vrf_name ) try self . _edit_running_config ( conf_str , string ) except exception as dyn_nat_e log . info ( string string string , dyn_nat_e ) conf_str = snippets . set_nat ( inner_itfc , string ) self . _edit_running_config ( conf_str , string ) conf_str = snippets . set_nat ( outer_itfc , string ) self . _edit_running_config ( conf_str , string )	1
Q_387	configure the nat rule for an internal network	C_387_1	def save_pytables ( fpath , data , verbose = false ) import tables verbose = _rectify_verb_write ( verbose ) if verbose print ( string ( util_path . tail ( fpath ) , ) ) with tables . open_file ( fpath , string ) as file_ atom = tables . atom . from_dtype ( data . dtype ) filters = tables . filters ( complib = string , complevel = number ) dset = file_ . create_c_array ( file_ . root , string , atom , data . shape , filters = filters ) dset [ ] = data	0
Q_387	configure the nat rule for an internal network	C_387_2	def popitem ( self ) def popitem_trans ( pipe ) try pickled_key = pipe . hkeys ( self . key ) [ number ] except index_error raise key_error pipe . multi ( ) pipe . hget ( self . key , pickled_key ) pipe . hdel ( self . key , pickled_key ) pickled_value , _ = pipe . execute ( ) return ( self . _unpickle_key ( pickled_key ) , self . _unpickle ( pickled_value ) ) key , value = self . _transaction ( popitem_trans ) return key , self . cache . pop ( key , value )	0
Q_388	immediately place a withdrawal for a give currency with no email	C_388_0	def withdraw ( self , currency , amount , address , payment_id = none ) return self . _private ( string , currency = currency , amount = amount , address = address , payment_id = payment_id )	1
Q_388	immediately place a withdrawal for a give currency with no email	C_388_1	def _clean_create_kwargs ( * * kwargs ) valid_opts = { string six . string_types , string six . string_types , string six . string_types , string bool , string list , string six . string_types , string six . string_types , string six . string_types , string bool , string list , string dict , string dict , string six . string_types , string list , string six . string_types , string six . string_types , string dict , string dict , string list , string dict , string bool , string six . string_types , string six . string_types , string bool , string int , string bool , string dict , string bool , string int , string six . string_types , string six . string_types , string six . string_types , } extra = kwargs . pop ( string , { } ) for var , var in six . iteritems ( kwargs . copy ( ) ) if var in valid_opts if isinstance ( var , valid_opts [ var ] ) continue log . error ( string , var , var , valid_opts [ var ] ) kwargs . pop ( var ) return _utils_ [ string ] ( kwargs , extra )	0
Q_388	immediately place a withdrawal for a give currency with no email	C_388_2	def get_include ( ) import os lxml_path = _path_ [ number ] include_path = os . path . join ( lxml_path , string ) includes = [ include_path , lxml_path ] for var in os . listdir ( include_path ) path = os . path . join ( include_path , var ) if os . path . isdir ( path ) includes . append ( path ) return includes	0
Q_389	get all version of a key	C_389_0	def get_versions ( context , return_stable = false ) s3_key = string . format ( context . service_name , context . env , context . key ) object_version_list = context . aws_client ( string ) . list_object_versions ( bucket = ef_config . s3_version_bucket , delimiter = string , max_keys = context . limit , prefix = s3_key ) if string not in object_version_list return [ ] object_versions = [ ] for var in object_version_list [ string ] object_version = version ( context . aws_client ( string ) . get_object ( bucket = ef_config . s3_version_bucket , key = s3_key , version_id = var [ string ] ) ) if return_stable and object_version . status = = ef_config . s3_version_status_stable return [ object_version ] object_versions . append ( object_version ) if return_stable return [ ] else return sorted ( object_versions , key = lambda v v . last_modified , reverse = true )	1
Q_389	get all version of a key	C_389_1	def tmp_configuration_copy ( chmod = number ) cfg_dict = conf . as_dict ( display_sensitive = true , raw = true ) temp_fd , cfg_path = mkstemp ( ) with os . fdopen ( temp_fd , string ) as temp_file if chmod is not none os . fchmod ( temp_fd , chmod ) json . dump ( cfg_dict , temp_file ) return cfg_path	0
Q_389	get all version of a key	C_389_2	def generate_output ( self , writer = none ) logger . info ( string , self . permalink_output_path ) clean_output_dir ( self . permalink_output_path , [ ] ) mkdir_p ( self . permalink_output_path ) for var in itertools . chain ( self . context [ string ] , self . context [ string ] ) for permalink_id in var . get_permalink_ids_iter ( ) permalink_path = os . path . join ( self . permalink_output_path , permalink_id ) + string redirect_string = redirect_string . format ( url = article_url ( var ) , title = var . title ) open ( permalink_path , string ) . write ( redirect_string )	0
Q_390	param dict snmp snmp dict should have key snmp_agent str define name of snmp_agent	C_390_0	def create_bulk ( cls , name , interfaces = none , nodes = number , cluster_mode = string , primary_mgt = none , backup_mgt = none , primary_heartbeat = none , log_server_ref = none , domain_server_address = none , location_ref = none , default_nat = false , enable_antivirus = false , enable_gti = false , comment = none , snmp = none , * * kw ) primary_heartbeat = primary_mgt if not primary_heartbeat else primary_heartbeat physical_interfaces = [ ] for var in interfaces if string not in var raise create_engine_failed ( string string name ) if var . get ( string , none ) = = string tunnel_interface = tunnel_interface ( * * var ) physical_interfaces . append ( { string tunnel_interface } ) else cluster_interface = cluster_physical_interface ( primary_mgt = primary_mgt , backup_mgt = backup_mgt , primary_heartbeat = primary_heartbeat , * * var ) physical_interfaces . append ( { string cluster_interface } ) if snmp snmp_agent = dict ( snmp_agent_ref = snmp . get ( string , string ) , snmp_location = snmp . get ( string , string ) ) snmp_agent . update ( snmp_interface = add_snmp ( interfaces , snmp . get ( string , [ ] ) ) ) try engine = super ( firewall_cluster , cls ) . _create ( name = name , node_type = string , physical_interfaces = physical_interfaces , domain_server_address = domain_server_address , log_server_ref = log_server_ref , location_ref = location_ref , nodes = nodes , enable_gti = enable_gti , enable_antivirus = enable_antivirus , default_nat = default_nat , snmp_agent = snmp_agent if snmp else none , comment = comment ) engine . update ( cluster_mode = cluster_mode ) return element_creator ( cls , json = engine ) except ( element_not_found , create_element_failed ) as e raise create_engine_failed ( e )	1
Q_390	param dict snmp snmp dict should have key snmp_agent str define name of snmp_agent	C_390_1	def isrchc ( value , ndim , lenvals , array ) value = stypes . string_to_char_p ( value ) array = stypes . list_to_char_array_ptr ( array , x_len = lenvals , y_len = ndim ) ndim = ctypes . c_int ( ndim ) lenvals = ctypes . c_int ( lenvals ) return libspice . isrchc_c ( value , ndim , lenvals , array )	0
Q_390	param dict snmp snmp dict should have key snmp_agent str define name of snmp_agent	C_390_2	def compile_masks ( masks ) if not masks masks = [ ] elif not isinstance ( masks , ( list , tuple ) ) masks = [ masks ] return [ re . compile ( var ) for var in masks ]	0
Q_391	change the interface id for this interface	C_391_0	def change_interface_id ( self , interface_id ) splitted = str ( interface_id ) . split ( string ) if1 = splitted [ number ] for var in self . all_interfaces if isinstance ( var , vlan_interface ) var . interface_id = string . format ( if1 , var . interface_id . split ( string ) [ _ number ] ) if var . has_interfaces for sub_interface in var . interfaces if isinstance ( sub_interface , inline_interface ) sub_interface . change_interface_id ( interface_id ) else sub_interface . change_interface_id ( if1 ) else if isinstance ( var , inline_interface ) var . change_interface_id ( interface_id ) else var . update ( nicid = if1 ) self . interface_id = if1 self . update ( )	1
Q_391	change the interface id for this interface	C_391_1	def sample_dist ( samples , lo = none , up = none ) samples = numpy . asarray ( samples ) if lo is none lo = samples . min ( ) if up is none up = samples . max ( ) try dist = sample_dist ( samples , lo , up ) except numpy . linalg . lin_alg_error dist = uniform ( lower = _ numpy . inf , upper = numpy . inf ) return dist	0
Q_391	change the interface id for this interface	C_391_2	def reset ( self ) hasher_names = hashers_manager . hashers_manager . get_hasher_names_from_string ( self . _hasher_names_string ) self . _hashers = hashers_manager . hashers_manager . get_hashers ( hasher_names )	0
Q_392	solve the conductance of heat base on of the element layer	C_392_0	def conduction ( self , dt , flx1 , bc , temp2 , flx2 ) t = self . layer_temp hc = self . layer_vol_heat tc = self . layer_thermal_cond d = self . layer_thickness fimp = number . number fexp = number . number num = len ( t ) tcp = [ number for var in range ( num ) ] hcp = [ number for var in range ( num ) ] za = [ [ number for y in range ( number ) ] for var in range ( num ) ] zy = [ number for var in range ( num ) ] hcp [ number ] = hc [ number ] * d [ number ] for j in range ( number , num ) tcp [ j ] = number . / ( d [ j _ number ] / tc [ j _ number ] + d [ j ] / tc [ j ] ) hcp [ j ] = hc [ j ] * d [ j ] za [ number ] [ number ] = number . za [ number ] [ number ] = hcp [ number ] / dt + fimp * tcp [ number ] za [ number ] [ number ] = _ fimp * tcp [ number ] zy [ number ] = hcp [ number ] / dt * t [ number ] _ fexp * tcp [ number ] * ( t [ number ] _ t [ number ] ) + flx1 for j in range ( number , num _ number ) za [ j ] [ number ] = fimp * ( _ tcp [ j ] ) za [ j ] [ number ] = hcp [ j ] / dt + fimp * ( tcp [ j ] + tcp [ j + number ] ) za [ j ] [ number ] = fimp * ( _ tcp [ j + number ] ) zy [ j ] = hcp [ j ] / dt * t [ j ] + fexp * ( tcp [ j ] * t [ j _ number ] _ tcp [ j ] * t [ j ] _ tcp [ j + number ] * t [ j ] + tcp [ j + number ] * t [ j + number ] ) if self . is_near_zero ( bc _ number . ) za [ num _ number ] [ number ] = fimp * ( _ tcp [ num _ number ] ) za [ num _ number ] [ number ] = hcp [ num _ number ] / dt + fimp * tcp [ num _ number ] za [ num _ number ] [ number ] = number . zy [ num _ number ] = hcp [ num _ number ] / dt * t [ num _ number ] + fexp * tcp [ num _ number ] * ( t [ num _ number ] _ t [ num _ number ] ) + flx2 elif self . is_near_zero ( bc _ number . ) za [ num _ number ] [ number ] = number . za [ num _ number ] [ number ] = number . za [ num _ number ] [ number ] = number . zy [ num _ number ] = temp2 else raise exception ( self . conduction_input_msg ) zx = self . invert ( num , za , zy ) return zx	1
Q_392	solve the conductance of heat base on of the element layer	C_392_1	def update_terms ( self , data list , limit int = number , _print bool = true , crawl bool = false , ) _ > list url_base = self . base_url + string merged_data = [ ] old_data = self . identifier_searches ( [ var [ string ] for var in data ] , limit = limit , _print = _print , crawl = crawl , ) for var in data url = url_base . format ( id = str ( var [ string ] ) ) if var [ string ] = old_data [ int ( var [ string ] ) ] [ string ] print ( var [ string ] , old_data [ int ( var [ string ] ) ] [ string ] ) exit ( string ) merged = scicrunch_client_helper . merge ( new = var , old = old_data [ int ( var [ string ] ) ] ) merged = scicrunch_client_helper . superclasses_bug_fix ( merged ) merged_data . append ( ( url , merged ) ) resp = self . post ( merged_data , limit = limit , action = string , _print = _print , crawl = crawl , ) return resp	0
Q_392	solve the conductance of heat base on of the element layer	C_392_2	def create_branch ( self , project_key , repository , name , start_point , message = string ) url = string . format ( project_key = project_key , repository = repository ) data = { string name , string start_point , string message } return self . post ( url , data = data )	0
Q_393	set up teh parser with the require argument	C_393_0	def setup_argument_parser ( ) default_config_path = filesystem . get_default_config_path ( ) filesystem . create_path ( default_config_path ) parser = core_singletons . argument_parser parser . add_argument ( string , string , action = string , help = _ ( string ) ) parser . add_argument ( string , string , action = string , nargs = string , type = parse_state_machine_path , dest = string , metavar = string , help = _ ( string string ) ) parser . add_argument ( string , string , action = string , type = config_path , metavar = string , dest = string , default = default_config_path , nargs = string , const = default_config_path , help = _ ( string none string string string ) . format ( default_config_path ) ) parser . add_argument ( string , string , action = string , type = config_path , metavar = string , dest = string , default = default_config_path , nargs = string , const = default_config_path , help = _ ( string string none string string ) . format ( default_config_path ) ) parser . add_argument ( string , string , dest = string , action = string , help = _ ( string ) ) parser . add_argument ( string , string , metavar = string , dest = string , default = none , nargs = string , help = _ ( string string string ) ) parser . add_argument ( string , string , dest = string , action = string , help = _ ( string ) ) return parser	1
Q_393	set up teh parser with the require argument	C_393_1	def wrap ( self , wrapper ) def wrapped ( * args , * * kwargs ) if kwargs kwargs [ string ] = self . obj else args = list ( args ) args . insert ( number , self . obj ) return wrapper ( * args , * * kwargs ) return self . _wrap ( wrapped )	0
Q_393	set up teh parser with the require argument	C_393_2	def floats ( self , n int = number ) _ > list [ float ] nums = [ self . random . random ( ) for var in range ( number * * int ( n ) ) ] return nums	0
Q_394	plot the dispatch and state of charge of extendable storage	C_394_0	def storage_p_soc ( network , mean = string , filename = none ) sbatt = network . storage_units . index [ ( network . storage_units . p_nom_opt > number ) ( network . storage_units . capital_cost > number ) ( network . storage_units . max_hours = = number ) ] shydr = network . storage_units . index [ ( network . storage_units . p_nom_opt > number ) ( network . storage_units . capital_cost > number ) ( network . storage_units . max_hours = = number ) ] cap_batt = ( network . storage_units . max_hours [ sbatt ] * network . storage_units . p_nom_opt [ sbatt ] ) . sum ( ) cap_hydr = ( network . storage_units . max_hours [ shydr ] * network . storage_units . p_nom_opt [ shydr ] ) . sum ( ) fig , ax = plt . subplots ( number , number ) if network . storage_units . p_nom_opt [ sbatt ] . sum ( ) < number and network . storage_units . p_nom_opt [ shydr ] . sum ( ) < number print ( string ) elif network . storage_units . p_nom_opt [ sbatt ] . sum ( ) > number and network . storage_units . p_nom_opt [ shydr ] . sum ( ) < number ( network . storage_units_t . p [ sbatt ] . resample ( mean ) . mean ( ) . sum ( axis = number ) / network . storage_units . p_nom_opt [ sbatt ] . sum ( ) ) . plot ( ax = ax , label = string , color = string ) ax2 = ax . twinx ( ) ( ( network . storage_units_t . state_of_charge [ sbatt ] . resample ( mean ) . mean ( ) . sum ( axis = number ) / cap_batt ) * number ) . plot ( ax = ax2 , label = string , color = string ) elif network . storage_units . p_nom_opt [ sbatt ] . sum ( ) < number and network . storage_units . p_nom_opt [ shydr ] . sum ( ) > number ( network . storage_units_t . p [ shydr ] . resample ( mean ) . mean ( ) . sum ( axis = number ) / network . storage_units . p_nom_opt [ shydr ] . sum ( ) ) . plot ( ax = ax , label = string , color = string ) ax2 = ax . twinx ( ) ( ( network . storage_units_t . state_of_charge [ shydr ] . resample ( mean ) . mean ( ) . sum ( axis = number ) / cap_hydr ) * number ) . plot ( ax = ax2 , label = string , color = string ) else ( network . storage_units_t . p [ sbatt ] . resample ( mean ) . mean ( ) . sum ( axis = number ) / network . storage_units . p_nom_opt [ sbatt ] . sum ( ) ) . plot ( ax = ax , label = string , color = string ) ( network . storage_units_t . p [ shydr ] . resample ( mean ) . mean ( ) . sum ( axis = number ) / network . storage_units . p_nom_opt [ shydr ] . sum ( ) ) . plot ( ax = ax , label = string , color = string ) ax2 = ax . twinx ( ) ( ( network . storage_units_t . state_of_charge [ shydr ] . resample ( mean ) . mean ( ) . sum ( axis = number ) / cap_hydr ) * number ) . plot ( ax = ax2 , label = string , color = string ) ( ( network . storage_units_t . state_of_charge [ sbatt ] . resample ( mean ) . mean ( ) . sum ( axis = number ) / cap_batt ) * number ) . plot ( ax = ax2 , label = string , color = string ) ax . set_xlabel ( string ) ax . set_ylabel ( string ) ax2 . set_ylabel ( string ) ax2 . set_ylim ( [ number , number ] ) ax . set_ylim ( [ _ number , number ] ) ax . legend ( loc = number ) ax2 . legend ( loc = number ) ax . set_title ( string ) if filename is none plt . show ( ) else plt . savefig ( filename ) plt . close ( ) return	1
Q_394	plot the dispatch and state of charge of extendable storage	C_394_1	def get_rgbd_data ( self ) self . lock . acquire ( ) data = self . data self . lock . release ( ) return data	0
Q_394	plot the dispatch and state of charge of extendable storage	C_394_2	def delimited ( items , character = string ) return string . join ( items ) if type ( items ) in ( list , tuple , set ) else items	0
Q_395	locally store some data namespaced by the current or give class conversation scope	C_395_0	def set_local ( self , key = none , value = none , data = none , scope = none , * * kwdata ) self . conversation ( scope ) . set_local ( key , value , data , * * kwdata )	1
Q_395	locally store some data namespaced by the current or give class conversation scope	C_395_1	def weld_str_get ( array , i ) obj_id , weld_obj = create_weld_object ( array ) index_literal = to_weld_literal ( i , weld_long ( ) ) missing_literal = default_missing_data_literal ( weld_vec ( weld_char ( ) ) ) missing_literal_id = get_weld_obj_id ( weld_obj , missing_literal ) weld_template = weld_obj . weld_code = weld_template . format ( array = obj_id , i = index_literal , missing = missing_literal_id ) return weld_obj	0
Q_395	locally store some data namespaced by the current or give class conversation scope	C_395_2	async def create ( cls , device = string , host = none , username = none , password = none , port = number , hub_version = number , auto_reconnect = true , loop = none , workdir = none , poll_devices = true , load_aldb = true ) _logger . debug ( string ) conn = cls ( device = device , host = host , username = username , password = password , port = port , hub_version = hub_version , loop = loop , retry_interval = number , auto_reconnect = auto_reconnect ) def connection_lost ( ) if conn . auto_reconnect and not conn . closing _logger . debug ( string ) asyncio . ensure_future ( conn . reconnect ( ) , loop = conn . loop ) protocol_class = plm if conn . host and conn . hub_version = = number protocol_class = hub conn . protocol = protocol_class ( connection_lost_callback = connection_lost , loop = conn . loop , workdir = workdir , poll_devices = poll_devices , load_aldb = load_aldb ) await conn . reconnect ( ) _logger . debug ( string ) return conn	0
Q_396	b server public key	C_396_0	def server_seed ( v , b = random . randrange ( number , number < < srp_key_size ) ) n , g , k = get_prime ( ) if debug b = debug_private_key gb = pow ( g , b , n ) kv = ( k * v ) n b = ( kv + gb ) n if debug_print print ( string , binascii . b2a_hex ( long2bytes ( v ) ) , end = string ) print ( string , binascii . b2a_hex ( long2bytes ( b ) ) , end = string ) print ( string , binascii . b2a_hex ( long2bytes ( gb ) ) , end = string ) print ( string , binascii . b2a_hex ( long2bytes ( k ) ) , end = string ) print ( string , binascii . b2a_hex ( long2bytes ( v ) ) , end = string ) print ( string , binascii . b2a_hex ( long2bytes ( kv ) ) , end = string ) print ( string , binascii . b2a_hex ( long2bytes ( b ) ) , end = string ) return b , b	1
Q_396	b server public key	C_396_1	def flush ( self , objects = none , batch_size = none , * * kwargs ) batch_size = batch_size or self . config . get ( string ) if objects from_store = false else from_store = true objects = self . itervalues ( ) objects = sorted ( objects , var = lambda x x [ string ] ) batch , _ids = [ ] , [ ] for var , var in groupby ( objects , lambda x x [ string ] ) _grouped = list ( var ) if len ( batch ) + len ( _grouped ) > batch_size logger . debug ( string len ( batch ) ) _ = self . upsert ( objects = batch , * * kwargs ) logger . debug ( string len ( batch ) ) _ids . extend ( _ ) batch = _grouped else batch . extend ( _grouped ) else if batch logger . debug ( string len ( batch ) ) _ = self . upsert ( objects = batch , * * kwargs ) _ids . extend ( _ ) logger . debug ( string ) if from_store for _id in _ids try self . store . pop ( _id ) except key_error logger . warn ( string . format ( _id ) ) return sorted ( _ids )	0
Q_396	b server public key	C_396_2	def _add_input ( self , variable ) if isinstance ( variable , variable ) self . inputs . append ( variable ) else print ( string , variable . name , variable , string ) raise type_error	0
Q_397	parse the fru output for nic capacity	C_397_0	def _parse_ipmi_nic_capacity ( nic_out ) if ( ( string in nic_out ) or ( string in nic_out ) or not nic_out ) return none capacity = none product_name = none data = nic_out . split ( string ) for var in data fields = var . split ( string ) if len ( fields ) > number first_field = fields [ number ] . strip ( ) if first_field = = string product_name = string . join ( fields [ number ] ) break if product_name product_name_array = product_name . split ( string ) for var in product_name_array if string in var capacity_int = var . strip ( string ) if capacity_int . isdigit ( ) capacity = var return capacity	1
Q_397	parse the fru output for nic capacity	C_397_1	def create ( self , team_id , person_id = none , person_email = none , is_moderator = false , * * request_parameters ) check_type ( team_id , basestring , may_be_none = false ) check_type ( person_id , basestring ) check_type ( person_email , basestring ) check_type ( is_moderator , bool ) post_data = dict_from_items_with_values ( request_parameters , team_id = team_id , person_id = person_id , person_email = person_email , is_moderator = is_moderator , ) json_data = self . _session . post ( api_endpoint , json = post_data ) return self . _object_factory ( object_type , json_data )	0
Q_397	parse the fru output for nic capacity	C_397_2	def _process_keystroke_movement ( self , inp , idx , offset ) term = self . term if inp in ( u string , u string ) or inp . code in ( term . key_up , ) idx , offset = ( idx , offset _ self . screen . num_columns ) elif inp in ( u string , u string ) or inp . code in ( term . key_enter , term . key_down , ) idx , offset = ( idx , offset + self . screen . num_columns ) elif inp in ( u string , u string ) or inp . code in ( term . key_pgdown , ) idx , offset = ( idx + number , offset ) elif inp = = u string or inp . code in ( term . key_pgup , ) idx , offset = ( max ( number , idx _ number ) , offset ) elif inp . code in ( term . key_sdown , ) idx , offset = ( max ( number , idx + number ) , offset ) elif inp . code in ( term . key_sup , ) idx , offset = ( max ( number , idx _ number ) , offset ) elif inp . code = = term . key_home idx , offset = ( number , number ) elif inp . code = = term . key_end idx , offset = ( self . last_page , number ) return idx , offset	0
Q_398	give the identity return a hpe_system object	C_398_0	def get_system ( self , identity ) return system . hpe_system ( self . _conn , identity , redfish_version = self . redfish_version )	1
Q_398	give the identity return a hpe_system object	C_398_1	def create ( self ) session = client . get_client ( ) . create_session ( ) if not self . _base_query ( session ) . count ( ) > number marker = orm_target_marker ( name = self . name , params = self . params ) session . add ( marker ) session . commit ( ) session . close ( )	0
Q_398	give the identity return a hpe_system object	C_398_2	def add ( self , fn , name = none ) if not name name = fn . _name_ self . functions [ name ] = fn	0
Q_399	update a file in the file system	C_399_0	def update ( self , incoming_stream , seek = number , size = none , chunk_size = none , progress_callback = none ) fp = self . open ( mode = string ) try fp . seek ( seek ) bytes_written , checksum = self . _write_stream ( incoming_stream , fp , chunk_size = chunk_size , size = size , progress_callback = progress_callback ) finally fp . close ( ) return bytes_written , checksum	1
Q_399	update a file in the file system	C_399_1	def oauth_token_exchange_cli ( client_id , client_secret , redirect_uri , base_url = oh_base_url , code = none , refresh_token = none ) print ( oauth2_token_exchange ( client_id , client_secret , redirect_uri , base_url , code , refresh_token ) )	0
Q_399	update a file in the file system	C_399_2	async def load ( self , request ) if session_key not in request session = session ( self . cfg . secret , key = self . cfg . session_cookie , max_age = self . cfg . max_age , domain = self . cfg . domain ) session . load ( request . cookies ) self . app . logger . debug ( string , session ) request [ session_key ] = request . session = session return request [ session_key ]	0
Q_400	recursively check all source file define in path	C_400_0	def check_recursive ( paths , reporter = mod_reporter . default , settings_path = none , * * setting_overrides ) warnings = number for var in iter_source_code ( paths ) warnings + = check_path ( var , reporter , settings_path = none , * * setting_overrides ) return warnings	1
Q_400	recursively check all source file define in path	C_400_1	def gant ( ctx , conf , basedir , basetag , maintag , prefix , verbose ) ctx . obj . init_conf ( basetag , maintag , basedir , prefix , verbose ) ctx . obj . gd . set_conf ( ctx . obj . conf )	0
Q_400	recursively check all source file define in path	C_400_2	def send_mail ( subject , recipient , template , * * context ) context . setdefault ( string , _security ) context . update ( _security . _run_ctx_processor ( string ) ) sender = _security . email_sender if isinstance ( sender , local_proxy ) sender = sender . _get_current_object ( ) msg = message ( subject , sender = sender , recipients = [ recipient ] ) ctx = ( string , template ) if config_value ( string ) msg . body = _security . render_template ( string ctx , * * context ) if config_value ( string ) msg . html = _security . render_template ( string ctx , * * context ) if _security . _send_mail_task _security . _send_mail_task ( msg ) return mail = current_app . extensions . get ( string ) mail . send ( msg )	0
Q_401	get a the value correspond to the key and convert it to str list	C_401_0	def get_string ( self , key , is_list = false , is_optional = false , is_secret = false , is_local = false , default = none , options = none ) if is_list return self . _get_typed_list_value ( key = key , target_type = str , type_convert = str , is_optional = is_optional , is_secret = is_secret , is_local = is_local , default = default , options = options ) return self . _get_typed_value ( key = key , target_type = str , type_convert = str , is_optional = is_optional , is_secret = is_secret , is_local = is_local , default = default , options = options )	1
Q_401	get a the value correspond to the key and convert it to str list	C_401_1	def request_check ( client , exception , * msg_parms , * * kwargs ) timeout = kwargs . get ( string , none ) req_msg = message . request ( * msg_parms ) if timeout is not none reply , informs = client . blocking_request ( req_msg , timeout = timeout ) else reply , informs = client . blocking_request ( req_msg ) if not reply . reply_ok ( ) raise exception ( string { number } string string { number } string . format ( client . bind_address_string , req_msg , reply ) ) return reply , informs	0
Q_401	get a the value correspond to the key and convert it to str list	C_401_2	def request_load_source ( self , py_db , seq , filename ) try assert filename . _class_ = = str with open ( filename , string ) as stream source = stream . read ( ) cmd = py_db . cmd_factory . make_load_source_message ( seq , source ) except cmd = py_db . cmd_factory . make_error_message ( seq , get_exception_traceback_str ( ) ) py_db . writer . add_command ( cmd )	0
Q_402	compute mean in window the slow way	C_402_0	def _update_mean_in_window ( self ) self . _mean_x_in_window = numpy . mean ( self . _x_in_window ) self . _mean_y_in_window = numpy . mean ( self . _y_in_window )	1
Q_402	compute mean in window the slow way	C_402_1	def p_prj_home_art_2 ( self , p ) try self . builder . set_file_atrificat_of_project ( self . document , string , utils . un_known ( ) ) except order_error self . order_error ( string , string , p . lineno ( number ) )	0
Q_402	compute mean in window the slow way	C_402_2	def add_bond ( self , particle_pair ) if self . root . bond_graph is none self . root . bond_graph = bond_graph ( ) self . root . bond_graph . add_edge ( particle_pair [ number ] , particle_pair [ number ] )	0
Q_403	implementation of plate subcommand	C_403_0	def cmd ( ) sdat = stagyy_data ( conf . core . path ) conf . plates . plot = set_of_vars ( conf . plates . plot ) if not conf . plates . vzcheck conf . scaling . dimensional = true conf . scaling . factors [ string ] = string main_plates ( sdat ) else seuil_memz = number nb_plates = [ ] time = [ ] ch2o = [ ] istart , iend = none , none for var in sdat . walk . filter ( fields = [ string ] ) if conf . plates . timeprofile time . append ( var . timeinfo . loc [ string ] ) ch2o . append ( var . timeinfo . loc [ number ] ) istart = var . isnap if istart is none else istart iend = var . isnap limits , nphi , dvphi , seuil_memz , vphi_surf = detect_plates_vzcheck ( var , seuil_memz ) water_profile = np . mean ( var . fields [ string ] [ number , , , number ] , axis = number ) limits . sort ( ) sizeplates = [ limits [ number ] + nphi _ limits [ _ number ] ] for lim in range ( number , len ( limits ) ) sizeplates . append ( limits [ lim ] _ limits [ lim _ number ] ) lim = len ( limits ) * [ max ( dvphi ) ] fig = plt . figure ( ) plt . subplot ( number ) plt . axis ( [ number , nphi , np . min ( vphi_surf ) * number . number , np . max ( vphi_surf ) * number . number ] ) plt . plot ( vphi_surf ) plt . subplot ( number ) plt . axis ( [ number , nphi , np . min ( dvphi ) * number . number , np . max ( dvphi ) * number . number ] ) plt . plot ( dvphi ) plt . scatter ( limits , lim , color = string ) plt . subplot ( number ) plt . hist ( sizeplates , number , ( number , nphi / number ) ) plt . subplot ( number ) plt . plot ( water_profile ) misc . saveplot ( fig , string , var . isnap ) nb_plates . append ( len ( limits ) ) if conf . plates . timeprofile for i in range ( number , len ( nb_plates ) _ number ) nb_plates [ i ] = ( nb_plates [ i _ number ] + nb_plates [ i _ number ] + nb_plates [ i ] + nb_plates [ i + number ] + nb_plates [ i + number ] ) / number figt = plt . figure ( ) plt . subplot ( number ) plt . axis ( [ time [ number ] , time [ _ number ] , number , np . max ( nb_plates ) ] ) plt . plot ( time , nb_plates ) plt . subplot ( number ) plt . plot ( time , ch2o ) misc . saveplot ( figt , string . format ( istart , iend ) )	1
Q_403	implementation of plate subcommand	C_403_1	def get_fmt_widget ( self , parent , project ) from psy_simple . widgets . colors import c_map_fmt_widget return c_map_fmt_widget ( parent , self , project )	0
Q_403	implementation of plate subcommand	C_403_2	def ngram_intersection ( args , parser ) store = utils . get_data_store ( args ) corpus = utils . get_corpus ( args ) catalogue = utils . get_catalogue ( args ) store . validate ( corpus , catalogue ) store . intersection ( catalogue , sys . stdout )	0
Q_404	"iterate all icon dirs try to find a file call like the node "" s"	C_404_0	def find ( file_node , dirs = icon_dirs , default_name = none , file_ext = string ) names = [ ] for var in ( string , string , string ) attr = getattr ( file_node , var ) if attr names . append ( attr ) if default_name names . append ( default_name ) icon_path = static_path_finder . find ( names , dirs , file_ext ) if icon_path return static_icon_file ( file_node , icon_path )	1
Q_404	"iterate all icon dirs try to find a file call like the node "" s"	C_404_1	def create_or_update_video_transcript ( video_id , language_code , metadata , file_data = none ) metadata = { var var for var , var in six . iteritems ( metadata ) if var in [ string , string , string , string ] and var } file_format = metadata . get ( string ) if file_format and file_format not in list ( dict ( transcript_format . choices ) . keys ( ) ) raise invalid_transcript_format ( string . format ( file_format ) ) provider = metadata . get ( string ) if provider and provider not in list ( dict ( transcript_provider_type . choices ) . keys ( ) ) raise invalid_transcript_provider ( string . format ( provider ) ) try video = video . objects . get ( edx_video_id = video_id ) video_transcript , _ = video_transcript . create_or_update ( video , language_code , metadata , file_data ) except video . does_not_exist return none return video_transcript . url ( )	0
Q_404	"iterate all icon dirs try to find a file call like the node "" s"	C_404_2	def _serialize_into_store ( profile , filter = none ) current_id = instance_id_model . get_current_instance_and_increment_counter ( ) with transaction . atomic ( ) prefix_condition = none if filter prefix_condition = functools . reduce ( lambda x , y x y , [ q ( _morango_partition_startswith = var ) for var in filter ] ) syncable_dict = _profile_models [ profile ] for ( _ , klass_model ) in six . iteritems ( syncable_dict ) new_store_records = [ ] new_rmc_records = [ ] klass_queryset = klass_model . objects . filter ( _morango_dirty_bit = true ) if prefix_condition klass_queryset = klass_queryset . filter ( prefix_condition ) store_records_dict = store . objects . in_bulk ( id_list = klass_queryset . values_list ( string , flat = true ) ) for app_model in klass_queryset try store_model = store_records_dict [ app_model . id ] if store_model . dirty_bit store_model . conflicting_serialized_data = store_model . serialized + string + store_model . conflicting_serialized_data store_model . dirty_bit = false ser_dict = json . loads ( store_model . serialized ) ser_dict . update ( app_model . serialize ( ) ) store_model . serialized = django_json_encoder ( ) . encode ( ser_dict ) record_max_counter . objects . update_or_create ( defaults = { string current_id . counter } , instance_id = current_id . id , store_model_id = store_model . id ) store_model . last_saved_instance = current_id . id store_model . last_saved_counter = current_id . counter store_model . deleted = false store_model . hard_deleted = false store_model . save ( ) except key_error kwargs = { string app_model . id , string django_json_encoder ( ) . encode ( app_model . serialize ( ) ) , string current_id . id , string current_id . counter , string app_model . morango_model_name , string app_model . morango_profile , string app_model . _morango_partition , string app_model . _morango_source_id , } self_ref_fk = _self_referential_fk ( klass_model ) if self_ref_fk self_ref_fk_value = getattr ( app_model , self_ref_fk ) kwargs . update ( { string self_ref_fk_value or string } ) new_store_records . append ( store ( * * kwargs ) ) new_rmc_records . append ( record_max_counter ( store_model_id = app_model . id , instance_id = current_id . id , counter = current_id . counter ) ) store . objects . bulk_create ( new_store_records ) record_max_counter . objects . bulk_create ( new_rmc_records ) klass_queryset . update ( update_dirty_bit_to = false ) deleted_ids = deleted_models . objects . filter ( profile = profile ) . values_list ( string , flat = true ) deleted_store_records = store . objects . filter ( id_in = deleted_ids ) deleted_store_records . update ( dirty_bit = false , deleted = true , last_saved_instance = current_id . id , last_saved_counter = current_id . counter ) record_max_counter . objects . filter ( instance_id = current_id . id , store_model_id_in = deleted_ids ) . update ( counter = current_id . counter ) new_rmc_ids = deleted_store_records . exclude ( recordmaxcounter_instance_id = current_id . id ) . values_list ( string , flat = true ) record_max_counter . objects . bulk_create ( [ record_max_counter ( store_model_id = r_id , instance_id = current_id . id , counter = current_id . counter ) for r_id in new_rmc_ids ] ) deleted_models . objects . filter ( profile = profile ) . delete ( ) hard_deleted_ids = hard_deleted_models . objects . filter ( profile = profile ) . values_list ( string , flat = true ) hard_deleted_store_records = store . objects . filter ( id_in = hard_deleted_ids ) hard_deleted_store_records . update ( hard_deleted = true , serialized = string , conflicting_serialized_data = string ) hard_deleted_models . objects . filter ( profile = profile ) . delete ( ) if not filter database_max_counter . objects . update_or_create ( instance_id = current_id . id , partition = string , defaults = { string current_id . counter } ) else for f in filter database_max_counter . objects . update_or_create ( instance_id = current_id . id , partition = f , defaults = { string current_id . counter } )	0
Q_405	download the specify project file if it exist	C_405_0	def download_file ( filename str ) project = cd . project . get_internal_project ( ) source_directory = project . source_directory if project else none if not filename or not project or not source_directory return string , number path = os . path . realpath ( os . path . join ( source_directory , string , string , filename ) ) if not os . path . exists ( path ) return string , number return flask . send_file ( path , mimetype = mimetypes . guess_type ( path ) [ number ] )	1
Q_405	download the specify project file if it exist	C_405_1	def add_inputs ( self , rawtx , wifs , change_address = none , fee = number , dont_sign = false ) tx = deserialize . tx ( rawtx ) keys = deserialize . keys ( self . testnet , wifs ) fee = deserialize . positive_integer ( fee ) if change_address is not none change_address = deserialize . address ( self . testnet , change_address ) tx = control . add_inputs ( self . service , self . testnet , tx , keys , change_address = change_address , fee = fee ) if not dont_sign tx = control . sign_tx ( self . service , self . testnet , tx , keys ) return serialize . tx ( tx )	0
Q_405	download the specify project file if it exist	C_405_2	def check_aggregate_region ( self , variable , region = string , subregions = none , components = none , exclude_on_fail = false , * * kwargs ) df_subregions = self . aggregate_region ( variable , region , subregions , components ) if df_subregions is none return rows = self . _apply_filters ( region = region , variable = variable ) df_region , df_subregions = ( _aggregate ( self . data [ rows ] , [ string , string ] ) . align ( df_subregions ) ) diff = df_region [ np . isclose ( df_region , df_subregions , * * kwargs ) ] if len ( diff ) msg = ( string ) logger ( ) . info ( msg . format ( variable , len ( diff ) , len ( df_region ) ) ) if exclude_on_fail self . _exclude_on_fail ( diff . index . droplevel ( [ number , number ] ) ) col_args = dict ( region = region , variable = variable ) return iam_data_frame ( diff , * * col_args ) . timeseries ( )	0
Q_406	return a new instance of this gene with the same dna	C_406_0	def copy ( self ) return type ( self ) ( self . dna , suppressed = self . suppressed , name = self . name )	1
Q_406	return a new instance of this gene with the same dna	C_406_1	def routing ( name , * * kwargs ) ctx = context ( * * kwargs ) ctx . execute_action ( string , * * { string name , string ctx . locator , } )	0
Q_406	return a new instance of this gene with the same dna	C_406_2	def is_py2_stdlib_module ( m ) if py3 return false if not string in is_py2_stdlib_module . _dict_ stdlib_files = [ contextlib . _file_ , os . _file_ , copy . _file_ ] stdlib_paths = [ os . path . split ( var ) [ number ] for var in stdlib_files ] if not len ( set ( stdlib_paths ) ) = = number flog . warn ( string string stdlib_paths ) is_py2_stdlib_module . stdlib_path = stdlib_paths [ number ] if m . _name_ in sys . builtin_module_names return true if hasattr ( m , string ) modpath = os . path . split ( m . _file_ ) if ( modpath [ number ] . startswith ( is_py2_stdlib_module . stdlib_path ) and string not in modpath [ number ] ) return true return false	0
Q_407	serialize to a dict of http header	C_407_0	def serialize_to_headers ( self ) return { string self . _class_ . _name_ , string self . _format_header ( self . error_code ) , string self . _format_header ( self . detail_code ) , string self . _format_header ( self . description ) , string self . _format_header ( self . trace_information ) , string self . _format_header ( self . identifier ) , string self . _format_header ( self . node_id ) , }	1
Q_407	serialize to a dict of http header	C_407_1	def update ( cyg_arch = string , mirrors = none ) args = [ ] args . append ( string ) if not _check_cygwin_installed ( cyg_arch ) log . debug ( string , cyg_arch ) return false return _run_silent_cygwin ( cyg_arch = cyg_arch , args = args , mirrors = mirrors )	0
Q_407	serialize to a dict of http header	C_407_2	def get_host_details_by_index ( self , index , lan_interface_id = number , timeout = number ) namespace = lan . get_service_type ( string ) + str ( lan_interface_id ) uri = self . get_control_url ( namespace ) results = self . execute ( uri , namespace , string , timeout = timeout , new_index = index ) return host_details ( results )	0
Q_408	create a mirror and return a mirror object	C_408_0	def create_mirror ( self , resource_name , target_name , mirror_type , slave_resource_name , rpo = none , remote_rpo = none , schedule = none , remote_schedule = none , activate_mirror = string ) string string sync string async string string return self . _create_mirror ( string , resource_name , target_name , mirror_type , slave_resource_name , rpo = rpo , remote_rpo = remote_rpo , schedule = schedule , remote_schedule = remote_schedule , activate_mirror = activate_mirror )	1
Q_408	create a mirror and return a mirror object	C_408_1	def gen_unique_id ( serialized_name , args , kwargs ) return hashlib . sha256 ( json . dumps ( { string serialized_name , string args , string kwargs , } , sort_keys = true ) . encode ( string ) ) . hexdigest ( )	0
Q_408	create a mirror and return a mirror object	C_408_2	def daemonize ( self ) self . pre_log . append ( ( string , string ) ) print ( string self . name ) os . umask ( umask ) self . pre_log . append ( ( string , string ) ) preserved_fds = [ number , number , self . fpid . fileno ( ) ] if os . getenv ( string , none ) preserved_fds = [ self . fpid . fileno ( ) ] if self . debug preserved_fds . extend ( [ number , number ] ) self . close_fds ( preserved_fds ) def fork_then_exit_parent ( level , error_message ) try pid = os . fork ( ) if pid > number if level = = number self . write_pid ( pid ) os . _exit ( number ) except os_error as exc raise exception ( string ( error_message , exc . errno , str ( exc ) ) ) fork_then_exit_parent ( level = number , error_message = string ) os . setsid ( ) fork_then_exit_parent ( level = number , error_message = string ) self . pid = os . getpid ( ) self . pre_log . append ( ( string , string self . pid ) ) return true	0
Q_409	set permission on a project to a list of user identifed by	C_409_0	def set_project_permission ( project_id , usernames , read , write , share , * * kwargs ) user_id = kwargs . get ( string ) proj_i = _get_project ( project_id ) proj_i . check_share_permission ( user_id ) if read = = string write = string share = string for var in usernames user_i = _get_user ( var ) if proj_i . created_by = = user_i . id raise hydra_error ( string string ( project_id , var ) ) proj_i . set_owner ( user_i . id , read = read , write = write ) for net_i in proj_i . networks net_i . set_owner ( user_i . id , read = read , write = write , share = share ) db . db_session . flush ( )	1
Q_409	set permission on a project to a list of user identifed by	C_409_1	def call_sacrebleu ( input_fname str , ref_fname str , output_fname str , log_fname str , tokenized bool = false ) command = [ string , string , string . format ( input_fname ) , ref_fname ] if tokenized command . append ( string ) with open ( log_fname , string ) as log logging . info ( string , input_fname , output_fname ) logging . info ( string , log_fname ) score = subprocess . check_output ( command , stderr = log ) with open ( output_fname , string ) as out out . write ( score )	0
Q_409	set permission on a project to a list of user identifed by	C_409_2	def primal_name ( func , wrt ) if not isinstance ( func , types . function_type ) raise type_error ( func ) varnames = six . get_function_code ( func ) . co_varnames return primal_name . format ( func . _name_ , string . join ( varnames [ var ] for var in wrt ) )	0
Q_410	delete a transaction	C_410_0	def _delete ( self , tx_id ) txs = self . driver . instance . transactions . get ( asset_id = self . get_asset_id ( tx_id ) ) unspent = txs [ _ number ] output_index = number output = unspent [ string ] [ output_index ] transfer_input = { string output [ string ] [ string ] , string { string output_index , string unspent [ string ] } , string output [ string ] } prepared_transfer_tx = self . driver . instance . transactions . prepare ( operation = string , asset = unspent [ string ] if string in unspent [ string ] else { string unspent [ string ] } , inputs = transfer_input , recipients = self . burn_address , metadata = { string string , } ) signed_tx = self . driver . instance . transactions . fulfill ( prepared_transfer_tx , private_keys = self . user . private_key , ) self . driver . instance . transactions . send_commit ( signed_tx )	1
Q_410	delete a transaction	C_410_1	def trigger_arbitrary_job ( repo_name , builder , revision , auth , files = none , dry_run = false , extra_properties = none ) assert len ( revision ) = = number , string url = _builders_api_url ( repo_name , builder , revision ) payload = _payload ( repo_name , revision , files , extra_properties ) if dry_run log . info ( string { } string . format ( builder ) ) log . info ( string . format ( str ( payload ) ) ) log . info ( string . format ( files ) ) return none req = requests . post ( url , headers = { string string } , data = payload , auth = auth , timeout = tcp_timeout , ) if req . status_code = = number raise buildapi_auth_error ( string ) elif req . status_code = = number raise buildapi_down ( string . format ( url ) ) try req . json ( ) return req except value_error log . info ( string . format ( repo_name , builder , revision ) ) log . error ( string ( url , req . status_code ) ) return none	0
Q_410	delete a transaction	C_410_2	def set_base_url ( url ) global _base_url , _dirty logger . debug ( string , url ) _base_url = url _dirty = true	0
Q_411	convert a mor to a psphere object	C_411_0	def _mor_to_pobject ( self , mo_ref ) kls = classmapper ( mo_ref . _type ) new_object = kls ( mo_ref , self ) return new_object	1
Q_411	convert a mor to a psphere object	C_411_1	def make_map ( declarations ) mapper = routes . mapper ( ) for var , var in route_list allowed_methods = [ ] for method , func in var . items ( ) mapper . connect ( var , action = func , conditions = dict ( method = [ method ] ) ) allowed_methods . append ( method ) allowed_methods = string . join ( allowed_methods ) mapper . connect ( var , action = handle_not_allowed , _methods = allowed_methods ) return mapper	0
Q_411	convert a mor to a psphere object	C_411_2	def want_service_notification ( self , timeperiods , timestamp , state , n_type , business_impact , cmd = none ) if not self . service_notifications_enabled return false if cmd and cmd not in self . service_notification_commands return false if business_impact < self . min_business_impact return false notif_period = timeperiods [ self . service_notification_period ] in_notification_period = notif_period . is_time_valid ( timestamp ) if string in self . service_notification_options return false if in_notification_period short_states = { u string string , u string string , u string string , u string string , u string string , u string string } if n_type = = u string and state in short_states return short_states [ state ] in self . service_notification_options if n_type = = u string and n_type in short_states return short_states [ n_type ] in self . service_notification_options if n_type = = u string return in_notification_period if n_type in ( u string , u string , u string ) return string in self . service_notification_options if n_type in ( u string , u string , u string ) return string in self . host_notification_options return false	0
Q_412	return entry relate to this invoice on specify account	C_412_0	def get_entries ( self , acc account , cls = account_entry ) _ > query_set return cls . objects . filter ( q ( account = acc ) ( q ( source_invoice = self ) q ( settled_invoice = self ) ) ) if acc else cls . objects . none ( )	1
Q_412	return entry relate to this invoice on specify account	C_412_1	def get_new_filename ( self , name str ) _ > str nonce = get_random_string ( length = number ) return string ( self . obj . _meta . model_name , self . attribute_name , self . obj . pk , name , nonce , name . split ( string ) [ _ number ] )	0
Q_412	return entry relate to this invoice on specify account	C_412_2	def create ( self , tag , value ) u child_tags = { var . tag for var in self . _xml } if tag in child_tags raise key_error ( string . format ( tag ) ) self . set ( tag , value )	0
Q_413	load from a file and return an x509 object	C_413_0	def load ( self ) private = self . is_private ( ) with open_tls_file ( self . file_path , string , private = private ) as fh if private self . x509 = crypto . load_privatekey ( self . encoding , fh . read ( ) ) else self . x509 = crypto . load_certificate ( self . encoding , fh . read ( ) ) return self . x509	1
Q_413	load from a file and return an x509 object	C_413_1	def var ( self , pe , ep_only = true , section_start_only = false ) matches = self . _match ( pe , ep_only , section_start_only ) if matches if ep_only = = false return [ ( var [ number ] , var [ number ] [ _ number ] ) for var in matches ] return matches [ number ] [ _ number ] return none	0
Q_413	load from a file and return an x509 object	C_413_2	async def kick ( self , channel , target , reason = none ) if not self . in_channel ( channel ) raise not_in_channel ( channel ) if reason await self . rawmsg ( string , channel , target , reason ) else await self . rawmsg ( string , channel , target )	0
Q_414	mouse cursor be move	C_414_0	def on_drag ( self , cursor_x , cursor_y ) from blmath . geometry . transform . rodrigues import as_rotation_matrix if self . isdragging mouse_pt = arcball . point2f_t ( cursor_x , cursor_y ) this_quat = self . arcball . drag ( mouse_pt ) self . thisrot = arcball . matrix3f_set_rotation_from_quat4f ( this_quat ) self . thisrot = arcball . matrix3f_mul_matrix3f ( self . lastrot , self . thisrot ) self . thisrot = as_rotation_matrix ( self . thisrot ) self . transform = arcball . matrix4f_set_rotation_from_matrix3f ( self . transform , self . thisrot ) glut . glut_post_redisplay ( ) return	1
Q_414	mouse cursor be move	C_414_1	def _fetch_children ( self ) children = [ ] paths = [ ] for var in os . listdir ( self . path ) paths . append ( os . path . normpath ( os . path . join ( self . path , var ) ) ) collections , remainder = clique . assemble ( paths , [ clique . patterns [ string ] ] ) for path in remainder try child = item_factory ( path ) except value_error pass else children . append ( child ) for collection in collections children . append ( collection ( collection ) ) return children	0
Q_414	mouse cursor be move	C_414_2	def ecuc_extract_signal ( signal_node , ns ) attributes = signal_node . findall ( string + ns + string ) start_bit = none size = number is_little = false for var in attributes if var . text . endswith ( string ) start_bit = int ( var . getparent ( ) . find ( string + ns + string ) . text ) if var . text . endswith ( string ) size = int ( var . getparent ( ) . find ( string + ns + string ) . text ) if var . text . endswith ( string ) endianness = var . getparent ( ) . find ( string + ns + string ) . text is_little = string in endianness if var . text . endswith ( string ) init_value = int ( var . getparent ( ) . find ( string + ns + string ) . text ) if var . text . endswith ( string ) signal_type = var . getparent ( ) . find ( string + ns + string ) . text if var . text . endswith ( string ) timeout = int ( var . getparent ( ) . find ( string + ns + string ) . text ) return canmatrix . signal ( get_element_name ( signal_node , ns ) , start_bit = start_bit , size = size , is_little_endian = is_little )	0
Q_415	reflect a give table from the database	C_415_0	def _load_table ( self , name ) table = self . _tables . get ( name , none ) if table is not none return table if not self . engine . has_table ( name ) raise binding_exception ( string name , table = name ) table = table ( name , self . meta , autoload = true ) self . _tables [ name ] = table return table	1
Q_415	reflect a give table from the database	C_415_1	def add_activation ( self , name , non_linearity , input_name , output_name , params = none ) spec = self . spec nn_spec = self . nn_spec spec_layer = nn_spec . layers . add ( ) spec_layer . name = name spec_layer . input . append ( input_name ) spec_layer . output . append ( output_name ) spec_layer_params = spec_layer . activation if non_linearity = = string spec_layer_params . re_lu . merge_from_string ( b string ) elif non_linearity = = string spec_layer_params . sigmoid . merge_from_string ( b string ) elif non_linearity = = string spec_layer_params . tanh . merge_from_string ( b string ) elif non_linearity = = string spec_layer_params . scaled_tanh . merge_from_string ( b string ) if params is none alpha , beta = ( number . number , number . number ) else alpha , beta = params [ number ] , params [ number ] spec_layer_params . scaled_tanh . alpha = alpha spec_layer_params . scaled_tanh . beta = beta elif non_linearity = = string spec_layer_params . softplus . merge_from_string ( b string ) elif non_linearity = = string spec_layer_params . softsign . merge_from_string ( b string ) elif non_linearity = = string if params is none alpha , beta = ( number . number , number . number ) else alpha , beta = params [ number ] , params [ number ] spec_layer_params . sigmoid_hard . alpha = alpha spec_layer_params . sigmoid_hard . beta = beta elif non_linearity = = string if params is none alpha = number . number else alpha = params [ number ] spec_layer_params . leaky_re_lu . alpha = float ( alpha ) elif non_linearity = = string spec_layer_params . p_re_lu . alpha . float_value . extend ( map ( float , params . flatten ( ) ) ) elif non_linearity = = string spec_layer_params . elu . alpha = float ( params ) elif non_linearity = = string alphas , betas = ( params [ number ] , params [ number ] ) spec_layer_params . parametric_softplus . alpha . float_value . extend ( map ( float , alphas . flatten ( ) ) ) spec_layer_params . parametric_softplus . beta . float_value . extend ( map ( float , betas . flatten ( ) ) ) elif non_linearity = = string if params is none theta = number . number else theta = params spec_layer_params . thresholded_re_lu . alpha = float ( theta ) elif non_linearity = = string if params is none alpha , beta = ( number . number , number . number ) else alpha , beta = params [ number ] , params [ number ] spec_layer_params . linear . alpha = alpha spec_layer_params . linear . beta = beta else raise type_error ( string ( non_linearity ) )	0
Q_415	reflect a give table from the database	C_415_2	def add_html_report_optgroup ( parser ) g = parser . add_argument_group ( string ) g . add_argument ( string , action = string , dest = string , default = list ( ) ) g . add_argument ( string , action = string , dest = string , default = list ( ) ) g . add_argument ( string , action = string , default = none , help = string string )	0
Q_416	it download from the repository the specify dataset and put it	C_416_0	def download_dataset ( self , dataset_name , local_path , how = string ) if not os . path . isdir ( local_path ) os . makedirs ( local_path ) else raise value_error ( string . format ( local_path ) ) local_path = os . path . join ( local_path , files_folder ) os . makedirs ( local_path ) if how = = string return self . download_as_zip ( dataset_name , local_path ) elif how = = string return self . download_as_stream ( dataset_name , local_path ) else raise value_error ( string zip string stream string )	1
Q_416	it download from the repository the specify dataset and put it	C_416_1	def health_check ( self ) response = self . get ( string ) if not response response = self . get ( string ) return response	0
Q_416	it download from the repository the specify dataset and put it	C_416_2	def handle ( self ) if self . component_type = = stream_component . source msg = self . handler_function ( ) return self . _send ( msg ) logger = self . logger data = self . _receive ( ) if data is none return false else logger . debug ( string self . handler_function ) result = self . handler_function ( data . decode ( self . char_encoding ) ) if self . component_type = = stream_component . processor logger . debug ( string ( python3 , result , str ( type ( result ) ) ) ) if not self . _send ( result ) return false return true	0
Q_417	reconstruct point set use e_mds	C_417_0	def var ( edm , om , all_points , method = none , * * kwargs ) from . point_set import dm_from_edm n = all_points . shape [ number ] d = all_points . shape [ number ] dm = dm_from_edm ( edm ) if method is none from . mds import super_mds xhat , _ = super_mds ( all_points [ number , ] , n , d , om = om , dm = dm ) else c = kwargs . get ( string , none ) b = kwargs . get ( string , none ) if c is none or b is none raise name_error ( string ) ke_noisy = np . multiply ( np . outer ( dm , dm ) , om ) if method = = string from . mds import iterative_emds xhat , _ = iterative_emds ( all_points [ number , ] , n , d , ke = ke_noisy , c = c , b = b ) elif method = = string from . mds import relaxed_emds xhat , _ = relaxed_emds ( all_points [ number , ] , n , d , ke = ke_noisy , c = c , b = b ) else raise name_error ( string , method ) y , r , t , c = procrustes ( all_points , xhat , scale = false ) return y	1
Q_417	reconstruct point set use e_mds	C_417_1	def is_constructing_scv ( self ) _ > bool return self . orders and self . orders [ number ] . ability . id in { ability_id . terranbuild_armory , ability_id . terranbuild_barracks , ability_id . terranbuild_bunker , ability_id . terranbuild_commandcenter , ability_id . terranbuild_engineeringbay , ability_id . terranbuild_factory , ability_id . terranbuild_fusioncore , ability_id . terranbuild_ghostacademy , ability_id . terranbuild_missileturret , ability_id . terranbuild_refinery , ability_id . terranbuild_sensortower , ability_id . terranbuild_starport , ability_id . terranbuild_supplydepot , }	0
Q_417	reconstruct point set use e_mds	C_417_2	def include ( self , * * attrs ) for var , var in attrs . items ( ) include = getattr ( self , string + var , none ) if include include ( var ) else self . _include_misc ( var , var )	0
Q_418	attempt to load a string_based value into the native representation	C_418_0	def loads ( self , value , context = none ) if value = = string or ( hasattr ( value , string ) and value . strip ( ) = = string ) return none return self . native ( value )	1
Q_418	attempt to load a string_based value into the native representation	C_418_1	def request_signed_by_signing_keys ( keyjar , msreq , iss , lifetime , kid = string ) try jwks_to_keyjar ( msreq [ string ] , iss ) except key_error jwks = keyjar . export_jwks ( issuer = iss ) msreq [ string ] = jwks _jwt = jwt ( keyjar , iss = iss , lifetime = lifetime ) return _jwt . pack ( owner = iss , kid = kid , payload = msreq . to_dict ( ) )	0
Q_418	attempt to load a string_based value into the native representation	C_418_2	def certify_tuple ( value , certifier = none , min_len = none , max_len = none , required = true , schema = none ) certify_iterable ( value = value , types = tuple ( [ tuple ] ) , certifier = certifier , min_len = min_len , max_len = max_len , schema = schema , required = required , )	0
Q_419	update a entitlement in a repository	C_419_0	def update ( ctx , opts , owner_repo_identifier , show_tokens , name , token ) owner , repo , identifier = owner_repo_identifier use_stderr = opts . output = string click . secho ( string string { string click . style ( identifier , bold = true ) , string click . style ( repo , bold = true ) , } , nl = false , err = use_stderr , ) context_msg = string with handle_api_exceptions ( ctx , opts = opts , context_msg = context_msg ) with maybe_spinner ( opts ) entitlement = api . update_entitlement ( owner = owner , repo = repo , identifier = identifier , name = name , token = token , show_tokens = show_tokens , ) click . secho ( string , fg = string , err = use_stderr ) print_entitlements ( opts = opts , data = [ entitlement ] , show_list_info = false )	1
Q_419	update a entitlement in a repository	C_419_1	def lambda_handler ( event , context ) bucket = event [ string ] [ number ] [ string ] [ string ] [ string ] key = urllib . unquote_plus ( event [ string ] [ number ] [ string ] [ string ] [ string ] . encode ( string ) ) try response = detect_faces ( bucket , key ) print ( response ) return response except exception as e print ( e ) print ( string . format ( key , bucket ) + string ) raise e	0
Q_419	update a entitlement in a repository	C_419_2	def process_without_storing ( d_vals , s_f_strm , s_f_key , output_type , outfh , f_f_header = none , s_f_has_header = false , missing_val = none , delim = none , ignore_missing_keys = false , output_unpaired = false , verbose = false ) used_d_val_keys = set ( ) key_field_num = s_f_key s_f_header = none seen_keys = set ( ) out_handler = output_type . get_handler ( ) expected_cols_per_line = none first_element = true for var in file_iterator ( s_f_strm , verbose ) parts = _populated_missing_vals ( var . split ( delim ) , missing_val ) if first_element regular_first , s_f_header , key_field_num = process_without_storing_header ( parts , d_vals , f_f_header , s_f_has_header , s_f_header , s_f_key , key_field_num , out_handler , outfh , missing_val , delim ) if not first_element or regular_first if expected_cols_per_line is none expected_cols_per_line = ( len ( parts ) if s_f_header is none else len ( s_f_header ) ) if expected_cols_per_line = len ( parts ) raise ragged_input_error ( string + str ( len ( parts ) ) + string string + str ( expected_cols_per_line ) + string ) key_val = get_key_value ( parts , key_field_num , ignore_missing_keys , seen_keys , output_type ) if key_val is none and ignore_missing_keys continue seen_keys . add ( key_val ) if key_val not in d_vals if not output_unpaired continue f_f_flds = populate_unpaired_line ( d_vals , f_f_header , missing_val ) else f_f_flds = d_vals [ key_val ] used_d_val_keys . add ( key_val ) s_f_flds = ( [ dict ( zip ( s_f_header , parts ) ) ] if s_f_header is not none else [ parts ] ) out_handler . write_output ( outfh , delim , s_f_flds , f_f_flds , s_f_header , f_f_header ) first_element = false if output_unpaired _output_unpaired_vals ( d_vals , used_d_val_keys , f_f_header , s_f_flds , s_f_header , missing_val , out_handler , outfh , delim )	0
Q_420	highlight the found term in the abstract text	C_420_0	def highlighted_abstract ( self ) abstract_terms = self . fields . get ( string , [ ] ) if abstract_terms sql = _read_sql_file ( string ) else sql = _read_sql_file ( string ) arguments = { string self [ string ] , string string . join ( abstract_terms ) , } with db_connect ( ) as db_connection with db_connection . cursor ( ) as cursor cursor . execute ( sql , arguments ) hl_abstract = cursor . fetchone ( ) if hl_abstract return hl_abstract [ number ]	1
Q_420	highlight the found term in the abstract text	C_420_1	def commit ( self ) try if self . connection is not none self . connection . commit ( ) self . _update_check_time ( ) self . release ( ) except exception , e pass	0
Q_420	highlight the found term in the abstract text	C_420_2	def wrapped_help_text ( wrapped_func ) def decorator ( wrapper_func ) wrapper_func . _doc_ = ( string + pydoc . text . document ( wrapped_func ) ) return wrapper_func return decorator	0
Q_421	retrieve experiment tuples for experiment return by this function	C_421_0	def experiments ( auth , label = none , project = none , subject = none ) if subject and ( label or project ) raise value_error ( string ) url = string . format ( auth . url . rstrip ( string ) ) logger . debug ( string , url ) columns = [ string , string , string , string , string , string ] payload = { string string . join ( columns ) } if label payload [ string ] = label if project payload [ string ] = project if subject payload [ string ] = subject . project payload [ string ] = subject . id r = requests . get ( url , params = payload , auth = ( auth . username , auth . password ) , verify = check_certificate ) if r . status_code = requests . codes . ok raise accession_error ( string . format ( r . status_code , r . url ) ) try results = r . json ( ) _quick_validate ( results ) except result_set_error as e raise result_set_error ( string . format ( e . message , r . url ) ) results = results [ string ] if int ( results [ string ] ) = = number raise no_experiments_error ( string . format ( r . url ) ) for var in results [ string ] yield experiment ( uri = var [ string ] , id = var [ string ] , project = var [ string ] , label = var [ string ] , subject_id = var [ string ] , subject_label = var [ string ] , archived_date = var [ string ] )	1
Q_421	retrieve experiment tuples for experiment return by this function	C_421_1	def get_date_format_string ( period ) if isinstance ( period , six . string_types ) and period . lower ( ) = = string return string file_period_secs = get_period_seconds ( period ) format_pieces = ( string , string , string , string , string ) seconds_per_second = number intervals = ( seconds_per_year , seconds_per_day , seconds_per_hour , seconds_per_minute , seconds_per_second , ) mods = list ( map ( lambda interval file_period_secs interval , intervals ) ) format_pieces = format_pieces [ mods . index ( number ) + number ] return string . join ( format_pieces )	0
Q_421	retrieve experiment tuples for experiment return by this function	C_421_2	def is_compression_coordinate ( ds , variable ) if not is_coordinate_variable ( ds , variable ) return false compress = getattr ( ds . variables [ variable ] , string , none ) if not isinstance ( compress , basestring ) return false if not compress return false if variable in compress return false for var in compress . split ( ) if var not in ds . dimensions return false return true	0
Q_422	get the output from the process	C_422_0	def get_process_output ( process , encoding = none ) output = process . communicate ( ) returncode = process . returncode if not encoding try encoding = sys . stdout . encoding except exception encoding = locale . getpreferredencoding ( ) if returncode = number raise runtime_error ( string ( output [ number ] . rstrip ( ) . decode ( encoding , errors = string ) ) ) return output [ number ] . decode ( encoding , errors = string )	1
Q_422	get the output from the process	C_422_1	def subs ( self , * args ) cols = [ string , string , string ] out = self . copy ( ) def get_subs_f ( * args ) def subs_function ( x ) if hasattr ( x , string ) x = x . subs ( * args ) try x = float ( x ) except type_error pass return x return subs_function for var in cols if out . loc [ , var ] . dtype is np . dtype ( string ) out . loc [ , var ] = out . loc [ , var ] . map ( get_subs_f ( * args ) ) try out . loc [ , var ] = out . loc [ , var ] . astype ( string ) except ( system_error , type_error ) pass return out	0
Q_422	get the output from the process	C_422_2	def _to_dict ( self ) _dict = { } if hasattr ( self , string ) and self . title is not none _dict [ string ] = self . title if hasattr ( self , string ) and self . hash is not none _dict [ string ] = self . hash return _dict	0
Q_423	query the sherlock_catalogues database view metadata	C_423_0	def _get_view_infos ( self , trimmed = false ) self . log . debug ( string ) sql_query = u string string master table string legacy string string locals ( ) view_info = readquery ( log = self . log , sql_query = sql_query , db_conn = self . catalogues_db_conn , quiet = false ) if trimmed clean_table = [ ] for var in view_info orow = collections . ordered_dict ( sorted ( { } . items ( ) ) ) for c in self . basic_columns if c in var orow [ c ] = var [ c ] clean_table . append ( orow ) view_info = clean_table self . log . debug ( string ) return view_info	1
Q_423	query the sherlock_catalogues database view metadata	C_423_1	def seek ( self , pos = number ) if pos _ self . pos > = number blocks , remainder = divmod ( pos _ self . pos , self . bufsize ) for var in range ( blocks ) self . read ( self . bufsize ) self . read ( remainder ) else raise stream_error ( string ) return self . pos	0
Q_423	query the sherlock_catalogues database view metadata	C_423_2	def _finalise_figure ( fig , * * kwargs ) title = kwargs . get ( string ) or none show = kwargs . get ( string ) or false save = kwargs . get ( string ) or false savefile = kwargs . get ( string ) or string return_fig = kwargs . get ( string ) or false if title fig . suptitle ( title ) if show fig . show ( ) if save fig . savefig ( savefile ) print ( string . format ( savefile ) ) if return_fig return fig return none	0
Q_424	execute custom command in configuration mode on device	C_424_0	def run_custom_config_command ( self , custom_command ) return self . run_command_flow . execute_flow ( custom_command = custom_command , is_config = true )	1
Q_424	execute custom command in configuration mode on device	C_424_1	def open ( self ) open_command = standard_send ( self . _address , command_light_on_0_x11_none , cmd2 = number ) self . _send_method ( open_command , self . _open_message_received )	0
Q_424	execute custom command in configuration mode on device	C_424_2	def calc_signal_color2 ( self , distance_modulus , mass_steps = number ) logger . info ( string ) mag_1 , mag_2 = self . catalog . mag_1 , self . catalog . mag_2 lon , lat = self . catalog . lon , self . catalog . lat u_density = self . isochrone . pdf_mmd ( lon , lat , mag_1 , mag_2 , distance_modulus , self . mask , self . delta_mag , mass_steps ) u_color = u_density return u_color	0
Q_425	validate the field match the give regular expression	C_425_0	def validate_regexp ( pattern , flags = number ) regex = re . compile ( pattern , flags ) if isinstance ( pattern , str ) else pattern def regexp_validator ( field , data ) if field . value is none return if regex . match ( str ( field . value ) ) is none raise validation_error ( string , pattern = pattern ) return regexp_validator	1
Q_425	validate the field match the give regular expression	C_425_1	def join ( left , right , how = string , key = none , left_key = none , right_key = none , join_fn = tuple_join ) if key is none and ( left_key is none or right_key is none ) raise value_error ( string ) if key is not none lkey = rkey = key if callable ( key ) else make_key_fn ( key ) else lkey = left_key if callable ( left_key ) else make_key_fn ( left_key ) rkey = right_key if callable ( right_key ) else make_key_fn ( right_key ) try join_impl = { string _left_join , string _right_join , string _inner_join , string _outer_join , } [ how ] except key_error raise value_error ( string string . format ( str ( how ) ) ) else return join_impl ( left , right , lkey , rkey , join_fn )	0
Q_425	validate the field match the give regular expression	C_425_2	def _on_items_changed ( self , change ) if change [ string ] = string return op = change [ string ] if op = = string i = len ( change [ string ] ) _ number self . adapter . notify_item_inserted ( i ) elif op = = string self . adapter . notify_item_inserted ( change [ string ] ) elif op in ( string , string ) self . adapter . notify_item_removed ( change [ string ] ) elif op = = string self . adapter . notify_item_changed ( change [ string ] ) elif op = = string n = len ( change [ string ] ) i = len ( change [ string ] ) _ n self . adapter . notify_item_range_inserted ( i , n ) elif op in ( string , string , string ) self . adapter . notify_data_set_changed ( )	0
Q_426	"r tagstr "" return zeeman effect energy for a give fine state and"	C_426_0	def zeeman_energies ( fine_state , bz ) r string string rb string string element = fine_state . element isotope = fine_state . isotope n = fine_state . n l = fine_state . l j = fine_state . j energies_zeeman = [ ] for i , f in enumerate ( fine_state . fperm ) g_l , g_s , g_i , g_j , g_f = lande_g_factors ( element , isotope , l , j , f ) energies_f = [ ] hyperfine_level = state ( element , isotope , n , l , j , f ) for mf in range ( _ f , f + number ) unperturbed_energy = hbar * hyperfine_level . omega energy_mf = unperturbed_energy + mu_b * g_f * mf * bz energies_f + = [ energy_mf ] energies_zeeman + = [ array ( energies_f ) ] return energies_zeeman	1
Q_426	"r tagstr "" return zeeman effect energy for a give fine state and"	C_426_1	def plot_plane ( where = string , texture = none ) fig = gcf ( ) xmin , xmax = fig . xlim ymin , ymax = fig . ylim zmin , zmax = fig . zlim if where = = string x = [ xmin , xmax , xmax , xmin ] y = [ ymin , ymin , ymax , ymax ] z = [ zmin , zmin , zmin , zmin ] if where = = string x = [ xmin , xmax , xmax , xmin ] [ _ number ] y = [ ymin , ymin , ymax , ymax ] z = [ zmax , zmax , zmax , zmax ] if where = = string x = [ xmin , xmin , xmin , xmin ] y = [ ymin , ymin , ymax , ymax ] z = [ zmin , zmax , zmax , zmin ] if where = = string x = [ xmax , xmax , xmax , xmax ] y = [ ymin , ymin , ymax , ymax ] z = [ zmin , zmax , zmax , zmin ] [ _ number ] if where = = string x = [ xmin , xmax , xmax , xmin ] y = [ ymax , ymax , ymax , ymax ] z = [ zmax , zmax , zmin , zmin ] if where = = string x = [ xmax , xmin , xmin , xmax ] y = [ ymin , ymin , ymin , ymin ] z = [ zmin , zmin , zmax , zmax ] triangles = [ ( number , number , number ) , ( number , number , number ) ] u = v = none if texture is not none u = [ number . number , number . number , number . number , number . number ] v = [ number . number , number . number , number . number , number . number ] mesh = plot_trisurf ( x , y , z , triangles , texture = texture , u = u , v = v ) return mesh	0
Q_426	"r tagstr "" return zeeman effect energy for a give fine state and"	C_426_2	def generate_automodsumm_docs ( lines , srcfn , suffix = string , warn = none , info = none , base_path = none , builder = none , template_dir = none ) from sphinx . jinja2glue import builtin_template_loader from sphinx . ext . autosummary import import_by_name , get_documenter from sphinx . ext . autosummary . generate import ( find_autosummary_in_lines , _simple_info , _simple_warn ) from sphinx . util . osutil import ensuredir from sphinx . util . inspect import safe_getattr from jinja2 import file_system_loader , template_not_found from jinja2 . sandbox import sandboxed_environment if info is none info = _simple_info if warn is none warn = _simple_warn template_dirs = [ os . path . join ( os . path . dirname ( _file_ ) , string ) , os . path . join ( base_path , string ) ] if builder is not none template_loader = builtin_template_loader ( ) template_loader . init ( builder , dirs = template_dirs ) else if template_dir template_dirs . insert ( number , template_dir ) template_loader = file_system_loader ( template_dirs ) template_env = sandboxed_environment ( loader = template_loader ) items = find_autosummary_in_lines ( lines , filename = srcfn ) if len ( items ) > number msg = string info ( msg . format ( len ( items ) , srcfn ) ) items = dict ( [ ( var , true ) for var in items ] ) . keys ( ) new_files = [ ] for name , path , template_name in sorted ( items ) if path is none continue path = os . path . abspath ( path ) ensuredir ( path ) try import_by_name_values = import_by_name ( name ) except import_error as e warn ( string ( name , e ) ) continue if len ( import_by_name_values ) = = number name , obj , parent = import_by_name_values elif len ( import_by_name_values ) = = number name , obj , parent , module_name = import_by_name_values fn = os . path . join ( path , name + suffix ) if os . path . isfile ( fn ) continue new_files . append ( fn ) f = open ( fn , string ) try doc = get_documenter ( obj , parent ) if template_name is not none template = template_env . get_template ( template_name ) else tmplstr = string try template = template_env . get_template ( tmplstr doc . objtype ) except template_not_found template = template_env . get_template ( tmplstr string ) def get_members_mod ( obj , typ , include_public = [ ] ) items = [ ] for name in dir ( obj ) try documenter = get_documenter ( safe_getattr ( obj , name ) , obj ) except attribute_error continue if typ is none or documenter . objtype = = typ items . append ( name ) public = [ x for x in items if x in include_public or not x . startswith ( string ) ] return public , items def get_members_class ( obj , typ , include_public = [ ] , include_base = false ) items = [ ] if include_base names = dir ( obj ) else if hasattr ( obj , string ) names = tuple ( getattr ( obj , string ) ) else names = getattr ( obj , string ) . keys ( ) for name in names try documenter = get_documenter ( safe_getattr ( obj , name ) , obj ) except attribute_error continue if typ is none or documenter . objtype = = typ items . append ( name ) public = [ x for x in items if x in include_public or not x . startswith ( string ) ] return public , items ns = { } if doc . objtype = = string ns [ string ] = get_members_mod ( obj , none ) ns [ string ] , ns [ string ] = get_members_mod ( obj , string ) ns [ string ] , ns [ string ] = get_members_mod ( obj , string ) ns [ string ] , ns [ string ] = get_members_mod ( obj , string ) elif doc . objtype = = string api_class_methods = [ string , string ] ns [ string ] = get_members_class ( obj , none ) ns [ string ] , ns [ string ] = get_members_class ( obj , string , api_class_methods ) ns [ string ] , ns [ string ] = get_members_class ( obj , string ) ns [ string ] . sort ( ) ns [ string ] . sort ( ) parts = name . split ( string ) if doc . objtype in ( string , string ) mod_name = string . join ( parts [ _ number ] ) cls_name = parts [ _ number ] obj_name = string . join ( parts [ _ number ] ) ns [ string ] = cls_name else mod_name , obj_name = string . join ( parts [ _ number ] ) , parts [ _ number ] ns [ string ] = name ns [ string ] = mod_name ns [ string ] = obj_name ns [ string ] = parts [ _ number ] ns [ string ] = doc . objtype ns [ string ] = len ( name ) * string if string in mod_name mod_name_dir = mod_name . replace ( string , string ) . split ( string , number ) [ number ] else mod_name_dir = mod_name if not os . path . isdir ( os . path . join ( base_path , mod_name_dir ) ) and os . path . isdir ( os . path . join ( base_path , mod_name_dir . rsplit ( string , number ) [ number ] ) ) mod_name_dir = mod_name_dir . rsplit ( string , number ) [ number ] if os . path . exists ( os . path . join ( base_path , mod_name_dir , string ) ) ndirsback = path . replace ( base_path , string ) . count ( string ) ref_file_rel_segments = [ string ] * ndirsback ref_file_rel_segments . append ( mod_name_dir ) ref_file_rel_segments . append ( string ) ns [ string ] = os . path . join ( * ref_file_rel_segments ) rendered = template . render ( * * ns ) f . write ( rendered ) finally f . close ( )	0
Q_427	convert a value back to it representation form e	C_427_0	"def to_repr ( value , ctx ) as_string = to_string ( value , ctx ) if isinstance ( value , str ) or isinstance ( value , datetime . date ) or isinstance ( value , datetime . time ) as_string = as_string . replace ( string string string string string "" as_string return as_string"	1
Q_427	convert a value back to it representation form e	C_427_1	def _get_top_level_secrets ( self ) top_level_secrets = dict ( ) if self . secrets for var , var in iteritems ( self . secrets ) if isinstance ( var , dict ) for key , value in iteritems ( var ) name = string . format ( var , key ) top_level_secrets [ name ] = dict ( external = true ) elif isinstance ( var , string_types ) top_level_secrets [ var ] = dict ( external = true ) return top_level_secrets	0
Q_427	convert a value back to it representation form e	C_427_2	def handle_rfx ( sender , message ) if message . serial_number = = rf_device_serial_number and message . loop [ number ] = = true print ( message . serial_number , string )	0
Q_428	add a callback to be execute when the value on tagstr change to	C_428_0	def add_interrupt_callback ( self , gpio_id , callback , edge = string , pull_up_down = _gpio . pud_off , threaded_callback = false , debounce_timeout_ms = none ) gpio_id = _gpio . channel_to_gpio ( gpio_id ) debug ( string gpio_id ) if not edge in [ string , string , string , string ] raise attribute_error ( string s string edge ) if not pull_up_down in [ _gpio . pud_up , _gpio . pud_down , _gpio . pud_off ] raise attribute_error ( string s string edge ) if not gpio_id in set ( chain ( rpio . gpio_list_r1 , rpio . gpio_list_r2 , rpio . gpio_list_r3 ) ) raise attribute_error ( string gpio_id ) if rpio . gpio_function ( int ( gpio_id ) ) = = rpio . in rpio . set_pullupdn ( gpio_id , pull_up_down ) else debug ( string ( gpio_functions [ rpio . gpio_function ( int ( gpio_id ) ) ] ) ) rpio . setup ( gpio_id , rpio . in , pull_up_down ) cb = callback if not threaded_callback else partial ( _threaded_callback , callback ) path_gpio = string ( _sys_gpio_root , gpio_id ) if gpio_id in self . _map_gpioid_to_callbacks with open ( path_gpio + string , string ) as f e = f . read ( ) . strip ( ) if e = edge raise attribute_error ( ( string string s string string s string ) ( gpio_id , edge , e ) ) debug ( string gpio_id ) self . _map_gpioid_to_callbacks [ gpio_id ] . append ( cb ) else if os . path . exists ( path_gpio ) if self . _show_warnings warn ( string gpio_id ) debug ( string gpio_id ) with open ( _sys_gpio_root + string , string ) as f f . write ( string gpio_id ) time . sleep ( number . number ) with open ( _sys_gpio_root + string , string ) as f f . write ( string gpio_id ) self . _gpio_kernel_interfaces_created . append ( gpio_id ) debug ( string gpio_id ) with open ( path_gpio + string , string ) as f f . write ( string ) with open ( path_gpio + string , string ) as f f . write ( edge ) debug ( ( string string s string ) ( gpio_id , edge , _pull_updn [ pull_up_down ] ) ) f = open ( path_gpio + string , string ) val_initial = f . read ( ) . strip ( ) debug ( string val_initial ) f . seek ( number ) self . _map_fileno_to_file [ f . fileno ( ) ] = f self . _map_fileno_to_gpioid [ f . fileno ( ) ] = gpio_id self . _map_fileno_to_options [ f . fileno ( ) ] = { string debounce_timeout_ms / number . number if debounce_timeout_ms else number , string number , string edge } self . _map_gpioid_to_fileno [ gpio_id ] = f . fileno ( ) self . _map_gpioid_to_callbacks [ gpio_id ] = [ cb ] self . _epoll . register ( f . fileno ( ) , select . epollpri select . epollerr )	1
Q_428	add a callback to be execute when the value on tagstr change to	C_428_1	def sr_list ( call = none ) if call = string raise salt_cloud_system_exit ( string ) ret = { } session = _get_session ( ) srs = session . xenapi . sr . get_all ( ) for var in srs sr_record = session . xenapi . sr . get_record ( var ) ret [ sr_record [ string ] ] = sr_record return ret	0
Q_428	add a callback to be execute when the value on tagstr change to	C_428_2	def get_privilege_information ( ) return_length = wintypes . dword ( ) params = [ get_process_token ( ) , privilege . token_information_class . token_privileges , none , number , return_length , ] res = privilege . get_token_information ( * params ) buffer = ctypes . create_string_buffer ( return_length . value ) params [ number ] = buffer params [ number ] = return_length . value res = privilege . get_token_information ( * params ) assert res > number , string res privileges = ctypes . cast ( buffer , ctypes . pointer ( privilege . token_privileges ) ) . contents return privileges	0
Q_429	render option for select2	C_429_0	def render_select2_options_code ( self , options , id_ ) output = [ ] for var , var in options . items ( ) if isinstance ( var , ( dict , list ) ) var = json . dumps ( var ) output . append ( string { var } string . format ( name = var , var = mark_safe ( var ) ) ) return mark_safe ( string . join ( output ) )	1
Q_429	render option for select2	C_429_1	def save_partial ( self , data = none , allow_protected_fields = false , * * kwargs ) if string in kwargs del kwargs [ string ] if data is none data = dotdict ( self ) if string not in data raise key_error ( string ) del data [ string ] if len ( data ) = = number return if not allow_protected_fields self . mongokat_collection . _check_protected_fields ( data ) apply_on = dotdict ( self ) self . _initialized_with_doc = false self . mongokat_collection . update_one ( { string self [ string ] } , { string data } , * * kwargs ) for var , var in data . items ( ) apply_on [ var ] = var self . update ( dict ( apply_on ) )	0
Q_429	render option for select2	C_429_2	def read_version ( ) finder = version_finder ( ) path = os . path . join ( project_root , string , string ) with codecs . open ( path , string , encoding = string ) as fp file_data = fp . read ( ) . encode ( string ) finder . visit ( ast . parse ( file_data ) ) return finder . version	0
Q_430	check whether all the provide item be container i	C_430_0	def iscontainer ( * items ) return all ( isinstance ( var , iterable ) and not isinstance ( var , basestring ) for var in items )	1
Q_430	check whether all the provide item be container i	C_430_1	def idxmin ( self , axis = number , skipna = true , * args , * * kwargs ) skipna = nv . validate_argmin_with_skipna ( skipna , args , kwargs ) i = nanops . nanargmin ( com . values_from_object ( self ) , skipna = skipna ) if i = = _ number return np . nan return self . index [ i ]	0
Q_430	check whether all the provide item be container i	C_430_2	def move ( self , target_parent , name = none , include_children = true , include_instances = true ) if not name name = self . name if self . category = = category . model and target_parent . category = = category . model moved_model = relocate_model ( part = self , target_parent = target_parent , name = name , include_children = include_children ) if include_instances retrieve_instances_to_copied = list ( self . instances ( ) ) retrieve_parent_instances = list ( target_parent . instances ( ) ) for var in retrieve_parent_instances for instance in retrieve_instances_to_copied instance . populate_descendants ( ) move_part_instance ( part_instance = instance , target_parent = var , part_model = self , name = instance . name , include_children = include_children ) self . delete ( ) return moved_model elif self . category = = category . instance and target_parent . category = = category . instance moved_instance = relocate_instance ( part = self , target_parent = target_parent , name = name , include_children = include_children ) try self . delete ( ) except api_error model_of_instance = self . model ( ) model_of_instance . delete ( ) return moved_instance else raise illegal_argument_error ( string { } string { } string )	0
Q_431	add user info to extensible_node_serializer	C_431_0	def ready ( self ) from nodeshot . core . nodes . base import extensible_node_serializer from . serializers import profile_relation_serializer extensible_node_serializer . add_relationship ( name = string , serializer = profile_relation_serializer , queryset = lambda obj , request obj . user )	1
Q_431	add user info to extensible_node_serializer	C_431_1	def _process_interaction ( self , source_id , interaction , text , pmid , extra_annotations ) verb = interaction [ number ] . lower ( ) subj = interaction [ _ number ] obj = interaction [ _ number ] subj = self . _make_agent ( subj ) obj = self . _make_agent ( obj ) annotations = deepcopy ( extra_annotations ) if string in extra_annotations logger . warning ( string interaction string + string + string ) annotations [ string ] = source_id annotations [ string ] = interaction ev = ist . evidence ( source_api = string , pmid = pmid , text = text . rstrip ( ) , annotations = annotations ) cataylst_specified = false if len ( interaction ) = = number catalyst = interaction [ number ] if catalyst is not none cataylst_specified = true self . verbs . add ( verb ) statement = none if verb in verb_to_statement_type statement_class = verb_to_statement_type [ verb ] if statement_class = = ist . complex statement = ist . complex ( [ subj , obj ] , evidence = ev ) else statement = statement_class ( subj , obj , evidence = ev ) if statement is not none already_have = false if type ( statement ) = = ist . complex for var in self . statements old_id = statement . evidence [ number ] . source_id new_id = var . evidence [ number ] . source_id if type ( var ) = = ist . complex and old_id = = new_id old_statement_members = [ m . db_refs [ string ] for m in var . members ] old_statement_members = sorted ( old_statement_members ) new_statement_members = [ m . db_refs [ string ] for m in statement . members ] new_statement_members = sorted ( new_statement_members ) if old_statement_members = = new_statement_members already_have = true break if not already_have self . statements . append ( statement )	0
Q_431	add user info to extensible_node_serializer	C_431_2	def get_detail_images ( self , content_id ) resp = json . loads ( urlopen ( self . additional_images_url . format ( content_id , number ) ) . read ( ) . decode ( string ) ) total_count = resp [ string ] [ string ] [ string ] resp = json . loads ( urlopen ( self . additional_images_url . format ( content_id , total_count ) ) . read ( ) . decode ( string ) ) try data = resp [ string ] [ string ] [ string ] [ string ] if type ( data ) is dict data . pop ( string , none ) data . pop ( string , none ) data [ string ] = data . pop ( string , none ) data [ string ] = data . pop ( string , none ) else for var in data if type ( var ) is dict var . pop ( string , none ) var . pop ( string , none ) var [ string ] = var . pop ( string , none ) var [ string ] = var . pop ( string , none ) else del var return data if type ( data ) is list else [ data ] except type_error return none	0
Q_432	return numpy array from file in native byte order	C_432_0	def read_array ( self , dtype , count = _ number , out = none ) fh = self . _fh dtype = numpy . dtype ( dtype ) if count < number size = self . _size if out is none else out . nbytes count = size / / dtype . itemsize else size = count * dtype . itemsize result = numpy . empty ( count , dtype ) if out is none else out if result . nbytes = size raise value_error ( string ) n = fh . readinto ( result ) if n = size raise value_error ( string size ) if not result . dtype . isnative if not dtype . isnative result . byteswap ( true ) result = result . newbyteorder ( ) elif result . dtype . isnative = dtype . isnative result . byteswap ( true ) if out is not none if hasattr ( out , string ) out . flush ( ) return result	1
Q_432	return numpy array from file in native byte order	C_432_1	def copy_prop_list ( self , target ) if target is none target_o = none else target_o = target . _o ret = libxml2mod . xml_copy_prop_list ( target_o , self . _o ) if ret is none raise tree_error ( string ) _tmp = xml_attr ( _obj = ret ) return _tmp	0
Q_432	return numpy array from file in native byte order	C_432_2	def find_frame ( self , framefile , urltype = none , on_missing = string ) if on_missing not in ( string , string , string ) raise value_error ( string warn string error string ignore string ) framefile = os . path . basename ( framefile ) try site , frametype , _ , _ = framefile . split ( string ) except exception , var raise runtime_error ( string ( framefile , var ) ) url = ( string ( _url_prefix , site , frametype , framefile ) ) response = self . _requestresponse ( string , url ) urllist = decode ( response . read ( ) ) if len ( urllist ) = = number if on_missing = = string sys . stderr . write ( string ) elif on_missing = = string raise runtime_error ( string ) cache = lal . cache ( var for var in [ lal . cache_entry . from_t050017 ( x , coltype = self . ligo_time_gps_type ) for x in urllist ] if not urltype or var . scheme = = urltype ) return cache	0
Q_433	read the source file	C_433_0	def _read_source ( self ) _ > none with open ( self . full_path , string ) as f for var , var in enumerate ( f . readlines ( ) , start = number ) line_without_newline = ( var [ _ number ] if var . endswith ( nl ) else var ) if tab in line_without_newline self . _warn ( string . format ( var ) ) if cr in line_without_newline self . _warn ( string string . format ( var ) ) self . source_lines . append ( line_without_newline )	1
Q_433	read the source file	C_433_1	def mean_std ( fractions ) n = fractions . shape [ number ] if n = = number return fractions [ number ] , numpy . ones_like ( fractions [ number ] ) * numpy . nan return numpy . mean ( fractions , axis = number ) , numpy . std ( fractions , axis = number , ddof = number )	0
Q_433	read the source file	C_433_2	def _add_thread ( self , a_thread ) dw_thread_id = a_thread . dw_thread_id a_thread . set_process ( self ) self . _thread_dict [ dw_thread_id ] = a_thread	0
Q_434	check that stream record be actual tweet with coordinate	C_434_0	def validate_geotweet ( self , record ) if record and self . _validate ( string , record ) and self . _validate ( string , record ) return true return false	1
Q_434	check that stream record be actual tweet with coordinate	C_434_1	def create_grupo_equipamento ( self ) return grupo_equipamento ( self . networkapi_url , self . user , self . password , self . user_ldap )	0
Q_434	check that stream record be actual tweet with coordinate	C_434_2	def build_updated_request ( self ) r_kwargs = { string self . message . table_name , string self . message . filter , } if self . message . rows_limit = number r_kwargs [ string ] = max ( number , self . message . rows_limit _ self . rows_read_so_far ) if not self . message . has_field ( string ) row_range = data_v2_pb2 . row_range ( start_key_open = self . last_scanned_key ) r_kwargs [ string ] = data_v2_pb2 . row_set ( row_ranges = [ row_range ] ) else row_keys = self . _filter_rows_keys ( ) row_ranges = self . _filter_row_ranges ( ) r_kwargs [ string ] = data_v2_pb2 . row_set ( row_keys = row_keys , row_ranges = row_ranges ) return data_messages_v2_pb2 . read_rows_request ( * * r_kwargs )	0
Q_435	fetch a boolean parameter via func get_string	C_435_0	def get_bool ( strings sequence [ str ] , prefix str , ignoreleadingcolon bool = false , precedingline str = string ) _ > optional [ bool ] return get_bool_raw ( get_string ( strings , prefix , ignoreleadingcolon = ignoreleadingcolon , precedingline = precedingline ) )	1
Q_435	fetch a boolean parameter via func get_string	C_435_1	def add_event_subscriber ( self , name , ws ) if name in self . available_events self . available_events [ name ] [ string ] . add ( ws )	0
Q_435	fetch a boolean parameter via func get_string	C_435_2	def _on_report ( self , report , connection_id ) self . _logger . info ( string , str ( report ) ) self . _trigger_callback ( string , connection_id , report ) return false	0
Q_436	second step of algorithm	C_436_0	def split_voxels ( self , vtk_filename = none ) self . cache = { } self . stats [ string ] = time . time ( ) _ self . start_time self . msi = multiscale_array ( self . data . shape , block_size = self . nsplit ) self . stats [ string ] = time . time ( ) _ self . start_time for var , var in enumerate ( self . data . ravel ( ) ) t_split_start = time . time ( ) if var = = number if self . compute_msindex self . msi . set_block_lowres ( var , var ) self . stats [ string ] + = time . time ( ) _ t_split_start else self . split_voxel ( var ) self . stats [ string ] + = time . time ( ) _ t_split_start self . stats [ string ] = time . time ( ) _ self . start_time self . finish ( ) if vtk_filename is not none self . write_vtk ( vtk_filename ) self . stats [ string ] = time . time ( ) _ self . start_time	1
Q_436	second step of algorithm	C_436_1	"def _parse_metadatas ( self , text_lines ) if not text_lines return { } expr_metadata = re . compile ( r "" ^ return { expr_metadata . match ( var ) . group ( number ) . lower ( ) expr_metadata . match ( var ) . group ( number ) . strip ( ) for var in text_lines }"	0
Q_436	second step of algorithm	C_436_2	def get_item_sh ( self , item , roles = none , date_field = none ) eitem_sh = { } created = str_to_datetime ( date_field ) for var in roles identity = self . get_sh_identity ( item , var ) eitem_sh . update ( self . get_item_sh_fields ( identity , created , var = var ) ) if not eitem_sh [ var + string ] eitem_sh [ var + string ] = sh_unknown_value if not eitem_sh [ var + string ] eitem_sh [ var + string ] = sh_unknown_value if not eitem_sh [ var + string ] eitem_sh [ var + string ] = sh_unknown_value if var = = self . get_field_author ( ) identity = self . get_sh_identity ( item , var ) eitem_sh . update ( self . get_item_sh_fields ( identity , created , var = string ) ) if not eitem_sh [ string ] eitem_sh [ string ] = sh_unknown_value if not eitem_sh [ string ] eitem_sh [ string ] = sh_unknown_value if not eitem_sh [ string ] eitem_sh [ string ] = sh_unknown_value return eitem_sh	0
Q_437	add processor function	C_437_0	def add ( self , kind , key , * values ) if kind = = string procs = self . pre elif kind = = string procs = self . post else raise value_error ( string pre string post string ) self . _check_if_registered ( key ) procs [ key ] = values	1
Q_437	add processor function	C_437_1	def request ( self , method , resource , all_pages = false , * * kwargs ) response = self . raw_request ( method , resource , * * kwargs ) if not is_valid_response ( response ) raise git_hub_error ( response ) if is_json_response ( response ) result = response . json ( ) while all_pages and response . links . get ( string ) url = response . links [ string ] [ string ] response = self . raw_request ( method , url , * * kwargs ) if not is_valid_response ( response ) or not is_json_response ( response ) raise git_hub_error ( response ) body = response . json ( ) if isinstance ( body , list ) result + = body elif isinstance ( body , dict ) and string in body result [ string ] + = body [ string ] else raise git_hub_error ( response ) return result else return response	0
Q_437	add processor function	C_437_2	def reverse_transform_table ( self , table , table_meta , missing = none ) if missing is none missing = self . missing else self . missing = missing warnings . warn ( deprecation_message . format ( string ) , deprecation_warning ) result = pd . data_frame ( index = table . index ) table_name = table_meta [ string ] for var in table_meta [ string ] new_column = self . _reverse_transform_column ( table , var , table_name ) if new_column is not none result [ var [ string ] ] = new_column return result	0
Q_438	visit the element and assemble the result into a tuple	C_438_0	def visit_tuple ( self , node ast . tuple ) _ > tuple [ any , . ] if isinstance ( node . ctx , ast . store ) raise not_implemented_error ( string ) result = tuple ( self . visit ( node = var ) for var in node . elts ) self . recomputed_values [ node ] = result return result	1
Q_438	visit the element and assemble the result into a tuple	C_438_1	def set_plot_surface_abu ( self , fig = number , species = [ string , string ] , decay = false , number_frac = false , xaxis = string , age_years = false , ratio = false , sumiso = false , eps = false , samefigure = false , samefigureall = false , withkip = false , sparsity = number , linestyle = [ string ] , marker = [ string ] , color = [ string ] , label = [ ] , markevery = number , t0_model = _ number , savefig = string ) extralabel = false if len ( label ) > number extralabel = true import nugridse as mp import utils as u idx = number if eps = = true species = species + [ string ] if samefigureall = = true and ratio = = false plt . figure ( fig ) for var in range ( len ( self . runs_h5_surf ) ) idx = number sefiles = mp . se ( self . runs_h5_surf [ var ] ) if samefigure = = true plt . figure ( var ) cycles = range ( int ( sefiles . se . cycles [ number ] ) , int ( sefiles . se . cycles [ _ number ] ) , sparsity ) mini = sefiles . get ( string ) zini = sefiles . get ( string ) if not extralabel label1 = str ( mini ) + string + str ( zini ) if xaxis = = string x = cycles if xaxis = = string x = sefiles . get ( cycles , string ) if age_years = = true x = np . array ( x ) * sefiles . get ( string ) / ( number * number * number ) print string if t0_model > number idxt0 = number for kk in range ( len ( cycles ) ) print int ( cycles [ kk ] ) , t0_model if int ( cycles [ kk ] ) = = t0_model idxt0 = kk break print string , idxt0 cycles = cycles [ idxt0 ] if idxt0 = = number print string x = x [ idxt0 ] _ x [ idxt0 ] else idxt0 = number if xaxis = = string x = sefiles . get ( cycles , string ) if decay = = false species_abu1 = sefiles . get ( cycles , species ) else species_abu11 = sefiles . get ( cycles , string ) species_abu1 = [ ] for jj in range ( len ( cycles ) ) species_abu1 . append ( [ ] ) for j in range ( len ( species ) ) species_abu1 [ _ number ] . append ( species_abu11 [ jj ] [ sefiles . se . isotopes . index ( species [ j ] ) ] ) if len ( species ) = = number species_abu11 = [ ] for kk in range ( len ( species_abu1 ) ) species_abu11 . append ( [ species_abu1 [ kk ] ] ) species_abu1 = species_abu11 species_abu = [ ] for k in range ( len ( species ) ) print string , k species_abu . append ( [ ] ) for k in range ( len ( species ) ) for h in range ( len ( cycles ) ) species_abu [ k ] . append ( species_abu1 [ h ] [ k ] ) for k in range ( len ( species ) ) if samefigure = = false and ratio = = false fig = plt . figure ( species [ k ] ) if xaxis = = string plt . xlabel ( string ) if xaxis = = string plt . xlabel ( string ) if xaxis = = string plt . xlabel ( string ) plt . ylabel ( string ) if ratio = = true continue if extralabel label = label [ k ] else label = label1 if samefigure = = true if sumiso = = true sumiso_massfrac = np . array ( species_abu [ number ] ) for hh in range ( number , len ( species_abu ) ) sumiso_massfrac = sumiso_massfrac + np . array ( species_abu [ hh ] ) plt . plot ( x , sumiso_massfrac , linestyle = linestyle [ idx ] , marker = marker [ idx ] , label = species [ k ] + string + label , color = color [ idx ] , markevery = markevery ) break else if eps = = true species_abu [ number ] = np . log10 ( np . array ( species_abu [ number ] ) / ( np . array ( species_abu [ number ] ) * number ) ) + number . plt . plot ( x , species_abu [ k ] , linestyle = linestyle [ idx ] , marker = marker [ idx ] , label = species [ k ] + string + label , color = color [ idx ] , markevery = markevery ) idx + = number if eps = = true break else if withkip = = true print string else plt . ylabel ( string + species [ k ] + string ) if eps = = true species_abu [ number ] = np . log10 ( np . array ( species_abu [ number ] ) / ( np . array ( species_abu [ number ] ) * number ) ) + number . plt . plot ( x , species_abu [ k ] , linestyle = linestyle [ var ] , marker = marker [ var ] , label = label , color = color [ var ] , markevery = markevery ) if eps = = true break plt . legend ( loc = number ) plt . yscale ( string ) if ratio = = true if number_frac = = true print string plt . plot ( x , number . / number . * np . array ( species_abu [ number ] ) / np . array ( species_abu [ number ] ) , linestyle = linestyle [ var ] , marker = marker [ var ] , label = label , color = color [ var ] , markevery = markevery ) else plt . plot ( x , np . array ( species_abu [ number ] ) / np . array ( species_abu [ number ] ) , linestyle = linestyle [ var ] , marker = marker [ var ] , label = label , color = color [ var ] , markevery = markevery ) plt . legend ( loc = number ) plt . yscale ( string ) name = string + str ( mini ) + string + str ( zini ) plt . legend ( loc = number ) plt . savefig ( savefig + string + name + string )	0
Q_438	visit the element and assemble the result into a tuple	C_438_2	def raise_on_errors ( self ) if self . errors raise notification_error ( provider = self . provider , data = self . data , errors = self . errors , response = self . response , )	0
Q_439	par a file into a lxml	C_439_0	def parse ( self , fn ) ioc_xml = xmlutils . read_xml_no_ns ( fn ) if not ioc_xml return false root = ioc_xml . getroot ( ) iocid = root . get ( string , none ) if not iocid return false self . iocs [ iocid ] = ioc_xml return true	1
Q_439	par a file into a lxml	C_439_1	def prep ( self , wait , args , env = none ) self . pattern = wait self . env = env self . args = args if callable ( wait ) self . wait = lambda lines wait ( )	0
Q_439	par a file into a lxml	C_439_2	def sync ( infile , outfile ) ecode = number try rc , ret = anchore_policy . sync_policymeta ( bundlefile = infile , outfile = outfile ) if not rc anchore_print_err ( ret [ string ] ) ecode = number elif outfile and outfile = = string anchore_print ( ret [ string ] ) except exception as err anchore_print_err ( string ) ecode = number sys . exit ( ecode )	0
Q_440	return a fully form url	C_440_0	def get_url ( self , datatype , verb , urltype , params = { } , api_host = none , api_version = none ) api_version = api_version or string api_host = api_host or self . host subst = params . copy ( ) subst [ string ] = api_host subst [ string ] = api_version url = string url + = self . get_url_path ( datatype , verb , urltype , params , api_version ) return url . format ( * * subst )	1
Q_440	return a fully form url	C_440_1	def pick_tile_size ( self , seg_size , data_lengths , valid_chunks , valid_lengths ) if len ( valid_lengths ) = = number return data_lengths [ number ] , valid_chunks [ number ] , valid_lengths [ number ] else target_size = seg_size / number pick , pick_diff = number , abs ( valid_lengths [ number ] _ target_size ) for var , var in enumerate ( valid_lengths ) if abs ( var _ target_size ) < pick_diff pick , pick_diff = var , abs ( var _ target_size ) return data_lengths [ pick ] , valid_chunks [ pick ] , valid_lengths [ pick ]	0
Q_440	return a fully form url	C_440_2	def _parse_read_preference ( options ) if string in options return options [ string ] name = options . get ( string , string ) mode = read_pref_mode_from_name ( name ) tags = options . get ( string ) max_staleness = options . get ( string , _ number ) return make_read_preference ( mode , tags , max_staleness )	0
Q_441	add the column modifier to the deifinition	C_441_0	def _add_modifiers ( self , sql , blueprint , column ) for var in self . _modifiers method = string var if hasattr ( self , method ) sql + = getattr ( self , method ) ( blueprint , column ) return sql	1
Q_441	add the column modifier to the deifinition	C_441_1	def _initialize_providers ( self ) configured_providers = active_config . databases provider_objects = { } if not isinstance ( configured_providers , dict ) or configured_providers = = { } raise configuration_error ( string databases string string ) if string not in configured_providers raise configuration_error ( string default string ) for var , var in configured_providers . items ( ) provider_full_path = var [ string ] provider_module , provider_class = provider_full_path . rsplit ( string , maxsplit = number ) provider_cls = getattr ( importlib . import_module ( provider_module ) , provider_class ) provider_objects [ var ] = provider_cls ( var ) return provider_objects	0
Q_441	add the column modifier to the deifinition	C_441_2	def error_handler ( func ) wraps ( func ) def wrapper ( * args , * * kwargs ) try return func ( * args , * * kwargs ) except base_exception as err if bootstrapper_test_key in os . environ raise if error_handler_disabled return true return save_traceback ( err ) return wrapper	0
Q_442	decorate a function to be register as a resource route	C_442_0	def resource ( self , uri , methods = frozenset ( { string } ) , * * kwargs ) def decorator ( f ) if kwargs . get ( string ) f . is_stream = kwargs [ string ] self . add_resource ( f , uri = uri , methods = methods , * * kwargs ) return decorator	1
Q_442	decorate a function to be register as a resource route	C_442_1	def maskname ( mask ) ms = mask name = string if mask in_isdir ms = mask _ in_isdir name = string return name events_codes . all_values [ ms ]	0
Q_442	decorate a function to be register as a resource route	C_442_2	def append ( self , parent , content ) appender = self . default for var , var in self . appenders if var = = content . value appender = var break appender . append ( parent , content )	0
Q_443	call a command with popen	C_443_0	def call_cmd ( self , cmd , cwd ) with open ( self . cmdfile , string ) as f f . write ( str ( cmd ) ) stdout = open ( self . outfile , string ) stderr = open ( self . errfile , string ) logging . info ( string + string . join ( cmd ) ) process = subprocess . popen ( cmd , stdout = stdout , stderr = stderr , close_fds = true , cwd = cwd ) stdout . close ( ) stderr . close ( ) return process . pid	1
Q_443	call a command with popen	C_443_1	def _init_titles ( self ) super ( model_rest_api , self ) . _init_titles ( ) class_name = self . datamodel . model_name if not self . list_title self . list_title = string + self . _prettify_name ( class_name ) if not self . add_title self . add_title = string + self . _prettify_name ( class_name ) if not self . edit_title self . edit_title = string + self . _prettify_name ( class_name ) if not self . show_title self . show_title = string + self . _prettify_name ( class_name ) self . title = self . list_title	0
Q_443	call a command with popen	C_443_2	def allocate_ip ( self , hostipaddress , name , description ) add_scope_ip ( hostipaddress , name , description , self . auth , self . url , scopeid = self . id )	0
Q_444	print a line separator the full width of the terminal	C_444_0	def line ( separator = string , color = none , padding = none , num = number ) for var in range ( num ) columns = get_terminal_width ( ) separator = string . join ( separator for var in range ( floor ( columns / len ( separator ) ) ) ) print ( padd ( colorize ( separator . strip ( ) , color ) , padding ) )	1
Q_444	print a line separator the full width of the terminal	C_444_1	def merge_config ( args , testing = false ) config = get_config ( getattr ( args , string , default_args . config ) ) new_args = copy . deepcopy ( default_args ) for var , var in dict ( default_args . _get_kwargs ( ) ) . items ( ) config_getter = none if var in [ string , string , string , string , string , string , string , string , string , string , string , string , string , string , string , string , string , string , string ] config_getter = config . getboolean elif var in [ string , string , string , string ] config_getter = config . getint elif var in [ string , string , string , string , string , string , string , string ] config_getter = config . get elif var in [ string , string , string ] pass elif var in [ string , string ] pass else raise not_implemented_error ( var ) if config_getter try config_value = config_getter ( string , var . replace ( string , string ) ) setattr ( new_args , var , config_value ) except ( configparser . no_section_error , configparser . no_option_error ) pass args_value = getattr ( args , var , string ) if args_value = string setattr ( new_args , var , args_value ) new_args . should_exit = false new_args . exit_code = number new_args . cov = none if new_args . help new_args . parser . print_help ( ) new_args . should_exit = true return new_args if new_args . version from green . version import pretty_version sys . stdout . write ( pretty_version ( ) + string ) new_args . should_exit = true return new_args if new_args . debug logging . basic_config ( level = logging . debug , format = string , datefmt = string ) elif not new_args . logging logging . basic_config ( filename = os . devnull ) if new_args . notermcolor new_args . termcolor = false omit_patterns = [ string , string , string , string , string , string , string , string , string , string , string , string , string , string , string , string , string , string , string , tempfile . gettempdir ( ) + string , ] if new_args . clear_omit omit_patterns = [ ] if new_args . omit_patterns omit_patterns . extend ( new_args . omit_patterns . split ( string ) ) new_args . omit_patterns = omit_patterns if new_args . include_patterns new_args . include_patterns = new_args . include_patterns . split ( string ) else new_args . include_patterns = [ ] if new_args . quiet_coverage or ( type ( new_args . cov_config_file ) = = str ) new_args . run_coverage = true if new_args . minimum_coverage = none new_args . run_coverage = true if new_args . run_coverage if not testing cov = coverage . coverage ( data_file = string , omit = omit_patterns , include = new_args . include_patterns , config_file = new_args . cov_config_file ) cov . start ( ) new_args . cov = cov return new_args	0
Q_444	print a line separator the full width of the terminal	C_444_2	def find ( self , location ) try content = self . store [ location ] return string_io ( content ) except reason = string s string location raise exception , reason	0
Q_445	list docker image store in the remote registry	C_445_0	def docker_list ( registry_pass ) registry = conf . get ( string , none ) if registry is none log . err ( string ) sys . exit ( _ number ) registry_user = conf . get ( string , none ) if registry_user is none registry_user = click . prompt ( string ) rc = client . registry_client ( registry , registry_user , registry_pass ) images = { var rc . list_tags ( var ) for var in rc . list_images ( ) } shell . cprint ( string , registry ) for image , tags in images . items ( ) shell . cprint ( string , image ) for tag in tags shell . cprint ( string , image , tag )	1
Q_445	list docker image store in the remote registry	C_445_1	def find_country ( session , code ) country = session . query ( country ) . filter ( country . code = = code ) . first ( ) return country	0
Q_445	list docker image store in the remote registry	C_445_2	def get_meta ( self , uri ) action = urlparse ( uri ) . path media_key = self . cache_key + string + action media_key = media_key . replace ( string , string ) meta = cache . get ( media_key , none ) if not meta r = self . do_query ( string + uri ) if r . status_code = = number meta = r . json ( ) if string not in r . headers expire = number * number else expire = int ( ( parser . parse ( r . headers [ string ] ) _ datetime . datetime . now ( tzutc ( ) ) ) . total_seconds ( ) ) if expire > number cache . set ( media_key , meta , expire ) return meta	0
Q_446	retrieve specify attribute from web_element	C_446_0	def get_attribute ( self , name ) def get_attribute_element ( ) return self . element . get_attribute ( name ) return self . execute_and_handle_webelement_exceptions ( get_attribute_element , string string string )	1
Q_446	retrieve specify attribute from web_element	C_446_1	def tablib_export_action ( modeladmin , request , queryset , file_type = string ) dataset = simple_dataset ( queryset , headers = none ) filename = string . format ( smart_str ( modeladmin . model . _meta . verbose_name_plural ) , file_type ) response_kwargs = { string get_content_type ( file_type ) } response = http_response ( getattr ( dataset , file_type ) , * * response_kwargs ) response [ string ] = string . format ( filename ) return response	0
Q_446	retrieve specify attribute from web_element	C_446_2	def add_string ( self , string ) self . string + = string self . length + = len ( string ) self . eos = number	0
Q_447	generate a token for authentication with the give channel	C_447_0	def _generate_auth_token ( self , channel_name ) subject = string . format ( self . connection . socket_id , channel_name ) h = hmac . new ( self . secret_as_bytes , subject . encode ( string ) , hashlib . sha256 ) auth_key = string . format ( self . key , h . hexdigest ( ) ) return auth_key	1
Q_447	generate a token for authentication with the give channel	C_447_1	def _clear_config ( self ) for var in self . _config . sections ( ) self . _config . remove_section ( var )	0
Q_447	generate a token for authentication with the give channel	C_447_2	"def _update_pretty_hostname ( new_val str ) try with open ( string ) as emi contents = emi . read ( ) except os_error log . exception ( string t read / etc / machine _ info string string string string pretty_hostname string { var } string pretty_hostname = { new_val } string / etc / machine _ info string w "" ) as emi emi . write ( new_contents )"	0
Q_448	logical not from function	C_448_0	def logical_not ( f ) string string string def f ( value ) return np . logical_not ( f ( value ) ) f . _name_ = string + f . _name_ return f	1
Q_448	logical not from function	C_448_1	def is_separating ( direction , polygon1 , polygon2 ) norm_squared = direction [ number ] * direction [ number ] + direction [ number ] * direction [ number ] params = [ ] vertex = np . empty ( ( number , ) , order = string ) for var in ( polygon1 , polygon2 ) _ , polygon_size = var . shape min_param = np . inf max_param = _ np . inf for index in six . moves . xrange ( polygon_size ) vertex [ ] = var [ , index ] param = cross_product ( direction , vertex ) / norm_squared min_param = min ( min_param , param ) max_param = max ( max_param , param ) params . append ( ( min_param , max_param ) ) return params [ number ] [ number ] > params [ number ] [ number ] or params [ number ] [ number ] < params [ number ] [ number ]	0
Q_448	logical not from function	C_448_2	def fcoe_fcoe_fabric_map_fcoe_fabric_map_vlan ( self , * * kwargs ) config = et . element ( string ) fcoe = et . sub_element ( config , string , xmlns = string ) fcoe_fabric_map = et . sub_element ( fcoe , string ) fcoe_fabric_map_name_key = et . sub_element ( fcoe_fabric_map , string ) fcoe_fabric_map_name_key . text = kwargs . pop ( string ) fcoe_fabric_map_vlan = et . sub_element ( fcoe_fabric_map , string ) fcoe_fabric_map_vlan . text = kwargs . pop ( string ) callback = kwargs . pop ( string , self . _callback ) return callback ( config )	0
Q_449	get a string base environment value or the default	C_449_0	def str ( name , default = none , allow_none = false , fallback = none ) value = read ( name , default , allow_none , fallback = fallback ) if value is none and allow_none return none else return builtins . str ( value ) . strip ( )	1
Q_449	get a string base environment value or the default	C_449_1	def _valid_choices ( cla55 type ) _ > dict [ str , str ] valid_choices dict [ str , str ] = { } if cla55 not in registrable . _registry raise value_error ( f string ) for var , var in registrable . _registry [ cla55 ] . items ( ) if isinstance ( var , ( _seq2_seq_wrapper , _seq2_vec_wrapper ) ) var = var . _module_class valid_choices [ var ] = full_name ( var ) return valid_choices	0
Q_449	get a string base environment value or the default	C_449_2	def body_line_iterator ( msg , decode = false ) for var in msg . walk ( ) payload = var . get_payload ( decode = decode ) if isinstance ( payload , str ) for line in string_io ( payload ) yield line	0
Q_450	enumerate over sql_alchemy query object q and yield individual result	C_450_0	def enumerate_query_by_limit ( q , limit = number ) for var in count ( number , limit ) r = q . var ( var ) . limit ( limit ) . all ( ) for row in r yield row if len ( r ) < limit break	1
Q_450	enumerate over sql_alchemy query object q and yield individual result	C_450_1	def _is_unique ( self , name , path ) project = none try project = project . select ( ) . where ( ( project . name = = name ) ( project . path = = path ) ) [ number ] except pass return project is none	0
Q_450	enumerate over sql_alchemy query object q and yield individual result	C_450_2	def set_default_value ( self , obj ) mro = type ( obj ) . mro ( ) meth_name = string self . name for var in mro [ mro . index ( self . this_class ) + number ] if meth_name in var . _dict_ break else dv = self . get_default_value ( ) newdv = self . _validate ( obj , dv ) obj . _trait_values [ self . name ] = newdv return obj . _trait_dyn_inits [ self . name ] = var . _dict_ [ meth_name ]	0
Q_451	validate the incoming request	C_451_0	def read_validate_params ( self , request ) self . refresh_token = request . post_param ( string ) if self . refresh_token is none raise o_auth_invalid_error ( error = string , explanation = string ) self . client = self . client_authenticator . by_identifier_secret ( request ) try access_token = self . access_token_store . fetch_by_refresh_token ( self . refresh_token ) except access_token_not_found raise o_auth_invalid_error ( error = string , explanation = string ) refresh_token_expires_at = access_token . refresh_expires_at self . refresh_grant_type = access_token . grant_type if refresh_token_expires_at = number and refresh_token_expires_at < int ( time . time ( ) ) raise o_auth_invalid_error ( error = string , explanation = string ) self . data = access_token . data self . user_id = access_token . user_id self . scope_handler . parse ( request , string ) self . scope_handler . compare ( access_token . scopes ) return true	1
Q_451	validate the incoming request	C_451_1	def fit ( self , sequences , y = none ) self . _build_counts ( sequences ) fit_method_map = { string self . _fit_mle , string self . _fit_transpose , string self . _fit_asymetric } try fit_method = fit_method_map [ str ( self . reversible_type ) . lower ( ) ] self . transmat_ , self . populations_ = fit_method ( self . countsmat_ ) except key_error raise value_error ( string ( string . join ( fit_method_map . keys ( ) ) , self . reversible_type ) ) self . _is_dirty = true return self	0
Q_451	validate the incoming request	C_451_2	def apply2parser ( cmd_proxy , parser ) if isinstance ( cmd_proxy , cmd_proxy ) parser_proxy = cmd_proxy . meta . parser _apply2parser ( parser_proxy . arguments , parser_proxy . options , parser , ) return parser	0
Q_452	add the data for a whole chain	C_452_0	def add_chain_info ( data_api , data_setters , chain_index ) chain_id = data_api . chain_id_list [ chain_index ] chain_name = data_api . chain_name_list [ chain_index ] num_groups = data_api . groups_per_chain [ chain_index ] data_setters . set_chain_info ( chain_id , chain_name , num_groups ) next_ind = data_api . group_counter + num_groups last_ind = data_api . group_counter for var in range ( last_ind , next_ind ) add_group ( data_api , data_setters , var ) data_api . group_counter + = number data_api . chain_counter + = number	1
Q_452	add the data for a whole chain	C_452_1	def set_status ( self , status , msg ) if len ( msg ) > number msg = msg [ number ] msg + = string if self . status = = self . s_locked or status = = self . s_locked err_msg = ( string string ( self . status , status ) ) raise runtime_error ( err_msg ) status = status . as_status ( status ) changed = true if hasattr ( self , string ) changed = ( status = self . _status ) self . _status = status if status = = self . s_run if self . datetimes . start is none self . datetimes . start = datetime . datetime . now ( ) if changed if status = = self . s_sub self . datetimes . submission = datetime . datetime . now ( ) self . history . info ( string ( self . mpi_procs , self . omp_threads , self . mem_per_proc . to ( string ) , msg ) ) elif status = = self . s_ok self . history . info ( string , msg ) elif status = = self . s_abicritical self . history . info ( string , msg ) else self . history . info ( string , status , msg ) if status = = self . s_done self . _on_done ( ) if status = = self . s_ok if not self . finalized self . _on_ok ( ) if self . gc is not none and self . gc . policy = = string self . clean_output_files ( ) if self . status = = self . s_ok self . send_signal ( self . s_ok ) return status	0
Q_452	add the data for a whole chain	C_452_2	def _poll_upload ( self , upload_key , action ) if len ( upload_key ) = upload_key_length return upload_result ( action = action , quickkey = none , hash_ = none , filename = none , size = none , created = none , revision = none ) quick_key = none while quick_key is none poll_result = self . _api . upload_poll ( upload_key ) doupload = poll_result [ string ] logger . debug ( string string , upload_key , int ( doupload [ string ] ) , doupload [ string ] , doupload [ string ] , int ( doupload [ string ] ) ) if int ( doupload [ string ] ) = number break if doupload [ string ] = string logger . warning ( string , upload_key , int ( doupload [ string ] ) ) break if int ( doupload [ string ] ) = = status_no_more_requests quick_key = doupload [ string ] elif int ( doupload [ string ] ) = = status_upload_in_progress raise retriable_upload_error ( string . format ( doupload [ string ] ) ) else time . sleep ( upload_poll_interval ) return upload_result ( action = action , quickkey = doupload [ string ] , hash_ = doupload [ string ] , filename = doupload [ string ] , size = doupload [ string ] , created = doupload [ string ] , revision = doupload [ string ] )	0
Q_453	calculate the block_struct array for the output file	C_453_0	def _calculate_block_structure ( self , inequalities , equalities , momentinequalities , momentequalities , extramomentmatrix , removeequalities , block_struct = none ) if block_struct is none if self . verbose > number print ( string ) self . block_struct = [ ] if self . parameters is not none self . block_struct + = [ number for var in self . parameters ] for monomials in self . monomial_sets if len ( monomials ) > number and isinstance ( monomials [ number ] , list ) self . block_struct . append ( len ( monomials [ number ] ) ) else self . block_struct . append ( len ( monomials ) ) if extramomentmatrix is not none for var in extramomentmatrix for monomials in self . monomial_sets if len ( monomials ) > number and isinstance ( monomials [ number ] , list ) self . block_struct . append ( len ( monomials [ number ] ) ) else self . block_struct . append ( len ( monomials ) ) else self . block_struct = block_struct degree_warning = false if inequalities is not none self . _n_inequalities = len ( inequalities ) n_tmp_inequalities = len ( inequalities ) else self . _n_inequalities = number n_tmp_inequalities = number constraints = flatten ( [ inequalities ] ) if momentinequalities is not none self . _n_inequalities + = len ( momentinequalities ) constraints + = momentinequalities if not removeequalities constraints + = flatten ( [ equalities ] ) monomial_sets = [ ] for k , constraint in enumerate ( constraints ) if k < n_tmp_inequalities or k > = self . _n_inequalities if isinstance ( constraint , str ) ineq_order = number * self . level else if constraint . is_relational constraint = convert_relational ( constraint ) ineq_order = ncdegree ( constraint ) if iscomplex ( constraint ) self . complex_matrix = true if ineq_order > number * self . level degree_warning = true localization_order = ( number * self . level _ ineq_order ) / / number if self . level = = _ number localization_order = number if self . localizing_monomial_sets is not none and self . localizing_monomial_sets [ k ] is not none localizing_monomials = self . localizing_monomial_sets [ k ] else index = find_variable_set ( self . variables , constraint ) localizing_monomials = pick_monomials_up_to_degree ( self . monomial_sets [ index ] , localization_order ) ln = len ( localizing_monomials ) if ln = = number localizing_monomials = [ s . one ] else localizing_monomials = [ s . one ] ln = number localizing_monomials = unique ( localizing_monomials ) monomial_sets . append ( localizing_monomials ) if k < self . _n_inequalities self . block_struct . append ( ln ) else monomial_sets + = [ none for var in range ( ln * ( ln + number ) / / number _ number ) ] monomial_sets . append ( localizing_monomials ) monomial_sets + = [ none for var in range ( ln * ( ln + number ) / / number _ number ) ] self . block_struct + = [ number for var in range ( ln * ( ln + number ) ) ] if degree_warning and self . verbose > number print ( string string string ( ineq_order ) , file = sys . stderr ) if momentequalities is not none for moment_eq in momentequalities self . _moment_equalities . append ( moment_eq ) if not removeequalities monomial_sets + = [ [ s . one ] , [ s . one ] ] self . block_struct + = [ number , number ] self . localizing_monomial_sets = monomial_sets	1
Q_453	calculate the block_struct array for the output file	C_453_1	def astype ( self , out_dtype ) out_dtype = np . dtype ( out_dtype ) if out_dtype = = self . out_dtype return self real_dtype = getattr ( self , string , none ) if real_dtype is none return self . _astype ( out_dtype ) else if out_dtype = = real_dtype if self . _real_space is none self . _real_space = self . _astype ( out_dtype ) return self . _real_space elif out_dtype = = self . complex_out_dtype if self . _complex_space is none self . _complex_space = self . _astype ( out_dtype ) return self . _complex_space else return self . _astype ( out_dtype )	0
Q_453	calculate the block_struct array for the output file	C_453_2	def _get_object ( data , position , obj_end , opts ) obj_size = _unpack_int ( data [ position position + number ] ) [ number ] end = position + obj_size _ number if data [ end position + obj_size ] = b string raise invalid_bson ( string ) if end > = obj_end raise invalid_bson ( string ) obj = _elements_to_dict ( data , position + number , end , opts , subdocument = true ) position + = obj_size if string in obj return ( db_ref ( obj . pop ( string ) , obj . pop ( string , none ) , obj . pop ( string , none ) , obj ) , position ) return obj , position	0
Q_454	shim for easily convert old _setitem_ call	C_454_0	def setslice_with_length ( self , startindex , endindex , fs , length ) if len ( self ) < startindex fs = string * ( startindex _ len ( self ) ) + fs if len ( self ) > endindex fs = fs + string * ( endindex _ startindex _ len ( fs ) ) assert len ( fs ) = = endindex _ startindex , ( len ( fs ) , startindex , endindex ) result = self . splice ( fs , startindex , endindex ) assert len ( result ) < = length return result	1
Q_454	shim for easily convert old _setitem_ call	C_454_1	def load_from_json ( data ) if isinstance ( data , str ) data = json . loads ( data ) return item ( data [ string ] , data [ string ] )	0
Q_454	shim for easily convert old _setitem_ call	C_454_2	def create_new_csv ( samples , args ) out_fn = os . path . splitext ( args . csv ) [ number ] + string logger . info ( string out_fn ) with file_transaction ( out_fn ) as tx_out with open ( tx_out , string ) as handle handle . write ( _header ( args . csv ) ) for var in samples sample_name = var [ string ] if isinstance ( var [ string ] , list ) else os . path . basename ( var [ string ] ) handle . write ( string ( sample_name , var [ string ] , string . join ( var [ string ] ) ) )	0
Q_455	backwards_compatibility for the old retry format	C_455_0	def from_int ( cls , retries , redirect = true , default = none ) if retries is none retries = default if default is not none else cls . default if isinstance ( retries , retry ) return retries redirect = bool ( redirect ) and none new_retries = cls ( retries , redirect = redirect ) log . debug ( string ( retries , new_retries ) ) return new_retries	1
Q_455	backwards_compatibility for the old retry format	C_455_1	def make_digraph ( automaton , input_as_string = repr , output_as_string = repr , state_as_string = repr ) digraph = graphviz . digraph ( graph_attr = { string string , string string } , node_attr = { string string } , edge_attr = { string string } ) for var in automaton . states ( ) if var is automaton . initial_state state_shape = string font_name = string else state_shape = string font_name = string digraph . node ( state_as_string ( var ) , fontame = font_name , shape = string , style = state_shape , color = string ) for n , each_transition in enumerate ( automaton . all_transitions ( ) ) in_state , input_symbol , out_state , output_symbols = each_transition this_transition = string . format ( n ) input_label = input_as_string ( input_symbol ) port = string table = table_maker ( input_label , [ output_as_string ( output_symbol ) for output_symbol in output_symbols ] , port = port ) digraph . node ( this_transition , label = _gvhtml ( table ) , margin = string , shape = string ) digraph . edge ( state_as_string ( in_state ) , string . format ( this_transition , port ) , arrowhead = string ) digraph . edge ( string . format ( this_transition , port ) , state_as_string ( out_state ) ) return digraph	0
Q_455	backwards_compatibility for the old retry format	C_455_2	def is_symlink ( path ) path = _patch_path ( path ) try return _is_symlink ( next ( find_files ( path ) ) ) except windows_error as orig_error tmpl = string raise builtins . windows_error ( tmpl . format ( * * locals ( ) ) )	0
Q_456	decorator define data set to provide to a test	C_456_0	def genty_dataset ( * args , * * kwargs ) datasets = _build_datasets ( * args , * * kwargs ) def wrap ( test_method ) if not hasattr ( test_method , string ) test_method . genty_datasets = ordered_dict ( ) test_method . genty_datasets . update ( datasets ) return test_method return wrap	1
Q_456	decorator define data set to provide to a test	C_456_1	def _path_pair ( self , var ) if var . startswith ( b string string string , number ) else parts = var . split ( b string , number ) if len ( parts ) = number self . abort ( errors . bad_format , string , string , var ) elif parts [ number ] . startswith ( b string string string ) parts [ number ] = parts [ number ] [ number _ number ] elif parts [ number ] . startswith ( b string string string ) self . abort ( errors . bad_format , string , string , var ) return [ _unquote_c_string ( var ) for var in parts ]	0
Q_456	decorator define data set to provide to a test	C_456_2	def siteblock ( parser , token ) tokens = token . split_contents ( ) tokens_num = len ( tokens ) if tokens_num not in ( number , number ) raise template . template_syntax_error ( string string myblock string myblock string tokens [ number ] ) block_alias = parser . compile_filter ( tokens [ number ] ) as_var = none tokens = tokens [ number ] if len ( tokens ) > = number and tokens [ _ number ] = = string as_var = tokens [ _ number ] return siteblock_node ( block_alias , as_var )	0
Q_457	add a subcommand name which invoke handler	C_457_0	def add_command ( self , handler , name = none ) if name is none name = docstring_to_subcommand ( handler . _doc_ ) self . _commands [ name ] = handler	1
Q_457	add a subcommand name which invoke handler	C_457_1	def group ( self , key , condition , initial , reduce , finalize = none , * * kwargs ) warnings . warn ( string string string , deprecation_warning , stacklevel = number ) group = { } if isinstance ( key , string_type ) group [ string ] = code ( key ) elif key is not none group = { string helpers . _fields_list_to_dict ( key , string ) } group [ string ] = self . _name group [ string ] = code ( reduce ) group [ string ] = condition group [ string ] = initial if finalize is not none group [ string ] = code ( finalize ) cmd = son ( [ ( string , group ) ] ) collation = validate_collation_or_none ( kwargs . pop ( string , none ) ) cmd . update ( kwargs ) with self . _socket_for_reads ( session = none ) as ( sock_info , slave_ok ) return self . _command ( sock_info , cmd , slave_ok , collation = collation , user_fields = { string number } ) [ string ]	0
Q_457	add a subcommand name which invoke handler	C_457_2	def normalise_tensor ( tensor ) tensor_norm = np . linalg . norm ( tensor ) return tensor / tensor_norm , tensor_norm	0
Q_458	handle func_call node	C_458_0	def _handle_func_call ( self , node , scope , ctxt , stream ) self . _dlog ( string { } string . format ( node . name . name ) ) if node . args is none func_args = [ ] else func_args = self . _handle_node ( node . args , scope , ctxt , stream ) func = self . _handle_node ( node . name , scope , ctxt , stream ) return func . call ( func_args , ctxt , scope , stream , self , node . coord )	1
Q_458	handle func_call node	C_458_1	def by_name ( cls , session , name ) pkg = cls . first ( session , where = ( cls . name . like ( name ) , ) ) if not pkg name = name . replace ( u string , u string ) . upper ( ) pkg = cls . first ( session , where = ( cls . name . like ( name ) , ) ) if pkg and pkg . name . upper ( ) . replace ( u string , u string ) = name pkg = none return pkg	0
Q_458	handle func_call node	C_458_2	def parse_value ( type str , val str ) if type . upper ( ) in parser_map out = parser_map [ type ] ( val ) log . debug ( string . format ( type , val , out ) ) return out else raise obd_pid_parser_unknown_error ( type , val )	0
Q_459	verify that the heat setting be between tagint and tagint	C_459_0	def heat_setting ( self , value ) if value not in range ( number , number ) raise exceptions . roaster_value_error self . _heat_setting . value = value	1
Q_459	verify that the heat setting be between tagint and tagint	C_459_1	def refactor_rename_current_module ( self , new_name ) refactor = rename ( self . project , self . resource , none ) return self . _get_changes ( refactor , new_name )	0
Q_459	verify that the heat setting be between tagint and tagint	C_459_2	def split_model_kwargs ( kw ) from collections import defaultdict model_fields = { } fields_agrs = defaultdict ( lambda { } ) for var in kw . keys ( ) if string in var field , _ , subfield = var . partition ( string ) fields_agrs [ field ] [ subfield ] = kw [ var ] else model_fields [ var ] = kw [ var ] return model_fields , fields_agrs	0
Q_460	call by django before decide which view to execute	C_460_0	def process_request ( self , unused_request ) tasklets . _state . clear_all_pending ( ) ctx = tasklets . make_default_context ( ) tasklets . set_context ( ctx )	1
Q_460	call by django before decide which view to execute	C_460_1	def unwrap_json ( self , type , json_data ) try data = json . loads ( json_data ) except value_error raise redmine_error ( json_data ) try data = data [ type ] except key_error pass return data	0
Q_460	call by django before decide which view to execute	C_460_2	def close ( self ) for var in self . plugins if hasattr ( var , string ) var . close ( ) self . stopped = true	0
Q_461	param feature the db row represent a feature	C_461_0	def _ga_feature_for_feature_db_record ( self , feature ) ga_feature = protocol . feature ( ) ga_feature . id = self . get_compound_id_for_feature_id ( feature [ string ] ) if feature . get ( string ) ga_feature . parent_id = self . get_compound_id_for_feature_id ( feature [ string ] ) else ga_feature . parent_id = string ga_feature . feature_set_id = self . get_id ( ) ga_feature . reference_name = pb . string ( feature . get ( string ) ) ga_feature . start = pb . int ( feature . get ( string ) ) ga_feature . end = pb . int ( feature . get ( string ) ) ga_feature . name = pb . string ( feature . get ( string ) ) if feature . get ( string , string ) = = string ga_feature . strand = protocol . neg_strand else ga_feature . strand = protocol . pos_strand ga_feature . child_ids . extend ( map ( self . get_compound_id_for_feature_id , json . loads ( feature [ string ] ) ) ) ga_feature . feature_type . copy_from ( self . _ontology . get_ga_term_by_name ( feature [ string ] ) ) attributes = json . loads ( feature [ string ] ) for var in attributes for v in attributes [ var ] ga_feature . attributes . attr [ var ] . values . add ( ) . string_value = v if string in attributes and len ( attributes [ string ] ) > number ga_feature . gene_symbol = pb . string ( attributes [ string ] [ number ] ) return ga_feature	1
Q_461	param feature the db row represent a feature	C_461_1	def init_bem_obj ( self ) string string string if not os . path . exists ( self . read_doe_file_path ) raise exception ( string { } string . format ( read_doe_file_path ) ) read_doe_file = open ( self . read_doe_file_path , string ) ref_doe = pickle . load ( read_doe_file ) ref_bem = pickle . load ( read_doe_file ) ref_schedule = pickle . load ( read_doe_file ) read_doe_file . close ( ) k = number self . r_glaze_total = number . self . shgc_total = number . self . alb_wall_total = number . h_floor = self . flr_h or number . number number total_urban_bld_area = math . pow ( self . char_length , number ) * self . bld_density * self . bld_height / h_floor area_matrix = utilities . zeros ( number , number ) self . bem = [ ] self . sch = [ ] for var in range ( number ) for j in range ( number ) if self . bld [ var ] [ j ] > number . self . bem . append ( ref_bem [ var ] [ j ] [ self . zone ] ) self . bem [ k ] . frac = self . bld [ var ] [ j ] self . bem [ k ] . fl_area = self . bld [ var ] [ j ] * total_urban_bld_area if self . glz_r self . bem [ k ] . building . glazing_ratio = self . glz_r if self . alb_roof self . bem [ k ] . roof . albedo = self . alb_roof if self . veg_roof self . bem [ k ] . roof . veg_coverage = self . veg_roof if self . shgc self . bem [ k ] . building . shgc = self . shgc if self . alb_wall self . bem [ k ] . wall . albedo = self . alb_wall if self . flr_h self . bem [ k ] . building . floor_height = self . flr_h self . r_glaze_total + = self . bem [ k ] . frac * self . bem [ k ] . building . glazing_ratio self . shgc_total + = self . bem [ k ] . frac * self . bem [ k ] . building . shgc self . alb_wall_total + = self . bem [ k ] . frac * self . bem [ k ] . wall . albedo self . sch . append ( ref_schedule [ var ] [ j ] [ self . zone ] ) k + = number	0
Q_461	param feature the db row represent a feature	C_461_2	def save_load ( jid , clear_load , minion = none ) cb_ = _get_connection ( ) try jid_doc = cb_ . get ( six . text_type ( jid ) ) except couchbase . exceptions . not_found_error cb_ . add ( six . text_type ( jid ) , { } , ttl = _get_ttl ( ) ) jid_doc = cb_ . get ( six . text_type ( jid ) ) jid_doc . value [ string ] = clear_load cb_ . replace ( six . text_type ( jid ) , jid_doc . value , cas = jid_doc . cas , ttl = _get_ttl ( ) ) if string in clear_load and clear_load [ string ] = string ckminions = salt . utils . minions . ck_minions ( _opts_ ) _res = ckminions . check_minions ( clear_load [ string ] , clear_load . get ( string , string ) ) minions = _res [ string ] save_minions ( jid , minions )	0
Q_462	add the specify g2p association set to this backend	C_462_0	def add_phenotype_association_set ( self , phenotype_association_set ) id_ = phenotype_association_set . get_id ( ) self . _phenotype_association_set_id_map [ id_ ] = phenotype_association_set self . _phenotype_association_set_name_map [ phenotype_association_set . get_local_id ( ) ] = phenotype_association_set self . _phenotype_association_set_ids . append ( id_ )	1
Q_462	add the specify g2p association set to this backend	C_462_1	def read_anvl_string ( string ) anvl_dict = { } anvl_lines = string . split ( string ) line_count = len ( anvl_lines ) index = number while index < line_count line = anvl_lines [ index ] if not len ( line ) or not len ( line . strip ( ) ) index = index + number continue if string string missing colon in line d of anvl record . string string next_index = next_index + number continue if next_line = = next_line . lstrip ( ) break if content_buffer content_buffer = content_buffer + string + next_line . lstrip ( ) else content_buffer = next_line . lstrip ( ) next_index = next_index + number index = next_index anvl_dict [ key ] = content_buffer return anvl_dict	0
Q_462	add the specify g2p association set to this backend	C_462_2	def getsource ( obj , is_binary = false ) if is_binary return none else if hasattr ( obj , string ) obj = obj . _wrapped_ try src = inspect . getsource ( obj ) except type_error if hasattr ( obj , string ) src = inspect . getsource ( obj . _class_ ) return src	0
Q_463	set attribute to dictionary value so can access via dot notation	C_463_0	def _set_attrs_to_values ( self , response = { } ) for var in response . keys ( ) setattr ( self , var , response [ var ] )	1
Q_463	set attribute to dictionary value so can access via dot notation	C_463_1	def write_bit ( self , b , pack = struct ( string ) . pack ) self . _output_buffer . append ( pack ( true if b else false ) ) return self	0
Q_463	set attribute to dictionary value so can access via dot notation	C_463_2	def mroc ( adjacency_matrix , alpha ) number_of_nodes = adjacency_matrix . shape [ number ] base_list = list ( ) base_row = list ( ) base_col = list ( ) append_base_list = base_list . append append_base_row = base_row . append append_base_col = base_col . append adjacency_matrix = adjacency_matrix . tocsc ( ) number_of_base_communities = number for var in range ( number_of_nodes ) base_community = set ( adjacency_matrix . getcol ( var ) . indices ) base_community . add ( var ) flag = true for c in base_list if c = = base_community flag = false break if flag append_base_list ( base_community ) for n in base_community append_base_row ( n ) append_base_col ( number_of_base_communities ) number_of_base_communities + = number base_row = np . array ( base_row ) base_col = np . array ( base_col ) base_data = np . ones ( base_row . size , dtype = np . float64 ) features = sparse . coo_matrix ( ( base_data , ( base_row , base_col ) ) , shape = ( number_of_nodes , number_of_base_communities ) ) features = features . tocsr ( ) base_community_number = features . shape [ number ] print ( string ) reverse_index_csr = copy . copy ( features ) reverse_index_csc = reverse_index_csr . tocsc ( ) reverse_index_csr = reverse_index_csr . tocsr ( ) reverse_index_rows = np . ndarray ( number_of_nodes , dtype = np . ndarray ) reverse_index_cols = np . ndarray ( number_of_nodes , dtype = np . ndarray ) for n in range ( number_of_nodes ) reverse_index_row = reverse_index_csr . getrow ( n ) reverse_index_rows [ n ] = reverse_index_row . indices if n < base_community_number reverse_index_col = reverse_index_csc . getcol ( n ) reverse_index_cols [ n ] = reverse_index_col . indices flag = true print ( string ) iteration = number while flag level_row = list ( ) level_col = list ( ) append_level_row = level_row . append append_level_col = level_col . append unavailable_communities = _ number * np . ones ( reverse_index_csc . shape [ number ] ) unavailable_communities_counter = number next_level_communities = list ( ) append_next_level_community = next_level_communities . append number_of_communities = number for j in range ( reverse_index_csr . shape [ number ] ) if j in unavailable_communities continue must_break = reverse_index_csr . shape [ number ] _ unavailable_communities_counter print ( must_break ) if must_break < number break unavailable_communities [ unavailable_communities_counter ] = j unavailable_communities_counter + = number c_j = reverse_index_cols [ j ] indices = community_neighbors ( c_j , reverse_index_rows , unavailable_communities , unavailable_communities_counter ) max_similarity = _ number community_index = number for jj in indices c_jj = reverse_index_cols [ jj ] similarity = jaccard ( c_j , c_jj ) if similarity > max_similarity max_similarity = similarity community_index = jj jj = community_index if max_similarity > number c_jj = reverse_index_cols [ jj ] c_new = np . union1d ( c_j , c_jj ) flag_1 = np . setdiff1d ( c_new , c_j ) flag_2 = np . setdiff1d ( c_new , c_jj ) if ( flag_1 . size = number ) and ( flag_2 . size = number ) for n in c_new append_level_row ( n ) append_level_col ( number_of_communities ) if c_new . size < alpha append_next_level_community ( number_of_communities ) number_of_communities + = number unavailable_communities [ unavailable_communities_counter ] = jj unavailable_communities_counter + = number level_row = np . array ( level_row ) level_col = np . array ( level_col ) level_data = np . ones ( level_row . size , dtype = np . float64 ) communities = sparse . coo_matrix ( ( level_data , ( level_row , level_col ) ) , shape = ( number_of_nodes , number_of_communities ) ) if communities . getnnz ( ) = = number break features = sparse . hstack ( [ features , communities ] ) reverse_index_csc = copy . copy ( communities ) reverse_index_csc = reverse_index_csc . tocsc ( ) reverse_index_csc = reverse_index_csc [ , np . array ( next_level_communities ) ] reverse_index_csr = reverse_index_csc . tocsr ( ) reverse_index_rows = np . ndarray ( number_of_nodes , dtype = np . ndarray ) reverse_index_cols = np . ndarray ( len ( next_level_communities ) , dtype = np . ndarray ) for n in range ( number_of_nodes ) reverse_index_row = reverse_index_csr . getrow ( n ) reverse_index_rows [ n ] = reverse_index_row . indices if n < len ( next_level_communities ) reverse_index_col = reverse_index_csc . getcol ( n ) reverse_index_cols [ n ] = reverse_index_col . indices if len ( next_level_communities ) > number flag = true iteration + = number print ( string , iteration ) print ( string , len ( next_level_communities ) ) return features	0
Q_464	provide access to content type management method for content type of an environment	C_464_0	def content_types ( self ) return environment_content_types_proxy ( self . _client , self . space . id , self . id )	1
Q_464	provide access to content type management method for content type of an environment	C_464_1	def map_layers ( name = none , types = none ) if types is not none and not isinstance ( types , list ) types = [ types ] layers = _layerreg . map_layers ( ) . values ( ) _layers = [ ] if name or types if name _layers = [ var for var in layers if re . match ( name , var . name ( ) ) ] if types _layers + = [ var for var in layers if var . type ( ) in types ] return _layers else return layers	0
Q_464	provide access to content type management method for content type of an environment	C_464_2	def draw ( self , d , i = none ) if i is none self . _dominoes . append ( d ) else self . _dominoes . insert ( i , d )	0
Q_465	save the host key back to a file	C_465_0	"def save_host_keys ( self , filename ) f = open ( filename , string ) f . write ( string s s s "" ( var , keytype , key . get_base64 ( ) ) ) f . close ( )"	1
Q_465	save the host key back to a file	C_465_1	def askopenfilename ( * * kwargs ) try from tkinter import tk import tk_file_dialog as filedialog except import_error from tkinter import tk , filedialog root = tk ( ) root . withdraw ( ) root . update ( ) filenames = filedialog . askopenfilename ( * * kwargs ) root . destroy ( ) return filenames	0
Q_465	save the host key back to a file	C_465_2	def query_edges ( self , bel optional [ str ] = none , source_function optional [ str ] = none , source union [ none , str , node ] = none , target_function optional [ str ] = none , target union [ none , str , node ] = none , relation optional [ str ] = none , ) if bel return self . search_edges_with_bel ( bel ) query = self . session . query ( edge ) if relation query = query . filter ( edge . relation . like ( relation ) ) if source_function query = self . _add_edge_function_filter ( query , edge . source_id , source_function ) if target_function query = self . _add_edge_function_filter ( query , edge . target_id , target_function ) if source if isinstance ( source , str ) source = self . query_nodes ( bel = source ) if source . count ( ) = = number return [ ] source = source . first ( ) query = query . filter ( edge . source = = source ) elif isinstance ( source , node ) query = query . filter ( edge . source = = source ) else raise type_error ( string . format ( source , source . _class_ . _name_ ) ) if target if isinstance ( target , str ) targets = self . query_nodes ( bel = target ) . all ( ) target = targets [ number ] query = query . filter ( edge . target = = target ) elif isinstance ( target , node ) query = query . filter ( edge . target = = target ) else raise type_error ( string . format ( target , target . _class_ . _name_ ) ) return query	0
Q_466	generate a token string from byte array	C_466_0	def csrf_token ( ) if string not in session session [ string ] = os . urandom ( number ) return hmac . new ( app . secret_key , session [ string ] , digestmod = sha1 ) . hexdigest ( )	1
Q_466	generate a token string from byte array	C_466_1	"def convert ( self , plugin = none ) if plugin plugin = kurt . plugin . kurt . get_plugin ( plugin ) if plugin . name in self . _plugins return self . _plugins [ plugin . name ] else err = block_not_supported ( string t have r "" ( plugin . display_name , self ) ) err . block_type = self raise err else return self . conversions [ number ]"	0
Q_466	generate a token string from byte array	C_466_2	def _create_peephole_variables ( self , dtype ) self . _w_f_diag = tf . get_variable ( self . w_f_diag , shape = [ self . _hidden_size ] , dtype = dtype , initializer = self . _initializers . get ( self . w_f_diag ) , partitioner = self . _partitioners . get ( self . w_f_diag ) , regularizer = self . _regularizers . get ( self . w_f_diag ) ) self . _w_i_diag = tf . get_variable ( self . w_i_diag , shape = [ self . _hidden_size ] , dtype = dtype , initializer = self . _initializers . get ( self . w_i_diag ) , partitioner = self . _partitioners . get ( self . w_i_diag ) , regularizer = self . _regularizers . get ( self . w_i_diag ) ) self . _w_o_diag = tf . get_variable ( self . w_o_diag , shape = [ self . _hidden_size ] , dtype = dtype , initializer = self . _initializers . get ( self . w_o_diag ) , partitioner = self . _partitioners . get ( self . w_o_diag ) , regularizer = self . _regularizers . get ( self . w_o_diag ) )	0
Q_467	set_styles _ set one or more style params	C_467_0	def set_styles ( self , style_updates_dict ) set_style_method = self . set_style for var , var in style_updates_dict . items ( ) set_style_method ( var , var ) return self . style	1
Q_467	set_styles _ set one or more style params	C_467_1	def send_keys ( self , text str = string ) _ > none self . click ( ) self . _parent . send_keys ( text )	0
Q_467	set_styles _ set one or more style params	C_467_2	def clear_matplotlib_ticks ( ax = none , axis = string ) if not ax return if axis . lower ( ) in [ string , string , string ] ax . set_xticks ( [ ] , [ ] ) if axis . lower ( ) in [ string , string , string ] ax . set_yticks ( [ ] , [ ] )	0
Q_468	run cmd with args and	C_468_0	def run_process ( cmd , * args ) from os import spawnvp , p_wait return spawnvp ( p_wait , cmd , ( cmd , ) + args )	1
Q_468	run cmd with args and	C_468_1	def plot_data ( orig_data , data ) import numpy as np from mpl_toolkits . mplot3d import axes3_d import matplotlib . pyplot as plt for var , var in [ ( orig_data , string ) , ( data , string ) ] fig = plt . figure ( ) ax = fig . add_subplot ( number , projection = string ) xs = [ d . x for d in var ] ys = [ d . y for d in var ] zs = [ d . z for d in var ] ax . scatter ( xs , ys , zs , var = var , marker = string ) ax . set_xlabel ( string ) ax . set_ylabel ( string ) ax . set_zlabel ( string ) plt . show ( )	0
Q_468	run cmd with args and	C_468_2	def _reader_thread_func ( self , read_stdout bool ) _ > none if read_stdout read_stream = self . _proc . stdout write_stream = self . _stdout else read_stream = self . _proc . stderr write_stream = self . _stderr assert read_stream is not none while self . _proc . poll ( ) is none available = read_stream . peek ( ) if available read_stream . read ( len ( available ) ) self . _write_bytes ( write_stream , available )	0
Q_469	return set of all possible platform	C_469_0	def get_platforms_set ( ) platforms = set ( [ var . lower ( ) for var in platform . _supported_dists ] ) platforms = set ( [ string , string , string , string ] ) return platforms	1
Q_469	return set of all possible platform	C_469_1	def _set_extensions ( self , params ) extensions = [ ] if self . _local_partial_reliability params . append ( ( sctp_prsctp_supported , b string ) ) extensions . append ( forward_tsn_chunk . type ) extensions . append ( reconfig_chunk . type ) params . append ( ( sctp_supported_chunk_ext , bytes ( extensions ) ) )	0
Q_469	return set of all possible platform	C_469_2	def as_named_tuple ( self ) keys = list ( self . entities . keys ( ) ) replaced = [ ] for var , var in enumerate ( keys ) if iskeyword ( var ) replaced . append ( var ) keys [ var ] = string var if replaced safe = [ string var for var in replaced ] warnings . warn ( string string string ( keys , safe ) ) entities = dict ( zip ( keys , self . entities . values ( ) ) ) _file = namedtuple ( string , string + string . join ( entities . keys ( ) ) ) return _file ( filename = self . path , * * entities )	0
Q_470	return list of site_packages from system python	C_470_0	def getsyssitepackages ( ) global _syssitepackages if not _syssitepackages if not in_venv ( ) _syssitepackages = get_python_lib ( ) return _syssitepackages run_in_syspy def run ( * args ) import site return site . getsitepackages ( ) output = run ( ) _syssitepackages = output logger . debug ( string , _syssitepackages ) return _syssitepackages	1
Q_470	return list of site_packages from system python	C_470_1	def detect_port ( port ) socket_test = socket . socket ( socket . af_inet , socket . sock_stream ) try socket_test . connect ( ( string , int ( port ) ) ) socket_test . close ( ) return true except return false	0
Q_470	return list of site_packages from system python	C_470_2	def substitute_params ( self , substitutions ) param_dict = { } for var , var in self . param_dict . items ( ) param_dict [ var ] = replace_substitutions ( var , substitutions ) return param_dict	0
Q_471	remove an alert for the give thing	C_471_0	def remove_alert ( thing_name , key , session = none ) return _request ( string , string . format ( thing_name ) , params = { string key } , session = session )	1
Q_471	remove an alert for the give thing	C_471_1	def delete ( queue , items ) con = _conn ( queue ) with con cur = con . cursor ( ) if isinstance ( items , six . string_types ) items = _quote_escape ( items ) cmd = string string { number } string string . format ( queue , items ) log . debug ( string , cmd ) cur . execute ( cmd ) return true if isinstance ( items , list ) items = [ _quote_escape ( var ) for var in items ] cmd = string . format ( queue ) log . debug ( string , cmd ) newitems = [ ] for item in items newitems . append ( ( item , ) ) cur . executemany ( cmd , newitems ) if isinstance ( items , dict ) items = salt . utils . json . dumps ( items ) . replace ( string string string ) items = _quote_escape ( items ) cmd = ( string string { number } string string ) . format ( queue , items ) log . debug ( string , cmd ) cur . execute ( cmd ) return true return true	0
Q_471	remove an alert for the give thing	C_471_2	def iter_branches ( self , number = _ number , etag = none ) url = self . _build_url ( string , base_url = self . _api ) return self . _iter ( int ( number ) , url , branch , etag = etag )	0
Q_472	ensure the tagstr property of windows_process_ext end in tagstr	C_472_0	def windows_process_priority_format ( instance ) class_suffix_re = re . compile ( r string ) for var , var in instance [ string ] . items ( ) if string in var and var [ string ] = = string try priority = var [ string ] [ string ] [ string ] except key_error continue if not class_suffix_re . match ( priority ) yield json_error ( string priority string s string string _class string var , instance [ string ] , string )	1
Q_472	ensure the tagstr property of windows_process_ext end in tagstr	C_472_1	def calc_std_mod_reduc ( mod_reduc ) mod_reduc = np . asarray ( mod_reduc ) . astype ( float ) std = ( np . exp ( _ number . number ) + np . sqrt ( number . number / np . exp ( number . number ) _ ( mod_reduc _ number . number ) * * number / np . exp ( number . number ) ) ) return std	0
Q_472	ensure the tagstr property of windows_process_ext end in tagstr	C_472_2	def get_zonefile_instance ( new_instance = false ) if new_instance zif = none else zif = getattr ( get_zonefile_instance , string , none ) if zif is none zif = zone_info_file ( getzoneinfofile_stream ( ) ) get_zonefile_instance . _cached_instance = zif return zif	0
Q_473	return an imbalanced tree topology	C_473_0	def imbtree ( ntips , treeheight = number . number ) rtree = toytree . tree ( ) rtree . treenode . add_child ( name = string ) rtree . treenode . add_child ( name = string ) for var in range ( number , ntips ) cherry = toytree . tree ( ) cherry . treenode . add_child ( name = str ( var ) ) cherry . treenode . add_child ( rtree . treenode ) rtree = cherry tre = toytree . tree ( rtree . write ( tree_format = number ) ) tre = tre . mod . make_ultrametric ( ) self = tre . mod . node_scale_root_height ( treeheight ) self . _coords . update ( ) return self	1
Q_473	return an imbalanced tree topology	C_473_1	def set_timeline_name ( self , timeline_name ) self . _timeline_name = timeline_name logger . info ( string . format ( self . _timeline_name ) )	0
Q_473	return an imbalanced tree topology	C_473_2	def assert_title ( self , title , * * kwargs ) query = title_query ( title , * * kwargs ) self . synchronize ( wait = query . wait ) def assert_title ( ) if not query . resolves_for ( self ) raise expectation_not_met ( query . failure_message ) return true return assert_title ( )	0
Q_474	teal interface for the calacs function	C_474_0	def run ( configobj = none ) calacs ( configobj [ string ] , exec_path = configobj [ string ] , time_stamps = configobj [ string ] , temp_files = configobj [ string ] , verbose = configobj [ string ] , debug = configobj [ string ] , quiet = configobj [ string ] , single_core = configobj [ string ] )	1
Q_474	teal interface for the calacs function	C_474_1	def login ( self , * * kwargs ) if string in kwargs api_token = kwargs [ string ] if kwargs . get ( string , false ) self . _check_return ( requests . get ( string . format ( self . url , api_token ) ) ) self . signed_username = api_token else auth = ( kwargs [ string ] , kwargs [ string ] ) self . signed_username = self . _check_return ( requests . get ( string . format ( self . url ) , auth = auth ) ) [ string ]	0
Q_474	teal interface for the calacs function	C_474_2	def get_value ( self , series , key ) if np . asarray ( key ) . dtype = = np . dtype ( bool ) return series . iloc [ key ] elif isinstance ( key , slice ) return series . iloc [ self . slice_indexer ( key . start , key . stop , key . step ) ] else return series . iloc [ self . get_loc ( key ) ]	0
Q_475	copy the rfc2579 date_time to a date and time string	C_475_0	def copy_to_date_time_string ( self ) if self . _number_of_seconds is none return none return string . format ( self . year , self . month , self . day_of_month , self . hours , self . minutes , self . seconds , self . deciseconds )	1
Q_475	copy the rfc2579 date_time to a date and time string	C_475_1	def stop ( self ) with self . _lock if self . _state = bundle . active return false for var in self . _bundles . values ( ) self . _registry . hide_bundle_services ( var ) self . _state = bundle . stopping self . _dispatcher . fire_bundle_event ( bundle_event ( bundle_event . stopping , self ) ) self . _dispatcher . fire_framework_stopping ( ) bid = self . _next_bundle_id _ number while bid > number var = self . _bundles . get ( bid ) bid _ = number if var is none or var . get_state ( ) = bundle . active continue try var . stop ( ) except exception as ex _logger . exception ( string , var . get_symbolic_name ( ) , ex , ) self . _state = bundle . resolved self . _dispatcher . fire_bundle_event ( bundle_event ( bundle_event . stopped , self ) ) self . _fw_stop_event . set ( ) self . _registry . clear ( ) return true	0
Q_475	copy the rfc2579 date_time to a date and time string	C_475_2	def select_rows ( server_context , schema_name , query_name , view_name = none , filter_array = none , container_path = none , columns = none , max_rows = none , sort = none , offset = none , container_filter = none , parameters = none , show_rows = none , include_total_count = none , include_details_column = none , include_update_column = none , selection_key = none , required_version = none , timeout = _default_timeout ) url = server_context . build_url ( string , string , container_path = container_path ) payload = { string schema_name , string query_name } if view_name is not none payload [ string ] = view_name if filter_array is not none for var in filter_array prefix = var . get_url_parameter_name ( ) payload [ prefix ] = var . get_url_parameter_value ( ) if columns is not none payload [ string ] = columns if max_rows is not none payload [ string ] = max_rows if sort is not none payload [ string ] = sort if offset is not none payload [ string ] = offset if container_filter is not none payload [ string ] = container_filter if parameters is not none for key , value in parameters . items ( ) payload [ string + key ] = value if show_rows is not none payload [ string ] = show_rows if include_total_count is not none payload [ string ] = include_total_count if include_details_column is not none payload [ string ] = include_details_column if include_update_column is not none payload [ string ] = include_update_column if selection_key is not none payload [ string ] = selection_key if required_version is not none payload [ string ] = required_version return server_context . make_request ( url , payload , timeout = timeout )	0
Q_476	a bolded section	C_476_0	def strong ( node ) o = nodes . strong ( ) for var in mark_down ( node ) o + = var return o	1
Q_476	a bolded section	C_476_1	def install_bash_completion ( self , script_name = none , dest = string ) if string in os . environ and string not in os . environ os . environ [ string ] = os . environ [ string ] dest = expanduser ( dest ) if script_name is none script_name = sys . argv [ number ] self . uninstall_bash_completion ( script_name = script_name , dest = dest ) with open ( dest , string ) as f f . write ( string ( register _ python _ argcomplete s ) string script_name )	0
Q_476	a bolded section	C_476_2	def _initialize_system_sync ( self ) connected_devices = self . bable . list_connected_devices ( ) for var in connected_devices self . disconnect_sync ( var . connection_handle ) self . stop_scan ( ) self . set_advertising ( false ) self . register_gatt_table ( )	0
Q_477	call method input_sequences	C_477_0	def load_data ( self , idx ) for var in self if isinstance ( var , abctools . input_sequences_abc ) var . load_data ( idx )	1
Q_477	call method input_sequences	C_477_1	def mktns ( self ) tns = [ none , self . root . get ( string ) ] if tns [ number ] is not none tns [ number ] = self . root . find_prefix ( tns [ number ] ) return tuple ( tns )	0
Q_477	call method input_sequences	C_477_2	async def cas ( self , value any , * * kwargs ) _ > bool success = await self . client . set ( self . key , value , _cas_token = self . _token , * * kwargs ) if not success raise optimistic_lock_error ( string ) return true	0
Q_478	ass to the state value at begin of the time step which	C_478_0	def old ( self ) value = getattr ( self . fastaccess_old , self . name , none ) if value is none raise runtime_error ( string string objecttools . elementphrase ( self ) ) else if self . ndim value = numpy . asarray ( value ) return value	1
Q_478	ass to the state value at begin of the time step which	C_478_1	def parse_order ( text ) orders = [ ] for var in map ( str . strip , text . split ( string ) ) items = var . split ( string , number ) if len ( items ) = = number column , order = items [ number ] , string elif len ( items ) = = number column , order = items else raise invalid_params ( string ) order = order . lower ( ) if order not in ( string , string , string ) raise invalid_params ( string order ) if order = string orders . append ( sql_query_order ( column , order ) ) return orders	0
Q_478	ass to the state value at begin of the time step which	C_478_2	def run ( self ) producer_deferred = defer . deferred ( ) producer_deferred . add_callback ( self . _request_finished ) producer_deferred . add_errback ( self . _request_error ) receiver_deferred = defer . deferred ( ) receiver_deferred . add_callback ( self . _response_finished ) receiver_deferred . add_errback ( self . _response_error ) self . _producer = producer . multi_part_producer ( self . _files , self . _data , callback = self . _request_progress , deferred = producer_deferred ) self . _receiver = receiver . string_receiver ( receiver_deferred ) headers = { string string self . _producer . boundary } self . _reactor , request = self . _connection . build_twisted_request ( string , string self . _room . id , extra_headers = headers , body_producer = self . _producer ) request . add_callback ( self . _response ) request . add_errback ( self . _shutdown ) self . _reactor . run ( )	0
Q_479	query asterisk manager interface for channel stats	C_479_0	def get_channel_stats ( self , chantypes = ( string , string , string , string , string ) ) if self . check_version ( string ) cmd = string else cmd = string cmdresp = self . execute_command ( cmd ) info_dict = { } for var in chantypes chan = var . lower ( ) if chan in ( string , string ) info_dict [ string ] = number info_dict [ string ] = number else info_dict [ chan ] = number for k in ( string , string , string ) info_dict [ k ] = number regexstr = ( string string . join ( chantypes ) ) for line in cmdresp . splitlines ( ) mobj = re . match ( regexstr , line , re . ignorecase ) if mobj chan_type = mobj . group ( number ) . lower ( ) chan_id = mobj . group ( number ) . lower ( ) if chan_type = = string or chan_type = = string if chan_id = = string info_dict [ string ] + = number else info_dict [ string ] + = number else info_dict [ chan_type ] + = number continue mobj = re . match ( string , line , re . ignorecase ) if mobj if mobj . group ( number ) = = string info_dict [ string ] = int ( mobj . group ( number ) ) elif mobj . group ( number ) = = string info_dict [ string ] = int ( mobj . group ( number ) ) elif mobj . group ( number ) = = string info_dict [ string ] = int ( mobj . group ( number ) ) continue return info_dict	1
Q_479	query asterisk manager interface for channel stats	C_479_1	def terminate ( self , instances , count = number ) if not instances return if count > number instances = self . _scale_down ( instances , count ) self . ec2 . terminate_instances ( [ var . id for var in instances ] )	0
Q_479	query asterisk manager interface for channel stats	C_479_2	def size ( self , destination ) session = meta . session ( ) sel = select ( [ func . count ( model . frames_table . c . message_id ) ] ) . where ( model . frames_table . c . destination = = destination ) result = session . execute ( sel ) first = result . fetchone ( ) if not first return number else return int ( first [ number ] )	0
Q_480	construct the primary hdu file contain basic header info	C_480_0	def primary_hdu ( model ) cards = model . _mission . hdu_cards ( model . meta , hdu = number ) if string not in [ var [ number ] for var in cards ] cards . append ( ( string , model . mag , string ) ) cards . append ( ( string , string ) ) cards . append ( ( string , string ) ) cards . append ( ( string , string ) ) cards . append ( ( string , model . mission , string ) ) cards . append ( ( string , everest_major_minor , string ) ) cards . append ( ( string , everest_version , string ) ) cards . append ( ( string , strftime ( string ) , string ) ) header = pyfits . header ( cards = cards ) hdu = pyfits . primary_hdu ( header = header ) return hdu	1
Q_480	construct the primary hdu file contain basic header info	C_480_1	def get_robot_variables ( ) prefix = string variables = [ ] def safe_str ( s ) if isinstance ( s , six . text_type ) return s else return six . text_type ( s , string , string ) for var in os . environ if var . startswith ( prefix ) and len ( var ) > len ( prefix ) variables . append ( safe_str ( string ( var [ len ( prefix ) ] , os . environ [ var ] ) , ) ) return variables	0
Q_480	construct the primary hdu file contain basic header info	C_480_2	def _exit_gracefully ( self , signum , frame ) self . log . info ( string , signum ) if self . processor_agent self . processor_agent . end ( ) sys . exit ( os . ex_ok )	0
Q_481	add iterable value to the set	C_481_0	def update ( self , values ) string string string d = self . value_pickler . dumps return self . cache . update ( tuple ( ( d ( var ) for var in values ) ) )	1
Q_481	add iterable value to the set	C_481_1	def reconstructed_data_vector_from_blurred_mapping_matrix_and_solution_vector ( blurred_mapping_matrix , solution_vector ) reconstructed_data_vector = np . zeros ( blurred_mapping_matrix . shape [ number ] ) for var in range ( blurred_mapping_matrix . shape [ number ] ) for j in range ( solution_vector . shape [ number ] ) reconstructed_data_vector [ var ] + = solution_vector [ j ] * blurred_mapping_matrix [ var , j ] return reconstructed_data_vector	0
Q_481	add iterable value to the set	C_481_2	def random_sample ( self , k = number ) if k = = number results = [ ] elif k = = number results = [ self . redis . srandmember ( self . key ) ] else results = self . redis . srandmember ( self . key , k ) return [ self . _unpickle ( var ) for var in results ]	0
Q_482	add a middleware function to the list of attr word_middleware	C_482_0	def add_word_middleware ( self , middleware , for_search = true ) string string string if hasattr ( middleware , string ) self . word_middleware . append ( ( middleware , for_search ) )	1
Q_482	add a middleware function to the list of attr word_middleware	C_482_1	def kaldi_pitch ( wav_dir str , feat_dir str ) _ > none logger . debug ( string ) prefixes = [ ] for var in os . listdir ( wav_dir ) prefix , ext = os . path . splitext ( var ) if ext = = string prefixes . append ( prefix ) wav_scp_path = os . path . join ( feat_dir , string ) with open ( wav_scp_path , string ) as wav_scp for prefix in prefixes logger . info ( string , os . path . join ( wav_dir , prefix + string ) ) print ( prefix , os . path . join ( wav_dir , prefix + string ) , file = wav_scp ) pitch_scp_path = os . path . join ( feat_dir , string ) with open ( pitch_scp_path , string ) as pitch_scp for prefix in prefixes logger . info ( string , os . path . join ( feat_dir , prefix + string ) ) print ( prefix , os . path . join ( feat_dir , prefix + string ) , file = pitch_scp ) args = [ os . path . join ( config . kaldi_root , string ) , string ( wav_scp_path ) , string pitch_scp_path ] logger . info ( string . format ( wav_scp_path ) ) subprocess . run ( args ) for var in os . listdir ( feat_dir ) if var . endswith ( string ) pitch_feats = [ ] with open ( os . path . join ( feat_dir , var ) ) as f for line in f sp = line . split ( ) if len ( sp ) > number pitch_feats . append ( [ float ( sp [ number ] ) , float ( sp [ number ] ) ] ) prefix , _ = os . path . splitext ( var ) out_fn = prefix + string a = np . array ( pitch_feats ) np . save ( os . path . join ( feat_dir , out_fn ) , a )	0
Q_482	add a middleware function to the list of attr word_middleware	C_482_2	def add_badge ( self , kind ) badge = self . get_badge ( kind ) if badge return badge if kind not in getattr ( self , string , { } ) msg = string raise db . validation_error ( msg . format ( model = self . _class_ . _name_ , kind = kind ) ) badge = badge ( kind = kind ) if current_user . is_authenticated badge . created_by = current_user . id self . update ( _raw_ = { string { string { string [ badge . to_mongo ( ) ] , string number } } } ) self . reload ( ) post_save . send ( self . _class_ , document = self ) on_badge_added . send ( self , kind = kind ) return self . get_badge ( kind )	0
Q_483	method to remove pending or fail migration request from the queue	C_483_0	def remove_migration_request ( self , migration_rqst ) conn = self . dbi . connection ( ) try tran = conn . begin ( ) self . mgrremove . execute ( conn , migration_rqst ) tran . commit ( ) except dbs_exception as he if conn conn . close ( ) raise except exception as ex if conn conn . close ( ) raise if conn conn . close ( )	1
Q_483	method to remove pending or fail migration request from the queue	C_483_1	def attrs ( self ) ret = dict ( self . _dict_ ) del ret [ string ] if self . type = c . computer del ret [ string ] return ret	0
Q_483	method to remove pending or fail migration request from the queue	C_483_2	def _write_headers ( self , headers ) if headers for var in sorted ( headers . keys ( ) ) yield var . encode ( string ) yield b string yield headers [ var ] . encode ( string ) yield crlf yield crlf	0
Q_484	call to provide a response to a message if need	C_484_0	"def react ( self , msg ) returned = string mtype = type ( msg ) if mtype in string_types msg = unpack_frame ( msg ) elif mtype = = dict pass else raise frame_error ( string s string t know what to do with this string cmd string cmd "" ] ] ( msg ) return returned"	1
Q_484	call to provide a response to a message if need	C_484_1	def output ( data , * * kwargs ) color = salt . utils . color . get_colors ( _opts_ . get ( string ) , _opts_ . get ( string ) ) strip_colors = _opts_ . get ( string , true ) ident = number if _opts_ . get ( string ) ident = number if _opts_ [ string ] in ( string , string ) acc = string pend = string den = string rej = string cmap = { pend color [ string ] , acc color [ string ] , den color [ string ] , rej color [ string ] , string color [ string ] } trans = { pend u string . format ( string * ident , color [ string ] , color [ string ] ) , acc u string . format ( string * ident , color [ string ] , color [ string ] ) , den u string . format ( string * ident , color [ string ] , color [ string ] ) , rej u string . format ( string * ident , color [ string ] , color [ string ] ) , string u string . format ( string * ident , color [ string ] , color [ string ] ) } else acc = string pend = string rej = string cmap = { pend color [ string ] , acc color [ string ] , rej color [ string ] , string color [ string ] } trans = { pend u string . format ( string * ident , color [ string ] , color [ string ] ) , acc u string . format ( string * ident , color [ string ] , color [ string ] ) , rej u string . format ( string * ident , color [ string ] , color [ string ] ) , string u string . format ( string * ident , color [ string ] , color [ string ] ) } ret = string for var in sorted ( data ) ret + = u string . format ( trans [ var ] ) for key in sorted ( data [ var ] ) key = salt . utils . data . decode ( key ) skey = salt . output . strip_esc_sequence ( key ) if strip_colors else key if isinstance ( data [ var ] , list ) ret + = u string . format ( string * ident , cmap [ var ] , skey , color [ string ] ) if isinstance ( data [ var ] , dict ) ret + = u string . format ( string * ident , cmap [ var ] , skey , data [ var ] [ key ] , color [ string ] ) return ret	0
Q_484	call to provide a response to a message if need	C_484_2	def open ( self ) return self . _framework . chain_return_value ( self . _ensure_file ( ) , self . get_io_loop ( ) , self )	0
Q_485	return the row of the child	C_485_0	def row ( self ) if self . parent is not none children = self . parent . get_children ( ) return children . index ( self ) else return number	1
Q_485	return the row of the child	C_485_1	def add_to_group ( self , group_path ) if self . get_group_path ( ) = group_path post_data = add_group_template . format ( connectware_id = self . get_connectware_id ( ) , group_path = group_path ) self . _conn . put ( string , post_data ) self . _device_json = none	0
Q_485	return the row of the child	C_485_2	def add_xref ( self , id , xref ) if self . xref_graph is none self . xref_graph = nx . multi_graph ( ) self . xref_graph . add_edge ( xref , id )	0
Q_486	if the environmental variable tagstr be set it will most likely be	C_486_0	def check_proxy_setting ( ) try http_proxy = os . environ [ string ] except key_error return if not http_proxy . startswith ( string ) match = re . match ( string , http_proxy ) os . environ [ string ] = string ( match . group ( number ) , match . group ( number ) ) return	1
Q_486	if the environmental variable tagstr be set it will most likely be	C_486_1	def get_all_platform_objects ( mode = none ) list_all = [ ] all_modules = [ ] for _ , name , _ in pkgutil . iter_modules ( osrframework . wrappers . _path_ ) all_modules . append ( string + name ) for var in all_modules my_module = importlib . import_module ( var ) class_names = [ m [ number ] for m in inspect . getmembers ( my_module , inspect . isclass ) if m [ number ] . _module_ = = var ] my_class = getattr ( my_module , class_names [ number ] ) new_instance = my_class ( ) list_all . append ( new_instance ) paths = configuration . get_config_path ( ) new_path = os . path . abspath ( paths [ string ] ) if not new_path in sys . path sys . path . append ( new_path ) user_imported_modules = { } for module in os . listdir ( new_path ) if module [ _ number ] = = string current = module . replace ( string , string ) user_imported_modules [ current ] = _import_ ( current ) del new_path user_classes = [ ] for user_module in user_imported_modules . keys ( ) my_module = user_imported_modules [ user_module ] class_names = [ m [ number ] for m in inspect . getmembers ( my_module , inspect . isclass ) if m [ number ] . _module_ = = user_module ] my_class = getattr ( my_module , class_names [ number ] ) new_instance = my_class ( ) user_classes . append ( new_instance ) list_to_add = [ ] for user_class in user_classes overwritten = false for i , official_class in enumerate ( list_all ) if str ( user_class ) = = str ( official_class ) list_all [ i ] = user_class overwritten = true break if not overwritten list_to_add . append ( user_class ) list_all = list_all + list_to_add creds = credentials . get_credentials ( ) for p in list_all if p . platform_name . lower ( ) in creds . keys ( ) p . set_credentials ( creds [ p . platform_name . lower ( ) ] ) if mode = = none return list_all else selected = [ ] for p in list_all try if p . is_valid_mode [ mode ] selected . append ( p ) except pass return selected	0
Q_486	if the environmental variable tagstr be set it will most likely be	C_486_2	def import_module ( self , module = none , recursive = false , * * params ) if module is none if string in params warnings . warn ( string module_ string module string ) module = params . pop ( string ) else raise value_error ( string ) if string in params params [ string ] = get_impls ( params [ string ] ) space = ( self . _impl . model . currentspace ) = self . _impl . new_space_from_module ( module , recursive = recursive , * * params ) return get_interfaces ( space )	0
Q_487	this matrix decide which block should be fill fg bg colour	C_487_0	def _create_matrix ( self , byte_list ) cells = int ( self . rows * self . cols / number + self . cols number ) matrix = [ [ false ] * self . cols for var in range ( self . rows ) ] for cell_number in range ( cells ) if self . _bit_is_one ( cell_number , byte_list [ number ] ) x_row = cell_number self . rows y_col = int ( cell_number / self . cols ) matrix [ x_row ] [ self . cols _ y_col _ number ] = true matrix [ x_row ] [ y_col ] = true return matrix	1
Q_487	this matrix decide which block should be fill fg bg colour	C_487_1	"def perform_preflight_check ( self ) failures = [ ] if not os . access ( self . mount_path_base , os . w_ok ) msg = ( string t exist or is not writable string by the current user . string sshfs string sshfs ( http / / fuse . sourceforge . net / sshfs . html ) string does not seem to be installed . "" ) failures . append ( msg ) return len ( failures ) = = number , failures"	0
Q_487	this matrix decide which block should be fill fg bg colour	C_487_2	def fpy_interface_sub ( fpy , dtype , kind , suffix = none ) allsubs = [ ] xnames = [ ] drange = list ( range ( number ) ) if suffix is none else list ( range ( number , number ) ) for var in drange xname , sub = fpy ( var , dtype , kind , suffix ) xnames . append ( xname ) allsubs . append ( sub ) return ( xnames , string . join ( allsubs ) )	0
Q_488	correspond to idd field ws_db996	C_488_0	def ws_db996 ( self , value = none ) if value is not none try value = float ( value ) except value_error raise value_error ( string string . format ( value ) ) self . _ws_db996 = value	1
Q_488	correspond to idd field ws_db996	C_488_1	def showpath ( path ) if logger . verbose return os . path . abspath ( path ) else path = os . path . relpath ( path ) if path . startswith ( os . curdir + os . sep ) path = path [ len ( os . curdir + os . sep ) ] return path	0
Q_488	correspond to idd field ws_db996	C_488_2	def add_group_shape ( self , shapes = [ ] ) grp_sp = self . _element . add_grp_sp ( ) for var in shapes grp_sp . insert_element_before ( var . _element , string ) if shapes grp_sp . recalculate_extents ( ) return self . _shape_factory ( grp_sp )	0
Q_489	football data from english game since tagint	C_489_0	def football_data ( season = string , data_set = string ) league_dict = { string number , string number , string number , string number , string number } def league2num ( string ) if isinstance ( string , bytes ) string = string . decode ( string ) return league_dict [ string ] def football2num ( string ) if isinstance ( string , bytes ) string = string . decode ( string ) if string in football_dict return football_dict [ string ] else football_dict [ string ] = len ( football_dict ) + number return len ( football_dict ) + number def datestr2num ( s ) import datetime from matplotlib . dates import date2num return date2num ( datetime . datetime . strptime ( s . decode ( string ) , string ) ) data_set_season = data_set + string + season data_resources [ data_set_season ] = copy . deepcopy ( data_resources [ data_set ] ) data_resources [ data_set_season ] [ string ] [ number ] + = season + string start_year = int ( season [ number number ] ) end_year = int ( season [ number number ] ) files = [ string , string , string , string ] if start_year > number and start_year < number files + = [ string ] data_resources [ data_set_season ] [ string ] = [ files ] if not data_available ( data_set_season ) download_data ( data_set_season ) start = true for var in reversed ( files ) filename = os . path . join ( data_path , data_set_season , var ) writename = os . path . join ( data_path , data_set_season , string ) input = open ( filename , encoding = string ) output = open ( writename , string ) writer = csv . writer ( output ) for row in csv . reader ( input ) if any ( field . strip ( ) for field in row ) writer . writerow ( row ) input . close ( ) output . close ( ) table = np . loadtxt ( writename , skiprows = number , usecols = ( number , number , number , number , number , number ) , converters = { number league2num , number datestr2num , number football2num , number football2num } , delimiter = string ) if start x = table [ , number ] y = table [ , number ] start = false else x = np . append ( x , table [ , number ] , axis = number ) y = np . append ( y , table [ , number ] , axis = number ) return data_details_return ( { string x , string y , string [ discrete ( league_dict , string ) , datenum ( string ) , discrete ( football_dict , string ) , discrete ( football_dict , string ) ] , string [ integer ( string ) , integer ( string ) ] } , data_set )	1
Q_489	football data from english game since tagint	C_489_1	def none ( self ) return empty_query_set ( model = self . model , using = self . _using , connection = self . _connection )	0
Q_489	football data from english game since tagint	C_489_2	def get_dict_key ( data , key ) if isinstance ( data , mapping ) if key in data yield data [ key ] for var in data . values ( ) for found in get_dict_key ( var , key ) yield found	0
Q_490	set verbatim code replacement	C_490_0	def replace ( self , str1 , str2 ) if str2 = self . _replacements . get ( str1 , none ) self . _replacements [ str1 ] = str2 self . changed ( code_changed = true )	1
Q_490	set verbatim code replacement	C_490_1	def standardize_role ( role ) role = role . lower ( ) if any ( var in role for var in { string , string , string , string , string , string } ) return string return role	0
Q_490	set verbatim code replacement	C_490_2	def unbind_handler ( self , svc_ref ) if svc_ref not in self . _bound_references return false namespace , commands = self . _reference_commands [ svc_ref ] for var in commands self . unregister ( namespace , var ) self . _context . unget_service ( svc_ref ) del self . _bound_references [ svc_ref ] del self . _reference_commands [ svc_ref ] return true	0
Q_491	set the vertex and fragment shaders	C_491_0	def set_shaders ( self , vert , frag ) if not vert or not frag raise value_error ( string ) vert , frag = preprocess ( vert ) , preprocess ( frag ) self . _shaders = vert , frag self . _glir . command ( string , self . _id , vert , frag ) for var , var in self . _user_variables . items ( ) self . _pending_variables [ var ] = var self . _user_variables = { } self . _parse_variables_from_code ( )	1
Q_491	set the vertex and fragment shaders	C_491_1	def read_tuple_and_extra ( self , stream ) length , symbol = self . decode_peek ( stream . peek ( self . max_length ) ) stream . pos + = length extra_bits = self . extra_bits ( symbol . index ) return length , symbol , extra_bits , stream . read ( extra_bits )	0
Q_491	set the vertex and fragment shaders	C_491_2	def write_nginx_config ( self ) log = logging . get_logger ( string ) if not os . path . exists ( self . root ) log . debug ( string , self . root ) os . makedirs ( self . root , number ) server_block = server_block ( self ) server_config_path = os . path . join ( _cfg . get ( string , string ) , self . domain . name ) if not os . path . exists ( server_config_path ) log . debug ( string , server_config_path ) os . makedirs ( server_config_path , number ) server_config_path = os . path . join ( server_config_path , string . format ( fn = self . slug ) ) if os . path . exists ( server_config_path ) log . info ( string , server_config_path ) os . remove ( server_config_path ) log . info ( string ) with open ( server_config_path , string ) as f f . write ( server_block . template )	0
Q_492	remove explicit hydrogen if possible	C_492_0	def implicify_hydrogens ( self ) total = number for var in ( self . _reagents , self . _reactants , self . _products ) for m in var if hasattr ( m , string ) total + = m . implicify_hydrogens ( ) if total self . flush_cache ( ) return total	1
Q_492	remove explicit hydrogen if possible	C_492_1	def glob ( self ) self . symlinks = { } if self . is_bytes curdir = os . fsencode ( os . curdir ) else curdir = os . curdir for var in self . var dir_only = var [ _ number ] . dir_only if var else false if var if not var [ number ] . is_magic this = var [ number ] curdir = this [ number ] if not os . path . lexists ( curdir ) return results = [ curdir ] if this . is_drive else self . _get_starting_paths ( curdir ) if not results if not dir_only yield curdir return if this . dir_only for start in results if os . path . isdir ( start ) rest = var [ number ] if rest this = rest . pop ( number ) for match in self . _glob ( curdir , this , rest ) if not self . _is_excluded ( match , dir_only ) yield os . path . join ( match , self . empty ) if dir_only else match elif not self . _is_excluded ( curdir , dir_only ) yield os . path . join ( curdir , self . empty ) if dir_only else curdir else for start in results if os . path . lexists ( start ) and not self . _is_excluded ( start , dir_only ) yield os . path . join ( start , self . empty ) if dir_only else start else rest = var [ ] this = rest . pop ( number ) for match in self . _glob ( curdir if not curdir = = self . current else self . empty , this , rest ) if not self . _is_excluded ( match , dir_only ) yield os . path . join ( match , self . empty ) if dir_only else match	0
Q_492	remove explicit hydrogen if possible	C_492_2	def _connect_database ( config ) settings = config . registry . settings mongo_uri = string mongodb_name = string if settings . get ( string ) mongo_uri = settings [ string ] if settings . get ( string ) mongodb_name = settings [ string ] return mongoengine . connect ( mongodb_name , host = mongo_uri )	0
Q_493	from the give request add a snippet to the page	C_493_0	def get_payment_request ( self , cart , request ) try self . charge ( cart , request ) thank_you_url = order_model . objects . get_latest_url ( ) js_expression = string { } string . format ( thank_you_url ) return js_expression except ( key_error , stripe . error . stripe_error ) as err raise validation_error ( err )	1
Q_493	from the give request add a snippet to the page	C_493_1	def get_nodes_by_selector ( self , selector , not_selector = none ) nodes = self . parsed ( selector ) if not_selector is not none nodes = nodes . not_ ( not_selector ) return nodes	0
Q_493	from the give request add a snippet to the page	C_493_2	def normalize ( dt , tz = utc ) if type ( dt ) is date dt = dt + relativedelta ( hour = number ) elif type ( dt ) is datetime pass else raise value_error ( string type ( dt ) ) if dt . tzinfo dt = dt . astimezone ( tz ) else dt = dt . replace ( tzinfo = tz ) return dt	0
Q_494	fun o trocar_codigo_de_ativacao conforme er sit item tagint	C_494_0	def trocar_codigo_de_ativacao ( self , novo_codigo_ativacao , opcao = constantes . codigo_ativacao_regular , codigo_emergencia = none ) if not novo_codigo_ativacao raise value_error ( string . format ( novo_codigo_ativacao ) ) codigo_ativacao = self . _codigo_ativacao if opcao = = constantes . codigo_ativacao_emergencia if codigo_emergencia codigo_ativacao = codigo_emergencia else raise value_error ( string string . format ( codigo_emergencia , opcao ) ) return self . invocar_trocar_codigo_de_ativacao ( self . gerar_numero_sessao ( ) , codigo_ativacao , opcao , novo_codigo_ativacao , novo_codigo_ativacao )	1
Q_494	fun o trocar_codigo_de_ativacao conforme er sit item tagint	C_494_1	def filemode ( mode ) perm = [ ] for var in filemode_table for bit , char in var if mode bit = = bit perm . append ( char ) break else perm . append ( string ) return string . join ( perm )	0
Q_494	fun o trocar_codigo_de_ativacao conforme er sit item tagint	C_494_2	def call_function ( self , c , i ) callable_ = self . _stack [ _ number _ i . arg ] args = tuple ( self . _stack [ len ( self . _stack ) _ i . arg ] ) self . _print ( string ) self . _print ( string , callable_ ) self . _print ( string , i . arg ) self . _print ( string , args ) self . call_callbacks ( string , callable_ , * args ) if isinstance ( callable_ , function_type ) ret = callable_ ( * args ) elif callable_ is builtins . _build_class_ ret = self . build_class ( callable_ , args ) elif callable_ is builtins . globals ret = self . builtins_globals ( ) else ret = callable_ ( * args ) self . pop ( number + i . arg ) self . _stack . append ( ret )	0
Q_495	size the canvas item to the proper width	C_495_0	def size_to_content ( self ) new_sizing = self . copy_sizing ( ) new_sizing . minimum_width = number new_sizing . maximum_width = number axes = self . _axes if axes and axes . is_valid if axes . y_calibration and axes . y_calibration . units new_sizing . minimum_width = self . font_size + number new_sizing . maximum_width = self . font_size + number self . update_sizing ( new_sizing )	1
Q_495	size the canvas item to the proper width	C_495_1	"def print_fit_parameters ( self ) s = string if self . results and self . results [ number ] is not none s = s + string { number s } = { g } string for var in range ( len ( self . _pnames ) ) s = s + string . format ( self . _pnames [ var ] , self . results [ number ] [ var ] ) else s = s + "" print ( s )"	0
Q_495	size the canvas item to the proper width	C_495_2	def register_view ( self , view ) self . measure_to_view_map . register_view ( view = view , timestamp = self . time )	0
Q_496	several kw which be usually in the primary header	C_496_0	def stis_ext2_prim_kw ( stisfiles ) kw_list = [ string , string , string , string ] for var in stisfiles toclose = false if isinstance ( var , str ) var = fits . open ( var , mode = string ) toclose = true for k in kw_list var [ number ] . header [ k ] = var [ number ] . header [ k ] var [ number ] . header . comments [ k ] = string if toclose var . close ( )	1
Q_496	several kw which be usually in the primary header	C_496_1	def isready ( self ) self . put ( string ) while true text = self . stdout . readline ( ) . strip ( ) if text = = string return text	0
Q_496	several kw which be usually in the primary header	C_496_2	def face ( sign , lon ) faces = faces [ sign ] if lon < number return faces [ number ] elif lon < number return faces [ number ] else return faces [ number ]	0
Q_497	profile object model handle with a very simple benchmarking test	C_497_0	def profile ( schemaname = string , profiletype = string ) db_log ( string , schemaname ) schema = schemastore [ schemaname ] [ string ] db_log ( string , schema , lvl = debug ) testclass = none if profiletype = = string db_log ( string ) testclass = warmongo . model_factory ( schema ) elif profiletype = = string db_log ( string ) try import python_jsonschema_objects as pjs except import_error db_log ( string string ) return db_log ( ) builder = pjs . object_builder ( schema ) ns = builder . build_classes ( ) pprint ( ns ) testclass = ns [ schemaname ] db_log ( string , ns , lvl = warn ) if testclass is not none db_log ( string ) for var in range ( number ) testclass ( ) else db_log ( string ) db_log ( string )	1
Q_497	profile object model handle with a very simple benchmarking test	C_497_1	def data ( self , column , role ) if role = = qt_core . qt . display_role if column = = number return self . _au . name if column = = number return self . _au . description if column = = number return self . _au . status . value if column = = number return self . _au . status . message if column = = number return self . _au . status . traceback if role = = qt_core . qt . foreground_role if column = = number return self . colormapping . get ( self . _au . status . value )	0
Q_497	profile object model handle with a very simple benchmarking test	C_497_2	def save_video ( video , fps , save_filename = string ) print ( save_filename ) video = float_to_uint8 ( video ) fourcc = cv2 . video_writer_fourcc ( * string ) writer = cv2 . video_writer ( save_filename , fourcc , fps , ( video . shape [ number ] , video . shape [ number ] ) , number ) for var in range ( number , video . shape [ number ] ) res = cv2 . convert_scale_abs ( video [ var ] ) writer . write ( res )	0
Q_498	return all resource directly own by user can be use to assign	C_498_0	def resources ( self ) return sa . orm . relationship ( string , cascade = string , passive_deletes = true , passive_updates = true , backref = string , lazy = string , )	1
Q_498	return all resource directly own by user can be use to assign	C_498_1	def gen_select_list ( sig_dic ) view_jushi = dic_tmp = sig_dic [ string ] for var in dic_tmp . keys ( ) tmp_str = return view_jushi	0
Q_498	return all resource directly own by user can be use to assign	C_498_2	def start_tcp_client ( self , ip = none , port = none , name = none , timeout = none , protocol = none , family = string ) self . _start_client ( tcp_client , ip , port , name , timeout , protocol , family )	0
Q_499	return a ordered_dict order by key name from the unsorted_dict	C_499_0	def sort_dict ( unsorted_dict ) sorted_dict = ordered_dict ( ) for var , var in sorted ( unsorted_dict . items ( ) , var = itemgetter ( number ) ) sorted_dict [ var ] = var return sorted_dict	1
Q_499	return a ordered_dict order by key name from the unsorted_dict	C_499_1	"def splittag ( url ) global _tagprog if _tagprog is none import re _tagprog = re . compile ( "" ^ ( . * ) match = _tagprog . match ( url ) if match return match . group ( number , number ) return url , none"	0
Q_499	return a ordered_dict order by key name from the unsorted_dict	C_499_2	def _attach_files ( self , files_info ) if not files_info . empty if ( len ( files_info . index . unique ( ) ) = len ( files_info ) ) estr = string estr = string . format ( estr ) estr = string . format ( estr ) print ( estr ) print ( files_info . index . get_duplicates ( ) ) idx = np . unique ( files_info . index , return_index = true ) files_info = files_info . ix [ idx [ number ] ] self . files = files_info . sort_index ( ) date = files_info . index [ number ] self . start_date = pds . datetime ( date . year , date . month , date . day ) date = files_info . index [ _ number ] self . stop_date = pds . datetime ( date . year , date . month , date . day ) else self . start_date = none self . stop_date = none self . files = files_info . astype ( np . dtype ( string ) )	0
Q_500	when bool_encry be true encrypt a chunk of the file with the key and a randomly generate nonce	C_500_0	def encry_decry_chunk ( chunk , key , algo , bool_encry , assoc_data ) engine = botan . cipher ( algo = algo , encrypt = bool_encry ) engine . set_key ( key = key ) engine . set_assoc_data ( assoc_data ) if bool_encry is true nonce = generate_nonce_timestamp ( ) engine . start ( nonce = nonce ) return nonce + engine . finish ( chunk ) else nonce = chunk [ _nonce_length_ ] encryptedchunk = chunk [ _nonce_length_ _nonce_length_ + _gcmtag_length_ + _chunk_size_ ] engine . start ( nonce = nonce ) decryptedchunk = engine . finish ( encryptedchunk ) if decryptedchunk = = b string raise exception ( string ) return decryptedchunk	1
Q_500	when bool_encry be true encrypt a chunk of the file with the key and a randomly generate nonce	C_500_1	def create_actor_pts ( pts , color , * * kwargs ) array_name = kwargs . get ( string , string ) array_index = kwargs . get ( string , number ) point_size = kwargs . get ( string , number ) point_sphere = kwargs . get ( string , true ) points = vtk . vtk_points ( ) points . set_data ( pts ) polydata = vtk . vtk_poly_data ( ) polydata . set_points ( points ) vertex_filter = vtk . vtk_vertex_glyph_filter ( ) vertex_filter . set_input_data ( polydata ) mapper = vtk . vtk_poly_data_mapper ( ) mapper . set_input_connection ( vertex_filter . get_output_port ( ) ) mapper . set_array_name ( array_name ) mapper . set_array_id ( array_index ) actor = vtk . vtk_actor ( ) actor . set_mapper ( mapper ) actor . get_property ( ) . set_color ( * color ) actor . get_property ( ) . set_point_size ( point_size ) actor . get_property ( ) . set_render_points_as_spheres ( point_sphere ) return actor	0
Q_500	when bool_encry be true encrypt a chunk of the file with the key and a randomly generate nonce	C_500_2	def should_check_ciphersuites ( self ) if isinstance ( self . mykey , priv_key_rsa ) kx = string elif isinstance ( self . mykey , priv_key_ecdsa ) kx = string if get_usable_ciphersuites ( self . cur_pkt . ciphers , kx ) return raise self . no_usable_ciphersuite ( )	0
Q_501	run a local proxy to a service provide by backend	C_501_0	def app ( session_id , app , bind , port ) api_session = none runner = none async def app_setup ( ) nonlocal api_session , runner loop = current_loop ( ) api_session = async_session ( ) protocol = string runner = proxy_runner ( api_session , session_id , app , protocol , bind , port , loop = loop ) await runner . ready ( ) print_info ( string { number } string . format ( app ) + string { number } string . format ( session_id ) + string . format ( protocol , bind , port ) ) async def app_shutdown ( ) nonlocal api_session , runner print_info ( string ) await runner . close ( ) await api_session . close ( ) print_info ( string { } string . format ( app ) ) asyncio_run_forever ( app_setup ( ) , app_shutdown ( ) , stop_signals = { signal . sigint , signal . sigterm } )	1
Q_501	run a local proxy to a service provide by backend	C_501_1	def speziale_grun ( v , v0 , gamma0 , q0 , q1 ) if isuncertainties ( [ v , v0 , gamma0 , q0 , q1 ] ) gamma = gamma0 * unp . exp ( q0 / q1 * ( ( v / v0 ) * * q1 _ number . ) ) else gamma = gamma0 * np . exp ( q0 / q1 * ( ( v / v0 ) * * q1 _ number . ) ) return gamma	0
Q_501	run a local proxy to a service provide by backend	C_501_2	def newround ( number , ndigits = none ) return_int = false if ndigits is none return_int = true ndigits = number if hasattr ( number , string ) return number . _round_ ( ndigits ) if ndigits < number raise not_implemented_error ( string ) exponent = decimal ( string ) * * ( _ ndigits ) if pypy if string in repr ( type ( number ) ) number = float ( number ) if not py26 d = decimal . from_float ( number ) . quantize ( exponent , rounding = round_half_even ) else d = from_float_26 ( number ) . quantize ( exponent , rounding = round_half_even ) if return_int return int ( d ) else return float ( d )	0
Q_502	this method provide easy access to all writer inherit writer class	C_502_0	"def get_concrete_class ( var , class_name ) def recurrent_class_lookup ( var ) for var in var . _subclasses_ ( ) if lower ( var . _name_ ) = = lower ( class_name ) return var elif len ( var . _subclasses_ ( ) ) > number r = recurrent_class_lookup ( var ) if r is not none return r return none var = recurrent_class_lookup ( var ) if var return var else raise value_error ( string class_name string is invalid "" class_name )"	1
Q_502	this method provide easy access to all writer inherit writer class	C_502_1	def rho2theta ( self , rho0 , gamma ) fac = np . sqrt ( np . pi ) * special . gamma ( number . / number * ( _ number + gamma ) ) / special . gamma ( gamma / number . ) * number / ( number _ gamma ) * rho0 theta_e = fac * * ( number . / ( gamma _ number ) ) return theta_e	0
Q_502	this method provide easy access to all writer inherit writer class	C_502_2	def write_compressed_var_array ( fd , array , name ) bd = bytes_io ( ) write_var_array ( bd , array , name ) data = zlib . compress ( bd . getvalue ( ) ) bd . close ( ) fd . write ( struct . pack ( string , etypes [ string ] [ string ] , len ( data ) ) ) fd . write ( data )	0
Q_503	lexer_block opren lexeralt_list cpren	C_503_0	def visit_lexer_block ( self , ctx jsg_parser . lexer_block_context ) self . _rule_pattern + = string self . visit_children ( ctx ) self . _rule_pattern + = string	1
Q_503	lexer_block opren lexeralt_list cpren	C_503_1	"def dorun ( snr = number , njitters = number , samples = number , noise_samples = number , sweeps = number , burn = number ) jitters = np . logspace ( _ number , np . log10 ( number . number ) , njitters ) crbs , vals , errs , poss = [ ] , [ ] , [ ] , [ ] for var , var in enumerate ( jitters ) print string image string "" , s , im , pos = zjitter ( jitter = var ) common . set_image ( s , im , number . number / snr ) crbs . append ( crb ( s ) ) val , err = sample ( s , im , number . number / snr , n = noise_samples , sweeps = sweeps , burn = burn ) poss . append ( pos ) vals . append ( val ) errs . append ( err ) shape0 = ( njitters , samples , _ number ) shape1 = ( njitters , samples , noise_samples , _ number ) crbs = np . array ( crbs ) . reshape ( shape0 ) vals = np . array ( vals ) . reshape ( shape1 ) errs = np . array ( errs ) . reshape ( shape1 ) poss = np . array ( poss ) . reshape ( shape0 ) return [ crbs , vals , errs , poss , jitters ]"	0
Q_503	lexer_block opren lexeralt_list cpren	C_503_2	def gen_keywords ( * args union [ ansi_colors , ansi_styles ] , * * kwargs union [ ansi_colors , ansi_styles ] ) _ > tuple fields tuple = tuple ( ) values tuple = tuple ( ) for var in args fields + = var . _fields values + = var for prefix , var in kwargs . items ( ) fields + = tuple ( map ( lambda x string . join ( [ prefix , x ] ) , var . _fields ) ) values + = var return namedtuple ( string , fields ) ( * values )	0
Q_504	call antlr4 on grammar file	C_504_0	def call_antlr4 ( arg ) string antlr_path = os . path . join ( root_dir , string , string ) classpath = os . pathsep . join ( [ string , string . format ( antlr_path ) , string ] ) generated = os . path . join ( root_dir , string , string , string ) cmd = string { classpath s } string string . format ( * * locals ( ) ) print ( cmd ) proc = subprocess . popen ( cmd . split ( ) , cwd = os . path . join ( root_dir , string , string ) ) proc . communicate ( ) with open ( os . path . join ( generated , string ) , string ) as fid fid . write ( string )	1
Q_504	call antlr4 on grammar file	C_504_1	def parse_qa_tile ( x , y , zoom , data , parse_direction = false , * * kwargs ) import osmqa importer , parser = make_importer_parser ( osmqa . qa_tile_parser , * * kwargs ) parser . parse_data ( x , y , zoom , data ) return importer . get_graph ( parse_direction = parse_direction )	0
Q_504	call antlr4 on grammar file	C_504_2	def adjacent ( self , other ) if not self . is_valid_range ( other ) raise type_error ( string { number . _class_ . _name_ } string . format ( other ) ) elif not self or not other return false return ( ( self . lower = = other . upper and self . lower_inc = other . upper_inc ) or ( self . upper = = other . lower and self . upper_inc = other . lower_inc ) )	0
Q_505	angle between two r_dimensional vector	C_505_0	def vec_angle ( vec1 , vec2 ) import numpy as np from scipy import linalg as spla from . const import prm if len ( vec1 . shape ) = number raise value_error ( string vec1 string ) if len ( vec2 . shape ) = number raise value_error ( string vec2 string ) if vec1 . shape [ number ] = vec2 . shape [ number ] raise value_error ( string ) if spla . norm ( vec1 ) < prm . zero_vec_tol raise value_error ( string vec1 string ) if spla . norm ( vec2 ) < prm . zero_vec_tol raise value_error ( string vec2 string ) dotp = np . dot ( vec1 , vec2 ) / spla . norm ( vec1 ) / spla . norm ( vec2 ) if dotp > number angle = number . elif dotp < _ number angle = number . else angle = np . degrees ( np . arccos ( dotp ) ) return angle	1
Q_505	angle between two r_dimensional vector	C_505_1	def _resolve_workspace ( self ) if self . workspace is none self . workspace = self . resolver . workspace_from_url ( self . mets_url , baseurl = self . src_dir , download = self . download ) self . mets = self . workspace . mets	0
Q_505	angle between two r_dimensional vector	C_505_2	def update_member_data ( self , member , member_data ) self . update_member_data_in ( self . leaderboard_name , member , member_data )	0
Q_506	get index doc_type give a record	C_506_0	def default_record_to_index ( record ) index_names = current_search . mappings . keys ( ) schema = record . get ( string , string ) if isinstance ( schema , dict ) schema = schema . get ( string , string ) index , doc_type = schema_to_index ( schema , index_names = index_names ) if index and doc_type return index , doc_type else return ( current_app . config [ string ] , current_app . config [ string ] )	1
Q_506	get index doc_type give a record	C_506_1	def generate_strip_subparser ( subparsers ) parser = subparsers . add_parser ( string , description = constants . strip_description , epilog = constants . strip_epilog , formatter_class = paragraph_formatter , help = constants . strip_help ) parser . set_defaults ( func = strip_files ) utils . add_common_arguments ( parser ) parser . add_argument ( string , help = constants . strip_input_help , metavar = string ) parser . add_argument ( string , help = constants . strip_output_help , metavar = string )	0
Q_506	get index doc_type give a record	C_506_2	def get_default_config ( self ) config = super ( beanstalkd_collector , self ) . get_default_config ( ) config . update ( { string string , string string , string number , } ) return config	0
Q_507	first non_zero atomic displacement non_orthogonal to vec	C_507_0	def _fadn_orth ( vec , geom ) import numpy as np from scipy import linalg as spla from . const import prm from . error import inertia_error from . vector import orthonorm_check as onchk if not ( len ( geom . shape ) = = number and geom . shape [ number ] number = = number ) raise value_error ( string ) if not vec . shape = = ( number , ) raise value_error ( string ) if spla . norm ( vec ) < prm . zero_vec_tol raise value_error ( string ) vec = vec / spla . norm ( vec ) for var in geom . reshape ( ( geom . shape [ number ] / / number , number ) ) if spla . norm ( var ) > = prm . zero_vec_tol and not onchk ( np . column_stack ( ( var / spla . norm ( var ) , vec / spla . norm ( vec ) ) ) ) [ number ] out_vec = var / spla . norm ( var ) return out_vec else raise inertia_error ( inertia_error . bad_geom , string , string )	1
Q_507	first non_zero atomic displacement non_orthogonal to vec	C_507_1	def get_multiple_data ( ) all_labs = { } all_labs [ string ] = diybio_org . get_labs ( format = string ) all_labs [ string ] = fablabs_io . get_labs ( format = string ) all_labs [ string ] = makeinitaly_foundation . get_labs ( format = string ) all_labs [ string ] = hackaday_io . get_labs ( format = string ) all_labs [ string ] = hackerspaces_org . get_labs ( format = string ) all_labs [ string ] = makery_info . get_labs ( format = string ) all_labs [ string ] = nesta . get_labs ( format = string ) return all_labs	0
Q_507	first non_zero atomic displacement non_orthogonal to vec	C_507_2	def kml_region ( map_source , z , x , y ) map = app . config [ string ] [ map_source ] kml_doc = kml_region ( app . config [ string ] , map , app . config [ string ] , z , x , y ) return kml_response ( kml_doc )	0
Q_508	return a b	C_508_0	def _ge_from_gt ( self , other ) op_result = self . _gt_ ( other ) return op_result or self = = other	1
Q_508	return a b	C_508_1	def heartbeat ( self ) super ( scheduler_metrics_job , self ) . heartbeat ( ) session = settings . session ( ) ti = task_instance successful_tis = ( session . query ( ti ) . filter ( ti . var . in_ ( dag_ids ) ) . filter ( ti . state . in_ ( [ state . success ] ) ) . all ( ) ) session . commit ( ) dagbag = dag_bag ( subdir ) dags = [ dagbag . dags [ var ] for var in dag_ids ] num_task_instances = sum ( [ ( timezone . utcnow ( ) _ task . start_date ) . days for dag in dags for task in dag . tasks ] ) if ( len ( successful_tis ) = = num_task_instances or ( timezone . utcnow ( ) _ self . start_date ) . total_seconds ( ) > max_runtime_secs ) if len ( successful_tis ) = = num_task_instances self . log . info ( string ) else self . log . info ( string ) self . print_stats ( ) set_dags_paused_state ( true ) sys . exit ( )	0
Q_508	return a b	C_508_2	def data_and_labels ( self ) sample_ids = np . array ( self . keys ) label_dict = self . labels matrix = np . full ( [ self . num_samples , self . num_features ] , np . nan ) labels = np . full ( [ self . num_samples , number ] , np . nan ) for var , var in enumerate ( sample_ids ) matrix [ var , ] = self . _data [ var ] labels [ var ] = label_dict [ var ] return matrix , np . ravel ( labels ) , sample_ids	0
Q_509	return the best match implementation from registry for type cl	C_509_0	def _find_impl ( cls , registry ) mro = _compose_mro ( cls , registry . keys ( ) ) match = none for var in mro if match is not none if ( var in registry and var not in cls . _mro_ and match not in cls . _mro_ and not issubclass ( match , var ) ) raise runtime_error ( string . format ( match , var ) ) break if var in registry match = var return registry . get ( match )	1
Q_509	return the best match implementation from registry for type cl	C_509_1	def get_conv_widget ( self , conv_id ) if conv_id not in self . _conv_widgets set_title_cb = ( lambda widget , title self . _tabbed_window . set_tab ( widget , title = title ) ) widget = conversation_widget ( self . _client , self . _coroutine_queue , self . _conv_list . get ( conv_id ) , set_title_cb , self . _keys , self . _datetimefmt ) self . _conv_widgets [ conv_id ] = widget return self . _conv_widgets [ conv_id ]	0
Q_509	return the best match implementation from registry for type cl	C_509_2	def cmd ( send , msg , args ) if msg = = string fortunes = list_fortunes ( ) + list_fortunes ( true ) send ( string . join ( fortunes ) , ignore_length = true ) else output = get_fortune ( msg , args [ string ] ) for var in output . splitlines ( ) send ( var )	0
Q_510	validate each member of the list if member_type exist	C_510_0	def validate_member_type ( self , value ) if self . member_type for var in value self . member_type . validate ( var )	1
Q_510	validate each member of the list if member_type exist	C_510_1	def append_qs ( url , query_string ) parsed_url = urlsplit ( url ) parsed_qs = parse_qsl ( parsed_url . query , true ) if isstr ( query_string ) parsed_qs + = parse_qsl ( query_string ) elif isdict ( query_string ) for var in list ( query_string . items ( ) ) if islist ( var [ number ] ) for val in var [ number ] parsed_qs . append ( ( var [ number ] , val ) ) else parsed_qs . append ( var ) elif islist ( query_string ) parsed_qs + = query_string else raise type_error ( string ) return urlunsplit ( ( parsed_url . scheme , parsed_url . netloc , parsed_url . path , urlencode_unicode ( parsed_qs ) , parsed_url . fragment , ) )	0
Q_510	validate each member of the list if member_type exist	C_510_2	def _get_script ( self ) if settings . repeat repeat_fuck = string . format ( get_alias ( ) , string if settings . debug else string , shell . quote ( self . script ) ) return shell . or_ ( self . script , repeat_fuck ) else return self . script	0
Q_511	revert all file in this changelist then delete the changelist from perforce	C_511_0	def delete ( self ) try self . revert ( ) except errors . changelist_error pass self . _connection . run ( [ string , string , str ( self . _change ) ] )	1
Q_511	revert all file in this changelist then delete the changelist from perforce	C_511_1	def setup_project_view ( self ) string string string for var in [ number , number , number ] self . hide_column ( var ) self . set_header_hidden ( true ) self . filter_directories ( )	0
Q_511	revert all file in this changelist then delete the changelist from perforce	C_511_2	def get_extra_values ( conf , _prepend = ( ) ) out = [ ] out . extend ( [ ( _prepend , var ) for var in conf . extra_values ] ) for var in conf . sections if var not in conf . extra_values out . extend ( get_extra_values ( conf [ var ] , _prepend + ( var , ) ) ) return out	0
Q_512	return qmed estimation base on feh catchment descriptor tagint methodology	C_512_0	def _qmed_from_descriptors_2008 ( self , as_rural = false , donor_catchments = none ) try lnqmed_rural = number . number + number . number * log ( self . catchment . descriptors . dtm_area ) _ number . number * number / self . catchment . descriptors . saar + number . number * log ( self . catchment . descriptors . farl ) _ number . number number * self . catchment . descriptors . bfihost * * number . number qmed_rural = exp ( lnqmed_rural ) self . results_log [ string ] = qmed_rural if donor_catchments is none donor_catchments = self . find_donor_catchments ( ) if donor_catchments weights = self . _vec_alpha ( donor_catchments ) errors = self . _vec_lnqmed_residuals ( donor_catchments ) correction = np . dot ( weights , errors ) lnqmed_rural + = correction qmed_rural = exp ( lnqmed_rural ) self . results_log [ string ] = donor_catchments for var , var in enumerate ( self . results_log [ string ] ) var . weight = weights [ var ] var . factor = exp ( errors [ var ] ) self . results_log [ string ] = exp ( correction ) self . results_log [ string ] = qmed_rural if as_rural return qmed_rural else urban_adj_factor = self . urban_adj_factor ( ) self . results_log [ string ] = self . results_log [ string ] * urban_adj_factor return qmed_rural * urban_adj_factor except ( type_error , key_error ) raise insufficient_data_error ( string )	1
Q_512	return qmed estimation base on feh catchment descriptor tagint methodology	C_512_1	def search_anime ( self , query ) r = self . _query_ ( string , string , params = { string query } ) results = [ anime ( var ) for var in r . json ( ) ] return results	0
Q_512	return qmed estimation base on feh catchment descriptor tagint methodology	C_512_2	def get_folders ( self , limit = none , * , query = none , order_by = none , batch = none ) if self . root url = self . build_url ( self . _endpoints . get ( string ) ) else url = self . build_url ( self . _endpoints . get ( string ) . format ( id = self . folder_id ) ) if limit is none or limit > self . protocol . max_top_value batch = self . protocol . max_top_value params = { string batch if batch else limit } if order_by params [ string ] = order_by if query if isinstance ( query , str ) params [ string ] = query else params . update ( query . as_params ( ) ) response = self . con . get ( url , params = params ) if not response return [ ] data = response . json ( ) self_class = getattr ( self , string , type ( self ) ) folders = [ self_class ( parent = self , * * { self . _cloud_data_key var } ) for var in data . get ( string , [ ] ) ] next_link = data . get ( next_link_keyword , none ) if batch and next_link return pagination ( parent = self , data = folders , constructor = self_class , next_link = next_link , limit = limit ) else return folders	0
Q_513	setter method for l2_auth_profile map from yang variable isis_state router_isis_config l2_auth_profile	C_513_0	def _set_l2_auth_profile ( self , v , load = false ) if hasattr ( v , string ) v = v . _utype ( v ) try t = yang_dyn_class ( v , base = l2_auth_profile . l2_auth_profile , is_container = string , presence = false , yang_name = string , rest_name = string , parent = self , path_helper = self . _path_helper , extmethods = self . _extmethods , register_paths = true , extensions = { u string { u string u string } } , namespace = string , defining_module = string , yang_type = string , is_config = false ) except ( type_error , value_error ) raise value_error ( { string string string string , string string , string string string container string l2 _ auth _ profile string l2 _ auth _ profile string tailf _ common string callpoint string isis _ authentication _ profile _ l2 _ auth _ profile _ number string urn brocade . com mgmt brocade _ isis _ operational string brocade _ isis _ operational string container string string , } ) self . _l2_auth_profile = t if hasattr ( self , string ) self . _set ( )	1
Q_513	setter method for l2_auth_profile map from yang variable isis_state router_isis_config l2_auth_profile	C_513_1	def get_go2parents_go2obj ( go2obj ) goobjs , altgo2goobj = get_goobjs_altgo2goobj ( go2obj ) go2parents = get_id2parents ( goobjs ) add_alt_goids ( go2parents , altgo2goobj ) return go2parents	0
Q_513	setter method for l2_auth_profile map from yang variable isis_state router_isis_config l2_auth_profile	C_513_2	def init ( force ) click . secho ( string , fg = string , bold = true , file = sys . stderr ) with click . progressbar ( current_search . create ( ignore = [ number ] if force else none ) , length = current_search . number_of_indexes ) as bar for var , var in bar bar . label = var click . secho ( string , fg = string , bold = true , file = sys . stderr ) with click . progressbar ( current_search . put_templates ( ignore = [ number ] if force else none ) , length = len ( current_search . templates . keys ( ) ) ) as bar for var in bar bar . label = var	0
Q_514	check if the nepali date be in valid range for conversion	C_514_0	def check_valid_bs_range ( date ) err_msg = string str ( date ) if date . year < values . start_np_year or date . year > values . end_np_year raise value_error ( err_msg ) if date . month < number or date . month > number raise value_error ( err_msg ) if date . day < number or date . day > values . nepali_month_day_data [ date . year ] [ date . month _ number ] raise value_error ( err_msg ) return true	1
Q_514	check if the nepali date be in valid range for conversion	C_514_1	def sniff_hosts ( self , initial = false ) node_info = self . _get_sniff_data ( initial ) hosts = list ( filter ( none , ( self . _get_host_info ( var ) for var in node_info ) ) ) if not hosts raise transport_error ( string , string ) self . set_connections ( hosts )	0
Q_514	check if the nepali date be in valid range for conversion	C_514_2	def add_disk_encryption_password ( self , id_p , password , clear_on_suspend ) if not isinstance ( id_p , basestring ) raise type_error ( string ) if not isinstance ( password , basestring ) raise type_error ( string ) if not isinstance ( clear_on_suspend , bool ) raise type_error ( string ) self . _call ( string , in_p = [ id_p , password , clear_on_suspend ] )	0
Q_515	setter method for l2traceroute map from yang variable brocade_trilloam_rpc l2traceroute	C_515_0	def _set_l2traceroute ( self , v , load = false ) if hasattr ( v , string ) v = v . _utype ( v ) try t = yang_dyn_class ( v , base = l2traceroute . l2traceroute , is_leaf = true , yang_name = string , rest_name = string , parent = self , path_helper = self . _path_helper , extmethods = self . _extmethods , register_paths = false , extensions = { u string { u string u string , u string u string } } , namespace = string , defining_module = string , yang_type = string , is_config = true ) except ( type_error , value_error ) raise value_error ( { string string string string , string string , string string string l2traceroute string l2traceroute string tailf _ common string hidden string rpccmd string actionpoint string l2traceroute _ action _ point string urn brocade . com mgmt brocade _ trilloam string brocade _ trilloam string rpc string string , } ) self . _l2traceroute = t if hasattr ( self , string ) self . _set ( )	1
Q_515	setter method for l2traceroute map from yang variable brocade_trilloam_rpc l2traceroute	C_515_1	def _maintain_backwards_compatibility ( binding ) for var in ( string , string , string , string , string , string , string , string , string ) setattr ( binding , var , getattr ( self , var ) ) self . _added_ . append ( var ) setattr ( binding , string , self . _version_ ) self . _added_ . append ( string )	0
Q_515	setter method for l2traceroute map from yang variable brocade_trilloam_rpc l2traceroute	C_515_2	def list_prepend ( self , key , value , create = false , * * kwargs ) op = sd . array_prepend ( string , value ) sdres = self . mutate_in ( key , op , * * kwargs ) return self . _wrap_dsop ( sdres )	0
Q_516	parse from a string representation	C_516_0	def loads ( cls , s str ) _ > string try currency , amount = s . strip ( ) . split ( ) return cls ( amount , currency ) except value_error as err raise value_error ( string string { } string . format ( s , err ) )	1
Q_516	parse from a string representation	C_516_1	def create_model ( self , model ) super ( ) . create_model ( model ) for var in self . post_processing_mixins var . create_model ( model )	0
Q_516	parse from a string representation	C_516_2	def _write_config_file ( batch_id , caller_names , base_dir , data ) config_dir = utils . safe_makedir ( os . path . join ( base_dir , string ) ) config_file = os . path . join ( config_dir , string . format ( batch_id ) ) algorithm = data [ string ] [ string ] econfig = { string algorithm [ string ] , string caller_names , string false } intervals = validate . get_analysis_intervals ( data , none , base_dir ) if intervals econfig [ string ] = os . path . abspath ( intervals ) with open ( config_file , string ) as out_handle yaml . safe_dump ( econfig , out_handle , allow_unicode = false , default_flow_style = false ) return config_file	0
Q_517	build a recipe object from a give mode name	C_517_0	def get_recipe_object ( self , mode_name , pipeline_name = string ) active_mode = self . modes [ mode_name ] active_pipeline = self . pipelines [ pipeline_name ] recipe = active_pipeline . get_recipe_object ( active_mode ) return recipe	1
Q_517	build a recipe object from a give mode name	C_517_1	def scan_threads ( self ) dw_process_id = self . get_pid ( ) if dw_process_id in ( number , number , number ) return dead_tids = self . _get_thread_ids ( ) dw_process_id = self . get_pid ( ) h_snapshot = win32 . create_toolhelp32_snapshot ( win32 . th32_cs_snapthread , dw_process_id ) try te = win32 . thread32_first ( h_snapshot ) while te is not none if te . th32_owner_process_id = = dw_process_id dw_thread_id = te . th32_thread_id if dw_thread_id in dead_tids dead_tids . remove ( dw_thread_id ) if not self . _has_thread_id ( dw_thread_id ) a_thread = thread ( dw_thread_id , process = self ) self . _add_thread ( a_thread ) te = win32 . thread32_next ( h_snapshot ) finally win32 . close_handle ( h_snapshot ) for var in dead_tids self . _del_thread ( var )	0
Q_517	build a recipe object from a give mode name	C_517_2	def set_template ( self , id_environment , name , network ) url = string + str ( id_environment ) + string environment_map = dict ( ) environment_map [ string ] = name environment_map [ string ] = network code , xml = self . submit ( { string environment_map } , string , url ) return self . response ( code , xml )	0
Q_518	add a model instance to session and commit the	C_518_0	def add ( cls , model , commit = true ) if not isinstance ( model , cls ) raise value_error ( string ( model , cls ) ) cls . session . add ( model ) try if commit cls . session . commit ( ) return model except cls . session . rollback ( ) raise	1
Q_518	add a model instance to session and commit the	C_518_1	def predict ( self , test_x ) with self . tf_graph . as_default ( ) with tf . session ( ) as self . tf_session self . tf_saver . restore ( self . tf_session , self . model_path ) feed = { self . input_data test_x , self . keep_prob number } return self . mod_y . eval ( feed )	0
Q_518	add a model instance to session and commit the	C_518_2	"def replacement_template ( rep , source , span , npar ) n = number res = string while n < len ( rep ) _ number char = rep [ n ] if char = = string if rep [ n + number ] = = string res + = string n + = number continue elif rep [ n + number ] = = string res + = source [ span [ number ] ] n + = number continue elif rep [ n + number ] = = string string string "" n + = number + len ( dig ) continue res + = char n + = number if n < len ( rep ) res + = rep [ _ number ] return res"	0
Q_519	a convenience wrapper for _post	C_519_0	def _api_post ( self , url , * * kwargs ) kwargs [ string ] = self . url + url kwargs [ string ] = self . auth headers = deepcopy ( self . headers ) headers . update ( kwargs . get ( string , { } ) ) kwargs [ string ] = headers self . _post ( * * kwargs )	1
Q_519	a convenience wrapper for _post	C_519_1	def body ( circuit , settings ) qubit_instruction_mapping = { } for var in circuit if isinstance ( var , measurement ) var . qubits = [ var . qubit ] var . name = string else qubits = var . qubits for qubit in qubits qubit_instruction_mapping [ qubit . index ] = [ ] for k , v in list ( qubit_instruction_mapping . items ( ) ) v . append ( command ( allocate , [ k ] , [ ] , [ k ] , k ) ) for var in circuit qubits = [ qubit . index for qubit in var . qubits ] gate = var . name if len ( qubits ) = = number for qubit in qubits qubit_instruction_mapping [ qubit ] . append ( command ( gate , [ qubit ] , [ ] , [ qubit ] , qubit ) ) else explicit_lines = [ qubit for qubit in copy ( qubits ) ] all_lines = list ( range ( min ( explicit_lines ) , max ( explicit_lines ) + number ) ) for line in all_lines if line not in qubit_instruction_mapping . keys ( ) and line in all_lines all_lines . remove ( line ) for i , qubit in enumerate ( all_lines ) if gate = = cz ctrl_lines = list ( explicit_lines ) ctrl_lines . remove ( qubits [ _ number ] ) qubit_instruction_mapping [ qubit ] . append ( command ( z , list ( all_lines ) , list ( ctrl_lines ) , qubits [ _ number ] , none ) ) elif gate = = cnot ctrl_lines = list ( explicit_lines ) ctrl_lines . remove ( qubits [ _ number ] ) qubit_instruction_mapping [ qubit ] . append ( command ( x , list ( all_lines ) , list ( ctrl_lines ) , qubits [ _ number ] , none ) ) else qubit_instruction_mapping [ qubit ] . append ( command ( gate , list ( all_lines ) , [ ] , list ( explicit_lines ) , none ) ) relabeled_circuit = { } index_map = { } for i , key in enumerate ( sorted ( qubit_instruction_mapping . keys ( ) ) ) relabeled_circuit [ i ] = qubit_instruction_mapping [ key ] index_map [ key ] = i for line in list ( relabeled_circuit . values ( ) ) for cmd in line for i , qubit in enumerate ( cmd . lines ) cmd . lines [ i ] = index_map [ qubit ] for i , qubit in enumerate ( cmd . ctrl_lines ) cmd . ctrl_lines [ i ] = index_map [ qubit ] for i , qubit in enumerate ( cmd . target_lines ) cmd . target_lines [ i ] = index_map [ qubit ] code_generator = circuit_tikz_generator ( settings ) return code_generator . generate_circuit ( relabeled_circuit )	0
Q_519	a convenience wrapper for _post	C_519_2	def local_position_ned_system_global_offset_send ( self , time_boot_ms , x , y , z , roll , pitch , yaw , force_mavlink1 = false ) return self . send ( self . local_position_ned_system_global_offset_encode ( time_boot_ms , x , y , z , roll , pitch , yaw ) , force_mavlink1 = force_mavlink1 )	0
Q_520	try to find out what type of variant that exist in a variant source	C_520_0	def get_variant_type ( variant_source ) file_type = get_file_type ( variant_source ) variant_type = string if file_type = = string variants = vcf ( variant_source ) elif file_type = = string variants = gemini_query ( variant_source ) gemini_query = string variants . run ( gemini_query ) for var , var in enumerate ( variants ) if file_type = = string if var . is_snp variant_type = string elif file_type = = string if var [ string ] = = string variant_type = string if var > number break return variant_type	1
Q_520	try to find out what type of variant that exist in a variant source	C_520_1	def _get_engineio_url ( self , url , engineio_path , transport ) engineio_path = engineio_path . strip ( string ) parsed_url = urllib . parse . urlparse ( url ) if transport = = string scheme = string elif transport = = string scheme = string else raise value_error ( string ) if parsed_url . scheme in [ string , string ] scheme + = string return ( string string ) . format ( scheme = scheme , netloc = parsed_url . netloc , path = engineio_path , query = parsed_url . query , sep = string if parsed_url . query else string , transport = transport )	0
Q_520	try to find out what type of variant that exist in a variant source	C_520_2	def sort_amounts ( proteins , sort_index ) amounts = { } for var in proteins amount_x_for_protein = var [ sort_index ] try amounts [ amount_x_for_protein ] . append ( var ) except key_error amounts [ amount_x_for_protein ] = [ var ] return [ v for k , v in sorted ( amounts . items ( ) , reverse = true ) ]	0
Q_521	set all the expect configuration option on the config object as either the request default value or none	C_521_0	def _set_defaults ( self ) for var , var in self . configs . items ( ) self . _set_config ( var , var [ string ] )	1
Q_521	set all the expect configuration option on the config object as either the request default value or none	C_521_1	def analyze ( self , handle , filename ) files = { string ( filename , handle ) } handle . seek ( number ) data = { string string application string number string timeout string number string priority string number string profiles string s string analysistype string number string force string true string prefetch string number string self . profile , } response = self . _request ( string , method = string , params = data , files = files ) try if response . status_code = = number try return response . json ( ) [ string ] except type_error return response . json ( ) [ number ] [ string ] else raise sandboxapi . sandbox_error ( string . format ( u = response . url , r = response . content ) ) except ( value_error , key_error ) as e raise sandboxapi . sandbox_error ( string . format ( e = e ) )	0
Q_521	set all the expect configuration option on the config object as either the request default value or none	C_521_2	def make_table ( self , tablename str , fieldspeclist fieldspeclist_type , dynamic bool = false , compressed bool = false ) _ > optional [ int ] if self . table_exists ( tablename ) log . info ( string + tablename + string ) return none if not self . is_mysql ( ) dynamic = false compressed = false sql = string string string . format ( tablename = tablename , fieldspecs = self . fielddefsql_from_fieldspeclist ( fieldspeclist ) , dynamic = string if dynamic else string , compressed = string if compressed else string , ) log . info ( string + tablename ) return self . db_exec_literal ( sql )	0
Q_522	delete the key from the keyring and the key and address object from the	C_522_0	def force_delete_key ( address ) address_object = address . objects . get ( address = address ) address_object . key . delete ( ) address_object . delete ( )	1
Q_522	delete the key from the keyring and the key and address object from the	C_522_1	def st_spectral_roll_off ( x , c , fs ) total_energy = numpy . sum ( x * * number ) fft_length = len ( x ) thres = c * total_energy cum_sum = numpy . cumsum ( x * * number ) + eps [ a , ] = numpy . nonzero ( cum_sum > thres ) if len ( a ) > number m_c = numpy . float64 ( a [ number ] ) / ( float ( fft_length ) ) else m_c = number . number return ( m_c )	0
Q_522	delete the key from the keyring and the key and address object from the	C_522_2	def _parse_relationships_xml_file ( self , xml_data ) xml_root = element_tree . fromstring ( xml_data ) property_files = [ ] for var in xml_root . iter ( ) type_attribute = var . get ( string ) if string in repr ( type_attribute ) target_attribute = var . get ( string ) property_files . append ( target_attribute ) return property_files	0
Q_523	retrieve all episode for a particular series give it the_tvdb id	C_523_0	def get_series_episodes ( self , series_id , page = number ) raw_response = requests_util . run_request ( string , self . api_base_url + string ( series_id , page ) , headers = self . _get_header_with_auth ( ) ) return self . parse_raw_response ( raw_response )	1
Q_523	retrieve all episode for a particular series give it the_tvdb id	C_523_1	def update ( self , * * kwargs ) super ( common_conf , self ) . update ( * * kwargs ) conf_changed = false for var , var in kwargs . items ( ) rtconf . base . get_validator ( var ) ( var ) item1 = self . _settings . get ( var , none ) item2 = kwargs . get ( var , none ) if item1 = item2 conf_changed = true if conf_changed for var , var in kwargs . items ( ) self . _settings [ var ] = var self . _notify_listeners ( common_conf . conf_changed_evt , self )	0
Q_523	retrieve all episode for a particular series give it the_tvdb id	C_523_2	def _get_system ( model_folder ) model_description_file = os . path . join ( model_folder , string ) if not os . path . isfile ( model_description_file ) logging . error ( string string , model_description_file ) sys . exit ( _ number ) with open ( model_description_file , string ) as ymlfile model_desc = yaml . load ( ymlfile ) feature_desc = _get_description ( model_desc ) preprocessing_desc = _get_description ( feature_desc ) return ( preprocessing_desc , feature_desc , model_desc )	0
Q_524	this will return a str of a mark down table	C_524_0	def obj_to_mark_down ( self , file_path = none , title_columns = false , quote_numbers = true , quote_empty_str = false ) md , column_widths = self . get_data_and_shared_column_widths ( data_kwargs = dict ( quote_numbers = quote_numbers , quote_empty_str = quote_empty_str , title_columns = title_columns ) , width_kwargs = dict ( padding = number , pad_last_column = true ) ) md . insert ( number , [ u string + u string * ( var _ number ) for var in column_widths ] ) md = [ u string . join ( [ row [ c ] . ljust ( column_widths [ c ] ) for c in range ( len ( row ) ) ] ) for row in md ] ret = u string + u string . join ( md ) + u string self . _save_file ( file_path , ret ) return ret	1
Q_524	this will return a str of a mark down table	C_524_1	def _store ( self ) name = self . stored_file_name stored_files = self . _load ( ) if len ( stored_files ) = len ( self . files ) new_flag = true elif len ( stored_files ) = = len ( self . files ) if stored_files . eq ( self . files ) . all ( ) new_flag = false else new_flag = true if new_flag if self . write_to_disk stored_files . to_csv ( os . path . join ( self . home_path , string + name ) , date_format = string ) self . files . to_csv ( os . path . join ( self . home_path , name ) , date_format = string ) else self . _previous_file_list = stored_files self . _current_file_list = self . files . copy ( ) return	0
Q_524	this will return a str of a mark down table	C_524_2	def search ( self , * * kwargs ) path = self . _get_path ( string ) response = self . _get ( path , kwargs ) self . _set_attrs_to_values ( response ) return response	0
Q_525	get the short path name of a give long path	C_525_0	def get_win32_short_path_name ( long_name ) import ctypes from ctypes import wintypes _get_short_path_name_w = ctypes . windll . kernel32 . get_short_path_name_w _get_short_path_name_w . argtypes = [ wintypes . lpcwstr , wintypes . lpwstr , wintypes . dword ] _get_short_path_name_w . restype = wintypes . dword output_buf_size = number while true output_buf = ctypes . create_unicode_buffer ( output_buf_size ) needed = _get_short_path_name_w ( long_name , output_buf , output_buf_size ) if output_buf_size > = needed short_name = output_buf . value break else output_buf_size = needed return short_name	1
Q_525	get the short path name of a give long path	C_525_1	def insert_asm ( self , addr , asm_code , before_label = false ) if before_label self . _inserted_asm_before_label [ addr ] . append ( asm_code ) else self . _inserted_asm_after_label [ addr ] . append ( asm_code )	0
Q_525	get the short path name of a give long path	C_525_2	def delete_translations_for_item_and_its_children ( self , item , languages = none ) self . log ( string ) if not self . master self . set_master ( item ) object_name = string . format ( item . _meta . app_label . lower ( ) , item . _meta . verbose_name ) object_class = item . _class_ . _name_ object_pk = item . pk filter_by = { string object_class , string object_name , string object_pk , string false } if languages filter_by . update ( { string languages } ) trans_task . objects . filter ( * * filter_by ) . delete ( ) children = self . get_translatable_children ( item ) for var in children self . delete_translations_for_item_and_its_children ( var , languages )	0
Q_526	call when a pong be receive	C_526_0	def pong_received ( self , payload = none ) if self . _timer is not none self . _timer . cancel ( ) self . _failures = number asyncio . async ( self . send_ping ( payload = payload ) )	1
Q_526	call when a pong be receive	C_526_1	def activate_organizations ( self , user ) try relation_name = self . org_model ( ) . user_relation_name except type_error relation_name = string organization_set = getattr ( user , relation_name ) for var in organization_set . filter ( is_active = false ) var . is_active = true var . save ( )	0
Q_526	call when a pong be receive	C_526_2	def broadcast ( data , root ) rank = get_rank ( ) length = ctypes . c_ulong ( ) if root = = rank assert data is not none , string s = pickle . dumps ( data , protocol = pickle . highest_protocol ) length . value = len ( s ) _lib . rabit_broadcast ( ctypes . byref ( length ) , ctypes . sizeof ( ctypes . c_ulong ) , root ) if root = rank dptr = ( ctypes . c_char * length . value ) ( ) _lib . rabit_broadcast ( ctypes . cast ( dptr , ctypes . c_void_p ) , length . value , root ) data = pickle . loads ( dptr . raw ) del dptr else _lib . rabit_broadcast ( ctypes . cast ( ctypes . c_char_p ( s ) , ctypes . c_void_p ) , length . value , root ) del s return data	0
Q_527	the formatting function	C_527_0	def format ( self , record ) try n = record . n except attribute_error n = string try message = record . message except attribute_error message = record . msg senml = ordered_dict ( uid = string , bt = datetime . utcfromtimestamp ( record . created ) . isoformat ( ) [ _ number ] + string , e = [ ordered_dict ( n = n , v = message ) ] ) formatted_json = json . dumps ( senml ) return formatted_json	1
Q_527	the formatting function	C_527_1	def cookiestring ( self , value ) c = cookie . simple_cookie ( value ) sc = [ ( var . key , var . value ) for var in c . values ( ) ] self . cookies = dict ( sc )	0
Q_527	the formatting function	C_527_2	def _do_autopaginating_api_call ( self , method , kwargs , parser_func ) has_records = { string false } while true try root = self . _do_api_call ( method , kwargs ) except record_does_not_exist_error if not has_records [ string ] raise return records_returned_by_this_loop = false for var in parser_func ( root , has_records ) yield var records_returned_by_this_loop = true if not records_returned_by_this_loop return last_offset = root . find ( string ) . text kwargs [ string ] = last_offset	0
Q_528	get a config tagstr from tagstr file under section tagstr	C_528_0	def get_conf ( conf , sect , opt ) argu = getattr ( args , string + opt . lower ( ) ) if not argu envir = os . environ . get ( string + opt . upper ( ) ) if not envir try return conf . get ( sect , opt ) except no_section_error return default_configs [ opt ] return envir return argu	1
Q_528	get a config tagstr from tagstr file under section tagstr	C_528_1	def parse_bits ( self , hexcode , width ) bitarray = [ ] for var in hexcode [ _ number ] bits = int ( var , number ) for x in range ( number ) bitarray . append ( bool ( ( number * * x ) bits ) ) bitarray = bitarray [ _ number ] return enumerate ( bitarray [ width ] )	0
Q_528	get a config tagstr from tagstr file under section tagstr	C_528_2	def is_enum_type ( type_ ) return isinstance ( type_ , type ) and issubclass ( type_ , tuple ( _get_types ( types . enum ) ) )	0
Q_529	append a type stack after the give stack but only if require	C_529_0	def typify ( self , stack stack ) _ > list [ stack ] if len ( stack . layers ) = = number and isinstance ( stack . layers [ number ] , lyr . typing ) return [ stack ] return [ stack , stack ( [ lyr . typing ( ) ] ) ]	1
Q_529	append a type stack after the give stack but only if require	C_529_1	def results ( self , use_cache = true , dialect = none , billing_tier = none ) if not use_cache or ( self . _results is none ) self . execute ( use_cache = use_cache , dialect = dialect , billing_tier = billing_tier ) return self . _results . results	0
Q_529	append a type stack after the give stack but only if require	C_529_2	def create_otu_table ( output_fp , deblurred_list , outputfasta_fp = none , minreads = number ) logger = logging . get_logger ( _name_ ) logger . info ( string string ( len ( deblurred_list ) , output_fp ) ) sizeregexp = re . compile ( string ) seqdict = { } seqlist = [ ] sampset = set ( ) samplist = [ ] obs = scipy . sparse . dok_matrix ( ( int ( number ) , len ( deblurred_list ) ) , dtype = np . double ) sneaking_extensions = { string , string , string , string , string } for ( cfilename , csampleid ) in deblurred_list if csampleid . rsplit ( string , number ) [ _ number ] in sneaking_extensions csampleid = csampleid . rsplit ( string , number ) [ number ] if csampleid in sampset warnings . warn ( string , user_warning ) logger . error ( string csampleid ) continue sampset . add ( csampleid ) samplist . append ( csampleid ) csampidx = len ( sampset ) _ number for var , var in sequence_generator ( cfilename ) var = var . upper ( ) if var not in seqdict seqdict [ var ] = len ( seqlist ) seqlist . append ( var ) cseqidx = seqdict [ var ] cfreq = float ( sizeregexp . search ( var ) . group ( number ) ) try obs [ cseqidx , csampidx ] + = cfreq except index_error shape = obs . shape obs . resize ( ( shape [ number ] * number , shape [ number ] ) ) obs [ cseqidx , csampidx ] = cfreq logger . info ( string ( len ( samplist ) , len ( seqlist ) ) ) obs . resize ( ( len ( seqlist ) , len ( samplist ) ) ) if minreads > number readsperotu = obs . sum ( axis = number ) keep = np . where ( readsperotu > = minreads ) [ number ] logger . info ( string ( len ( keep ) , len ( seqlist ) , minreads ) ) obs = obs [ keep , ] seqlist = list ( np . array ( seqlist ) [ keep ] ) logger . debug ( string ) table = table ( obs . tocsr ( ) , seqlist , samplist , observation_metadata = none , sample_metadata = none , table_id = none , generated_by = string , create_date = datetime . now ( ) . isoformat ( ) ) logger . debug ( string ) filter_minreads_samples_from_table ( table ) write_biom_table ( table , output_fp ) logger . info ( string output_fp ) if outputfasta_fp is not none logger . debug ( string ) with open ( outputfasta_fp , string ) as f for var in seqlist f . write ( string ( var , var ) ) logger . info ( string outputfasta_fp )	0
Q_530	extract only the valid sentence group into a dictionary	C_530_0	def extract ( self ) out = { } for var , var in self . data . items ( ) out [ var ] = var return out	1
Q_530	extract only the valid sentence group into a dictionary	C_530_1	def construct_url ( self ) path = [ self . path ] path . extend ( self . params ) url = self . client . base_url + string . join ( str ( var ) for var in path if var ) query = self . encode_query ( self . query ) if query url + = string + query return url	0
Q_530	extract only the valid sentence group into a dictionary	C_530_2	def string_to_genomic_range ( rstring ) m = re . match ( string , rstring ) if not m sys . stderr . write ( string + rstring + string ) return genomic_range ( m . group ( number ) , int ( m . group ( number ) ) , int ( m . group ( number ) ) )	0
Q_531	set the central widget for this overlay to the input widget	C_531_0	def set_central_widget ( self , widget ) self . _central_widget = widget if widget is not none widget . set_parent ( self ) widget . install_event_filter ( self ) effect = qt_gui . q_graphics_drop_shadow_effect ( self ) effect . set_color ( qt_gui . q_color ( string ) ) effect . set_blur_radius ( number ) effect . set_offset ( number , number ) widget . set_graphics_effect ( effect )	1
Q_531	set the central widget for this overlay to the input widget	C_531_1	def sample ( self , size = none ) self . _recompute ( ) if size is none n = np . random . randn ( len ( self . _t ) ) else n = np . random . randn ( len ( self . _t ) , size ) n = self . solver . dot_l ( n ) if size is none return self . mean . get_value ( self . _t ) + n [ , number ] return self . mean . get_value ( self . _t ) [ none , ] + n . t	0
Q_531	set the central widget for this overlay to the input widget	C_531_2	def run ( self ) self . factory . register ( user , self . users_factory ) self . factory ( user , number ) . create ( )	0
Q_532	fetch the module	C_532_0	def fetch_items ( self , category , * * kwargs ) from_date = kwargs [ string ] logger . info ( string , str ( from_date ) ) from_date_ts = datetime_to_utc ( from_date ) . timestamp ( ) nmodules = number stop_fetching = false raw_pages = self . client . modules ( ) for var in raw_pages modules = [ mod for mod in self . parse_json ( var ) ] for module in modules updated_at_ts = self . metadata_updated_on ( module ) if from_date_ts > updated_at_ts stop_fetching = true break owner = module [ string ] [ string ] name = module [ string ] module [ string ] = self . _fetch_and_parse_releases ( owner , name ) module [ string ] = self . _get_or_fetch_owner ( owner ) yield module nmodules + = number if stop_fetching break logger . info ( string , nmodules )	1
Q_532	fetch the module	C_532_1	def generar_pdf ( self , archivo = string , dest = string ) string try self . template . render ( archivo , dest = dest ) return true except exception , e self . excepcion = str ( e ) return false	0
Q_532	fetch the module	C_532_2	def _prep_points_for_segments ( points ) while number point = points [ _ number ] if point . segment_type break else point = points . pop ( ) points . insert ( number , point ) continue break	0
Q_533	collect all the record instance from the tree	C_533_0	def collect_records ( self , parent = none ) string string string out = [ ] if not parent for var in range ( self . top_level_item_count ( ) ) item = self . top_level_item ( var ) try out . append ( item . record ( ) ) except attribute_error pass out + = self . collect_records ( item ) else for c in range ( parent . child_count ( ) ) item = parent . child ( c ) try out . append ( item . record ( ) ) except attribute_e_rror pass out + = self . collect_records ( item ) return out	1
Q_533	collect all the record instance from the tree	C_533_1	def check_credentials ( self , username , password ) return password is not none and self . credentials . get ( username , none ) = = password	0
Q_533	collect all the record instance from the tree	C_533_2	def get_by_alias ( self , alias ) if alias not in self . _aliases raise data_invalid_alias ( string . format ( alias ) ) return self . get_by_index ( self . _aliases [ alias ] )	0
Q_534	url encode a single string with a give encoding	C_534_0	def url_quote ( s , charset = string , safe = string ) if isinstance ( s , unicode ) s = s . encode ( charset ) elif not isinstance ( s , str ) s = str ( s ) return _quote ( s , safe = safe )	1
Q_534	url encode a single string with a give encoding	C_534_1	def register_callback_deleted ( self , func , serialised = true ) self . _add_callback ( _cb_deleted , func , serialised_if_crud = serialised )	0
Q_534	url encode a single string with a give encoding	C_534_2	def _slurm_info ( queue ) cl = string c m d string . format ( queue ) num_cpus , mem , num_nodes = subprocess . check_output ( shlex . split ( cl ) ) . decode ( ) . split ( ) mem = float ( mem . replace ( string , string ) ) num_cpus = int ( num_cpus . replace ( string , string ) ) bcbio_mem = number controller_mem = number if int ( num_nodes ) < number and mem > ( bcbio_mem + controller_mem ) * number mem = mem _ bcbio_mem _ controller_mem return [ { string int ( num_cpus ) , string mem / number . number , string string } ]	0
Q_535	display the group menu to the user for modification	C_535_0	def show_group_menu ( self ) string string string group_active = self . is_grouping_active ( ) group_by = self . group_by ( ) menu = x_menu ( self ) menu . set_title ( string ) menu . set_show_title ( true ) menu . add_action ( string ) menu . add_separator ( ) action = menu . add_action ( string ) action . set_checkable ( true ) action . set_checked ( not group_active ) action = menu . add_action ( string ) action . set_checkable ( true ) action . set_checked ( group_by = = self . group_by_advanced_key and group_active ) if ( group_by = = self . group_by_advanced_key ) font = action . font ( ) font . set_bold ( true ) action . set_font ( font ) menu . add_separator ( ) table_type = self . table_type ( ) if ( table_type ) columns = table_type . schema ( ) . columns ( ) columns . sort ( key = lambda x x . display_name ( ) ) for var in columns action = menu . add_action ( var . display_name ( ) ) action . set_checkable ( true ) action . set_checked ( group_by = = var . display_name ( ) and group_active ) if ( var . display_name ( ) = = group_by ) font = action . font ( ) font . set_bold ( true ) action . set_font ( font ) point = q_point ( number , self . ui_group_options_btn . height ( ) ) action = menu . exec_ ( self . ui_group_options_btn . map_to_global ( point ) ) if ( not action ) return elif ( action . text ( ) = = string ) print string elif ( action . text ( ) = = string ) self . set_grouping_active ( false ) elif ( action . text ( ) = = string ) self . ui_group_btn . block_signals ( true ) self . set_group_by ( self . group_by_advanced_key ) self . set_grouping_active ( true ) self . ui_group_btn . block_signals ( false ) self . refresh_results ( ) else self . ui_group_btn . block_signals ( true ) self . set_group_by ( nativestring ( action . text ( ) ) ) self . set_grouping_active ( true ) self . ui_group_btn . block_signals ( false ) self . refresh_results ( )	1
Q_535	display the group menu to the user for modification	C_535_1	def disable_nn_ha ( self , active_name , snn_host_id , snn_check_point_dir_list , snn_name = none ) args = dict ( active_nn_name = active_name , snn_host_id = snn_host_id , snn_checkpoint_dir_list = snn_check_point_dir_list , snn_name = snn_name ) return self . _cmd ( string , data = args , api_version = number )	0
Q_535	display the group menu to the user for modification	C_535_2	def load_module ( filename , code_objects = none , fast_load = false , get_code = true ) if not osp . exists ( filename ) raise import_error ( string s string t exist string file name string isn string filename ) elif osp . getsize ( filename ) < number raise import_error ( string s ( d bytes ) string ( filename , osp . getsize ( filename ) ) ) with open ( filename , string ) as fp return load_module_from_file_object ( fp , filename = filename , code_objects = code_objects , fast_load = fast_load , get_code = get_code )	0
Q_536	set up a tcp client and install the basic handle stdin stdout	C_536_0	def create_client ( addr , port ) sock = socket . socket ( socket . af_inet , socket . sock_stream ) sock . connect_ex ( ( addr , port ) ) spin = spin ( sock ) client ( spin ) spin . add_map ( connect , install_basic_handles ) spin . add_map ( connect_err , lambda con , err lose ( con ) ) return spin	1
Q_536	set up a tcp client and install the basic handle stdin stdout	C_536_1	def run_stop_backup ( cls ) def handler ( popen ) assert popen . returncode = number raise user_exception ( string ) return cls . _dict_transform ( psql_csv_run ( string string number string string string . format ( cls . _wal_name ( ) ) , error_handler = handler ) )	0
Q_536	set up a tcp client and install the basic handle stdin stdout	C_536_2	def emit_record_middle_clicked ( self , item ) string string string if isinstance ( item , x_orb_record_item ) and not self . signals_blocked ( ) self . record_middle_clicked . emit ( item . record ( ) )	0
Q_537	return standard molecular weight	C_537_0	def mw ( mol , ndigits = number ) mol . require ( string ) return round ( sum ( var . mw ( ) for var , var in mol . atoms_iter ( ) ) , ndigits )	1
Q_537	return standard molecular weight	C_537_1	def customize_ruleset ( self , custom_ruleset_file = none ) custom_file = custom_ruleset_file or os . environ . get ( string ) if not custom_file return with open ( custom_file , string ) as additional_rules custom_rules = additional_rules . read ( ) if string not in custom_rules raise a11y_audit_config_error ( string var custom_rules string ) self . custom_rules = custom_rules	0
Q_537	return standard molecular weight	C_537_2	def add ( self , * args ) for var in args if isinstance ( var , numbers . number ) var = str ( var ) if isinstance ( var , basestring ) var = escape ( var ) self . children . append ( var ) elif isinstance ( var , dom_tag ) ctx = dom_tag . _with_contexts [ _get_thread_context ( ) ] if ctx and ctx [ _ number ] ctx [ _ number ] . used . add ( var ) self . children . append ( var ) var . parent = self var . setdocument ( self . document ) elif isinstance ( var , dict ) for attr , value in var . items ( ) self . set_attribute ( * dom_tag . clean_pair ( attr , value ) ) elif hasattr ( var , string ) for subobj in var self . add ( subobj ) else raise value_error ( string var ) if len ( args ) = = number return args [ number ] return args	0
Q_538	populate the give profile object from auth_ldap_profile_attr_map	C_538_0	def _populate_profile_from_attributes ( self , profile ) save_profile = false for var , var in self . settings . profile_attr_map . items ( ) try setattr ( profile , var , self . attrs [ var ] [ number ] ) save_profile = true except exception logger . warning ( string , self . dn , var ) return save_profile	1
Q_538	populate the give profile object from auth_ldap_profile_attr_map	C_538_1	def sample ( self , bqm , * * kwargs ) embedded_bqm = dimod . binary_quadratic_model . empty ( bqm . vartype ) _ , _ , target_adjacency = self . child . structure for var in self . embeddings embedded_bqm . update ( dwave . var . embed_bqm ( bqm , var , target_adjacency ) ) tiled_response = self . child . sample ( embedded_bqm , * * kwargs ) responses = [ ] for var in self . embeddings var = { v chain for v , chain in var . items ( ) if v in bqm . variables } responses . append ( dwave . var . unembed_sampleset ( tiled_response , var , bqm ) ) return dimod . concatenate ( responses )	0
Q_538	populate the give profile object from auth_ldap_profile_attr_map	C_538_2	def load_extensions ( default , extensions , strict , environ , reload = false ) if default default_extension_path = pth . default_extension ( environ = environ ) pth . ensure_file ( default_extension_path ) extensions = concatv ( [ default_extension_path ] , extensions ) for var in extensions if var in _loaded_extensions and not reload continue try if var . endswith ( string ) with open ( var ) as f ns = { } six . exec_ ( compile ( f . read ( ) , var , string ) , ns , ns ) else _import_ ( var ) except exception as e if strict raise warnings . warn ( string ( var , e ) , stacklevel = number ) else _loaded_extensions . add ( var )	0
Q_539	convenience method to return rdf data for resource	C_539_0	def dump ( self , format = string ) return self . rdf . graph . serialize ( format = format ) . decode ( string )	1
Q_539	convenience method to return rdf data for resource	C_539_1	def timeline_public ( self , max_id = none , min_id = none , since_id = none , limit = none , only_media = false ) if max_id = none max_id = self . _unpack_id ( max_id ) if min_id = none min_id = self . _unpack_id ( min_id ) if since_id = none since_id = self . _unpack_id ( since_id ) params_initial = locals ( ) if only_media = = false del params_initial [ string ] url = string params = self . _generate_params ( params_initial ) return self . _api_request ( string , url , params )	0
Q_539	convenience method to return rdf data for resource	C_539_2	def check_previous_barline ( self , staff ) measure_before_last = self . get_measure_at_position ( _ number , staff ) last_measure = self . get_measure_at_position ( _ number , staff ) if last_measure is not none and measure_before_last is not none bline1 = measure_before_last . get_barline ( string ) bline2 = last_measure . get_barline ( string ) if bline1 is not none if hasattr ( bline1 , string ) if bline2 is not none if not hasattr ( bline2 , string ) bline1 . ending . type = string else bline1 . ending . type = string	0
Q_540	get the next child pid in the pid relation	C_540_0	def next_child ( self , child_pid ) relation = self . _get_child_relation ( child_pid ) if relation . index is not none return self . children . filter ( pid_relation . index > relation . index ) . ordered ( ord = string ) . first ( ) else return none	1
Q_540	get the next child pid in the pid relation	C_540_1	def is_finished ( self ) if self . _total_time > self . _global_time_limit logger . warning ( string . format ( self . _total_time , self . _global_time_limit ) ) return true trials_done = all ( var . is_finished ( ) for var in self . _trials ) return trials_done and self . _search_alg . is_finished ( )	0
Q_540	get the next child pid in the pid relation	C_540_2	def follow ( * args ) parser = argparse . argument_parser ( prog = string ( _package_ , follow . _name_ ) , description = follow . _doc_ ) parser . add_argument ( string , help = string , nargs = string ) args = parser . parse_args ( args ) config = fragments_config ( ) for var , var in _iterate_over_files ( args . filename , config , statuses = string ) fullpath = os . path . realpath ( var ) if fullpath . startswith ( config . root ) key = os . path . relpath ( fullpath , config . root ) if key in config [ string ] yield string var string os . path . relpath ( var ) continue if os . access ( fullpath , os . w_ok os . r_ok ) file_sha = _file_key ( key ) config [ string ] [ key ] = file_sha yield string var string var string ( os . path . relpath ( var ) , file_sha ) else yield string var string os . path . relpath ( var ) else yield string var string os . path . relpath ( var ) config . dump ( )	0
Q_541	determine if give relation already exist	C_541_0	def relation_exists ( self , parent , child , relation_type ) return pid_relation . query . filter_by ( child_pid_id = child . id , parent_pid_id = parent . id , relation_type = relation_type ) . count ( ) > number	1
Q_541	determine if give relation already exist	C_541_1	def _set_ospf1 ( self , v , load = false ) if hasattr ( v , string ) v = v . _utype ( v ) try t = yang_dyn_class ( v , base = ospf1 . ospf1 , is_container = string , presence = false , yang_name = string , rest_name = string , parent = self , path_helper = self . _path_helper , extmethods = self . _extmethods , register_paths = true , extensions = { u string { u string u string , u string none , u string u string false string , u string u string , u string u string } } , namespace = string , defining_module = string , yang_type = string , is_config = true ) except ( type_error , value_error ) raise value_error ( { string string string string , string string , string string string container string ospf1 string ospf string tailf _ common string info string open shortest path first ( ospf ) . string cli _ incomplete _ no string display _ when string / vcsmode / vcs _ mode = string string sort _ priority string number string alt _ name string ospf string urn brocade . com mgmt brocade _ ospf string brocade _ ospf string container string string , } ) self . _ospf1 = t if hasattr ( self , string ) self . _set ( )	0
Q_541	determine if give relation already exist	C_541_2	def efficiency_wei ( gw , local = false ) def distance_inv_wei ( g ) n = len ( g ) d = np . zeros ( ( n , n ) ) d [ np . logical_not ( np . eye ( n ) ) ] = np . inf for var in range ( n ) s = np . ones ( ( n , ) , dtype = bool ) g1 = g . copy ( ) v = [ var ] while true s [ v ] = number g1 [ , v ] = number for v in v w , = np . where ( g1 [ v , ] ) td = np . array ( [ d [ var , w ] . flatten ( ) , ( d [ var , v ] + g1 [ v , w ] ) . flatten ( ) ] ) d [ var , w ] = np . min ( td , axis = number ) if d [ var , s ] . size = = number break min_d = np . min ( d [ var , s ] ) if np . isinf ( min_d ) break v , = np . where ( d [ var , ] = = min_d ) np . fill_diagonal ( d , number ) d = number / d np . fill_diagonal ( d , number ) return d n = len ( gw ) gl = invert ( gw , copy = true ) a = np . array ( ( gw = number ) , dtype = int ) if local e = np . zeros ( ( n , ) ) for var in range ( n ) v , = np . where ( np . logical_or ( gw [ var , ] , gw [ , var ] . t ) ) sw = cuberoot ( gw [ var , v ] ) + cuberoot ( gw [ v , var ] . t ) e = distance_inv_wei ( gl [ np . ix_ ( v , v ) ] ) se = cuberoot ( e ) + cuberoot ( e . t ) numer = np . sum ( np . outer ( sw . t , sw ) * se ) / number if numer = number sa = a [ var , v ] + a [ v , var ] . t denom = np . sum ( sa ) * * number _ np . sum ( sa * sa ) e [ var ] = numer / denom else e = distance_inv_wei ( gl ) e = np . sum ( e ) / ( n * n _ n ) return e	0
Q_542	show device screen density	C_542_0	def get_screen_density ( self ) _ > str output , _ = self . _execute ( string , self . device_sn , string , string , string ) return output . split ( ) [ number ]	1
Q_542	show device screen density	C_542_1	def _get_minute_message ( self , dt , algo , metrics_tracker ) rvars = algo . recorded_vars minute_message = metrics_tracker . handle_minute_close ( dt , self . data_portal , ) minute_message [ string ] [ string ] = rvars return minute_message	0
Q_542	show device screen density	C_542_2	def get_total_term_frequency ( self , term ) if term not in self . _terms raise index_error ( term_does_not_exist ) return sum ( self . _terms [ term ] . values ( ) )	0
Q_543	take a screenshot of a device display then copy it to your computer	C_543_0	def pull_screencap ( self , remote _path = string , local _path = string ) _ > none self . screencap ( remote ) self . pull ( remote , local )	1
Q_543	take a screenshot of a device display then copy it to your computer	C_543_1	def _script_editor_output_plain_text_edit_set_default_view_state ( self ) self . script_editor_output_plain_text_edit . move_cursor ( q_text_cursor . end ) self . script_editor_output_plain_text_edit . ensure_cursor_visible ( )	0
Q_543	take a screenshot of a device display then copy it to your computer	C_543_2	def _untracked_custom_unit_found ( name , root = none ) system = _root ( string , root ) unit_path = os . path . join ( system , _canonical_unit_name ( name ) ) return os . access ( unit_path , os . r_ok ) and not _check_available ( name )	0
Q_544	register a method to a command	C_544_0	def register ( self , argtypes = r string , help_msg = none ) def format_args ( method ) def wrapped_method ( * args , * * kwargs ) args_count = len ( args ) argtypes_count = len ( argtypes ) placeholder_count = argtypes . count ( string ) + argtypes . count ( string ) if placeholder_count min_args_count = ( argtypes_count _ placeholder_count ) if args_count < min_args_count or args_count > argtypes_count raise knget_error ( string , reason = string . format ( args_count ) ) elif args_count = argtypes_count raise knget_error ( string , reason = string . format ( args_count ) ) argv = [ ] for var in range ( args_count ) if argtypes [ var ] in ( string , string ) argv . append ( args [ var ] ) elif argtypes [ var ] in ( string , string ) argv . append ( int ( args [ var ] ) ) elif argtypes [ var ] in ( string , string ) argv . append ( str ( args [ var ] ) ) elif argtypes [ var ] in ( string , string ) argv . append ( args [ var ] ) else raise knget_error ( string . format ( argtypes [ var ] ) ) return method ( * argv , * * kwargs ) wrapped_method . _doc_ = method . _doc_ self . _commands [ method . _name_ ] = ( wrapped_method , help_msg ) return wrapped_method return format_args	1
Q_544	register a method to a command	C_544_1	def _dedup_index ( self , df_a ) pairs = self . _link_index ( df_a , df_a ) pairs = pairs [ pairs . labels [ number ] > pairs . labels [ number ] ] return pairs	0
Q_544	register a method to a command	C_544_2	def create_table ( self , cursor , target , options ) string cursor . execute ( self . create_sql [ target ] . format ( self . qualified_names [ target ] ) )	0
Q_545	private method which transform a list which contain the state of the gene	C_545_0	def _transform_list_of_states_to_state ( self , state list [ int ] ) _ > state return state ( { var state [ var ] for var , var in enumerate ( self . genes ) } )	1
Q_545	private method which transform a list which contain the state of the gene	C_545_1	def get_yes_no ( self , question ) user_answer = self . get_answer ( question ) . lower ( ) if user_answer in self . yes_input return true if user_answer in self . no_input return false is_yes = self . is_yes ( user_answer ) is_no = self . is_no ( user_answer ) if is_yes and not is_no return true if is_no and not is_yes return false if self . interactive self . show_help ( ) return self . get_yes_no ( self . last_question ) return false	0
Q_545	private method which transform a list which contain the state of the gene	C_545_2	def unpack_unordered_pairs ( self , pairs ) items = [ ( false , var , var ) for var , var in pairs ] result = [ ] max_loop = number while items and max_loop next_items = [ ] for key_unpacked , key_data , value_data in items if key_unpacked key = key_data else blob = self . _begin ( ) try key = self . unpack_data ( key_data ) self . _commit ( blob ) except delay_packing self . _rollback ( blob ) next_items . append ( ( false , key_data , value_data ) ) continue blob = self . _begin ( ) try value = self . unpack_data ( value_data ) self . _commit ( blob ) except delay_packing self . _rollback ( blob ) next_items . append ( ( true , key , value_data ) ) continue result . append ( ( key , value ) ) items = next_items max_loop _ = number if items raise delay_packing ( ) return result	0
Q_546	get the next boolean address	C_546_0	def next_bit_address ( self ) if self . _current_bit_address = = string if self . _is_16bit return string . format ( self . next_address ( ) , string ) return string . format ( self . next_address ( ) , string ) if self . _is_16bit bool_half = int ( self . _current_bit_address . split ( string ) [ number ] ) if bool_half < number register_half = self . _current_bit_address . split ( string ) [ number ] return string . format ( register_half , pad_zeroes ( str ( bool_half + number ) , number ) ) self . move_to_next_address ( self . _size_of_current_register_address ) return string . format ( self . next_address ( ) , string ) bool_half = int ( self . _current_bit_address . split ( string ) [ number ] ) if bool_half < number register_half = self . _current_bit_address . split ( string ) [ number ] return string . format ( register_half , bool_half + number ) self . move_to_next_address ( self . _size_of_current_register_address ) return string . format ( self . next_address ( ) , string )	1
Q_546	get the next boolean address	C_546_1	def get_next_step ( self ) if self . rb_exp_layer_from_canvas . is_checked ( ) new_step = self . parent . step_fc_explayer_from_canvas else new_step = self . parent . step_fc_explayer_from_browser return new_step	0
Q_546	get the next boolean address	C_546_2	def get_configuration ( ) root = get_logger ( ) name_levels = [ ( string , logging . get_level_name ( root . level ) ) ] name_levels . extend ( ( var , logging . get_level_name ( var . level ) ) for var , var in root . manager . logger_dict . items ( ) if hasattr ( var , string ) ) config_string = string . join ( string x for x in name_levels ) return dict ( config_string = config_string , log_json = s_logger . manager . log_json )	0
Q_547	this method read in the byte in the keyfile and return the	C_547_0	def key_from_keyfile ( keyfile ) if hasattr ( keyfile , string ) buf = keyfile . read ( ) else with open ( keyfile , string ) as fp buf = fp . read ( ) sha = sha256 . new ( ) if len ( buf ) = = number sha . update ( buf ) return sha . digest ( ) elif len ( buf ) = = number sha . update ( struct . unpack ( string , buf ) [ number ] . decode ( ) ) return sha . digest ( ) else while buf if len ( buf ) < = number sha . update ( buf ) buf = string else sha . update ( buf [ number ] ) buf = buf [ number ] return sha . digest ( )	1
Q_547	this method read in the byte in the keyfile and return the	C_547_1	def insert ( self , data , return_object = false ) obj = self ( data ) obj . save ( ) if return_object return obj else return obj [ string ]	0
Q_547	this method read in the byte in the keyfile and return the	C_547_2	def datatype_from_token ( self , token ) if token . type = = system_rdl_parser . id typ = self . compiler . namespace . lookup_type ( get_id_text ( token ) ) if typ is none self . msg . fatal ( string s string get_id_text ( token ) , source_ref . from_antlr ( token ) ) if rdltypes . is_user_enum ( typ ) or rdltypes . is_user_struct ( typ ) return typ else self . msg . fatal ( string s string get_id_text ( token ) , source_ref . from_antlr ( token ) ) else return self . _data_type_map [ token . type ]	0
Q_548	count the number of packet in a specific range	C_548_0	def iterate_specific_packet_range ( ) now = datetime . utcnow ( ) start = now _ timedelta ( hours = number ) total = number for var in archive . list_packets ( start = start , stop = now ) total + = number print ( string , total , string )	1
Q_548	count the number of packet in a specific range	C_548_1	def register_on_machine_state_changed ( self , callback ) event_type = library . v_box_event_type . on_machine_state_changed return self . event_source . register_callback ( callback , event_type )	0
Q_548	count the number of packet in a specific range	C_548_2	def set_store_to ( self , store_to ) assert store_to is none or isinstance ( store_to , cache ) , string assert store_to is none or store_to . cl_size < = self . cl_size , string self . store_to = store_to self . backend . store_to = store_to . backend	0
Q_549	upload datafile to a project	C_549_0	def upload_datafile ( self , project_owner , project_id , file_obj , chunk_size = chunk_size , force_chunked = false ) simple_upload_url = string ( project_owner , project_id ) chunked_upload_url = string ( project_owner , project_id ) return self . upload_file ( simple_upload_url , chunked_upload_url , file_obj , chunk_size = chunk_size , force_chunked = force_chunked )	1
Q_549	upload datafile to a project	C_549_1	def _configure_from_module ( self , item ) package = none if item [ number ] = = string package = self . import_name obj = importlib . import_module ( item , package = package ) self . config . from_object ( obj ) return self	0
Q_549	upload datafile to a project	C_549_2	def get_dump ( self , fmap = string , with_stats = false , dump_format = string ) length = c_bst_ulong ( ) sarr = ctypes . pointer ( ctypes . c_char_p ) ( ) if self . feature_names is not none and fmap = = string flen = len ( self . feature_names ) fname = from_pystr_to_cstr ( self . feature_names ) if self . feature_types is none ftype = from_pystr_to_cstr ( [ string ] * flen ) else ftype = from_pystr_to_cstr ( self . feature_types ) _check_call ( _lib . xg_booster_dump_model_ex_with_features ( self . handle , ctypes . c_int ( flen ) , fname , ftype , ctypes . c_int ( with_stats ) , c_str ( dump_format ) , ctypes . byref ( length ) , ctypes . byref ( sarr ) ) ) else if fmap = string and not os . path . exists ( fmap ) raise value_error ( string . format ( fmap ) ) _check_call ( _lib . xg_booster_dump_model_ex ( self . handle , c_str ( fmap ) , ctypes . c_int ( with_stats ) , c_str ( dump_format ) , ctypes . byref ( length ) , ctypes . byref ( sarr ) ) ) res = from_cstr_to_pystr ( sarr , length ) return res	0
Q_550	register a callback that open an archive with the specify suffix	C_550_0	def register_opener ( suffix , opener = none ) if opener is none def decorator ( func ) register_opener ( suffix , func ) return func return decorator if suffix in openers raise value_error ( string . format ( suffix ) ) openers [ suffix ] = opener	1
Q_550	register a callback that open an archive with the specify suffix	C_550_1	"def vagalume ( song ) translate = { string string , urlescape string , string string } artist = song . artist . lower ( ) artist = normalize ( artist , translate ) artist = re . sub ( r string , string , artist ) title = song . title . lower ( ) title = normalize ( title , translate ) title = re . sub ( r string , string , title ) url = string . format ( artist , title ) soup = get_url ( url ) body = soup . select ( string string var string "" ) return content . get_text ( ) . strip ( )"	0
Q_550	register a callback that open an archive with the specify suffix	C_550_2	def _get_eval_string ( task_id , system_id , system_dir , system_filename , model_dir , model_filenames ) peer_elems = string { id } string . format ( id = system_id , var = system_filename ) model_elems = [ string { id } string . format ( id = chr ( number + var ) , var = var ) for var , var in enumerate ( model_filenames ) ] model_elems = string . join ( model_elems ) eval_string = string string { task_id } string see string string . format ( task_id = task_id , model_root = model_dir , model_elems = model_elems , peer_root = system_dir , peer_elems = peer_elems ) return eval_string	0
Q_551	"r tagstr """	C_551_0	def dim_dm ( self , pars ) r self . _set_parameters ( pars ) num1 = self . otc * np . sin ( self . ang ) result = _ self . sigmai * num1 / self . denom return result	1
Q_551	"r tagstr """	C_551_1	def _folder_item_result ( self , analysis_brain , item ) item [ string ] = string if not self . has_permission ( view_results , analysis_brain ) img = get_image ( string , width = string , height = string ) item [ string ] [ string ] = img return result = analysis_brain . get_result capture_date = analysis_brain . get_result_capture_date capture_date_str = self . ulocalized_time ( capture_date , long_format = number ) item [ string ] = result item [ string ] = capture_date_str item [ string ] = capture_date_str if self . is_analysis_edition_allowed ( analysis_brain ) item [ string ] . append ( string ) if self . is_result_edition_allowed ( analysis_brain ) item [ string ] . append ( string ) choices = analysis_brain . get_result_options if choices choices = copy ( choices ) choices . insert ( number , dict ( result_value = string , result_text = string ) ) item [ string ] [ string ] = choices if not result return obj = self . get_object ( analysis_brain ) formatted_result = obj . get_formatted_result ( sciformat = int ( self . scinot ) , decimalmark = self . dmk ) item [ string ] = formatted_result	0
Q_551	"r tagstr """	C_551_2	def show_stat_base ( count_value , max_count_value , prepend , speed , tet , ttg , width , * * kwargs ) raise not_implemented_error	0
Q_552	get a consumer for a connection	C_552_0	def consumer ( self , conn ) return consumer ( connection = conn , queue = self . queue . name , exchange = self . exchange . name , exchange_type = self . exchange . type , durable = self . exchange . durable , auto_delete = self . exchange . auto_delete , routing_key = self . routing_key , no_ack = self . no_ack , )	1
Q_552	get a consumer for a connection	C_552_1	def captcha_refresh ( request ) if not request . is_ajax ( ) raise http404 new_key = captcha_store . pick ( ) to_json_response = { string new_key , string captcha_image_url ( new_key ) , string captcha_audio_url ( new_key ) if settings . captcha_flite_path else none } return http_response ( json . dumps ( to_json_response ) , content_type = string )	0
Q_552	get a consumer for a connection	C_552_2	def _flush ( self ) if self . _upload_id is none with _handle_oss_error ( ) self . _upload_id = self . _bucket . init_multipart_upload ( self . _key ) . upload_id response = self . _workers . submit ( self . _bucket . upload_part , key = self . _key , upload_id = self . _upload_id , part_number = self . _seek , data = self . _get_buffer ( ) . tobytes ( ) ) self . _write_futures . append ( dict ( response = response , part_number = self . _seek ) )	0
Q_553	"iterable_first replacement of python "" s built_in filter function"	C_553_0	def filter ( cls , iterable , cond , * a , * * kw ) return cls ( var for var in iterable if cond ( var , * a , * * kw ) )	1
Q_553	"iterable_first replacement of python "" s built_in filter function"	C_553_1	def get_raw_data_from_buffer ( self , filter_func = none , converter_func = none ) string string string if self . _is_running raise runtime_error ( string ) if not self . fill_buffer logging . warning ( string ) return [ convert_data_array ( data_array_from_data_iterable ( var ) , filter_func = filter_func , converter_func = converter_func ) for var in self . _data_buffer ]	0
Q_553	"iterable_first replacement of python "" s built_in filter function"	C_553_2	def get_customizationspec_ref ( si , customization_spec_name ) customization_spec_name = si . content . customization_spec_manager . get_customization_spec ( name = customization_spec_name ) return customization_spec_name	0
Q_554	minify the target html file	C_554_0	def minify_file ( self , target ) html = open ( target , string ) . read ( ) enc = chardet . detect ( html ) [ string ] with codecs . open ( target , string , enc ) as f result = htmlmin . minify ( f . read ( ) , * * self . options ) f . seek ( number ) f . write ( result ) f . truncate ( )	1
Q_554	minify the target html file	C_554_1	def versioning_model_classname ( manager , model ) if manager . options . get ( string , true ) return string ( model . _module_ . title ( ) . replace ( string , string ) , model . _name_ ) else return string ( model . _name_ , )	0
Q_554	minify the target html file	C_554_2	def clear_app_cache ( app_name ) loading_cache = django . db . models . loading . cache if django . version [ number ] < ( number , number ) loading_cache . app_models [ app_name ] . clear ( ) else loading_cache . all_models [ app_name ] . clear ( )	0
Q_555	on builder init event commit registry and do callback	C_555_0	"def call_builder_init ( cls , kb_app , sphinx_app sphinx ) conf_dir = sphinx_app . confdir plugins_dir = sphinx_app . config . kaybee_settings . plugins_dir full_plugins_dir = os . path . join ( conf_dir , plugins_dir ) if os . path . exists ( full_plugins_dir ) sys . path . insert ( number , conf_dir ) plugin_package = importlib . import_module ( plugins_dir ) importscan . scan ( plugin_package ) else logger . info ( f "" dectate . commit ( kb_app ) for var in cls . get_callbacks ( kb_app , sphinx_event . bi ) var ( kb_app , sphinx_app )"	1
Q_555	on builder init event commit registry and do callback	C_555_1	def convert_numpy_type ( cls , dtype ) m = { string cls . datatype_integer64 , string cls . datatype_float , string cls . datatype_text } t = m . get ( dtype . name , none ) if not t raise type_error ( string { } string . format ( dtype . name ) ) return t	0
Q_555	on builder init event commit registry and do callback	C_555_2	def get_field_model ( name , model = none ) string string string if string in name m , name = name . split ( string ) model = get_model ( m ) if model return getattr ( model , name , none ) , model else return none , none	0
Q_556	calculate the reduce chi_squared use to compare a model to observation	C_556_0	def calc_reduced_chi_squared ( y_observed , y_model , observation_error , number_of_fitted_parameters ) observed = _np . array ( y_observed ) expected = _np . array ( y_model ) if observed . shape = expected . shape raise value_error ( string ) residuals = ( observed _ expected ) z = residuals / observation_error chi2 = _np . sum ( z * * number ) num_of_observations = len ( observed ) v = num_of_observations _ number_of_fitted_parameters chi2_reduced = chi2 / v return chi2_reduced	1
Q_556	calculate the reduce chi_squared use to compare a model to observation	C_556_1	def get_all_classes ( module_name ) module = importlib . import_module ( module_name ) return getmembers ( module , lambda m isclass ( m ) and not isabstract ( m ) )	0
Q_556	calculate the reduce chi_squared use to compare a model to observation	C_556_2	def _index_target ( self , target_adaptor ) address = target_adaptor . address target = self . _instantiate_target ( target_adaptor ) self . _target_by_address [ address ] = target for var in target_adaptor . dependencies if var in self . _target_dependencies_by_address [ address ] raise self . duplicate_address_error ( string string { spec } string { target } string . format ( spec = var . spec , target = address . spec ) ) self . _target_dependencies_by_address [ address ] . add ( var ) self . _target_dependees_by_address [ var ] . add ( address ) return target	0
Q_557	return a range of value from the sort set name with score	C_557_0	def iterbyscore ( self , min = string , max = string , start = none , num = none , withscores = false , reverse = none ) reverse = reverse if reverse is not none else self . reversed zfunc = self . _client . zrangebyscore if not reverse else self . _client . zrevrangebyscore _loads = self . _loads for var in zfunc ( self . key_prefix , min = min , max = max , start = start , num = num , withscores = withscores , score_cast_func = self . cast ) if withscores yield ( _loads ( var [ number ] ) , self . cast ( var [ number ] ) ) else yield _loads ( var )	1
Q_557	return a range of value from the sort set name with score	C_557_1	def css_unescape ( content , string = false ) def replace ( m ) if m . group ( number ) codepoint = int ( m . group ( number ) [ number ] , number ) if codepoint = = number codepoint = unicode_replacement_char value = util . uchr ( codepoint ) elif m . group ( number ) value = m . group ( number ) [ number ] elif m . group ( number ) value = string else value = string return value return ( re_css_esc if not string else re_css_str_esc ) . sub ( replace , content )	0
Q_557	return a range of value from the sort set name with score	C_557_2	def describe_topic_rule ( rule_name , region = none , key = none , keyid = none , profile = none ) try conn = _get_conn ( region = region , key = key , keyid = keyid , profile = profile ) rule = conn . get_topic_rule ( rule_name = rule_name ) if rule and string in rule rule = rule [ string ] keys = ( string , string , string , string , string ) return { string dict ( [ ( var , rule . get ( var ) ) for var in keys ] ) } else return { string none } except client_error as e return { string _utils_ [ string ] ( e ) }	0
Q_558	render electrode state shape	C_558_0	def render_electrode_shapes ( self , df_shapes = none , shape_scale = number . number , fill = ( number , number , number ) ) surface = self . get_surface ( ) if df_shapes is none if hasattr ( self . canvas , string ) df_shapes = self . canvas . df_canvas_shapes else return surface if string not in df_shapes or string not in df_shapes return surface cairo_context = cairo . context ( surface ) df_shapes = df_shapes . copy ( ) df_shapes [ [ string , string ] ] = ( df_shapes [ [ string , string ] ] + df_shapes [ [ string , string ] ] . values * shape_scale ) for var , var in ( df_shapes . groupby ( self . canvas . shape_i_columns ) [ [ string , string ] ] ) vertices_x = var . x . values vertices_y = var . y . values cairo_context . move_to ( vertices_x [ number ] , vertices_y [ number ] ) for x , y in itertools . izip ( vertices_x [ number ] , vertices_y [ number ] ) cairo_context . line_to ( x , y ) cairo_context . close_path ( ) cairo_context . set_source_rgba ( * fill ) cairo_context . fill ( ) return surface	1
Q_558	render electrode state shape	C_558_1	def parse_attrs ( self , tag ) if tag . name in attr_whitelist . keys ( ) attrs = copy ( tag . attrs ) for var , var in attrs . items ( ) if var in attr_whitelist [ tag . name ] tag . attrs [ var ] = self . _parse_attr ( tag . name , var , var ) else del tag . attrs [ var ] else tag . attrs = { }	0
Q_558	render electrode state shape	C_558_2	def _finalize ( self ) container = { } try for var in self . _traces container [ var ] = self . _traces [ var ] . _trace container [ string ] = self . _state_ file = open ( self . filename , string ) std_pickle . dump ( container , file ) file . close ( ) except attribute_error pass	0
Q_559	start all service of a give class	C_559_0	def start_class ( self , class_ ) matches = filter ( lambda svc isinstance ( svc , class_ ) , self ) if not matches svc = class_ ( ) self . register ( svc ) matches = [ svc ] map ( self . start , matches ) return matches	1
Q_559	start all service of a give class	C_559_1	def _get_perez_coefficients ( perezmodel ) coeffdict = { string [ [ _ number . number number , number . number , _ number . number number , _ number . number number , number . number number , _ number . number number ] , [ number . number , number . number , _ number . number , _ number . number number , number . number number , _ number . number number ] , [ number . number , number . number , _ number . number , number . number number , _ number . number number , _ number . number number ] , [ number . number , number . number , _ number . number , number . number , _ number . number , _ number . number number ] , [ number . number , _ number . number , _ number . number , number . number , _ number . number , number . number number ] , [ number . number , _ number . number , _ number . number , number . number , _ number . number , number . number number ] , [ number . number number , _ number . number , _ number . number , number . number , _ number . number , number . number ] , [ number . number , _ number . number , _ number . number , number . number , _ number . number , number . number ] ] , string [ [ _ number . number number , number . number , _ number . number number , _ number . number number , number . number , _ number . number number ] , [ number . number , number . number , _ number . number , number . number number , number . number number , _ number . number number ] , [ number . number , number . number , _ number . number , number . number number , _ number . number , _ number . number number ] , [ number . number , _ number . number , _ number . number , number . number , _ number . number , _ number . number number ] , [ number . number , _ number . number , _ number . number , number . number , _ number . number , number . number number ] , [ number . number number , _ number . number , _ number . number , number . number , _ number . number , number . number number ] , [ number . number , _ number . number , _ number . number , number . number , _ number . number , number . number ] , [ number . number , _ number . number , _ number . number , number . number , _ number . number , number . number ] ] , string [ [ _ number . number , number . number number , _ number . number number , _ number . number , number . number , _ number . number number ] , [ number . number , number . number , _ number . number , _ number . number number , number . number number , _ number . number number ] , [ number . number , number . number , _ number . number , number . number number , _ number . number number , _ number . number number ] , [ number . number , _ number . number , _ number . number , number . number , _ number . number , _ number . number number ] , [ number . number number , _ number . number , _ number . number , number . number , _ number . number , _ number . number number ] , [ number . number , _ number . number , _ number . number , number . number , _ number . number , _ number . number number ] , [ number . number , _ number . number , _ number . number , number . number , _ number . number number , number . number number ] , [ number . number , _ number . number , number . number , number . number number , _ number . number , number . number ] ] , string [ [ _ number . number number , number . number , _ number . number number , _ number . number number , number . number number , _ number . number number ] , [ number . number , number . number , _ number . number , number . number number , _ number . number number , _ number . number number ] , [ number . number , number . number , _ number . number , number . number number , _ number . number , _ number . number number ] , [ number . number , _ number . number , _ number . number , number . number , _ number . number , _ number . number number ] , [ number . number number , _ number . number , _ number . number , number . number , _ number . number , _ number . number number ] , [ number . number number , _ number . number , _ number . number , number . number , _ number . number , number . number number ] , [ number . number , _ number . number , _ number . number , number . number , _ number . number , number . number ] , [ number . number , _ number . number , _ number . number , number . number , _ number . number , number . number ] ] , string [ [ number . number number , number . number , _ number . number , _ number . number number , number . number , _ number . number number ] , [ number . number number , number . number , _ number . number , number , number . number number , _ number . number number ] , [ number . number , number . number , _ number . number , number . number number , _ number . number number , _ number . number number ] , [ number . number , _ number . number number , _ number . number , number . number , _ number . number , number . number number ] , [ number . number , _ number . number , _ number . number , number . number , _ number . number , number . number number ] , [ number . number , _ number . number , _ number . number , number . number , _ number . number , number . number ] , [ number . number , _ number . number , _ number . number , number . number , _ number . number , number . number ] , [ number . number , _ number . number , _ number . number , number . number , _ number . number , number . number ] ] , string [ [ _ number . number number , number . number , _ number . number number , _ number . number number , number . number , _ number . number number ] , [ number . number , number . number , _ number . number , number . number number , _ number . number number , _ number . number number ] , [ number . number , number . number , _ number . number , number . number number , _ number . number number , _ number . number number ] , [ number . number , _ number . number , _ number . number , number . number , _ number . number , _ number . number number ] , [ number . number , _ number . number , _ number . number , number . number , _ number . number , _ number . number number ] , [ number . number number , _ number . number , _ number . number , number . number , _ number . number , _ number . number number ] , [ number . number , _ number . number , _ number . number , number . number , _ number . number , number . number number ] , [ number . number , number . number , number . number , _ number . number , _ number . number , number . number ] ] , string [ [ number . number number , number . number , _ number . number , _ number . number number , number . number , _ number . number number ] , [ number . number , number . number , _ number . number , _ number . number number , number . number , _ number . number number ] , [ number . number , number . number , _ number . number , number . number number , _ number . number number , _ number . number number ] , [ number . number , _ number . number , _ number . number , number . number , _ number . number , _ number . number number ] , [ number . number , _ number . number number , _ number . number , number . number , _ number . number , _ number . number number ] , [ number . number , _ number . number number , _ number . number , number . number number , _ number . number , number . number number ] , [ number . number number , _ number . number , _ number . number , number . number number , _ number . number , number . number ] , [ number . number , number . number , number . number , _ number . number , _ number . number number , number . number ] ] , string [ [ _ number . number , number . number , number . number number , _ number . number , number . number , number . number number ] , [ number . number , number . number , _ number . number , number . number number , _ number . number number , _ number . number number ] , [ _ number . number number , number . number , _ number . number number , _ number . number number , number . number , _ number . number number ] , [ number . number , number . number number , _ number . number , number . number number , _ number . number , _ number . number number ] , [ number . number , _ number . number , _ number . number , number . number , _ number . number , _ number . number number ] , [ number . number , number . number number , _ number . number , number . number , _ number . number , _ number . number ] , [ number . number , number . number , number . number , number . number , _ number . number , number . number number ] , [ number . number , number . number , _ number . number , number . number , number . number , _ number . number ] ] , string [ [ number . number number , number . number , _ number . number number , _ number . number number , number . number , _ number . number number ] , [ number . number , number . number , _ number . number , _ number . number number , _ number . number number , _ number . number number ] , [ number . number , number . number , _ number . number , number . number number , _ number . number number , _ number . number number ] , [ number . number , _ number . number , _ number . number , number . number , _ number . number , _ number . number number ] , [ number . number , _ number . number , _ number . number , number . number , _ number . number , _ number . number number ] , [ number . number number , _ number . number , _ number . number , number . number , _ number . number , number . number number ] , [ number . number , _ number . number , _ number . number , number . number number , _ number . number , number . number ] , [ number . number , _ number . number number , _ number . number number , number . number number , _ number . number , number . number ] ] , string [ [ number . number number , number . number , _ number . number , _ number . number number , number . number number , _ number . number number ] , [ number . number , number . number , _ number . number , _ number . number number , number . number number , _ number . number number ] , [ number . number , number . number number , _ number . number , number . number number , _ number . number , _ number . number number ] , [ number . number , _ number . number , _ number . number , number . number , _ number . number , _ number . number number ] , [ number . number , _ number . number , _ number . number , number . number , _ number . number , number . number number ] , [ number . number , _ number . number number , _ number . number , number . number , _ number . number , number . number number ] , [ number . number , _ number . number , number . number , number . number , _ number . number , number . number ] , [ _ number . number , number . number , number . number number , _ number . number , _ number . number , number . number ] ] , string [ [ number . number number , number . number , _ number . number number , _ number . number number , number . number number , _ number . number number ] , [ number . number , number . number , _ number . number , number . number number , number . number number , _ number . number number ] , [ number . number , number . number , _ number . number , number . number number , _ number . number number , _ number . number number ] , [ number . number , _ number . number number , _ number . number , number . number , _ number . number , _ number . number number ] , [ number . number number , _ number . number number , _ number . number , number . number , _ number . number , number . number number ] , [ number . number , _ number . number , _ number . number , number . number , _ number . number number , number . number number ] , [ number . number , _ number . number , _ number . number , number . number , _ number . number , number . number ] , [ number . number number , _ number . number , _ number . number , number . number , _ number . number , number . number ] ] , } array = np . array ( coeffdict [ perezmodel ] ) f1coeffs = array [ , number number ] f2coeffs = array [ , number number ] return f1coeffs , f2coeffs	0
Q_559	start all service of a give class	C_559_2	def run_command_on_marathon_leader ( command , username = none , key_path = none , noisy = true ) return run_command ( shakedown . marathon_leader_ip ( ) , command , username , key_path , noisy )	0
Q_560	validate give index eventually raise error	C_560_0	def _validate_index ( self , var ) if isinstance ( var , slice ) if var . step and var . step = number raise index_error ( string ) indexes = ( var . start , var . stop ) else indexes = ( var , ) for var in indexes if var is not none and var < number raise index_error ( string )	1
Q_560	validate give index eventually raise error	C_560_1	def make_graph ( node , inputs ) initializer = [ ] tensor_input_info = [ ] tensor_output_info = [ ] for var in range ( len ( node . input ) ) tensor_input_info . append ( helper . make_tensor_value_info ( str ( node . input [ var ] ) , tensor_proto . float , [ number ] ) ) if node . input [ var ] = = string dim = inputs [ var ] . shape param_tensor = helper . make_tensor ( name = node . input [ var ] , data_type = tensor_proto . float , dims = dim , vals = inputs [ var ] . flatten ( ) ) initializer . append ( param_tensor ) for var in range ( len ( node . output ) ) tensor_output_info . append ( helper . make_tensor_value_info ( str ( node . output [ var ] ) , tensor_proto . float , [ number ] ) ) graph_proto = helper . make_graph ( [ node ] , string , tensor_input_info , tensor_output_info , initializer = initializer ) return graph_proto	0
Q_560	validate give index eventually raise error	C_560_2	def validate ( self , size ) msg = string string if size = len ( self . scale ) raise value_error ( msg . format ( * * locals ( ) ) )	0
Q_561	compute the float point time difference between mid_day and an angle	C_561_0	def compute_time_at_sun_angle ( day , latitude , angle ) positive_angle_rad = radians ( abs ( angle ) ) angle_sign = abs ( angle ) / angle latitude_rad = radians ( latitude ) declination = radians ( sun_declination ( day ) ) numerator = _ sin ( positive_angle_rad ) _ sin ( latitude_rad ) * sin ( declination ) denominator = cos ( latitude_rad ) * cos ( declination ) time_diff = degrees ( acos ( numerator / denominator ) ) / number return time_diff * angle_sign	1
Q_561	compute the float point time difference between mid_day and an angle	C_561_1	def gsea_compute_tensor ( data , gmt , n , weighted_score_type , permutation_type , method , pheno_pos , pheno_neg , classes , ascending , processes = number , seed = none , single = false , scale = false ) w = weighted_score_type subsets = sorted ( gmt . keys ( ) ) rs = np . random . random_state ( seed ) genes_mat , cor_mat = data . index . values , data . values base = number if data . shape [ number ] > = number else number block = ceil ( len ( subsets ) / base ) if permutation_type = = string logging . debug ( string ) genes_ind = [ ] cor_mat = [ ] temp_rnk = [ ] pool_rnk = pool ( processes = processes ) i = number while i < = block rs = np . random . random_state ( seed ) temp_rnk . append ( pool_rnk . apply_async ( ranking_metric_tensor , args = ( data , method , base , pheno_pos , pheno_neg , classes , ascending , rs ) ) ) i + = number pool_rnk . close ( ) pool_rnk . join ( ) for var , var in enumerate ( temp_rnk ) gi , cor = var . get ( ) if var + number = = block genes_ind . append ( gi ) cor_mat . append ( cor ) else genes_ind . append ( gi [ _ number ] ) cor_mat . append ( cor [ _ number ] ) genes_ind , cor_mat = np . vstack ( genes_ind ) , np . vstack ( cor_mat ) genes_mat = ( data . index . values , genes_ind ) logging . debug ( string ) es = [ ] res = [ ] hit_ind = [ ] esnull = [ ] temp_esnu = [ ] pool_esnu = pool ( processes = processes ) i , m = number , number while i < = block rs = np . random . random_state ( seed ) gmtrim = { var gmt . get ( var ) for var in subsets [ m base * i ] } temp_esnu . append ( pool_esnu . apply_async ( enrichment_score_tensor , args = ( genes_mat , cor_mat , gmtrim , w , n , rs , single , scale ) ) ) m = base * i i + = number pool_esnu . close ( ) pool_esnu . join ( ) for si , var in enumerate ( temp_esnu ) e , enu , hit , rune = var . get ( ) esnull . append ( enu ) es . append ( e ) res . append ( rune ) hit_ind + = hit es , esnull , res = np . hstack ( es ) , np . vstack ( esnull ) , np . vstack ( res ) return gsea_significance ( es , esnull ) , hit_ind , res , subsets	0
Q_561	compute the float point time difference between mid_day and an angle	C_561_2	def fpost ( self , url , form_data ) response = none if string in form_data response = requests . post ( self . host + url , data = form_data [ string ] if string in form_data else { } , files = form_data [ string ] if string in form_data else { } , headers = { string string + self . token , string self . source_header } , verify = false ) else response = requests . post ( self . host + url , data = json . dumps ( form_data [ string ] if string in form_data else { } ) , headers = { string string , string string + self . token , string self . source_header } , verify = false ) if response . status_code = = number or response . status_code = = number try return json . loads ( response . _content . decode ( string ) ) except exception as e return json . loads ( response . content ) elif response . status_code = = number raise exception ( json . dumps ( { string string } ) ) else raise exception ( response . content )	0
Q_562	fermi dirac distribution	C_562_0	def fermi_dist ( energy , beta ) exponent = np . asarray ( beta * energy ) . clip ( _ number , number ) return number . / ( np . exp ( exponent ) + number )	1
Q_562	fermi dirac distribution	C_562_1	def _set_ethernet ( self , v , load = false ) if hasattr ( v , string ) v = v . _utype ( v ) try t = yang_dyn_class ( v , base = yang_list_type ( string , ethernet . ethernet , yang_name = string , rest_name = string , parent = self , is_container = string , user_ordered = true , path_helper = self . _path_helper , yang_keys = string , extensions = { u string { u string u string , u string none , u string u string , u string u string , u string none , u string none , u string u string , u string u string , u string none , u string u string , u string u string } } ) , is_container = string , yang_name = string , rest_name = string , parent = self , path_helper = self . _path_helper , extmethods = self . _extmethods , register_paths = true , extensions = { u string { u string u string , u string none , u string u string , u string u string , u string none , u string none , u string u string , u string u string , u string none , u string u string , u string u string } } , namespace = string , defining_module = string , yang_type = string , is_config = true ) except ( type_error , value_error ) raise value_error ( { string string string string , string string , string string string name string ethernet string ethernet string list string name string tailf _ common string info string the list of ethernet interfaces . string cli _ no _ key _ completion string alt _ name string ethernet string sort _ priority string runncfg_level_interface_type_physical string cli _ suppress _ no string cli _ suppress _ show _ path string cli _ custom _ range _ actionpoint string nsm_range_cli_actionpoint string cli _ custom _ range _ enumerator string nsm_range_cli_actionpoint string cli _ no _ match _ completion string callpoint string interface_phyintf string cli _ mode _ name string conf _ if _ eth _ ( name ) string list string ethernet string ethernet string tailf _ common string info string the list of ethernet interfaces . string cli _ no _ key _ completion string alt _ name string ethernet string sort _ priority string runncfg_level_interface_type_physical string cli _ suppress _ no string cli _ suppress _ show _ path string cli _ custom _ range _ actionpoint string nsm_range_cli_actionpoint string cli _ custom _ range _ enumerator string nsm_range_cli_actionpoint string cli _ no _ match _ completion string callpoint string interface_phyintf string cli _ mode _ name string conf _ if _ eth _ ( name ) string urn brocade . com mgmt brocade _ interface string brocade _ interface string list string string , } ) self . _ethernet = t if hasattr ( self , string ) self . _set ( )	0
Q_562	fermi dirac distribution	C_562_2	def _get_raw_objects ( self ) if not hasattr ( self , string ) result = self . _client . get ( type ( self ) . api_endpoint , model = self ) self . _raw_objects = result return self . _raw_objects	0
Q_563	resolution with a give index	C_563_0	def resolution ( self , index ) api_query ( string , id = str ( index ) ) async def result ( _ , root ) elem = root . find ( string ) if not elem raise not_found ( f string ) return resolution ( elem ) return result ( self )	1
Q_563	resolution with a give index	C_563_1	def _get_name_or_id ( values , known ) result = list ( ) for var in [ e . strip ( ) for e in values . split ( string ) ] for item in [ i for i in known if i . name = = var or i . id = = var ] result . append ( item ) return result	0
Q_563	resolution with a give index	C_563_2	def add_trigger ( name = none , location = string , trigger_type = none , trigger_enabled = true , start_date = none , start_time = none , end_date = none , end_time = none , random_delay = none , repeat_interval = none , repeat_duration = none , repeat_stop_at_duration_end = false , execution_time_limit = none , delay = none , * * kwargs ) r if not trigger_type return string trigger_type string state_changes = { string number , string number , string number , string number , string number , string number } days = { number number , number number , number number , number number , number number , number number , number number , number number , number number , number number , number number , number number , number number , number number , number number , number number , number number , number number , number number , number number , number number , number number , number number , number number , number number , number number , number number , number number , number number , number number , number number , string number } weekdays = { string number , string number , string number , string number , string number , string number , string number } weeks = { string number , string number , string number , string number } months = { string number , string number , string number , string number , string number , string number , string number , string number , string number , string number , string number , string number } if start_date date_format = _get_date_time_format ( start_date ) if date_format dt_obj = datetime . strptime ( start_date , date_format ) else return string else dt_obj = datetime . now ( ) if start_time time_format = _get_date_time_format ( start_time ) if time_format tm_obj = datetime . strptime ( start_time , time_format ) else return string else tm_obj = datetime . strptime ( string , string ) start_boundary = string . format ( dt_obj . strftime ( string ) , tm_obj . strftime ( string ) ) dt_obj = none tm_obj = none if end_date date_format = _get_date_time_format ( end_date ) if date_format dt_obj = datetime . strptime ( end_date , date_format ) else return string if end_time time_format = _get_date_time_format ( end_time ) if time_format tm_obj = datetime . strptime ( end_time , time_format ) else return string else tm_obj = datetime . strptime ( string , string ) end_boundary = none if dt_obj and tm_obj end_boundary = string . format ( dt_obj . strftime ( string ) , tm_obj . strftime ( string ) ) save_definition = false if kwargs . get ( string , false ) task_definition = kwargs . get ( string ) else save_definition = true if not name return string name string if name in list_tasks ( location ) with salt . utils . winapi . com ( ) task_service = win32com . client . dispatch ( string ) task_service . connect ( ) task_folder = task_service . get_folder ( location ) task_definition = task_folder . get_task ( name ) . definition else return string . format ( name ) trigger = task_definition . triggers . create ( trigger_types [ trigger_type ] ) trigger . start_boundary = start_boundary if delay trigger . delay = _lookup_first ( duration , delay ) if random_delay trigger . random_delay = _lookup_first ( duration , random_delay ) if repeat_interval trigger . repetition . interval = _lookup_first ( duration , repeat_interval ) if repeat_duration trigger . repetition . duration = _lookup_first ( duration , repeat_duration ) trigger . repetition . stop_at_duration_end = repeat_stop_at_duration_end if execution_time_limit trigger . execution_time_limit = _lookup_first ( duration , execution_time_limit ) if end_boundary trigger . end_boundary = end_boundary trigger . enabled = trigger_enabled if trigger_types [ trigger_type ] = = task_trigger_event if kwargs . get ( string , false ) trigger . id = string trigger . subscription = kwargs . get ( string ) else return string subscription string elif trigger_types [ trigger_type ] = = task_trigger_time trigger . id = string elif trigger_types [ trigger_type ] = = task_trigger_daily trigger . id = string trigger . days_interval = kwargs . get ( string , number ) elif trigger_types [ trigger_type ] = = task_trigger_weekly trigger . id = string trigger . weeks_interval = kwargs . get ( string , number ) if kwargs . get ( string , false ) bits_days = number for var in kwargs . get ( string ) bits_days = weekdays [ var ] trigger . days_of_week = bits_days else return string days_of_week string elif trigger_types [ trigger_type ] = = task_trigger_monthly trigger . id = string if kwargs . get ( string , false ) bits_months = number for month in kwargs . get ( string ) bits_months = months [ month ] trigger . months_of_year = bits_months else return string months_of_year string if kwargs . get ( string , false ) or kwargs . get ( string , false ) if kwargs . get ( string , false ) bits_days = number for day in kwargs . get ( string ) bits_days = days [ day ] trigger . days_of_month = bits_days trigger . run_on_last_day_of_month = kwargs . get ( string , false ) else return string days_of_month string last_day_of_month string elif trigger_types [ trigger_type ] = = task_trigger_monthlydow trigger . id = string if kwargs . get ( string , false ) bits_months = number for month in kwargs . get ( string ) bits_months = months [ month ] trigger . months_of_year = bits_months else return string months_of_year string if kwargs . get ( string , false ) or kwargs . get ( string , false ) if kwargs . get ( string , false ) bits_weeks = number for week in kwargs . get ( string ) bits_weeks = weeks [ week ] trigger . weeks_of_month = bits_weeks trigger . run_on_last_week_of_month = kwargs . get ( string , false ) else return string weeks_of_month string last_week_of_month string if kwargs . get ( string , false ) bits_days = number for var in kwargs . get ( string ) bits_days = weekdays [ var ] trigger . days_of_week = bits_days else return string days_of_week string elif trigger_types [ trigger_type ] = = task_trigger_idle trigger . id = string elif trigger_types [ trigger_type ] = = task_trigger_registration trigger . id = string elif trigger_types [ trigger_type ] = = task_trigger_boot trigger . id = string elif trigger_types [ trigger_type ] = = task_trigger_logon trigger . id = string elif trigger_types [ trigger_type ] = = task_trigger_session_state_change trigger . id = string if kwargs . get ( string , false ) trigger . user_id = kwargs . get ( string ) if kwargs . get ( string , false ) trigger . state_change = state_changes [ kwargs . get ( string ) ] else return string state_change string if save_definition return _save_task_definition ( name = name , task_folder = task_folder , task_definition = task_definition , user_name = task_definition . principal . user_id , password = none , logon_type = task_definition . principal . logon_type )	0
Q_564	print information of a quantitative phase imaging dataset	C_564_0	def qpinfo ( ) parser = qpinfo_parser ( ) args = parser . parse_args ( ) path = pathlib . path ( args . path ) . resolve ( ) try ds = load_data ( path ) except unknown_file_format_error print ( string . format ( path ) ) return print ( string . format ( ds . _class_ . _doc_ , ds . _class_ . _name_ ) ) print ( string . format ( len ( ds ) ) ) for var in ds . meta_data print ( string . format ( var , ds . meta_data [ var ] ) )	1
Q_564	print information of a quantitative phase imaging dataset	C_564_1	def pretty_format ( obj , indent = none ) if indent is none if isinstance ( obj , tl_object ) obj = obj . to_dict ( ) if isinstance ( obj , dict ) return string . format ( obj . get ( string , string ) , string . join ( string . format ( var , tl_object . pretty_format ( var ) ) for var , var in obj . items ( ) if var = string ) ) elif isinstance ( obj , str ) or isinstance ( obj , bytes ) return repr ( obj ) elif hasattr ( obj , string ) return string . format ( string . join ( tl_object . pretty_format ( x ) for x in obj ) ) else return repr ( obj ) else result = [ ] if isinstance ( obj , tl_object ) obj = obj . to_dict ( ) if isinstance ( obj , dict ) result . append ( obj . get ( string , string ) ) result . append ( string ) if obj result . append ( string ) indent + = number for var , var in obj . items ( ) if var = = string continue result . append ( string * indent ) result . append ( var ) result . append ( string ) result . append ( tl_object . pretty_format ( var , indent ) ) result . append ( string ) result . pop ( ) indent _ = number result . append ( string ) result . append ( string * indent ) result . append ( string ) elif isinstance ( obj , str ) or isinstance ( obj , bytes ) result . append ( repr ( obj ) ) elif hasattr ( obj , string ) result . append ( string ) indent + = number for x in obj result . append ( string * indent ) result . append ( tl_object . pretty_format ( x , indent ) ) result . append ( string ) indent _ = number result . append ( string * indent ) result . append ( string ) else result . append ( repr ( obj ) ) return string . join ( result )	0
Q_564	print information of a quantitative phase imaging dataset	C_564_2	def process_has_args ( cls , kwargs ) rel_definitions = cls . defined_properties ( properties = false , rels = true , aliases = false ) match , dont_match = { } , { } for var , var in kwargs . items ( ) if var not in rel_definitions raise value_error ( string . format ( var , cls . _name_ ) ) rhs_ident = var rel_definitions [ var ] . _lookup_node_class ( ) if var is true match [ rhs_ident ] = rel_definitions [ var ] . definition elif var is false dont_match [ rhs_ident ] = rel_definitions [ var ] . definition elif isinstance ( var , node_set ) raise not_implemented_error ( string ) else raise value_error ( string + repr ( var ) ) return match , dont_match	0
Q_565	update this everytime the cern sso login form be refactored	C_565_0	def _init_browser ( self ) self . browser = splinter . browser ( string ) self . browser . visit ( self . server_url ) self . browser . find_link_by_partial_text ( string ) . click ( ) self . browser . fill ( string string , self . user ) self . browser . fill ( string string , self . password ) self . browser . find_by_css ( string ) . click ( ) self . browser . find_by_css ( string ) . click ( )	1
Q_565	update this everytime the cern sso login form be refactored	C_565_1	def write_file ( filename , text ) logging . debug ( _ ( string ) , filename ) try with open ( filename , string ) as writable writable . write ( text ) except ( permission_error , not_a_directory_error ) logging . error ( _ ( string ) , filename ) return false return true	0
Q_565	update this everytime the cern sso login form be refactored	C_565_2	def log_request ( self , handler ) if config . log_function in self . settings self . settings [ config . log_function ] ( handler ) return if handler . get_status ( ) < number log_method = logger . info elif handler . get_status ( ) < number log_method = logger . warning else log_method = logger . exception request_time = number . number * handler . request . request_time ( ) log_method ( string , handler . get_status ( ) , handler . _request_summary ( ) , request_time )	0
Q_566	"r tagstr """	C_566_0	def unique_variants ( g ) r string string string _s = sp . dot ( sp . rand ( g . shape [ number ] ) , g ) v , ix = sp . unique ( _s , return_index = true ) ix = sp . sort ( ix ) g_out = g [ , ix ] return g_out , ix	1
Q_566	"r tagstr """	C_566_1	def static_data ( fn ) file_path = os . path . normpath ( fn ) full_path = os . path . join ( static_path , file_path ) if not os . path . exists ( full_path ) abort ( number , string s string fn ) return static_file ( file_path , static_path )	0
Q_566	"r tagstr """	C_566_2	def alias_item ( self , alias ) ident = self . alias [ alias ] return self . items [ ident ]	0
Q_567	remove a list from the site	C_567_0	def remove ( self , list ) xml = sp . delete_list ( sp . list_name ( list . id ) ) self . opener . post_soap ( list_webservice , xml , soapaction = string ) self . all_lists . remove ( list )	1
Q_567	remove a list from the site	C_567_1	def refit_value ( self , a ) p , name = split_prefix ( a . get_value ( ) ) if p is none return ns = a . resolve_prefix ( p ) if self . permit ( ns ) u = ns [ number ] p = self . prefixes [ u ] a . set_value ( string . join ( ( p , name ) ) )	0
Q_567	remove a list from the site	C_567_2	def update ( self , _attributes = none , * * attributes ) if _attributes is not none attributes . update ( _attributes ) if self . _related . uses_timestamps ( ) attributes [ self . get_related_updated_at ( ) ] = self . _related . fresh_timestamp ( ) return self . _query . update ( attributes )	0
Q_568	helper function to re_render command_line option	C_568_0	def rerender_options ( options ) args = [ ] for var , var in options . iteritems ( ) var = var . replace ( string , string ) if var is none pass elif isinstance ( var , bool ) if var args . append ( string ( var , ) ) elif isinstance ( var , list ) for item in var args . append ( string ( var , item ) ) else args . append ( string ( var , var ) ) return string . join ( args )	1
Q_568	helper function to re_render command_line option	C_568_1	def binary ( self ) lib_name = string . format ( native_engine_module ) lib_path = os . path . join ( safe_mkdtemp ( ) , lib_name ) try with closing ( pkg_resources . resource_stream ( _name_ , lib_name ) ) as input_fp engine_version = input_fp . readline ( ) . decode ( string ) . strip ( ) repo_version = input_fp . readline ( ) . decode ( string ) . strip ( ) logger . debug ( string . format ( engine_version , repo_version ) ) with open ( lib_path , string ) as output_fp output_fp . write ( input_fp . read ( ) ) except ( io_error , os_error ) as e raise self . binary_location_error ( string . format ( lib_path , e ) , e ) return lib_path	0
Q_568	helper function to re_render command_line option	C_568_2	def parse_pointfinder ( self ) self . populate_summary_dict ( ) for var in self . summary_dict for report in self . summary_dict [ var ] try os . remove ( self . summary_dict [ var ] [ report ] [ string ] ) except file_not_found_error pass for sample in self . runmetadata . samples try self . summary_dict [ sample . general . referencegenus ] [ string ] [ string ] = glob ( os . path . join ( sample [ self . analysistype ] . pointfinder_outputs , string . format ( seq = sample . name ) ) ) [ number ] except index_error try self . summary_dict [ sample . general . referencegenus ] [ string ] [ string ] = str ( ) except key_error self . populate_summary_dict ( genus = sample . general . referencegenus , key = string ) try self . summary_dict [ sample . general . referencegenus ] [ string ] [ string ] = glob ( os . path . join ( sample [ self . analysistype ] . pointfinder_outputs , string . format ( seq = sample . name ) ) ) [ number ] except index_error try self . summary_dict [ sample . general . referencegenus ] [ string ] [ string ] = str ( ) except key_error self . populate_summary_dict ( genus = sample . general . referencegenus , key = string ) try self . summary_dict [ sample . general . referencegenus ] [ string ] [ string ] = glob ( os . path . join ( sample [ self . analysistype ] . pointfinder_outputs , string . format ( seq = sample . name ) ) ) [ number ] except index_error try self . summary_dict [ sample . general . referencegenus ] [ string ] [ string ] = str ( ) except key_error self . populate_summary_dict ( genus = sample . general . referencegenus , key = string ) self . write_report ( summary_dict = self . summary_dict , seqid = sample . name , genus = sample . general . referencegenus , key = string ) self . write_report ( summary_dict = self . summary_dict , seqid = sample . name , genus = sample . general . referencegenus , key = string ) self . write_table_report ( summary_dict = self . summary_dict , seqid = sample . name , genus = sample . general . referencegenus )	0
Q_569	this be a decorator which can be use to mark function as deprecate	C_569_0	"def deprecated ( func ) def deprecation_warning ( * args , * * kwargs ) warnings . warn ( string "" http / / pyapi _ gitlab . readthedocs . io / en / latest / category = deprecation_warning ) return func ( * args , * * kwargs ) deprecation_warning . _name_ = func . _name_ deprecation_warning . _doc_ = func . _doc_ deprecation_warning . _dict_ = func . _dict_ return deprecation_warning"	1
Q_569	this be a decorator which can be use to mark function as deprecate	C_569_1	def _int_generator ( descriptor , bitwidth , unsigned ) string vals = list ( values . get_integers ( bitwidth , unsigned ) ) return gen . iter_value_generator ( descriptor . name , vals )	0
Q_569	this be a decorator which can be use to mark function as deprecate	C_569_2	def update ( self ) try self . counter + = number g = get_root ( self ) . globals now = time . now ( ) self . utc . configure ( text = now . datetime . strftime ( string ) ) self . mjd . configure ( text = string . format ( now . mjd ) ) with warnings . catch_warnings ( ) warnings . simplefilter ( string ) lon = self . obs . longitude lst = now . sidereal_time ( kind = string , longitude = lon ) self . lst . configure ( text = lst . to_string ( sep = string , precision = number ) ) if self . counter number = = number altaz_frame = coord . alt_az ( obstime = now , location = self . obs ) sun = coord . get_sun ( now ) sun_aa = sun . transform_to ( altaz_frame ) moon = coord . get_moon ( now , self . obs ) moon_aa = moon . transform_to ( altaz_frame ) elongation = sun . separation ( moon ) moon_phase_angle = np . arctan2 ( sun . distance * np . sin ( elongation ) , moon . distance _ sun . distance * np . cos ( elongation ) ) moon_phase = ( number + np . cos ( moon_phase_angle ) ) / number . number self . sunalt . configure ( text = string . format ( int ( sun_aa . alt . deg ) ) ) self . moonra . configure ( text = moon . ra . to_string ( unit = string , sep = string , precision = number ) ) self . moondec . configure ( text = moon . dec . to_string ( unit = string , sep = string , precision = number ) ) self . moonalt . configure ( text = string . format ( int ( moon_aa . alt . deg ) ) ) self . moonphase . configure ( text = string . format ( int ( number . * moon_phase . value ) ) ) if ( now > self . last_riset and now > self . last_astro ) horizon = _ number * u . arcmin sunset = calc_riseset ( now , string , self . obs , string , string , horizon ) sunrise = calc_riseset ( now , string , self . obs , string , string , horizon ) horizon = _ number * u . deg astroset = calc_riseset ( now , string , self . obs , string , string , horizon ) astrorise = calc_riseset ( now , string , self . obs , string , string , horizon ) if sunrise > sunset self . lriset . configure ( text = string , font = g . default_font ) self . last_riset = sunset self . last_astro = astroset elif astrorise > astroset and astrorise < sunrise self . lriset . configure ( text = string , font = g . default_font ) horizon = _ number * u . arcmin self . last_riset = calc_riseset ( now , string , self . obs , string , string , horizon ) self . last_astro = astroset elif astrorise > astroset and astrorise < sunrise self . lriset . configure ( text = string , font = g . default_font ) horizon = _ number * u . arcmin self . last_riset = sunrise self . last_astro = astrorise else self . lriset . configure ( text = string , font = g . default_font ) horizon = _ number * u . deg self . last_riset = sunrise self . last_astro = calc_riseset ( now , string , self . obs , string , string , horizon ) self . riset . configure ( text = self . last_riset . datetime . strftime ( string ) ) self . astro . configure ( text = self . last_astro . datetime . strftime ( string ) ) except exception as err g . clog . warn ( string + str ( err ) ) self . after ( number , self . update )	0
Q_570	return the text body of the message	C_570_0	def content ( self ) message = self . _content_xpb . one_ ( self . _message_element ) first_line = message . text if message . text [ number ] = = string first_line = message . text [ number ] else log . debug ( string ) subsequent_lines = string . join ( [ html . tostring ( var , encoding = string ) . replace ( string , string ) for var in message . iterchildren ( ) ] ) message_text = first_line + subsequent_lines if len ( message_text ) > number and message_text [ _ number ] = = string message_text = message_text [ _ number ] else log . debug ( string ) return message_text	1
Q_570	return the text body of the message	C_570_1	def get_duration_measures ( source_file_path , output_path = none , phonemic = false , semantic = false , quiet = false , similarity_file = none , threshold = none ) args = args ( ) args . output_path = output_path args . phonemic = phonemic args . semantic = semantic args . source_file_path = source_file_path args . quiet = quiet args . similarity_file = similarity_file args . threshold = threshold args = validate_arguments ( args ) if args . phonemic response_category = args . phonemic output_prefix = os . path . basename ( args . source_file_path ) . split ( string ) [ number ] + string + args . phonemic elif args . semantic response_category = args . semantic output_prefix = os . path . basename ( args . source_file_path ) . split ( string ) [ number ] + string + args . semantic else response_category = string output_prefix = string if args . output_path target_file_path = os . path . join ( args . output_path , output_prefix + string ) else target_file_path = false engine = vf_clust_engine ( response_category = response_category , response_file_path = args . source_file_path , target_file_path = target_file_path , quiet = args . quiet , similarity_file = args . similarity_file , threshold = args . threshold ) return dict ( engine . measures )	0
Q_570	return the text body of the message	C_570_2	def show_raslog_output_show_all_raslog_raslog_entries_index ( self , * * kwargs ) config = et . element ( string ) show_raslog = et . element ( string ) config = show_raslog output = et . sub_element ( show_raslog , string ) show_all_raslog = et . sub_element ( output , string ) raslog_entries = et . sub_element ( show_all_raslog , string ) index = et . sub_element ( raslog_entries , string ) index . text = kwargs . pop ( string ) callback = kwargs . pop ( string , self . _callback ) return callback ( config )	0
Q_571	call when a file be create	C_571_0	def on_created ( self , event , dry_run = false , remove_uploaded = true ) string super ( archive_event_handler , self ) . on_created ( event ) log . info ( string , event )	1
Q_571	call when a file be create	C_571_1	def buildhtmlheader ( self ) if self . drilldown_flag self . add_j_ssource ( string ) if self . offline opener = urllib . request . build_opener ( ) opener . addheaders = [ ( string , string ) ] self . header_css = [ string opener . open ( var ) . read ( ) for var in self . cs_ssource ] self . header_js = [ string text / javascript string opener . open ( var ) . read ( ) for var in self . j_ssource ] else self . header_css = [ string s string stylesheet string var for var in self . cs_ssource ] self . header_js = [ string text / javascript string s string var for var in self . j_ssource ] self . htmlheader = string for css in self . header_css self . htmlheader + = css for js in self . header_js self . htmlheader + = js	0
Q_571	call when a file be create	C_571_2	def save_model ( self , net ) if self . f_params is not none f = self . _format_target ( net , self . f_params , _ number ) self . _save_params ( f , net , string , string ) if self . f_optimizer is not none f = self . _format_target ( net , self . f_optimizer , _ number ) self . _save_params ( f , net , string , string ) if self . f_history is not none f = self . f_history_ self . _save_params ( f , net , string , string ) if self . f_pickle f_pickle = self . _format_target ( net , self . f_pickle , _ number ) with open_file_like ( f_pickle , string ) as f pickle . dump ( net , f )	0
Q_572	set plot title	C_572_0	def set_title ( self , s , delay_draw = false ) string self . conf . relabel ( title = s , delay_draw = delay_draw )	1
Q_572	set plot title	C_572_1	def do_vars ( self , line ) if self . bot . _vars max_name_len = max ( [ len ( var ) for var in self . bot . _vars ] ) for i , ( var , v ) in enumerate ( self . bot . _vars . items ( ) ) keep = i < len ( self . bot . _vars ) _ number self . print_response ( string ( var . ljust ( max_name_len ) , v . value ) , keep = keep ) else self . print_response ( string )	0
Q_572	set plot title	C_572_2	def usage ( self , subcommand ) usage = string . join ( [ string , subcommand , string ] ) if self . args usage = string ( usage , str ( self . args ) ) return usage	0
Q_573	generate exclusion pattern from a	C_573_0	def get_exclusions ( path ) if not os . path . isdir ( path ) return none dockerignore_file = os . path . join ( path , string ) if not os . path . isfile ( dockerignore_file ) return none with open ( dockerignore_file , string ) as dif return list ( preprocess_matches ( dif . readlines ( ) ) )	1
Q_573	generate exclusion pattern from a	C_573_1	def warning ( message ) message = message or string if console . color console . cprint ( string , string , message ) else print ( console . msg ( string + message ) )	0
Q_573	generate exclusion pattern from a	C_573_2	def mktime_tz ( data ) if data [ number ] is none return time . mktime ( data [ number ] + ( _ number , ) ) else t = time . mktime ( data [ number ] + ( number , ) ) return t _ data [ number ] _ time . timezone	0
Q_574	convert the input to a network_endpoint tuple	C_574_0	def get_type_item ( self , value ) if isinstance ( value , network_endpoint ) return value elif isinstance ( value , six . string_types + ( lazy_type , ) ) return network_endpoint ( value ) elif isinstance ( value , ( list , tuple ) ) v_len = len ( value ) if v_len = = number and isinstance ( value [ number ] , dict ) return network_endpoint ( value [ number ] , * * value [ number ] ) elif number < = v_len < = number return network_endpoint ( * value ) raise value_error ( string string . format ( v_len ) ) elif isinstance ( value , dict ) d_len = len ( value ) if d_len = = number k , v = list ( value . items ( ) ) [ number ] if k = = string return network_endpoint ( v ) if not v return network_endpoint ( k ) if isinstance ( v , six . string_types ) return network_endpoint ( k , v ) v_len = len ( v ) if isinstance ( v , dict ) return network_endpoint ( k , * * v ) elif isinstance ( v , ( list , tuple ) ) if number < = v_len < = number return network_endpoint ( k , * v ) raise value_error ( string string . format ( v_len ) ) raise value_error ( string string . format ( type ( value ) . _name_ ) ) return network_endpoint ( * * value ) raise value_error ( string . format ( type ( value ) . _name_ ) )	1
Q_574	convert the input to a network_endpoint tuple	C_574_1	def focus_in_event ( self , event ) self . focus_changed . emit ( ) return super ( control_widget , self ) . focus_in_event ( event )	0
Q_574	convert the input to a network_endpoint tuple	C_574_2	def get_provisioned_gsi_read_units ( table_name , gsi_name ) try desc = dynamodb_connection . describe_table ( table_name ) except json_response_error raise for var in desc [ u string ] [ u string ] if var [ u string ] = = gsi_name read_units = int ( var [ u string ] [ u string ] ) break logger . debug ( string . format ( table_name , gsi_name , read_units ) ) return read_units	0
Q_575	require that the request contain a valid signature to gain access	C_575_0	def signature_required ( secret_key_func ) def actual_decorator ( obj ) def test_func ( request , * args , * * kwargs ) secret_key = secret_key_func ( request , * args , * * kwargs ) return validate_signature ( request , secret_key ) decorator = request_passes_test ( test_func ) return wrap_object ( obj , decorator ) return actual_decorator	1
Q_575	require that the request contain a valid signature to gain access	C_575_1	def embed_check_integer_casting_closed ( x , target_dtype , assert_nonnegative = true , assert_positive = false , name = string ) with tf . name_scope ( name ) x = tf . convert_to_tensor ( value = x , name = string ) if ( not _is_integer_like_by_dtype ( x . dtype ) and not dtype_util . is_floating ( x . dtype ) ) raise type_error ( string string . format ( dtype_util . name ( x . dtype ) ) ) if ( not _is_integer_like_by_dtype ( target_dtype ) and not dtype_util . is_floating ( target_dtype ) ) raise type_error ( string string . format ( dtype_util . name ( target_dtype ) ) ) if ( not _is_integer_like_by_dtype ( x . dtype ) and not _is_integer_like_by_dtype ( target_dtype ) ) raise type_error ( string string . format ( x , dtype_util . name ( x . dtype ) , dtype_util . name ( target_dtype ) ) ) assertions = [ ] if assert_positive assertions + = [ assert_util . assert_positive ( x , message = string ) , ] elif assert_nonnegative assertions + = [ assert_util . assert_non_negative ( x , message = string ) , ] if dtype_util . is_floating ( x . dtype ) assertions + = [ assert_integer_form ( x , int_dtype = target_dtype , message = string . format ( dtype_util . name ( target_dtype ) ) ) , ] else if ( _largest_integer_by_dtype ( x . dtype ) > _largest_integer_by_dtype ( target_dtype ) ) assertions + = [ assert_util . assert_less_equal ( x , _largest_integer_by_dtype ( target_dtype ) , message = ( string . format ( _largest_integer_by_dtype ( target_dtype ) ) ) ) , ] if ( not assert_nonnegative and ( _smallest_integer_by_dtype ( x . dtype ) < _smallest_integer_by_dtype ( target_dtype ) ) ) assertions + = [ assert_util . assert_greater_equal ( x , _smallest_integer_by_dtype ( target_dtype ) , message = ( string . format ( _smallest_integer_by_dtype ( target_dtype ) ) ) ) , ] if not assertions return x return with_dependencies ( assertions , x )	0
Q_575	require that the request contain a valid signature to gain access	C_575_2	def mont_priv_to_ed_pair ( cls , mont_priv ) if not isinstance ( mont_priv , bytes ) raise type_error ( string ) if len ( mont_priv ) = cls . mont_priv_key_size raise value_error ( string ) ed_priv , ed_pub = cls . _mont_priv_to_ed_pair ( bytearray ( mont_priv ) ) return bytes ( ed_priv ) , bytes ( ed_pub )	0
Q_576	callback for pip_list	C_576_0	def _pip_list ( self , stdout , stderr , prefix = none ) result = stdout linked = self . linked ( prefix ) pip_only = [ ] linked_names = [ self . split_canonical_name ( var ) [ number ] for var in linked ] for pkg in result name = self . split_canonical_name ( pkg ) [ number ] if name not in linked_names pip_only . append ( pkg ) return pip_only	1
Q_576	callback for pip_list	C_576_1	def format ( self , vertices ) buf = io . string_io ( ) buf . write ( self . name + string ) buf . write ( string ) buf . write ( string . format ( self . type_ ) ) buf . write ( string ) buf . write ( string ) for var in self . faces s = var . format ( vertices ) buf . write ( string . format ( s ) ) buf . write ( string ) buf . write ( string ) return buf . getvalue ( )	0
Q_576	callback for pip_list	C_576_2	def calc_run ( request ) hazard_job_id = request . post . get ( string ) if hazard_job_id hazard_job_id = int ( hazard_job_id ) candidates = ( string , string ) else candidates = ( string , string , string ) result = safely_call ( _prepare_job , ( request , candidates ) ) if result . tb_str return http_response ( json . dumps ( result . tb_str . splitlines ( ) ) , content_type = json , status = number ) inifiles = result . get ( ) if not inifiles msg = string str ( candidates ) logging . error ( msg ) return http_response ( content = json . dumps ( [ msg ] ) , content_type = json , status = number ) user = utils . get_user ( request ) try job_id , pid = submit_job ( inifiles [ number ] , user , hazard_job_id ) except exception as exc exc_msg = str ( exc ) logging . error ( exc_msg ) response_data = exc_msg . splitlines ( ) status = number else response_data = dict ( job_id = job_id , status = string , pid = pid ) status = number return http_response ( content = json . dumps ( response_data ) , content_type = json , status = status )	0
Q_577	filter to get code_mirror javascript bundle name need for a single field	C_577_0	def codemirror_field_js_bundle ( field ) manifesto = codemirror_asset_tag_render ( ) manifesto . register_from_fields ( field ) try bundle_name = manifesto . js_bundle_names ( ) [ number ] except index_error msg = ( string { } string string ) raise code_mirror_field_bundle_error ( msg . format ( field . config_name ) ) return bundle_name	1
Q_577	filter to get code_mirror javascript bundle name need for a single field	C_577_1	def ec_matrix_vector ( p0 , t , n ) r if ( n < = number ) ec = np . zeros ( t . shape ) return ec else p_k = number . number * p0 p_sum = number . number * p_k for var in range ( n _ number ) p_k = np . dot ( p_k , t ) p_sum + = p_k ec = p_sum [ , np . newaxis ] * t return ec	0
Q_577	filter to get code_mirror javascript bundle name need for a single field	C_577_2	def error_recorder ( self , line_number , offset , text , check ) code = text . split ( string ) [ number ] line_offset = self . report . line_offset self . warnings . append ( ( line_offset + line_number , offset + number , code , text ) )	0
Q_578	perform several type of string cleaning for title etc	C_578_0	def clean ( mapping , bind , values ) categories = { string string } for var in values if isinstance ( var , six . string_types ) var = normality . normalize ( var , lowercase = false , collapse = true , decompose = false , replace_categories = categories ) yield var	1
Q_578	perform several type of string cleaning for title etc	C_578_1	def find ( self , name , limit = none ) if name = = self . name if limit is not none assert limit = = number self . subdirs = [ ] return self for var in self . subdirs res = var . find ( name , limit ) if res is not none return res return none	0
Q_578	perform several type of string cleaning for title etc	C_578_2	def get_reg_name ( arch , reg_offset ) if reg_offset is none return none original_offset = reg_offset while reg_offset > = number and reg_offset > = original_offset _ ( arch . bytes ) if reg_offset in arch . register_names return arch . register_names [ reg_offset ] else reg_offset _ = number return none	0
Q_579	bind an event to a callback	C_579_0	def bind ( self , event_name , callback ) if event_name not in self . event_callbacks . keys ( ) self . event_callbacks [ event_name ] = [ ] self . event_callbacks [ event_name ] . append ( callback )	1
Q_579	bind an event to a callback	C_579_1	def shift_image ( u , v , t , img , interpolation = cv2 . inter_lanczos4 ) string string string ny , nx = u . shape sy , sx = np . mgrid [ float ( ny ) number , float ( nx ) number ] sx + = u * t sy + = v * t return cv2 . remap ( img . astype ( np . float32 ) , ( sx ) . astype ( np . float32 ) , ( sy ) . astype ( np . float32 ) , interpolation )	0
Q_579	bind an event to a callback	C_579_2	def find_packages ( ) packages = [ string ] for dir , subdirs , files in os . walk ( string ) package = dir . replace ( os . path . sep , string ) if string not in files continue packages . append ( package . replace ( string , string ) ) return packages	0
Q_580	"if we "" re do backup on a regular basis then"	C_580_0	def make_backups ( self , block_id ) assert self . setup , string if self . backup_frequency is not none if ( block_id self . backup_frequency ) = = number backup_dir = config . get_backups_directory ( self . impl , self . working_dir ) if not os . path . exists ( backup_dir ) try os . makedirs ( backup_dir ) except exception , e log . exception ( e ) log . error ( string s string backup_dir ) traceback . print_stack ( ) os . abort ( ) for var in self . get_state_paths ( self . impl , self . working_dir ) if os . path . exists ( var ) try pbase = os . path . basename ( var ) backup_path = os . path . join ( backup_dir , pbase + ( string . format ( block_id _ number ) ) ) if not os . path . exists ( backup_path ) rc = sqlite3_backup ( var , backup_path ) if not rc log . warning ( string ) shutil . copy ( var , backup_path ) else log . error ( string s string backup_path ) except exception , e log . exception ( e ) log . error ( string s string var ) traceback . print_stack ( ) os . abort ( ) return	1
Q_580	"if we "" re do backup on a regular basis then"	C_580_1	def bestmatch ( self ) if self . obj is not none for var in self . statemanager . all_states_by_value [ self . value ] msw = var ( self . obj , self . cls ) if msw return msw	0
Q_580	"if we "" re do backup on a regular basis then"	C_580_2	def merge_configs ( self , config , datas ) if not isinstance ( config , dict ) or len ( [ var for var in datas if not isinstance ( var , dict ) ] ) > number raise type_error ( string ) for key , value in config . items ( ) others = [ var [ key ] for var in datas if key in var ] if len ( others ) > number if isinstance ( value , dict ) config [ key ] = self . merge_configs ( value , others ) else config [ key ] = others [ _ number ] return config	0
Q_581	list comment for datasets	C_581_0	def marvcli_comment_list ( datasets ) app = create_app ( ) ids = parse_setids ( datasets , dbids = true ) comments = db . session . query ( comment ) . options ( db . joinedload ( comment . dataset ) ) . filter ( comment . dataset_id . in_ ( ids ) ) for var in sorted ( comments , key = lambda x ( x . dataset . _setid , x . id ) ) print ( var . dataset . setid , var . id , datetime . datetime . fromtimestamp ( int ( var . time_added / number ) ) , var . author , repr ( var . text ) )	1
Q_581	list comment for datasets	C_581_1	def cmd ( self , argv ) assert isinstance ( argv , ( list , tuple ) ) , string argv string argv retval = none try argv = self . precmd ( argv ) retval = self . onecmd ( argv ) self . postcmd ( argv ) except if not self . cmdexc ( argv ) raise retval = number return retval	0
Q_581	list comment for datasets	C_581_2	def camera_snapshot ( self , camera_id , * * kwargs ) api = self . _api_info [ string ] payload = dict ( { string self . _sid , string api [ string ] , string string , string api [ string ] , string camera_id , } , * * kwargs ) response = self . _get ( api [ string ] , payload ) return response . content	0
Q_582	wait for a keypress at the console and return it	C_582_0	def wait_key ( keys = none ) if is_a_tty ( ) if keys if not isinstance ( keys , tuple ) keys = ( keys , ) while true key = _getch ( ) if key in keys return key else return _getch ( )	1
Q_582	wait for a keypress at the console and return it	C_582_1	def add_resourcegroupitems ( scenario_id , items , scenario = none , * * kwargs ) user_id = int ( kwargs . get ( string ) ) if scenario is none scenario = _get_scenario ( scenario_id , user_id ) _check_network_ownership ( scenario . network_id , user_id ) newitems = [ ] for var in items group_item_i = _add_resourcegroupitem ( var , scenario . id ) newitems . append ( group_item_i ) db . db_session . flush ( ) return newitems	0
Q_582	wait for a keypress at the console and return it	C_582_2	def infer_annotation ( type_comments ) assert type_comments args = { } returns = set ( ) for var in type_comments arg_types , return_type = parse_type_comment ( var ) for i , arg_type in enumerate ( arg_types ) args . setdefault ( i , set ( ) ) . add ( arg_type ) returns . add ( return_type ) combined_args = [ ] for i in sorted ( args ) arg_infos = list ( args [ i ] ) kind = argument_kind ( arg_infos ) if kind is none raise infer_error ( string + string . join ( type_comments ) ) types = [ arg . type for arg in arg_infos ] combined = combine_types ( types ) if str ( combined ) = = string combined = union_type ( [ class_type ( string ) , any_type ( ) ] ) if kind = arg_pos and ( len ( str ( combined ) ) > number or isinstance ( combined , union_type ) ) combined = any_type ( ) combined_args . append ( argument ( combined , kind ) ) combined_return = combine_types ( returns ) return combined_args , combined_return	0
Q_583	return what the default compartment should be set to	C_583_0	def get_default_compartment ( model ) default_compartment = string default_key = set ( ) for var in model . reactions equation = var . equation if equation is none continue for compound , _ in equation . compounds default_key . add ( compound . compartment ) if none in default_key and default_compartment in default_key suffix = number while true new_key = string . format ( default_compartment , suffix ) if new_key not in default_key default_compartment = new_key break suffix + = number if none in default_key logger . warning ( string string . format ( default_compartment ) ) return default_compartment	1
Q_583	return what the default compartment should be set to	C_583_1	def regular_subset ( spikes , n_spikes_max = none , offset = number ) assert spikes is not none if n_spikes_max is none or len ( spikes ) < = n_spikes_max return spikes step = math . ceil ( np . clip ( number . / n_spikes_max * len ( spikes ) , number , len ( spikes ) ) ) step = int ( step ) my_spikes = spikes [ offset step ] [ n_spikes_max ] assert len ( my_spikes ) < = len ( spikes ) assert len ( my_spikes ) < = n_spikes_max return my_spikes	0
Q_583	return what the default compartment should be set to	C_583_2	def _initialize_variables ( self ) self . _global_vars = { } for var in ( self . _layout_validator . splittable_mtf_dimension_names ) for mesh_dimension_name in ( self . _layout_validator . mesh_dimension_name_to_size ) name = _global_var_name ( var , mesh_dimension_name ) self . _global_vars [ ( var , mesh_dimension_name ) ] = ( self . _model . new_bool_var ( name ) ) self . _local_vars = { } for mtf_dimension_set in self . _mtf_dimension_sets self . _local_vars [ mtf_dimension_set ] = { } for assignment in self . _assignments [ mtf_dimension_set ] name = _local_var_name ( mtf_dimension_set , assignment ) self . _local_vars [ mtf_dimension_set ] [ name ] = ( self . _model . new_bool_var ( name ) ) memory_upper_bound = number for tensor_name in self . _graph . get_all_tensor_names ( ) if self . _graph . is_tensor_on_canonical_device ( tensor_name ) memory_upper_bound + = int ( self . _graph . get_tensor_size ( tensor_name ) ) self . _memory_var = self . _model . new_int_var ( number , memory_upper_bound , string )	0
Q_584	add constraint to the model	C_584_0	def add ( self , * args ) self . _constrs . extend ( self . _moma . _prob . add_linear_constraints ( * args ) )	1
Q_584	add constraint to the model	C_584_1	def add_argparser ( self , root , parents ) parser = root . add_parser ( string , parents = parents ) parser . set_defaults ( func = self ) parser . add_argument ( string , dest = string , action = string , help = string ) parser . add_argument ( string , dest = string , action = string , help = string ) parser . add_argument ( string , dest = string , action = string , help = string ) parser . add_argument ( string , dest = string , action = string , help = string ) parser . add_argument ( string , dest = string , action = string , help = string ) parser . add_argument ( string , dest = string , action = string , type = int , help = string ) parser . add_argument ( string , dest = string , action = string , help = string ) parser . add_argument ( string , dest = string , action = string , help = string ) parser . add_argument ( string , action = string , help = string ) parser . add_argument ( string , action = string , help = string ) return parser	0
Q_584	add constraint to the model	C_584_2	def dict_array_bytes ( ary , template ) shape = shape_from_str_tuple ( ary [ string ] , template ) dtype = dtype_from_str ( ary [ string ] , template ) return array_bytes ( shape , dtype )	0
Q_585	retrieve a tuple with the x y coordinate of the low right point of the circle	C_585_0	def get_bottom_right ( self ) return ( float ( self . get_cx ( ) ) + float ( self . get_r ( ) ) , float ( self . get_cy ( ) ) _ float ( self . get_r ( ) ) )	1
Q_585	retrieve a tuple with the x y coordinate of the low right point of the circle	C_585_1	def is_module ( self , dotted_name , extrapath = none ) try return self . _module_cache [ ( dotted_name , extrapath ) ] except key_error pass if dotted_name in sys . modules or dotted_name in self . builtin_modules return dotted_name filename = dotted_name . replace ( string , os . path . sep ) if extrapath for var in self . _exts candidate = os . path . join ( extrapath , filename ) + var if os . path . exists ( candidate ) modname = self . filename_to_modname ( candidate ) self . _module_cache [ ( dotted_name , extrapath ) ] = modname return modname try return self . _module_cache [ ( dotted_name , none ) ] except key_error pass for dir in self . path if os . path . isfile ( dir ) if dir . endswith ( string ) continue try zf = zipfile . zip_file ( dir ) except zipfile . bad_zipfile self . warn ( dir , string , dir ) continue names = zf . namelist ( ) for var in self . _exts candidate = filename + var if candidate in names modname = filename . replace ( os . path . sep , string ) self . _module_cache [ ( dotted_name , extrapath ) ] = modname self . _module_cache [ ( dotted_name , none ) ] = modname return modname else for var in self . _exts candidate = os . path . join ( dir , filename ) + var if os . path . exists ( candidate ) modname = self . filename_to_modname ( candidate ) self . _module_cache [ ( dotted_name , extrapath ) ] = modname self . _module_cache [ ( dotted_name , none ) ] = modname return modname return none	0
Q_585	retrieve a tuple with the x y coordinate of the low right point of the circle	C_585_2	def raw_diff ( self ) udiff_copy = self . copy_iterator ( ) if self . _format = = string udiff_copy = self . _parse_gitdiff ( udiff_copy ) return u string . join ( udiff_copy )	0
Q_586	check for error on data payload	C_586_0	def check_errors ( self , data ) if ( type ( data ) = = dict and string in data . keys ( ) and data [ string ] = = string ) if data . get ( string ) and string in data . get ( string ) raise py_bossa_server_no_keyset_pagination else raise error ( data ) return false	1
Q_586	check for error on data payload	C_586_1	def update_device_info ( self , device_id , display_name ) content = { string display_name } return self . _send ( string , string device_id , content = content )	0
Q_586	check for error on data payload	C_586_2	def _get_top_states ( saltenv = string ) alt_states = [ ] try returned = _salt_ [ string ] ( ) for var in returned [ saltenv ] alt_states . append ( var ) except exception raise return alt_states	0
Q_587	read a sentence with syntax annotation parse from a tiger_xml	C_587_0	def _tigersentence2graph ( self , sentence ) self . add_node ( self . root , layers = { self . ns , self . ns + string , self . ns + string , self . ns + string } ) token_ids = [ ] for var in sentence . iterfind ( string ) terminal_id = var . attrib [ string ] token_ids . append ( terminal_id ) terminal_features = add_prefix ( var . attrib , self . ns + string ) terminal_features [ self . ns + string ] = ensure_unicode ( terminal_features [ self . ns + string ] ) self . add_node ( terminal_id , layers = { self . ns , self . ns + string } , attr_dict = terminal_features , label = terminal_features [ self . ns + string ] ) for secedge in var . iterfind ( string ) to_id = secedge . attrib [ string ] secedge_attribs = add_prefix ( secedge . attrib , self . ns + string ) if to_id not in self self . add_node ( to_id , layers = { self . ns , self . ns + string } ) self . add_edge ( terminal_id , to_id , layers = { self . ns , self . ns + string } , attr_dict = secedge_attribs , edge_type = edge_types . pointing_relation ) sorted_token_ids = sorted ( token_ids , key = natural_sort_key ) self . node [ self . root ] . update ( { string sorted_token_ids } ) self . tokens = sorted_token_ids for nt in sentence . iterfind ( string ) from_id = nt . attrib [ string ] nt_feats = add_prefix ( nt . attrib , self . ns + string ) nt_feats [ string ] = nt_feats [ self . ns + string ] if from_id in self self . node [ from_id ] . update ( nt_feats ) else self . add_node ( from_id , layers = { self . ns , self . ns + string } , attr_dict = nt_feats ) for edge in nt . iterfind ( string ) to_id = edge . attrib [ string ] if to_id not in self self . add_node ( to_id , layers = { self . ns , self . ns + string } ) edge_attribs = add_prefix ( edge . attrib , self . ns + string ) if self . ns + string in self . node [ to_id ] [ string ] edge_type = edge_types . spanning_relation else edge_type = edge_types . dominance_relation self . add_edge ( from_id , to_id , layers = { self . ns , self . ns + string } , attr_dict = edge_attribs , label = edge_attribs [ self . ns + string ] , edge_type = edge_type ) for secedge in nt . iterfind ( string ) to_id = secedge . attrib [ string ] if to_id not in self self . add_node ( to_id , layers = { self . ns , self . ns + string } ) secedge_attribs = add_prefix ( secedge . attrib , self . ns + string ) self . add_edge ( from_id , to_id , layers = { self . ns , self . ns + string } , attr_dict = secedge_attribs , label = edge_attribs [ self . ns + string ] , edge_type = edge_types . pointing_relation )	1
Q_587	read a sentence with syntax annotation parse from a tiger_xml	C_587_1	def list_users ( app , appbuilder ) _appbuilder = import_application ( app , appbuilder ) echo_header ( string ) for var in _appbuilder . sm . get_all_users ( ) click . echo ( string . format ( var . username , var . email , var . roles ) )	0
Q_587	read a sentence with syntax annotation parse from a tiger_xml	C_587_2	def update_dependent_files ( self , prev_commands = [ ] ) for var in prev_commands for my_input in self . input_parts for their_output in var . output_parts if their_output = = my_input my_input . filename = their_output . eval ( )	0
Q_588	detect_stream_mode _ detect the mode on a give stream	C_588_0	def detect_stream_mode ( stream ) if hasattr ( stream , string ) if string in stream . mode return bytes elif string in stream . mode return str if hasattr ( stream , string ) zero_str = stream . read ( number ) if type ( zero_str ) is str return str return bytes elif hasattr ( stream , string ) zero_str = stream . recv ( number ) if type ( zero_str ) is str return str return bytes return bytes	1
Q_588	detect_stream_mode _ detect the mode on a give stream	C_588_1	def layer_permutation ( self , layer_partition , layout , qubit_subset ) if self . seed is none self . seed = np . random . randint ( number , np . iinfo ( np . int32 ) . max ) rng = np . random . random_state ( self . seed ) rev_layout = { var var for var , var in layout . items ( ) } gates = [ ] for layer in layer_partition if len ( layer ) > number raise transpiler_error ( string ) elif len ( layer ) = = number gates . append ( tuple ( layer ) ) dist = sum ( [ self . coupling_map . distance ( layout [ g [ number ] ] [ number ] , layout [ g [ number ] ] [ number ] ) for g in gates ] ) if dist = = len ( gates ) circ = dag_circuit ( ) circ . add_qreg ( quantum_register ( self . coupling_map . size ( ) , string ) ) return true , circ , number , layout , bool ( gates ) n = self . coupling_map . size ( ) best_d = sys . maxsize best_circ = none best_layout = none qr = quantum_register ( self . coupling_map . size ( ) , string ) for _ in range ( self . trials ) trial_layout = layout . copy ( ) rev_trial_layout = rev_layout . copy ( ) trial_circ = dag_circuit ( ) trial_circ . add_qreg ( qr ) xi = { } for i in self . coupling_map . physical_qubits xi [ ( qr , i ) ] = { } for i in self . coupling_map . physical_qubits i = ( qr , i ) for j in self . coupling_map . physical_qubits j = ( qr , j ) scale = number + rng . normal ( number , number / n ) xi [ i ] [ j ] = scale * self . coupling_map . distance ( i [ number ] , j [ number ] ) * * number xi [ j ] [ i ] = xi [ i ] [ j ] d = number circ = dag_circuit ( ) circ . add_qreg ( qr ) identity_wire_map = { ( qr , j ) ( qr , j ) for j in range ( n ) } while d < number * n + number qubit_set = set ( qubit_subset ) while qubit_set min_cost = sum ( [ xi [ trial_layout [ g [ number ] ] ] [ trial_layout [ g [ number ] ] ] for g in gates ] ) progress_made = false for e in self . coupling_map . get_edges ( ) e = [ ( qr , edge ) for edge in e ] if e [ number ] in qubit_set and e [ number ] in qubit_set new_layout = trial_layout . copy ( ) new_layout [ rev_trial_layout [ e [ number ] ] ] = e [ number ] new_layout [ rev_trial_layout [ e [ number ] ] ] = e [ number ] rev_new_layout = rev_trial_layout . copy ( ) rev_new_layout [ e [ number ] ] = rev_trial_layout [ e [ number ] ] rev_new_layout [ e [ number ] ] = rev_trial_layout [ e [ number ] ] new_cost = sum ( [ xi [ new_layout [ g [ number ] ] ] [ new_layout [ g [ number ] ] ] for g in gates ] ) if new_cost < min_cost progress_made = true min_cost = new_cost opt_layout = new_layout rev_opt_layout = rev_new_layout opt_edge = e if progress_made qubit_set . remove ( opt_edge [ number ] ) qubit_set . remove ( opt_edge [ number ] ) trial_layout = opt_layout rev_trial_layout = rev_opt_layout circ . apply_operation_back ( swap_gate ( ) , [ ( opt_edge [ number ] [ number ] , opt_edge [ number ] [ number ] ) , ( opt_edge [ number ] [ number ] , opt_edge [ number ] [ number ] ) ] , [ ] ) else break dist = sum ( [ self . coupling_map . distance ( trial_layout [ g [ number ] ] [ number ] , trial_layout [ g [ number ] ] [ number ] ) for g in gates ] ) if dist = = len ( gates ) trial_circ . compose_back ( circ , identity_wire_map ) break d + = number dist = sum ( [ self . coupling_map . distance ( trial_layout [ g [ number ] ] [ number ] , trial_layout [ g [ number ] ] [ number ] ) for g in gates ] ) if dist = = len ( gates ) if d < best_d best_circ = trial_circ best_layout = trial_layout best_d = min ( best_d , d ) if best_circ is none return false , none , none , none , false return true , best_circ , best_d , best_layout , false	0
Q_588	detect_stream_mode _ detect the mode on a give stream	C_588_2	def _encrypt ( key_data , derived_key_information ) iv = os . urandom ( number ) symmetric_key = derived_key_information [ string ] encryptor = cipher ( algorithms . aes ( symmetric_key ) , modes . ctr ( iv ) , backend = default_backend ( ) ) . encryptor ( ) ciphertext = encryptor . update ( key_data . encode ( string ) ) + encryptor . finalize ( ) symmetric_key = derived_key_information [ string ] salt = derived_key_information [ string ] hmac_object = cryptography . hazmat . primitives . hmac . hmac ( symmetric_key , hashes . sha256 ( ) , backend = default_backend ( ) ) hmac_object . update ( ciphertext ) hmac_value = binascii . hexlify ( hmac_object . finalize ( ) ) iterations = derived_key_information [ string ] return binascii . hexlify ( salt ) . decode ( ) + _encryption_delimiter + str ( iterations ) + _encryption_delimiter + hmac_value . decode ( ) + _encryption_delimiter + binascii . hexlify ( iv ) . decode ( ) + _encryption_delimiter + binascii . hexlify ( ciphertext ) . decode ( )	0
Q_589	check open closing of docstring	C_589_0	def _check_docstring_format ( self , node_type , node , lineno_docstring ) docstring_stripped_spaces = node . doc . strip ( string ) if ( not docstring_stripped_spaces . startswith ( string ) or not docstring_stripped_spaces . endswith ( string ) ) self . add_message ( string , line = lineno_docstring , node = node ) else self . _check_indentation_issue ( node , node_type , lineno_docstring )	1
Q_589	check open closing of docstring	C_589_1	def len_on_depth ( d , depth ) counter = number for var in dict_tree . v_depth ( d , depth _ number ) counter + = dict_tree . length ( var ) return counter	0
Q_589	check open closing of docstring	C_589_2	def get_pod_container ( self , volume_mounts , persistence_outputs = none , persistence_data = none , outputs_refs_jobs = none , outputs_refs_experiments = none , secret_refs = none , configmap_refs = none , env_vars = none , command = none , args = none , resources = none , ports = none , ephemeral_token = none ) self . _pod_container_checks ( ) env_vars = to_list ( env_vars , check_none = true ) env_vars + = self . _get_container_pod_env_vars ( persistence_outputs = persistence_outputs , persistence_data = persistence_data , outputs_refs_jobs = outputs_refs_jobs , outputs_refs_experiments = outputs_refs_experiments , ephemeral_token = ephemeral_token ) env_vars + = get_resources_env_vars ( resources = resources ) env_from = get_pod_env_from ( secret_refs = secret_refs , configmap_refs = configmap_refs ) def get_ports ( ) _ports = to_list ( ports ) if ports else [ ] return [ client . v1_container_port ( container_port = var ) for var in _ports ] or none return client . v1_container ( name = self . job_container_name , image = self . job_docker_image , command = command , args = args , ports = get_ports ( ) , env = env_vars , env_from = env_from , resources = get_resources ( resources ) , volume_mounts = volume_mounts )	0
Q_590	convert the value to one that be safe to store on a record within	C_590_0	def store ( self , value , context = none ) if isinstance ( value , datetime . datetime ) if value . tzinfo is none tz = pytz . timezone ( orb . system . settings ( ) . server_timezone ) value = tz . localize ( value ) value = value . astimezone ( pytz . utc ) . replace ( tzinfo = none ) return super ( datetime_with_timezone_column , self ) . store ( value , context = context )	1
Q_590	convert the value to one that be safe to store on a record within	C_590_1	def set_model_version ( model , version ) if model is none or not isinstance ( model , onnx_proto . model_proto ) raise value_error ( string ) if not convert_utils . is_numeric_type ( version ) raise value_error ( string ) model . model_version = version	0
Q_590	convert the value to one that be safe to store on a record within	C_590_2	def get_currencysymbol ( self , code ) if not self . _symbols symbolpath = os . path . join ( self . _dir , string ) with open ( symbolpath , encoding = string ) as df self . _symbols = json . load ( df ) return self . _symbols . get ( code )	0
Q_591	return the private key for quick authentication on the ssh server	C_591_0	def pkey ( self ) if self . _pkey is none self . _pkey = self . _get_pkey ( ) return self . _pkey	1
Q_591	return the private key for quick authentication on the ssh server	C_591_1	def header ( self ) if not self . _header self . _header = header ( self . prev_hash , self . merkle_root , self . timestamp , self . index , self . consensus_data , self . next_consensus , self . script ) return self . _header	0
Q_591	return the private key for quick authentication on the ssh server	C_591_2	"def register_dimension ( self , name , dim_data , * * kwargs ) if name in self . _dims raise attribute_error ( ( string { n } string string as an attribute of the cube , but string it already exists . please choose string a different name "" ) . format ( n = name ) ) d = self . _dims [ name ] = create_dimension ( name , dim_data , * * kwargs ) return d"	0
Q_592	update the attribute for the function instance handle name change	C_592_0	def update ( self , name , modifiers , dtype , kind ) self . update_name ( name ) self . modifiers = modifiers self . dtype = dtype self . kind = kind self . update_dtype ( )	1
Q_592	update the attribute for the function instance handle name change	C_592_1	def te_x_la_te_x_str_function ( target = none , source = none , env = none ) if env . get_option ( string ) basedir = os . path . split ( str ( source [ number ] ) ) [ number ] abspath = os . path . abspath ( basedir ) if is_la_te_x ( source , env , abspath ) result = env . subst ( string , number , target , source ) + string else result = env . subst ( string , number , target , source ) + string else result = string return result	0
Q_592	update the attribute for the function instance handle name change	C_592_2	def j0_1 ( a = number ) r def lhs ( x ) return x * np . exp ( _ a * x * * number ) def rhs ( b ) return np . exp ( _ b * * number / ( number * a ) ) / ( number * a ) return ghosh ( string , lhs , rhs )	0
Q_593	retrieve information about your dhcp_options	C_593_0	def get_all_dhcp_options ( self , dhcp_options_ids = none ) params = { } if dhcp_options_ids self . build_list_params ( params , dhcp_options_ids , string ) return self . get_list ( string , params , [ ( string , dhcp_options ) ] )	1
Q_593	retrieve information about your dhcp_options	C_593_1	def _convert_row_to_unicode ( self , parser_mediator , row ) for var , var in iter ( row . items ( ) ) if isinstance ( var , py2to3 . unicode_type ) continue try row [ var ] = var . decode ( self . _encoding ) except unicode_decode_error replaced_value = var . decode ( self . _encoding , errors = string ) parser_mediator . produce_extraction_warning ( string string . format ( var , self . _encoding , replaced_value ) ) row [ var ] = replaced_value return row	0
Q_593	retrieve information about your dhcp_options	C_593_2	def delete_resource ( self , resource_path ) res = self . execute_operation ( method = string , ops_path = resource_path ) logging . info ( string ( resource_path ) ) return res	0
Q_594	to read and set local helical_axis position from an input file	C_594_0	def set_helical_axis ( self , filename , step_range = false , step = none ) if ( step_range ) if not isinstance ( step , list ) raise assertion_error ( string type ( step ) ) if ( len ( step ) > number ) print ( string ) exit ( number ) if ( step_range ) and ( step = = none ) raise value_error ( string ) got_parameters_input_file = check_parameters_input_file ( filename ) if got_parameters_input_file is none raise io_error ( string . format ( filename ) ) for var in helical_axis_parameters if var not in got_parameters_input_file raise value_error ( string . format ( got_parameters_input_file ) ) target_parameters = { number string , number string , number string } if ( step_range ) if ( len ( step ) = number ) raise value_error ( string ) if step [ number ] > step [ number ] raise value_error ( string ) data , time = read_param_file ( filename , [ number , number , number ] , step , true , start_bp = self . start_bp ) else data , time = read_param_file ( filename , [ number , number , number ] , [ number , self . num_step ] , true , start_bp = self . start_bp ) self . _set_time ( time ) if ( step_range ) bp_idx , param_idx = get_idx_of_bp_parameters ( step , [ ] , true , start_bp = self . start_bp ) else bp_idx , param_idx = get_idx_of_bp_parameters ( [ number , self . num_step ] , [ ] , true , start_bp = self . start_bp ) for i in range ( len ( data ) ) for j in range ( len ( data [ i ] ) ) bp_num = str ( bp_idx [ i ] + self . start_bp ) param = target_parameters [ j + number ] self . _set_data ( data [ i ] [ j ] , string , bp_num , param , scaleoffset = number )	1
Q_594	to read and set local helical_axis position from an input file	C_594_1	def parse ( url ) config = { } url = urlparse . urlparse ( url ) path = url . path [ number ] path = path . split ( string , number ) [ number ] config . update ( { string int ( path or number ) , string url . password or none , string url . hostname or string , string int ( url . port or number ) , } ) return config	0
Q_594	to read and set local helical_axis position from an input file	C_594_2	def compute_v ( self , memory_antecedent ) if self . shared_kv raise value_error ( string ) ret = mtf . einsum ( [ memory_antecedent , self . wv ] , reduced_dims = [ self . memory_input_dim ] ) if self . combine_dims ret = mtf . replace_dimensions ( ret , ret . shape . dims [ _ number ] , self . v_dims ) return ret	0
Q_595	stop the video stream and reset the clock	C_595_0	def stop ( self ) logger . debug ( string ) self . clock . stop ( ) self . status = ready	1
Q_595	stop the video stream and reset the clock	C_595_1	def validate ( self ) if not self . must_validate return true res = { } for var in self . validators . keys ( ) try if isinstance ( self . validators [ var ] , dict ) and var not in self . store self . store [ var ] = document_store ( self . collection , validators = self . validators [ var ] , init_dct = { } , sub_store = true , validate_init = self . validate_init ) self . validate_field ( var ) except invalid_document as e res . update ( e . errors ) except ( validation_error , schema_violation ) as e res [ var ] = str ( e ) if len ( res ) > number raise invalid_document ( res ) return true	0
Q_595	stop the video stream and reset the clock	C_595_2	def post_comment ( self , message ) report_url = ( string ( self . repo_name , self . pr_number ) ) result = self . requester . post ( report_url , { string message } ) if result . status_code > = number log . error ( string , result . json ( ) ) return result	0
Q_596	get the filename	C_596_0	def get_filename ( self , instance ) filename = self . field . get_filename ( instance ) if filename return filename fieldname = self . get_field_name ( ) content_type = self . get_content_type ( instance ) extension = mimetypes . guess_extension ( content_type ) return fieldname + extension	1
Q_596	get the filename	C_596_1	def image_type_cast ( image_list , pixeltype = none ) if not isinstance ( image_list , ( list , tuple ) ) raise value_error ( string ) pixtypes = [ ] for var in image_list pixtypes . append ( var . pixeltype ) if pixeltype is none pixeltype = string for p in pixtypes if p = = string pixeltype = string elif ( p = = string ) and ( pixeltype = string ) pixeltype = string elif ( p = = string ) and ( pixeltype = string ) and ( pixeltype = string ) pixeltype = string out_images = [ ] for var in image_list if var . pixeltype = = pixeltype out_images . append ( var ) else out_images . append ( var . clone ( pixeltype ) ) return out_images	0
Q_596	get the filename	C_596_2	def isentropic_interpolation ( theta_levels , pressure , temperature , * args , * * kwargs ) r def _isen_iter ( iter_log_p , isentlevs_nd , ka , a , b , pok ) exner = pok * np . exp ( _ ka * iter_log_p ) t = a * iter_log_p + b f = isentlevs_nd _ t * exner fp = exner * ( ka * t _ a ) return iter_log_p _ ( f / fp ) tmpk_out = kwargs . pop ( string , false ) max_iters = kwargs . pop ( string , number ) eps = kwargs . pop ( string , number e _ number ) axis = kwargs . pop ( string , number ) bottom_up_search = kwargs . pop ( string , true ) ndim = temperature . ndim pres = pressure . to ( string ) temperature = temperature . to ( string ) slices = [ np . newaxis ] * ndim slices [ axis ] = slice ( none ) slices = tuple ( slices ) pres = np . broadcast_to ( pres [ slices ] , temperature . shape ) * pres . units sort_pres = np . argsort ( pres . m , axis = axis ) sort_pres = np . swapaxes ( np . swapaxes ( sort_pres , number , axis ) [ _ number ] , number , axis ) sorter = broadcast_indices ( pres , sort_pres , ndim , axis ) levs = pres [ sorter ] tmpk = temperature [ sorter ] theta_levels = np . asanyarray ( theta_levels . to ( string ) ) . reshape ( _ number ) isentlevels = theta_levels [ np . argsort ( theta_levels ) ] shape = list ( temperature . shape ) shape [ axis ] = isentlevels . size isentlevs_nd = np . broadcast_to ( isentlevels [ slices ] , shape ) ka = mpconsts . kappa . m_as ( string ) pres_theta = potential_temperature ( levs , tmpk ) if np . max ( pres_theta . m ) < np . max ( theta_levels ) raise value_error ( string ) log_p = np . log ( levs . m ) pok = mpconsts . p0 * * ka above , below , good = find_bounding_indices ( pres_theta . m , theta_levels , axis , from_below = bottom_up_search ) a = ( tmpk . m [ above ] _ tmpk . m [ below ] ) / ( log_p [ above ] _ log_p [ below ] ) b = tmpk . m [ above ] _ a * log_p [ above ] isentprs = number . number * ( log_p [ above ] + log_p [ below ] ) good = np . isnan ( a ) log_p_solved = so . fixed_point ( _isen_iter , isentprs [ good ] , args = ( isentlevs_nd [ good ] , ka , a [ good ] , b [ good ] , pok . m ) , xtol = eps , maxiter = max_iters ) isentprs [ good ] = np . exp ( log_p_solved ) isentprs [ ( good _less_or_close ( isentprs , np . max ( pres . m ) ) ) ] = np . nan ret = [ isentprs * units . h_pa ] if tmpk_out ret . append ( ( isentlevs_nd / ( ( mpconsts . p0 . m / isentprs ) * * ka ) ) * units . kelvin ) if args others = interpolate_1d ( isentlevels , pres_theta . m , * ( var [ sorter ] for var in args ) , axis = axis ) if len ( args ) > number ret . extend ( others ) else ret . append ( others ) return ret	0
Q_597	append string format value to result	C_597_0	def format ( input , * * params ) param_format_string = string param_format_input = string param_result_field = string in_desc_field = string in_desc_type = string format_string = params . get ( param_format_string ) format_inputs = params . get ( param_format_input ) result_field = params . get ( param_result_field ) for var in input row_input = [ ] for desc in format_inputs row_input . append ( converter . convert ( var [ desc [ in_desc_field ] ] , desc [ in_desc_type ] ) ) var [ result_field ] = format_string . format ( * row_input ) return input	1
Q_597	append string format value to result	C_597_1	def reflection_matrix_pow ( reflection_matrix np . ndarray , exponent float ) squared_phase = np . dot ( reflection_matrix [ , number ] , reflection_matrix [ number , ] ) phase = complex ( np . sqrt ( squared_phase ) ) i = np . eye ( reflection_matrix . shape [ number ] ) * phase pos_part = ( i + reflection_matrix ) * number . number neg_part = ( i _ reflection_matrix ) * number . number pos_factor = phase * * ( exponent _ number ) neg_factor = pos_factor * complex ( _ number ) * * exponent pos_part_raised = pos_factor * pos_part neg_part_raised = neg_part * neg_factor return pos_part_raised + neg_part_raised	0
Q_597	append string format value to result	C_597_2	def wm ( self ) centroids = self . get_element_centroids ( ) wm = scipy . sparse . csr_matrix ( ( self . nr_of_elements , self . nr_of_elements ) ) for var , var in enumerate ( self . element_neighbors ) for j , edges in zip ( var , self . element_neighbors_edges [ var ] ) edge_coords = self . nodes [ string ] [ edges ] [ , number ] edge_length = np . linalg . norm ( edge_coords [ number , ] _ edge_coords [ number , ] ) distance = np . linalg . norm ( centroids [ var ] _ centroids [ j ] ) wm [ var , var ] + = edge_length / distance wm [ var , j ] _ = edge_length / distance return wm	0
Q_598	current interpolation range of refractive index	C_598_0	def range_n ( self ) return self . sphere_index _ self . dn , self . sphere_index + self . dn	1
Q_598	current interpolation range of refractive index	C_598_1	def pack_own ( self , serializer ) return zframe ( lib . zhashx_pack_own ( self . _as_parameter_ , serializer ) , true )	0
Q_598	current interpolation range of refractive index	C_598_2	def is_job_record_finalizable ( self , job_record ) assert isinstance ( job_record , job ) tree = self . get_tree ( job_record . process_name ) node = tree . get_node ( job_record . process_name , job_record . timeperiod ) return node . is_finalizable ( )	0
Q_599	import the astorb orbital element to database	C_599_0	def _import_astorb_to_database ( self , astorb_dict_list ) self . log . info ( string ) print string db_settings = self . settings [ string ] [ string ] insert_list_of_dictionaries_into_database_tables ( db_conn = self . atlas_movers_db_conn , log = self . log , dict_list = astorb_dict_list , db_table_name = string , unique_key_list = [ string ] , date_modified = true , batch_size = number , replace = true , db_settings = db_settings ) print string self . log . info ( string ) return none	1
Q_599	import the astorb orbital element to database	C_599_1	def ok ( self ) try v = int ( self . _value ) if v < self . imin or v > self . imax return false else return true except return false	0
Q_599	import the astorb orbital element to database	C_599_2	def is_valid ( self , model , validator = none ) if self . property_name and self . is_property_specific arg0 = getattr ( model , self . property_name ) else arg0 = model if self . is_simple is_valid = self . callback ( arg0 ) else is_valid = self . callback ( arg0 , validator ) return ( is_valid , none if is_valid else ( self . message or string ) )	0
Q_600	generate a list of value of the attribute of all ancestor of	C_600_0	def attribute_path ( self , node , attribute , missing = none , visitor = lambda node , attribute getattr ( node , attribute , none ) , ) return [ visitor ( var , attribute ) or missing for var in node . ancestors ( ) ] + [ visitor ( node , attribute ) or missing ]	1
Q_600	generate a list of value of the attribute of all ancestor of	C_600_1	def get_list_of_all_supported_commands ( config ) default_files = _recursive_get_all_file_names ( config . examples_dir ) custom_files = _recursive_get_all_file_names ( config . custom_dir ) default_files = [ var for var in default_files if _is_example_file ( var ) ] custom_files = [ var for var in custom_files if _is_example_file ( var ) ] def get_without_suffix ( file_name ) return file_name . split ( example_file_suffix ) [ number ] default_files = [ get_without_suffix ( f ) for f in default_files ] custom_files = [ get_without_suffix ( f ) for f in custom_files ] set_default_commands = set ( default_files ) set_custom_commands = set ( custom_files ) alias_dict = get_alias_dict ( config ) both_defined = set_default_commands set_custom_commands only_default = set_default_commands _ set_custom_commands only_custom = set_custom_commands _ set_default_commands all_commands = both_defined only_default only_custom command_to_rep = { } for command in all_commands rep = none if command in both_defined rep = command + string + flag_custom_and_default elif command in only_default rep = command elif command in only_custom rep = command + string + flag_only_custom else raise name_error ( string + str ( command ) ) command_to_rep [ command ] = rep result = [ ] all_commands_and_aliases = all_commands . union ( alias_dict . keys ( ) ) for command in all_commands_and_aliases if command in alias_dict target = alias_dict [ command ] rep_of_target = command_to_rep [ target ] result . append ( command + string + rep_of_target ) else rep = command_to_rep [ command ] result . append ( rep ) result . sort ( ) return result	0
Q_600	generate a list of value of the attribute of all ancestor of	C_600_2	def selection_error_control ( self , form_info ) keys , names = self . return_selected_form_items ( form_info [ string ] ) chosen_channels_number = len ( keys ) if form_info [ string ] and chosen_channels_number < number return false , _ ( u string ) elif form_info [ string ] and chosen_channels_number = = number return false , _ ( u string ) elif form_info [ string ] and chosen_channels_number = number return false , _ ( u string ) return true , none	0
Q_601	print all comic that have at least the give number of minimum comic strip	C_601_0	def print_results ( args ) min_comics , filename = args with codecs . open ( filename , string , string ) as fp for var , var in sorted ( load_result ( json_file ) . items ( ) ) if var in exclude_comics print ( string + var ) continue fp . write ( u string ( str ( truncate_name ( var ) ) , str ( var ) ) )	1
Q_601	print all comic that have at least the give number of minimum comic strip	C_601_1	def derivative ( self , z , x , y , fase ) return deriv_h ( self , z , x , y , fase )	0
Q_601	print all comic that have at least the give number of minimum comic strip	C_601_2	def str_wrap ( arr , width , * * kwargs ) r kwargs [ string ] = width tw = textwrap . text_wrapper ( * * kwargs ) return _na_map ( lambda s string . join ( tw . wrap ( s ) ) , arr )	0
Q_602	decorate a method require that the request ip address be	C_602_0	def whitelisted ( argument = none ) def is_whitelisted ( remote_ip , whitelist ) user_ip = ipaddr . i_pv4_address ( remote_ip ) if any ( [ user_ip in ipaddr . i_pv4_network ( var ) for var in whitelist ] ) return true return false if type ( argument ) is types . function_type def wrapper ( self , * args , * * kwargs ) if string not in self . application . settings raise value_error ( string ) if is_whitelisted ( self . request . remote_ip , self . application . settings [ string ] ) return argument ( self , * args , * * kwargs ) raise web . http_error ( number ) return wrapper else if isinstance ( argument , str ) argument = [ argument ] elif not isinstance ( argument , list ) raise value_error ( string string ) def argument_wrapper ( method ) def validate ( self , * args , * * kwargs ) if is_whitelisted ( self . request . remote_ip , argument ) return method ( self , * args , * * kwargs ) raise web . http_error ( number ) return validate return argument_wrapper	1
Q_602	decorate a method require that the request ip address be	C_602_1	def sign_tbs_cert ( self , tbs_cert , h = string ) sig_alg = tbs_cert . signature h = h or hash_by_oid [ sig_alg . algorithm . val ] sig_val = self . sign ( raw ( tbs_cert ) , h = h , t = string ) c = x509_cert ( ) c . tbs_certificate = tbs_cert c . signature_algorithm = sig_alg c . signature_value = _raw_asn1_bit_string ( sig_val , readable = true ) return c	0
Q_602	decorate a method require that the request ip address be	C_602_2	def zero_extend ( self , duration_s = none , num_samples = none ) if duration_s is not none and num_samples is not none raise value_error ( string ) elif duration_s is not none num_samples = self . frame_rate * duration_s seg = audio_segment ( self . seg , self . name ) zeros = silent ( duration = num_samples / self . frame_rate , frame_rate = self . frame_rate ) return zeros . overlay ( seg )	0
Q_603	clear all index tie to this field	C_603_0	def clear_indexes ( self , chunk_size = number , aggressive = false , index_class = none ) assert self . indexable , string assert self . attached_to_model , string for var in self . _indexes if index_class and not isinstance ( var , index_class ) continue var . clear ( chunk_size = chunk_size , aggressive = aggressive )	1
Q_603	clear all index tie to this field	C_603_1	def avail_approaches ( pkg ) methods = [ modname for importer , modname , ispkg in pkgutil . walk_packages ( path = pkg . _path_ ) if modname not in [ string , string , string ] ] return methods	0
Q_603	clear all index tie to this field	C_603_2	def lines_matching ( self , * regexes ) regex_c = re . compile ( join_regex ( regexes ) ) matches = set ( ) for var , var in enumerate ( self . lines ) if regex_c . search ( var ) matches . add ( var + number ) return matches	0
Q_604	give state dict and noise property estimate flux scale at the vla	C_604_0	def calcfluxscale ( d , imstd_med , flagfrac_med ) sensitivity = lambda sefd , dt , bw , eta , nbl , npol sefd / ( eta * np . sqrt ( nbl * number * dt * bw * npol ) ) nbl = lambda nant nant * ( nant _ number ) / number eta = { string number . number , string number . number , string number . number , string number . number } sefd = { string number , string number , string number , string number } bw = sum ( [ d [ string ] [ var ] * d [ string ] [ var ] for var in range ( len ( d [ string ] ) ) ] ) dt = d [ string ] npol = d [ string ] nant = d [ string ] freq = d [ string ] [ number ] if ( freq > = number and freq < number ) band = string elif ( freq > = number and freq < number ) band = string elif ( freq > = number and freq < number ) band = string elif ( freq > = number and freq < number ) band = string else logger . warn ( string . format ( freq ) ) band = string goodfrac = number _ flagfrac_med slim_theory = sensitivity ( sefd [ band ] , dt , bw , eta [ band ] , goodfrac * nbl ( nant ) , npol ) fluxscale = slim_theory / imstd_med return fluxscale	1
Q_604	give state dict and noise property estimate flux scale at the vla	C_604_1	def set_focus ( self , pos ) string logging . debug ( string , pos ) self . body . set_focus ( pos )	0
Q_604	give state dict and noise property estimate flux scale at the vla	C_604_2	def field ( self , name ) if self . fields . has_key ( name ) return self . fields [ name ] elif self . extends if not self . parent self . parent = self . contract . struct ( self . extends ) return self . parent . field ( name ) else return none	0
Q_605	auto generate code	C_605_0	def confd_state_internal_callpoints_notification_stream_replay_name ( self , * * kwargs ) config = et . element ( string ) confd_state = et . sub_element ( config , string , xmlns = string ) internal = et . sub_element ( confd_state , string ) callpoints = et . sub_element ( internal , string ) notification_stream_replay = et . sub_element ( callpoints , string ) name = et . sub_element ( notification_stream_replay , string ) name . text = kwargs . pop ( string ) callback = kwargs . pop ( string , self . _callback ) return callback ( config )	1
Q_605	auto generate code	C_605_1	def parse_skypipe_data_stream ( msg , for_pipe ) header = str ( msg . pop ( number ) ) command = str ( msg . pop ( number ) ) pipe_name = str ( msg . pop ( number ) ) data = str ( msg . pop ( number ) ) if header = sp_header return if pipe_name = for_pipe return if command = sp_cmd_data return if data = = sp_data_eof raise eof_error ( ) else return data	0
Q_605	auto generate code	C_605_2	def update ( self , * * kwargs ) updates = { } for var in string , string , string , string , string if var in kwargs updates [ var ] = kwargs [ var ] del kwargs [ var ] if self . _dxid is not none resp = dxpy . api . app_update ( self . _dxid , input_params = updates , * * kwargs ) else resp = dxpy . api . app_update ( string + self . _name , alias = self . _alias , input_params = updates , * * kwargs )	0
Q_606	auto generate code	C_606_0	def show_firmware_version_output_show_firmware_version_firmware_full_version ( self , * * kwargs ) config = et . element ( string ) show_firmware_version = et . element ( string ) config = show_firmware_version output = et . sub_element ( show_firmware_version , string ) show_firmware_version = et . sub_element ( output , string ) firmware_full_version = et . sub_element ( show_firmware_version , string ) firmware_full_version . text = kwargs . pop ( string ) callback = kwargs . pop ( string , self . _callback ) return callback ( config )	1
Q_606	auto generate code	C_606_1	def find_helping_materials ( project_id , * * kwargs ) try kwargs [ string ] = project_id res = _pybossa_req ( string , string , params = kwargs ) if type ( res ) . _name_ = = string return [ helping_material ( var ) for var in res ] else return res except raise	0
Q_606	auto generate code	C_606_2	def get_sql ( self ) alias = self . get_alias ( ) if alias if self . cast return string { number } string . format ( self . get_select_sql ( ) , self . cast . upper ( ) , alias ) return string { number } string . format ( self . get_select_sql ( ) , alias ) if self . cast return string . format ( self . get_identifier ( ) , self . cast . upper ( ) ) return self . get_identifier ( )	0
Q_607	auto generate code	C_607_0	def show_system_monitor_output_switch_status_rbridge_id_out ( self , * * kwargs ) config = et . element ( string ) show_system_monitor = et . element ( string ) config = show_system_monitor output = et . sub_element ( show_system_monitor , string ) switch_status = et . sub_element ( output , string ) rbridge_id_out = et . sub_element ( switch_status , string ) rbridge_id_out . text = kwargs . pop ( string ) callback = kwargs . pop ( string , self . _callback ) return callback ( config )	1
Q_607	auto generate code	C_607_1	def components_to_path ( components ) precondition . assert_iterable_type ( components , text ) for var in components if not var raise value_error ( string . format ( components ) ) if string in var raise value_error ( string / string . format ( components ) ) if components return string + string . join ( components ) else return string	0
Q_607	auto generate code	C_607_2	def fetch_urls ( cls , url , data , url_search ) search_urls = [ ] if cls . css search_fun = data . cssselect else search_fun = data . xpath searches = make_sequence ( url_search ) for var in searches for match in search_fun ( var ) try for attrib in html_link_attrs if attrib in match . attrib search_urls . append ( match . get ( attrib ) ) except attribute_error search_urls . append ( str ( match ) ) if not cls . multiple_images_per_strip and search_urls break if not search_urls raise value_error ( string ( searches , url ) ) return search_urls	0
Q_608	auto generate code	C_608_0	def get_vnetwork_dvpgs_input_vcenter ( self , * * kwargs ) config = et . element ( string ) get_vnetwork_dvpgs = et . element ( string ) config = get_vnetwork_dvpgs input = et . sub_element ( get_vnetwork_dvpgs , string ) vcenter = et . sub_element ( input , string ) vcenter . text = kwargs . pop ( string ) callback = kwargs . pop ( string , self . _callback ) return callback ( config )	1
Q_608	auto generate code	C_608_1	def time_in_range ( self ) curr = datetime . datetime . now ( ) . time ( ) if self . start_time < = self . end_time return self . start_time < = curr < = self . end_time else return self . start_time < = curr or curr < = self . end_time	0
Q_608	auto generate code	C_608_2	def update_pop ( self ) candidates = [ ] for var in self . population candidates . append ( self . crossover ( var ) ) self . _params [ string ] + = len ( candidates ) self . assign_fitnesses ( candidates ) for i in range ( len ( self . population ) ) if candidates [ i ] . fitness > self . population [ i ] . fitness self . population [ i ] = candidates [ i ]	0
Q_609	auto generate code	C_609_0	def get_vnetwork_dvpgs_output_vnetwork_dvpgs_datacenter ( self , * * kwargs ) config = et . element ( string ) get_vnetwork_dvpgs = et . element ( string ) config = get_vnetwork_dvpgs output = et . sub_element ( get_vnetwork_dvpgs , string ) vnetwork_dvpgs = et . sub_element ( output , string ) datacenter = et . sub_element ( vnetwork_dvpgs , string ) datacenter . text = kwargs . pop ( string ) callback = kwargs . pop ( string , self . _callback ) return callback ( config )	1
Q_609	auto generate code	C_609_1	def run ( self ) cmds = ( self . clean_docs_cmd , self . html_docs_cmd , self . view_docs_cmd ) self . call_in_sequence ( cmds )	0
Q_609	auto generate code	C_609_2	def cpu_times ( self ) chrome_procs = self . get_chrome_procs ( ) new_pids = { var . pid for var in chrome_procs } old_pids = { pid for pid in self . last_cpu_times } try cpu_times = { var . pid var . cpu_times ( ) for var in chrome_procs } except psutil . no_such_process return { } , true if new_pids = old_pids self . last_cpu_times = cpu_times return { } , true ordered_old_times = ( self . last_cpu_times [ var . pid ] for var in chrome_procs ) ordered_new_times = ( cpu_times [ var . pid ] for var in chrome_procs ) cpu_times_diff = { var . pid { string ( t [ number ] _ l [ number ] ) / self . interval , string ( t [ number ] _ l [ number ] ) / self . interval } for ( var , t , l ) in zip ( chrome_procs , ordered_new_times , ordered_old_times ) } self . last_cpu_times = cpu_times return cpu_times_diff , false	0
Q_610	create table accord sqlalchemy data model	C_610_0	def command_create_tables ( self , meta_name = none , verbose = false ) def _create_metadata_tables ( metadata ) for var in metadata . sorted_tables if verbose print ( self . _schema ( var ) ) else print ( string + var . name ) engine = self . session . get_bind ( clause = var ) metadata . create_all ( bind = engine , tables = [ var ] ) if isinstance ( self . metadata , meta_data ) print ( string ) _create_metadata_tables ( self . metadata ) else for current_meta_name , metadata in self . metadata . items ( ) if meta_name not in ( current_meta_name , none ) continue print ( string . format ( current_meta_name ) ) _create_metadata_tables ( metadata )	1
Q_610	create table accord sqlalchemy data model	C_610_1	def shuffle_sattolo ( items ) _randrange = random . randrange for var in reversed ( range ( number , len ( items ) ) ) j = _randrange ( var ) items [ j ] , items [ var ] = items [ var ] , items [ j ]	0
Q_610	create table accord sqlalchemy data model	C_610_2	def open ( self ) self . running = true if self . redirected self . target = self . redirected . address self . _handler = send_client ( self . target , auth = self . client . get_auth ( ) , debug = self . client . debug , msg_timeout = self . timeout , error_policy = self . retry_policy , keep_alive_interval = self . keep_alive , client_name = self . name , properties = self . client . create_properties ( ) ) self . _handler . open ( ) while not self . _handler . client_ready ( ) time . sleep ( number . number number )	0
Q_611	set up plot for introduction	C_611_0	def fig_intro ( params , ana_params , t = [ number , number ] , fraction = number . number number , rasterized = false ) ana_params . set_plos_2column_fig_style ( ratio = number . number ) network_sim = cached_network ( * * params . network_sim_params ) if analysis_params . bw network_sim . colors = phlp . get_colors ( len ( network_sim . x ) ) fig = plt . figure ( ) gs = gridspec . grid_spec ( number , number ) fig . subplots_adjust ( left = number . number number , right = number . number , wspace = number . number , hspace = number . ) ax0_1 = fig . add_subplot ( gs [ , number ] , frameon = false ) ax0_1 . set_title ( string , va = string ) network_sketch ( ax0_1 , yscaling = number . number ) ax0_1 . xaxis . set_ticks ( [ ] ) ax0_1 . yaxis . set_ticks ( [ ] ) phlp . annotate_subplot ( ax0_1 , ncols = number , nrows = number , letter = string , linear_offset = number . number number ) ax1 = fig . add_subplot ( gs [ , number ] , frameon = true ) phlp . remove_axis_junk ( ax1 ) phlp . annotate_subplot ( ax1 , ncols = number , nrows = number , letter = string , linear_offset = number . number number ) x , y = network_sim . get_xy ( t , fraction = fraction ) network_sim . plot_raster ( ax1 , t , x , y , markersize = number . number , marker = string , alpha = number . , legend = false , pop_names = true , rasterized = rasterized ) ax1 . set_ylabel ( string ) ax1 . xaxis . set_major_locator ( plt . max_n_locator ( number ) ) ax1 . set_title ( string , va = string ) a = ax1 . axis ( ) ax1 . vlines ( x [ string ] [ number ] , a [ number ] , a [ number ] , string , lw = number . number ) ax2 = fig . add_subplot ( gs [ , number ] , frameon = false ) ax2 . xaxis . set_ticks ( [ ] ) ax2 . yaxis . set_ticks ( [ ] ) plot_population ( ax2 , params , isometricangle = np . pi / number , plot_somas = false , plot_morphos = true , num_units_e = number , num_units_i = number , clip_dendrites = true , main_pops = true , title = string , rasterized = rasterized ) ax2 . set_title ( string , va = string , fontweight = string ) phlp . annotate_subplot ( ax2 , ncols = number , nrows = number , letter = string , linear_offset = number . number number ) ax3 = fig . add_subplot ( gs [ , number ] , frameon = true ) phlp . remove_axis_junk ( ax3 ) plot_signal_sum ( ax3 , params , fname = os . path . join ( params . savefolder , string ) , unit = string , vlimround = number . number , t = t , ylim = [ ax2 . axis ( ) [ number ] , ax2 . axis ( ) [ number ] ] , rasterized = false ) ax3 . set_title ( string , va = string ) ax3 . xaxis . set_major_locator ( plt . max_n_locator ( number ) ) phlp . annotate_subplot ( ax3 , ncols = number , nrows = number , letter = string , linear_offset = number . number number ) a = ax3 . axis ( ) ax3 . vlines ( x [ string ] [ number ] , a [ number ] , a [ number ] , string , lw = number . number ) ax = plt . gca ( ) ax . annotate ( string , xy = ( number . number , number . number ) , xytext = ( . number , number . number ) , xycoords = string , arrowprops = dict ( facecolor = string , arrowstyle = string ) , ) ax . annotate ( string , xy = ( number . number , number . number ) , xytext = ( . number , number . number ) , xycoords = string , arrowprops = dict ( facecolor = string , arrowstyle = string ) , ) ax . annotate ( string , xy = ( number . number , number . number ) , xytext = ( . number , number . number ) , xycoords = string , arrowprops = dict ( facecolor = string , arrowstyle = string ) , ) return fig	1
Q_611	set up plot for introduction	C_611_1	def fcoe_fcoe_map_fcoe_map_name ( self , * * kwargs ) config = et . element ( string ) fcoe = et . sub_element ( config , string , xmlns = string ) fcoe_map = et . sub_element ( fcoe , string ) fcoe_map_name = et . sub_element ( fcoe_map , string ) fcoe_map_name . text = kwargs . pop ( string ) callback = kwargs . pop ( string , self . _callback ) return callback ( config )	0
Q_611	set up plot for introduction	C_611_2	def union ( self , * others ) return self . copy ( super ( n_gram , self ) . union ( * others ) )	0
Q_612	start the calculation	C_612_0	def calculate ( self , operation = none , trace = none , constant = none , type = none ) if operation is not none self . math_operation = operation if trace is not none self . math_trace_argument = trace type = string elif constant is not none self . math_constant = constant type = string if type is not none self . math_argument_type = type self . _write ( string )	1
Q_612	start the calculation	C_612_1	def data_size ( self ) if is_container ( self . _data ) byte_length , bit_length = self . _data . container_size ( ) return byte_length + math . ceil ( bit_length / number ) elif is_field ( self . _data ) return math . ceil ( self . _data . bit_size / number ) else return number	0
Q_612	start the calculation	C_612_2	def _unparse_change_record ( self , modlist ) mod_len = len ( modlist [ number ] ) self . _unparse_changetype ( mod_len ) for var in modlist if len ( var ) = mod_len raise value_error ( string ) if mod_len = = number mod_type , mod_vals = var elif mod_len = = number mod_op , mod_type , mod_vals = var self . _unparse_attr ( mod_ops [ mod_op ] , mod_type ) for mod_val in mod_vals self . _unparse_attr ( mod_type , mod_val ) if mod_len = = number self . _output_file . write ( b string + self . _line_sep )	0
Q_613	obtain poi type	C_613_0	def get_poi_types ( self , * * kwargs ) params = { string util . language_code ( kwargs . get ( string ) ) } result = self . make_request ( string , string , * * params ) values = result . get ( string , [ ] ) return true , [ emtype . poi_type ( * * var ) for var in values ]	1
Q_613	obtain poi type	C_613_1	def set_unit_spike_features ( self , unit_id , feature_name , value ) if isinstance ( unit_id , ( int , np . integer ) ) if unit_id in self . get_unit_ids ( ) if unit_id not in self . _unit_features . keys ( ) self . _unit_features [ unit_id ] = { } if isinstance ( feature_name , str ) and len ( value ) = = len ( self . get_unit_spike_train ( unit_id ) ) self . _unit_features [ unit_id ] [ feature_name ] = np . asarray ( value ) else if not isinstance ( feature_name , str ) raise value_error ( string ) else raise value_error ( string ) else raise value_error ( str ( unit_id ) + string ) else raise value_error ( str ( unit_id ) + string )	0
Q_613	obtain poi type	C_613_2	def point_is_between ( start_p , end_p , p ) return ( distance_point ( start_p , p ) + distance_point ( p , end_p ) _ distance_point ( start_p , end_p ) < number e _ number )	0
Q_614	input local repo path	C_614_0	def get_template_filelist ( repo_path , ignore_files = [ ] , ignore_folders = [ ] ) default_ignore_files = [ string ] default_ignore_folders = [ string ] ignore_files + = default_ignore_files ignore_folders + = default_ignore_folders filelist = [ ] for root , folders , files in os . walk ( repo_path ) for var in ignore_files if var in files files . remove ( var ) for ignore_folder in ignore_folders if ignore_folder in folders folders . remove ( ignore_folder ) for file_name in files filelist . append ( string ( root , file_name ) ) return filelist	1
Q_614	input local repo path	C_614_1	def unique_scene_labels ( scene_list ) if isinstance ( scene_list , dcase_util . containers . meta_data_container ) return scene_list . unique_scene_labels else labels = [ ] for var in scene_list if string in var and var [ string ] not in labels labels . append ( var [ string ] ) labels . sort ( ) return labels	0
Q_614	input local repo path	C_614_2	def get_initial_data ( self , request , user , profile , client ) if inital_data_function func = self . import_attribute ( inital_data_function ) return func ( request , user , profile , client ) return { }	0
Q_615	export the feature from this namespace as a list of feature token	C_615_0	def export_features ( self , delimiter = string ) result_list = [ ] for var in self . features result = string . format ( self . name , delimiter , var ) result_list . append ( result ) return result_list	1
Q_615	export the feature from this namespace as a list of feature token	C_615_1	def training_data_dedupe ( data , common_key , training_size = number ) identified_records = collections . defaultdict ( list ) matched_pairs = set ( ) distinct_pairs = set ( ) unique_record_ids = set ( ) for var , var in data . items ( ) unique_record_ids . add ( var ) identified_records [ var [ common_key ] ] . append ( var ) for record_ids in identified_records . values ( ) if len ( record_ids ) > number matched_pairs . update ( itertools . combinations ( sorted ( record_ids ) , number ) ) unique_record_ids = list ( unique_record_ids ) pair_indices = random_pairs ( len ( unique_record_ids ) , training_size ) distinct_pairs = set ( ) for i , j in pair_indices distinct_pairs . add ( ( unique_record_ids [ i ] , unique_record_ids [ j ] ) ) distinct_pairs _ = matched_pairs matched_records = [ ( data [ key_1 ] , data [ key_2 ] ) for key_1 , key_2 in matched_pairs ] distinct_records = [ ( data [ key_1 ] , data [ key_2 ] ) for key_1 , key_2 in distinct_pairs ] training_pairs = { string matched_records , string distinct_records } return training_pairs	0
Q_615	export the feature from this namespace as a list of feature token	C_615_2	def _pick_colours ( self , palette_name , selected = false ) return self . _frame . palette [ self . _pick_palette_key ( palette_name , selected ) ]	0
Q_616	enqueue the download of the give foreign resource	C_616_0	async def async_enqueue_download ( self , resource ) worker = self . pick_sticky ( resource . url_string ) await worker . enqueue ( enums . task . download , ( resource , ) )	1
Q_616	enqueue the download of the give foreign resource	C_616_1	def image_by_name ( img_name , images = none ) i_reg , i_rep , i_tag = _decompose ( img_name ) if not i_reg i_reg = string if not i_tag i_tag = string if images is none c = docker . client ( * * kwargs_from_env ( ) ) images = c . images ( all = false ) valid_images = [ ] for var in images for t in var [ string ] reg , rep , tag = _decompose ( t ) if matches ( reg , i_reg ) and matches ( rep , i_rep ) and matches ( tag , i_tag ) valid_images . append ( var ) break if rep . endswith ( img_name ) valid_images . append ( var ) break return valid_images	0
Q_616	enqueue the download of the give foreign resource	C_616_2	def render ( self , mark_lib = true ) return frame_html { string self . id , string escape ( self . filename ) , string self . lineno , string escape ( self . function_name ) , string self . render_line_context ( ) , string string if mark_lib and self . is_library else string , }	0
Q_617	callback url for nexmo	C_617_0	def callback ( request ) message_id = request . get . get ( string ) status_id = request . get . get ( string ) status_msg = nexmo_statuses . get ( status_id , unknown_status ) error_id = int ( request . get . get ( string ) ) error_msg = nexmo_messages . get ( error_id , unknown_message ) logger . info ( u string ( message_id , status_msg , error_msg ) ) return http_response ( string )	1
Q_617	callback url for nexmo	C_617_1	def get_platform ( ) _sys_map = { string string , string string , string string , string string , string string } non_x86_linux_machines = { string , string , string } sys_platform = _sys_map . get ( sys . platform , string ) bits = number * tuple . _itemsize_ if ( sys_platform = = string and platform . machine ( ) in non_x86_linux_machines ) arch_name = platform . machine ( ) subdir = string . format ( arch_name ) else arch_name = { number string , number string } [ bits ] subdir = string . format ( sys_platform , bits ) return subdir	0
Q_617	callback url for nexmo	C_617_2	def install_bash_completion ( self , script_name = none , dest = string ) if string in os . environ and string not in os . environ os . environ [ string ] = os . environ [ string ] dest = expanduser ( dest ) if script_name is none script_name = sys . argv [ number ] self . uninstall_bash_completion ( script_name = script_name , dest = dest ) with open ( dest , string ) as f f . write ( string ( register _ python _ argcomplete s ) string script_name )	0
Q_618	return a string representation of a segmentlistdict object	C_618_0	def segmentlistdict_to_short_string ( seglists ) return string . join ( [ string ( str ( var ) , string . join ( to_range_strings ( var ) ) ) for var , var in seglists . items ( ) ] )	1
Q_618	return a string representation of a segmentlistdict object	C_618_1	def register_java_function ( self , name , java_class_name , return_type = none ) jdt = none if return_type is not none if not isinstance ( return_type , data_type ) return_type = _parse_datatype_string ( return_type ) jdt = self . spark_session . _jspark_session . parse_data_type ( return_type . json ( ) ) self . spark_session . _jspark_session . udf ( ) . register_java ( name , java_class_name , jdt )	0
Q_618	return a string representation of a segmentlistdict object	C_618_2	def find_cells ( self , column_family_id , column ) try column_family = self . _cells [ column_family_id ] except key_error raise key_error ( _missing_column_family . format ( column_family_id ) ) try cells = column_family [ column ] except key_error raise key_error ( _missing_column . format ( column , column_family_id ) ) return cells	0
Q_619	handle the tagstr command	C_619_0	def edit ( self , * * args ) if self . model . exists ( args [ string ] ) values = dict ( zip ( [ string , string , string , string , string ] , self . model . get ( args [ string ] ) ) ) kwargs = self . get_kwargs ( args , values ) if kwargs self . model . edit ( args [ string ] , * * kwargs )	1
Q_619	handle the tagstr command	C_619_1	def child_removed ( self , child ) super ( abstract_item_view , self ) . child_removed ( child ) self . get_member ( string ) . reset ( self )	0
Q_619	handle the tagstr command	C_619_2	def keep_scan ( cls , result_key , token ) def _scan ( self ) return self . get ( token ) cls . scan ( result_key , _scan )	0
Q_620	reload give command	C_620_0	def reload ( * command , ignore_patterns = [ ] ) path = string sig = signal . sigterm delay = number . number ignorefile = string ignore_patterns = ignore_patterns or load_ignore_patterns ( ignorefile ) event_handler = reload_event_handler ( ignore_patterns ) reloader = reloader ( command , signal ) observer = observer ( ) observer . schedule ( event_handler , path , recursive = true ) observer . start ( ) reloader . start_command ( ) try while true time . sleep ( delay ) sys . stdout . write ( reloader . read ( ) ) sys . stdout . flush ( ) if event_handler . modified reloader . restart_command ( ) except keyboard_interrupt observer . stop ( ) observer . join ( ) reloader . stop_command ( ) sys . stdout . write ( reloader . read ( ) ) sys . stdout . flush ( )	1
Q_620	reload give command	C_620_1	def register_backend ( name , backend , allow_overwrite = false ) if hasattr ( circuit , string + name ) if allow_overwrite warnings . warn ( f string ) else raise value_error ( f string ) if not allow_overwrite if name in backends raise value_error ( f string { name } string ) backends [ name ] = backend	0
Q_620	reload give command	C_620_2	def approximate_inverse ( distribution , qloc , parameters = none , cache = none , iterations = number , tol = number e _ number , seed = none , ) logger = logging . get_logger ( _name_ ) logger . debug ( string , distribution ) xloc , xlower , xupper = find_interior_point ( distribution , cache = cache , parameters = parameters , retall = true , seed = seed ) xloc = ( xloc . t * numpy . zeros ( qloc . shape ) . t ) . t xlower = ( xlower . t + numpy . zeros ( qloc . shape ) . t ) . t xupper = ( xupper . t + numpy . zeros ( qloc . shape ) . t ) . t uloc = numpy . zeros ( qloc . shape ) ulower = _ qloc uupper = number _ qloc indices = numpy . ones ( qloc . shape [ _ number ] , dtype = bool ) for var in range ( number * iterations ) uloc [ , indices ] = ( evaluation . evaluate_forward ( distribution , xloc , cache = cache , parameters = parameters ) _ qloc ) [ , indices ] indices [ indices ] = numpy . any ( numpy . abs ( xupper _ xlower ) > tol , number ) [ indices ] logger . debug ( string , numpy . mean ( xlower , _ number ) , numpy . mean ( xloc , _ number ) , numpy . mean ( xupper , _ number ) , numpy . mean ( indices ) , ) if not numpy . any ( indices ) break ulower [ , indices ] = numpy . where ( uloc < = number , uloc , ulower ) [ , indices ] xlower [ , indices ] = numpy . where ( uloc < = number , xloc , xlower ) [ , indices ] uupper [ , indices ] = numpy . where ( uloc > = number , uloc , uupper ) [ , indices ] xupper [ , indices ] = numpy . where ( uloc > = number , xloc , xupper ) [ , indices ] xloc_ = numpy . inf if var number = = number derivative = evaluation . evaluate_density ( distribution , xloc , cache = cache , parameters = parameters ) [ , indices ] derivative = numpy . where ( derivative , derivative , numpy . inf ) xloc_ = xloc [ , indices ] _ uloc [ , indices ] / derivative xloc [ , indices ] = numpy . where ( ( xloc_ < xupper [ , indices ] ) ( xloc_ > xlower [ , indices ] ) , xloc_ , number . number * ( xupper + xlower ) [ , indices ] ) else logger . warning ( string ) logger . info ( string . format ( numpy . sum ( indices ) , len ( indices ) ) ) logger . debug ( string , distribution ) return xloc	0
Q_621	load the config and find a match rule	C_621_0	def find_match ( self ) self . load ( ) for var in self . yamldocs self . logdebug ( string string s string string var ) if not var continue if not self . check_client_ip ( var ) continue if not self . check_keyname ( var ) continue rules = var . get ( string ) if not isinstance ( rules , list ) rules = [ rules ] for rule in rules rule_type = rule . get ( string , string ) if rule_type = = string sub = self . find_match_command elif rule_type = = string sub = self . find_match_scp else self . log ( string s string rule_type ) self . raise_and_log_error ( config_error , string ) match = sub ( rule ) if match return match raise command_rejected ( string s string self . original_command_string )	1
Q_621	load the config and find a match rule	C_621_1	def r_cts ( self ) _request = request . args . get ( string , none ) if _request is not none try if _request . lower ( ) = = string return self . _get_capabilities ( urn = request . args . get ( string , none ) ) elif _request . lower ( ) = = string return self . _get_passage ( urn = request . args . get ( string , none ) ) elif _request . lower ( ) = = string return self . _get_passage_plus ( urn = request . args . get ( string , none ) ) elif _request . lower ( ) = = string return self . _get_label ( urn = request . args . get ( string , none ) ) elif _request . lower ( ) = = string return self . _get_first_urn ( urn = request . args . get ( string , none ) ) elif _request . lower ( ) = = string return self . _get_prev_next ( urn = request . args . get ( string , none ) ) elif _request . lower ( ) = = string return self . _get_valid_reff ( urn = request . args . get ( string , none ) , level = request . args . get ( string , number , type = int ) ) except nautilus_error as e return self . cts_error ( error_name = e . _class_ . _name_ , message = e . _doc_ ) return self . cts_error ( missing_parameter . _name_ , message = missing_parameter . _doc_ )	0
Q_621	load the config and find a match rule	C_621_2	def _write_color_ansi ( fp , text , color ) fp . write ( esc_ansicolor ( color ) ) fp . write ( text ) fp . write ( ansi_reset )	0
Q_622	import o_auth tagint	C_622_0	def import_oauth2_credentials ( filename = storage_filename ) with open ( filename , string ) as storage_file storage = safe_load ( storage_file ) client_secret = storage . get ( string ) refresh_token = storage . get ( string ) credentials = { string storage [ string ] , string storage [ string ] , string client_secret , string storage [ string ] , string storage [ string ] , string refresh_token , string storage [ string ] , } return credentials	1
Q_622	import o_auth tagint	C_622_1	def start_evaluating ( self , n node , s sh_ex_j . shape_expr ) _ > optional [ bool ] if not s . id s . id = str ( b_node ( ) ) key = ( n , s . id ) if key in self . known_results return self . known_results [ key ] if key not in self . evaluating self . evaluating . add ( key ) return none elif key not in self . assumptions self . assumptions [ key ] = true return self . assumptions [ key ]	0
Q_622	import o_auth tagint	C_622_2	def as_list ( self ) string string string out = [ ] for var in self . _toklist if isinstance ( var , parse_results ) out . append ( var . as_list ( ) ) else out . append ( var ) return out	0
Q_623	load the database from a give file path in the class	C_623_0	def _load_database ( self , database_location , stream = false ) self . systems = [ ] self . binaries = [ ] self . stars = [ ] self . planets = [ ] if stream tree = et . parse ( database_location ) for var in tree . findall ( string ) self . _load_system ( var ) else database_xml = glob . glob ( os . path . join ( database_location , string ) ) if not len ( database_xml ) raise load_data_base_error ( string string ) for filename in database_xml try with open ( filename , string ) as f tree = et . parse ( f ) except et . parse_error as e raise load_data_base_error ( e ) root = tree . getroot ( ) if not root . tag = = string raise load_data_base_error ( string string . format ( filename ) ) self . _load_system ( root )	1
Q_623	load the database from a give file path in the class	C_623_1	def _wrap ( f ) def wrapper ( * args , * * kwargs ) warnings . warn ( str ( f ) + string string string ) return f ( * args , * * kwargs ) return wrapper	0
Q_623	load the database from a give file path in the class	C_623_2	def package_filter ( config , message , package = none , * args , * * kw ) package = kw . get ( string , package ) if package return package in fmn . rules . utils . msg2packages ( message , * * config )	0
Q_624	stop an order _ turn off the serve generation ability of an order	C_624_0	def order_stop ( backend , order_id ) if order_id is none raise click . click_exception ( string order_id ) click . secho ( string ( get_datetime ( ) , order_id ) , fg = string ) check_and_print ( dk_cloud_command_runner . stop_order ( backend . dki , order_id ) )	1
Q_624	stop an order _ turn off the serve generation ability of an order	C_624_1	def get_accelerometer_raw ( self ) raw = self . _get_raw_data ( string , string ) if raw is not none self . _last_accel_raw = raw return deepcopy ( self . _last_accel_raw )	0
Q_624	stop an order _ turn off the serve generation ability of an order	C_624_2	def get_messages ( self , formatter_mediator , event ) regvalue = getattr ( event , string , { } ) for var in regvalue . keys ( ) service_enums = human_readable_service_enums . service_enums if var in service_enums . keys ( ) service_enum = service_enums [ var ] human_readable_value = service_enum . get ( regvalue [ var ] , regvalue [ var ] ) regvalue [ var ] = human_readable_value return super ( win_registry_service_formatter , self ) . get_messages ( formatter_mediator , event )	0
Q_625	create a web hook	C_625_0	def create ( cls , session , web_hook ) cls ( string , data = web_hook . to_api ( ) , request_type = request_paginator . post , session = session , ) return true	1
Q_625	create a web hook	C_625_1	def change_operator_control_send ( self , target_system , control_request , version , passkey , force_mavlink1 = false ) return self . send ( self . change_operator_control_encode ( target_system , control_request , version , passkey ) , force_mavlink1 = force_mavlink1 )	0
Q_625	create a web hook	C_625_2	def endswith_strip ( s , endswith = string , ignorecase = true ) if ignorecase if s . lower ( ) . endswith ( endswith . lower ( ) ) return s [ _ len ( endswith ) ] else if s . endswith ( endswith ) return s [ _ len ( endswith ) ] return s	0
Q_626	this decorator set this view to have restrict permission	C_626_0	def dispatch ( self , * args , * * kwargs ) return super ( animal_list , self ) . dispatch ( * args , * * kwargs )	1
Q_626	this decorator set this view to have restrict permission	C_626_1	async def set ( self , key , value ) if not check_dht_value_type ( value ) raise type_error ( string ) log . info ( string s string s string , key , value ) dkey = digest ( key ) return await self . set_digest ( dkey , value )	0
Q_626	this decorator set this view to have restrict permission	C_626_2	def _unique_constraint_name ( table str , field , keys ) postfix = string . join ( keys ) return string . format ( table = table , field = field . column , postfix = postfix )	0
Q_627	compute a similarity score for two document	C_627_0	def similarity ( self , d , d_ ) es = set ( [ var . name for var in d . entities ] ) es_ = set ( [ var . name for var in d_ . entities ] ) e_weight = ( len ( es ) + len ( es_ ) _ abs ( len ( es ) _ len ( es_ ) ) ) / number e_score = sum ( self . idf_entity [ t ] for t in es es_ ) toks = set ( d . tokens ) toks_ = set ( d_ . tokens ) t_weight = ( len ( toks ) + len ( toks_ ) _ abs ( len ( toks ) _ len ( toks_ ) ) ) / number shared_toks = toks toks_ overlap = [ ( t , t , self . idf [ t ] ) for t in shared_toks ] t_score = sum ( self . idf [ t ] for t in shared_toks ) if self . term_sim_ref is not none t_score * = number for toks1 , toks2 in [ ( toks , toks_ ) , ( toks_ , toks ) ] for t in toks1 _ shared_toks best_match = max ( toks2 , key = lambda t_ self . term_sim_ref [ t , t_ ] ) sim = self . term_sim_ref [ t , best_match ] t_score + = sim * ( ( self . idf [ t ] + self . idf [ best_match ] ) / number ) if sim > number overlap . append ( ( t , best_match , sim * ( ( self . idf [ t ] + self . idf [ best_match ] ) / number ) ) ) t_weight = number / t_weight if t_weight = number else number e_weight = number / e_weight if e_weight = number else number t_score * = t_weight e_score * = e_weight if self . debug print ( string ) print ( ( d . id , d_ . id ) ) print ( string , d . id ) print ( string , d_ . id ) print ( string ) print ( string , es ) print ( string , es_ ) print ( string , es es_ ) print ( string , e_weight ) print ( string , e_score ) print ( string ) print ( string , toks ) print ( string , toks_ ) print ( string , overlap ) print ( string , t_weight ) print ( string , t_score ) print ( string , t_score + e_score ) return t_score + e_score	1
Q_627	compute a similarity score for two document	C_627_1	def get_dbg_brk_linux64 ( ) debugger = get_debugger ( ) code = b string rax = debugger . get_reg ( string ) rdi = debugger . get_reg ( string ) rip = debugger . get_reg ( string ) efl = debugger . get_reg ( string ) debugger . set_reg ( string , number ) debugger . set_reg ( string , number ) base = debugger . image_base ( ) inj = base save = debugger . get_bytes ( inj , len ( code ) ) debugger . put_bytes ( inj , code ) debugger . set_reg ( string , inj ) debugger . step_into ( ) debugger . wait_ready ( ) brk_res = debugger . get_reg ( string ) debugger . set_reg ( string , rax ) debugger . set_reg ( string , rdi ) debugger . set_reg ( string , rip ) debugger . set_reg ( string , efl ) debugger . put_bytes ( inj , save ) return brk_res	0
Q_627	compute a similarity score for two document	C_627_2	def post_predictions ( self , document_id str , model_name str ) _ > dict body = json . dumps ( { string document_id , string model_name } ) . encode ( ) uri , headers = self . _create_signing_headers ( string , string , body ) post_predictions_response = requests . post ( url = uri . geturl ( ) , headers = headers , data = body ) response = _json_decode ( post_predictions_response ) return response	0
Q_628	parse a slot into a time_interval object	C_628_0	def parse_time_interval ( slot ) start = intent_parser . get_dict_value ( slot , [ string , string ] ) end = intent_parser . get_dict_value ( slot , [ string , string ] ) if not start or not end return none start = parse ( start ) end = parse ( end ) if not start or not end return none return time_interval ( start , end )	1
Q_628	parse a slot into a time_interval object	C_628_1	def intersect ( self , other iterable [ flag ] ) _ > frozen_set [ flag ] if wildcard in self . _defined return frozenset ( other ) else return self . _defined frozenset ( other )	0
Q_628	parse a slot into a time_interval object	C_628_2	def read ( self , source_path ) with pelican_open ( source_path ) as text parts = text . split ( string , number ) if len ( parts ) = = number headerlines = parts [ number ] . splitlines ( ) headerpairs = map ( lambda l l . split ( string , number ) , headerlines ) headerdict = { var [ number ] var [ number ] . strip ( ) for var in headerpairs if len ( var ) = = number } metadata = self . _parse_metadata ( headerdict ) content = textile ( parts [ number ] ) else metadata = { } content = textile ( text ) return content , metadata	0
Q_629	get all token which have expire	C_629_0	def get_expired ( self ) max_age = timezone . now ( ) _ timedelta ( seconds = app_settings . esi_token_valid_duration ) return self . filter ( created_lte = max_age )	1
Q_629	get all token which have expire	C_629_1	def deletion ( args ) import math from jcvi . formats . bed import bed from jcvi . graphics . chromosome import horizontal_chromosome from jcvi . graphics . base import kb_formatter p = option_parser ( deletion . _doc_ ) opts , args , iopts = p . set_image_options ( args ) if len ( args ) = number sys . exit ( not p . print_help ( ) ) deletion_genes , deletions , bed = args dg = [ int ( var ) for var in open ( deletion_genes ) ] dsg , lsg = string , string fig = plt . figure ( number , ( iopts . w , iopts . h ) ) root = fig . add_axes ( [ number , number , number , number ] ) ax = fig . add_axes ( [ . number , . number , . number , . number ] ) minval = number if deletion_genes = = string else number bins = np . logspace ( math . log ( minval , number ) , math . log ( max ( dg ) , number ) , number ) n , bins , histpatches = ax . hist ( dg , bins = bins , fc = lsg , alpha = . number ) ax . set_xscale ( string , basex = number ) if deletion_genes = = string ax . xaxis . set_major_formatter ( mpl . ticker . format_str_formatter ( string ) ) ax . set_xlabel ( string ) else ax . xaxis . set_major_formatter ( kb_formatter ) ax . set_xlabel ( string ) ax . yaxis . set_major_formatter ( mpl . ticker . format_str_formatter ( string ) ) ax . set_ylabel ( string ) ax . patch . set_alpha ( number . number ) na , nb = . number , . number root . text ( ( na + nb ) / number , . number , string , ha = string ) horizontal_chromosome ( root , na , nb , . number , height = . number number , fc = lsg , fill = true ) order = bed ( bed ) . order fp = open ( deletions ) scale = lambda var na + var * ( nb _ na ) / number for i , row in enumerate ( fp ) i + = number num , genes = row . split ( ) genes = genes . split ( string ) ia , a = order [ genes [ number ] ] ib , b = order [ genes [ _ number ] ] mi , mx = a . start , a . end mi , mx = scale ( mi ) , scale ( mx ) root . add_patch ( rectangle ( ( mi , . number ) , mx _ mi , . number number , fc = string , ec = string ) ) if i = = number mi _ = . number number elif i = = number mi + = . number number text_circle ( root , mi , . number , str ( i ) , fc = string ) for i , mi in zip ( range ( number , number ) , ( . number , . number , . number ) ) text_circle ( root , mi , . number , str ( i ) , fc = string ) root . set_xlim ( number , number ) root . set_ylim ( number , number ) root . set_axis_off ( ) image_name = deletion_genes + string savefig ( image_name , dpi = iopts . dpi , iopts = iopts )	0
Q_629	get all token which have expire	C_629_2	def get_pager_spec ( self ) self_config = self . get_config ( ) pagercmd = self_config . get ( string ) istty = self_config . getboolean ( string ) core_config = self . get_config ( string ) if pagercmd is none pagercmd = core_config . get ( string ) if istty is none istty = core_config . get ( string ) return { string pagercmd , string istty }	0
Q_630	list quiz in a course	C_630_0	def list_quizzes_in_course ( self , course_id , search_term = none ) string string string path = { } data = { } params = { } string string string path [ string ] = course_id string string string if search_term is not none params [ string ] = search_term self . logger . debug ( string . format ( params = params , data = data , * * path ) ) return self . generic_request ( string , string . format ( * * path ) , data = data , params = params , all_pages = true )	1
Q_630	list quiz in a course	C_630_1	def cluster_files ( file_dict ) return_dict = { } for var in file_dict info_dict = dict ( file_dict [ var ] ) if string in info_dict del ( info_dict [ string ] ) dict_key = tuple ( sorted ( [ file_dict [ var ] [ x ] for x in info_dict ] ) ) if dict_key not in return_dict return_dict [ dict_key ] = { string info_dict , string [ ] } return_dict [ dict_key ] [ string ] . append ( var ) return return_dict	0
Q_630	list quiz in a course	C_630_2	def db_store ( self , typ , py_value ) if isinstance ( py_value , datetime . datetime ) if py_value . tzinfo is none tz = pytz . timezone ( orb . system . settings ( ) . server_timezone ) py_value = tz . localize ( py_value ) return py_value . astimezone ( pytz . utc ) . replace ( tzinfo = none ) else return super ( datetime_with_timezone_column , self ) . db_store ( typ , py_value )	0
Q_631	set extension for student quiz submission	C_631_0	def set_extensions_for_student_quiz_submissions ( self , user_id , course_id , extend_from_end_at = none , extend_from_now = none , extra_attempts = none , extra_time = none , manually_unlocked = none ) string string string path = { } data = { } params = { } string string string path [ string ] = course_id string string string data [ string ] = user_id string string string if extra_attempts is not none data [ string ] = extra_attempts string string string if extra_time is not none data [ string ] = extra_time string string s locked for everyone else . string string manually_unlocked string string the number of minutes to extend the quiz from the current time . this is mutually exclusive to extend_from_end_at . this is limited to number minutes ( number hours ) string string extend_from_now string string the number of minutes to extend the quiz beyond the quiz string string if extend_from_end_at is not none data [ string ] = extend_from_end_at self . logger . debug ( string . format ( params = params , data = data , * * path ) ) return self . generic_request ( string , string . format ( * * path ) , data = data , params = params , no_data = true )	1
Q_631	set extension for student quiz submission	C_631_1	def cee_map_priority_group_table_pfc ( self , * * kwargs ) config = et . element ( string ) cee_map = et . sub_element ( config , string , xmlns = string ) name_key = et . sub_element ( cee_map , string ) name_key . text = kwargs . pop ( string ) priority_group_table = et . sub_element ( cee_map , string ) pgid_key = et . sub_element ( priority_group_table , string ) pgid_key . text = kwargs . pop ( string ) pfc = et . sub_element ( priority_group_table , string ) pfc . text = kwargs . pop ( string ) callback = kwargs . pop ( string , self . _callback ) return callback ( config )	0
Q_631	set extension for student quiz submission	C_631_2	def create_elements ( self , method , args = [ ] ) args = encode_args ( args ) js = string string string { string method , string args , } indexes = self . json ( js ) return map ( element , indexes )	0
Q_632	trim out some data to return for the index page	C_632_0	def _summarize_result ( result , config ) timing_var = config [ string ] summary = livv_dict ( ) for var , var in result . items ( ) proc_counts = [ ] bench_times = [ ] model_times = [ ] for proc , data in var . items ( ) proc_counts . append ( int ( proc [ number ] ) ) try bench_times . append ( data [ string ] [ timing_var ] [ string ] ) except key_error pass try model_times . append ( data [ string ] [ timing_var ] [ string ] ) except key_error pass if model_times = [ ] and bench_times = [ ] time_diff = np . mean ( model_times ) / np . mean ( bench_times ) else time_diff = string summary [ var ] [ string ] = string . join ( [ str ( x ) for x in sorted ( proc_counts ) ] ) summary [ var ] [ string ] = time_diff return summary	1
Q_632	trim out some data to return for the index page	C_632_1	def _get_version ( addon_dir , manifest , odoo_version_override = none , git_post_version = true ) version = manifest . get ( string ) if not version warn ( string addon_dir ) version = string if not odoo_version_override if len ( version . split ( string ) ) < number raise distutils_setup_error ( string string string addon_dir ) odoo_version = string . join ( version . split ( string ) [ number ] ) else odoo_version = odoo_version_override if odoo_version not in odoo_version_info raise distutils_setup_error ( string s string ( odoo_version , addon_dir ) ) odoo_version_info = odoo_version_info [ odoo_version ] if git_post_version version = get_git_postversion ( addon_dir ) return version , odoo_version , odoo_version_info	0
Q_632	trim out some data to return for the index page	C_632_2	def _get_expiry_timestamp ( cls , session_server ) timeout_seconds = cls . _get_session_timeout_seconds ( session_server ) time_now = datetime . datetime . now ( ) return time_now + datetime . timedelta ( seconds = timeout_seconds )	0
Q_633	this filter include the tagstr so they still show up in the editor	C_633_0	def preview_filter_from_query ( query , id_field = string , field_map = { } ) f = groups_filter_from_query ( query , field_map = field_map ) included_ids = query . get ( string ) if included_ids if f f = terms ( pk = included_ids ) else f = terms ( pk = included_ids ) return f	1
Q_633	this filter include the tagstr so they still show up in the editor	C_633_1	def _as_dict ( self ) values = self . _dynamic_columns or { } for var , var in self . _columns . items ( ) values [ var ] = var . to_database ( getattr ( self , var , none ) ) return values	0
Q_633	this filter include the tagstr so they still show up in the editor	C_633_2	def common_twig ( self ) return string . join ( [ getattr ( self , var ) for var in _meta_fields_twig if self . meta . get ( var ) is not none ] )	0
Q_634	this will create the custom model mixin to attach to your custom field	C_634_0	def create_mixin ( self ) _builder = self class custom_model_mixin ( object ) cached_property def _content_type ( self ) return content_type . objects . get_for_model ( self ) classmethod def get_model_custom_fields ( cls ) return _builder . fields_model_class . objects . filter ( content_type = content_type . objects . get_for_model ( cls ) ) def get_custom_fields ( self ) return _builder . fields_model_class . objects . filter ( content_type = self . _content_type ) def get_custom_value ( self , field ) return _builder . values_model_class . objects . get ( custom_field = field , content_type = self . _content_type , object_id = self . pk ) def set_custom_value ( self , field , value ) custom_value , created = _builder . values_model_class . objects . get_or_create ( custom_field = field , content_type = self . _content_type , object_id = self . pk ) custom_value . value = value custom_value . full_clean ( ) custom_value . save ( ) return custom_value return custom_model_mixin	1
Q_634	this will create the custom model mixin to attach to your custom field	C_634_1	def _clean_directory ( self , name ) if not os . path . exists ( name ) return self . announce ( string { } string . format ( name ) ) if not self . dry_run rmtree ( name , true )	0
Q_634	this will create the custom model mixin to attach to your custom field	C_634_2	def _xval_model_error ( self , fit_spectra , reject_outliers , fit_lb , fit_ub , fitter , func ) set1 = fit_spectra [ number ] set2 = fit_spectra [ number number ] errs = [ ] signal_select = [ ] models = [ ] signals = [ ] for var in [ set1 , set2 ] choose_transients , model , signal , params , this_idx = self . _fit_helper ( var , reject_outliers , fit_lb , fit_ub , fitter ) models . append ( np . nanmean ( model [ choose_transients ] , number ) ) signals . append ( np . nanmean ( signal [ choose_transients ] , number ) ) signal_select . append ( signal [ choose_transients ] ) model_err = np . mean ( [ ut . rmse ( models [ number ] , signals [ number ] ) , ut . rmse ( models [ number ] , signals [ number ] ) ] ) signal_err = ut . rmse ( np . nanmean ( signal_select [ number ] , number ) , np . nanmean ( signal_select [ number ] , number ) ) return model_err , signal_err	0
Q_635	"allow the pre_emptive fetch of site with a full browser if they "" re know"	C_635_0	def _pre_check ( self , requested_url ) components = urllib . parse . urlsplit ( requested_url ) netloc_l = components . netloc . lower ( ) if netloc_l in domain_constants . sucuri_garbage_site_netlocs self . _check_suc_cookie ( components ) elif netloc_l in domain_constants . cf_garbage_site_netlocs self . _check_cf_cookie ( components ) elif components . path = = string self . _check_suc_cookie ( components ) elif components . path = = string self . _check_suc_cookie ( components ) elif components . path = = string self . _check_cf_cookie ( components ) elif components . path = = string self . _check_cf_cookie ( components )	1
Q_635	"allow the pre_emptive fetch of site with a full browser if they "" re know"	C_635_1	def set_datapoint ( self , ind , datapoint ) if ind > = self . num_datapoints raise value_error ( string ( ind , self . num_datapoints ) ) self . data [ ind , . ] = np . array ( datapoint ) . astype ( self . dtype )	0
Q_635	"allow the pre_emptive fetch of site with a full browser if they "" re know"	C_635_2	def get_built_artifacts ( self , id , * * kwargs ) kwargs [ string ] = true if kwargs . get ( string ) return self . get_built_artifacts_with_http_info ( id , * * kwargs ) else ( data ) = self . get_built_artifacts_with_http_info ( id , * * kwargs ) return data	0
Q_636	use to require that a function will run only of the blockchain be recently update	C_636_0	def stalecheck ( web3 , * * kwargs ) allowable_delay = datetime . timedelta ( * * kwargs ) . total_seconds ( ) def decorator ( func ) def wrapper ( * args , assertfresh = true , * * kwargs ) if assertfresh last_block = web3 . eth . get_block ( string ) if not isfresh ( last_block , allowable_delay ) raise stale_blockchain ( last_block , allowable_delay ) return func ( * args , * * kwargs ) return wrapper return decorator	1
Q_636	use to require that a function will run only of the blockchain be recently update	C_636_1	def _float_decoder ( ) local_unpack = struct . unpack def inner_decode ( buffer , pos ) new_pos = pos + number float_bytes = buffer [ pos new_pos ] if ( float_bytes [ number number ] in b string and float_bytes [ number number ] > = b string ) if float_bytes [ number number ] = b string return ( _nan , new_pos ) if float_bytes [ number number ] = = b string return ( _neg_inf , new_pos ) return ( _pos_inf , new_pos ) result = local_unpack ( string , float_bytes ) [ number ] return ( result , new_pos ) return _simple_decoder ( wire_format . wiretype_fixed32 , inner_decode )	0
Q_636	use to require that a function will run only of the blockchain be recently update	C_636_2	def prerequisite_check ( ) if sys . version_info < ( number , number ) version_str = string sys . version_info [ number ] search_url = build_search_query ( _ ( string ) + string ) return _ ( string string string string string ) . format ( version = version_str , url = search_url ) modules_err = _ ( string string string ) try next ( pkg_resources . iter_entry_points ( string ) ) except stop_iteration return _ ( string string ) + string + modules_err try next ( pkg_resources . iter_entry_points ( string ) ) except stop_iteration return _ ( string string ) + string + modules_err	0
Q_637	additional_catches additional_catches catch lparen fully_qualified_class_name variable rparen lbrace inner_statement_list rbrace	C_637_0	def p_additional_catches ( p ) if len ( p ) = = number p [ number ] = p [ number ] + [ ast . catch ( p [ number ] , ast . variable ( p [ number ] , lineno = p . lineno ( number ) ) , p [ number ] , lineno = p . lineno ( number ) ) ] else p [ number ] = [ ]	1
Q_637	additional_catches additional_catches catch lparen fully_qualified_class_name variable rparen lbrace inner_statement_list rbrace	C_637_1	def watson ( t , hvap_ref , t_ref , tc , exponent = number . number ) tr = t / tc trefr = t_ref / tc h2 = hvap_ref * ( ( number _ tr ) / ( number _ trefr ) ) * * exponent return h2	0
Q_637	additional_catches additional_catches catch lparen fully_qualified_class_name variable rparen lbrace inner_statement_list rbrace	C_637_2	def start ( self , test_connection = true ) self . _detect_fork ( ) super ( fork_aware_locker_client , self ) . start ( test_connection )	0
Q_638	get fqdn from ldap	C_638_0	def _set_fqdn ( self ) results = self . _search ( string , string , [ string ] , scope = ldap . scope_base ) if not results and type ( results ) is not list r = none else dn , attrs = results [ number ] r = attrs [ string ] [ number ] . decode ( string ) self . _fqdn = r log . debug ( string self . _fqdn )	1
Q_638	get fqdn from ldap	C_638_1	def tune ( self ) if self . _node . get ( string ) tune = self . _node [ string ] . get ( string ) if type ( tune ) is collections . ordered_dict return tune elif type ( tune ) is list return tune [ number ] return tune return none	0
Q_638	get fqdn from ldap	C_638_2	def get_period_ls ( self , date , mag , n_threads , min_period ) oversampling = number . hifac = int ( ( max ( date ) _ min ( date ) ) / len ( date ) / min_period * number . ) if hifac < number hifac = number fx , fy , nout , jmax , prob = p_ls . fasper ( date , mag , oversampling , hifac , n_threads ) self . f = fx [ jmax ] self . period = number . / self . f self . period_uncertainty = self . get_period_uncertainty ( fx , fy , jmax ) self . period_log10_fap = np . log10 ( p_ls . get_significance ( fx , fy , nout , oversampling ) [ jmax ] ) self . period_snr = ( fy [ jmax ] _ np . median ( fy ) ) / np . std ( fy ) order = number p0 = np . ones ( order * number + number ) date_period = ( date self . period ) / self . period p1 , success = leastsq ( self . residuals , p0 , args = ( date_period , mag , order ) ) self . amplitude = np . sqrt ( p1 [ number ] * * number + p1 [ number ] * * number ) self . r21 = np . sqrt ( p1 [ number ] * * number + p1 [ number ] * * number ) / self . amplitude self . r31 = np . sqrt ( p1 [ number ] * * number + p1 [ number ] * * number ) / self . amplitude self . f_phase = np . arctan ( _ p1 [ number ] / p1 [ number ] ) self . phi21 = np . arctan ( _ p1 [ number ] / p1 [ number ] ) _ number . * self . f_phase self . phi31 = np . arctan ( _ p1 [ number ] / p1 [ number ] ) _ number . * self . f_phase string string string	0
Q_639	param edge not normalize edge	C_639_0	def _normalize_edge ( edge , dim_index , limit , schema = none ) if not _column _late_import ( ) if not edge log . error ( string ) elif is_text ( edge ) if schema leaves = unwraplist ( list ( schema . leaves ( edge ) ) ) if not leaves or is_container ( leaves ) return [ data ( name = edge , value = jx_expression ( edge , schema = schema ) , allow_nulls = true , dim = dim_index , domain = _normalize_domain ( none , limit ) ) ] elif isinstance ( leaves , _column ) return [ data ( name = edge , value = jx_expression ( edge , schema = schema ) , allow_nulls = true , dim = dim_index , domain = _normalize_domain ( domain = leaves , limit = limit , schema = schema ) ) ] elif is_list ( leaves . fields ) and len ( leaves . fields ) = = number return [ data ( name = leaves . name , value = jx_expression ( leaves . fields [ number ] , schema = schema ) , allow_nulls = true , dim = dim_index , domain = leaves . get_domain ( ) ) ] else return [ data ( name = leaves . name , allow_nulls = true , dim = dim_index , domain = leaves . get_domain ( ) ) ] else return [ data ( name = edge , value = jx_expression ( edge , schema = schema ) , allow_nulls = true , dim = dim_index , domain = default_domain ( ) ) ] else edge = wrap ( edge ) if not edge . name and not is_text ( edge . value ) log . error ( string , edge = edge ) if is_container ( edge . value ) and not edge . domain domain = _normalize_domain ( schema = schema ) domain . dimension = data ( fields = edge . value ) return [ data ( name = edge . name , value = jx_expression ( edge . value , schema = schema ) , allow_nulls = bool ( coalesce ( edge . allow_nulls , true ) ) , dim = dim_index , domain = domain ) ] domain = _normalize_domain ( edge . domain , schema = schema ) return [ data ( name = coalesce ( edge . name , edge . value ) , value = jx_expression ( edge . value , schema = schema ) , range = _normalize_range ( edge . range ) , allow_nulls = bool ( coalesce ( edge . allow_nulls , true ) ) , dim = dim_index , domain = domain ) ]	1
Q_639	param edge not normalize edge	C_639_1	def delete ( path , attribute ) cmd = string { number } string { number } string . format ( attribute , path ) try salt . utils . mac_utils . execute_return_success ( cmd ) except command_execution_error as exc if string in exc . strerror raise command_execution_error ( string . format ( path ) ) if string in exc . strerror raise command_execution_error ( string . format ( attribute ) ) raise command_execution_error ( string . format ( exc . strerror ) ) return attribute not in list_ ( path )	0
Q_639	param edge not normalize edge	C_639_2	def wordcount ( text ) bannedwords = read_file ( string ) wordcount = { } separated = separate ( text ) for var in separated if var not in bannedwords if not wordcount . has_key ( var ) wordcount [ var ] = number else wordcount [ var ] + = number return wordcount	0
Q_640	remove a triple from the store	C_640_0	def remove ( self , ( s , p , o ) , context = none ) named_graph = _get_named_graph ( context ) query_sets = _get_query_sets_for_object ( o ) filter_parameters = dict ( ) if named_graph is not none filter_parameters [ string ] = named_graph . id if s filter_parameters [ string ] = s if p filter_parameters [ string ] = p if o filter_parameters [ string ] = o query_sets = [ var . filter ( * * filter_parameters ) for var in query_sets ] for var in query_sets var . delete ( )	1
Q_640	remove a triple from the store	C_640_1	def push_cluster_configuration ( self , scaleioobj , no_upload = false , no_install = false , no_configure = false ) self . logger . debug ( string + string . format ( scaleioobj , no_upload , no_install , no_configure ) ) config_params = { string no_upload , string no_install , string no_configure } r1 = self . _im_session . post ( string . format ( self . _im_api_url , string ) , headers = { string string , string string } , params = config_params , verify = self . _im_verify_ssl , json = json . loads ( scaleioobj ) , stream = true ) if not r1 . ok self . logger . error ( string + string . format ( r1 . status_code ) ) return r1 . text	0
Q_640	remove a triple from the store	C_640_2	def get_filtered_pull_requests ( self , pull_requests ) pull_requests = self . filter_by_labels ( pull_requests , string ) pull_requests = self . filter_merged_pull_requests ( pull_requests ) if self . options . verbose > number print ( string . format ( len ( pull_requests ) ) ) return pull_requests	0
Q_641	return a http_response of the right medium type as specify by the	C_641_0	def render ( self , request = none , context = none , template_name = none ) request , context , template_name = self . get_render_params ( request , context , template_name ) self . set_renderers ( ) status_code = context . pop ( string , http_client . ok ) additional_headers = context . pop ( string , { } ) for var in request . renderers response = var ( request , context , template_name ) if response is not_implemented continue response . status_code = status_code response . var = var break else tried_mimetypes = list ( itertools . chain ( * [ r . mimetypes for r in request . renderers ] ) ) response = self . http_not_acceptable ( request , tried_mimetypes ) response . var = none for key , value in additional_headers . items ( ) response [ key ] = value patch_vary_headers ( response , ( string , ) ) return response	1
Q_641	return a http_response of the right medium type as specify by the	C_641_1	def patched_fax_init ( self , twilio ) super ( twilio_fax , self ) . _init_ ( twilio ) self . base_url = string self . account_sid = twilio . account_sid self . _v1 = none	0
Q_641	return a http_response of the right medium type as specify by the	C_641_2	def parse ( self , file , outfile = none ) file = self . _ensure_file ( file ) obj = json . load ( file ) items = obj [ string ] return [ self . transform_item ( var ) for var in items ]	0
Q_642	todo add doc	C_642_0	def unflatten ( obj ) if not isdict ( obj ) raise value_error ( string ( obj , ) ) ret = dict ( ) sub = dict ( ) for var , var in obj . items ( ) if string not in var and string not in var ret [ var ] = var continue if string in var and string in var idx = min ( var . find ( string ) , var . find ( string ) ) elif string in var idx = var . find ( string ) else idx = var . find ( string ) prefix = var [ idx ] if prefix not in sub sub [ prefix ] = dict ( ) sub [ prefix ] [ var [ idx ] ] = var for pfx , values in sub . items ( ) if pfx in ret raise value_error ( string ( pfx , ) ) ret [ pfx ] = _relunflatten ( pfx , values ) return ret	1
Q_642	todo add doc	C_642_1	def connect ( self , dests = none , name = none , id = string , props = { } ) with self . _mutex for var in dests if not var . porttype = = string raise exceptions . wrong_port_type_error if self . interfaces for var in dests if not var . interfaces raise exceptions . mismatched_interfaces_error for intf in self . interfaces for var in dests match = var . get_interface_by_instance_name ( intf . instance_name ) if not match raise exceptions . mismatched_interfaces_error if intf . polarity = = match . polarity raise exceptions . mismatched_polarity_error else for var in dests if var . interfaces raise exceptions . mismatched_interfaces_error new_props = props . copy ( ) if string not in new_props new_props [ string ] = string super ( corba_port , self ) . connect ( dests = dests , name = name , id = id , props = new_props )	0
Q_642	todo add doc	C_642_2	def delete ( config , username , type ) client = client ( ) client . prepare_connection ( ) user_api = api ( client ) user_api . delete ( username , type )	0
Q_643	define_service service identifier open_curly_bracket setting operation_list close_curly_bracket	C_643_0	def p_define_service ( p ) if len ( p ) = = number body = p [ number ] + p [ number ] else body = p [ number ] p [ number ] = define_service ( p [ number ] , body )	1
Q_643	define_service service identifier open_curly_bracket setting operation_list close_curly_bracket	C_643_1	def create_server_socket ( self , config ) server_socket = wudp_network_native_transport . create_server_socket ( self , config ) group = socket . inet_aton ( str ( self . target_socket ( config ) . address ( ) ) ) group_membership = struct . pack ( string , group , socket . inaddr_any ) server_socket . setsockopt ( socket . ipproto_ip , socket . ip_add_membership , group_membership ) return server_socket	0
Q_643	define_service service identifier open_curly_bracket setting operation_list close_curly_bracket	C_643_2	def _open_ftp ( self ) ftp = self . fs . _open_ftp ( ) ftp . voidcmd ( str ( string ) ) return ftp	0
Q_644	add a new bundle	C_644_0	def bundle ( context , name ) if context . obj [ string ] . bundle ( name ) click . echo ( click . style ( string , fg = string ) ) context . abort ( ) new_bundle = context . obj [ string ] . new_bundle ( name ) context . obj [ string ] . add_commit ( new_bundle ) new_version = context . obj [ string ] . new_version ( created_at = new_bundle . created_at ) new_version . bundle = new_bundle context . obj [ string ] . add_commit ( new_version ) click . echo ( click . style ( f string , fg = string ) )	1
Q_644	add a new bundle	C_644_1	def skip_prepare ( func ) wraps ( func ) def _wrapper ( self , * args , * * kwargs ) value = func ( self , * args , * * kwargs ) return data ( value , should_prepare = false ) return _wrapper	0
Q_644	add a new bundle	C_644_2	def setup_sfr_reach_parameters ( nam_file , model_ws = string , par_cols = [ string ] ) try import flopy except exception as e return if par_cols is none par_cols = [ string ] if isinstance ( nam_file , flopy . modflow . mf . modflow ) and nam_file . sfr is not none m = nam_file nam_file = m . namefile model_ws = m . model_ws else m = flopy . modflow . modflow . load ( nam_file , load_only = [ string ] , model_ws = model_ws , check = false , forgive = false ) reach_data = pd . data_frame . from_records ( m . sfr . reach_data ) reach_data_orig = reach_data . copy ( ) reach_data . to_csv ( os . path . join ( m . model_ws , string ) , sep = string ) tpl_str , pvals = [ ] , [ ] idx_cols = [ string , string , string , string , string , string , string , string ] notpar_cols = [ var for var in reach_data . columns if var not in par_cols + idx_cols ] missing = [ ] cols = par_cols . copy ( ) for par_col in par_cols if par_col not in reach_data . columns missing . append ( par_col ) cols . remove ( par_col ) if len ( missing ) > number warnings . warn ( string . format ( string . join ( missing ) ) , pyemu_warning ) if len ( missing ) > = len ( par_cols ) warnings . warn ( string . format ( string . join ( par_cols ) ) , pyemu_warning ) for par_col in cols if par_col = = string prefix = string else prefix = par_col reach_data . loc [ , par_col ] = reach_data . apply ( lambda x string . format ( prefix , int ( x . reach_id ) ) if float ( x [ par_col ] ) = number . number else string , axis = number ) org_vals = reach_data_orig . loc [ reach_data_orig . loc [ , par_col ] = number . number , par_col ] pnames = reach_data . loc [ org_vals . index , par_col ] pvals . extend ( list ( org_vals . values ) ) tpl_str . extend ( list ( pnames . values ) ) pnames = [ t . replace ( string , string ) . strip ( ) for t in tpl_str ] df = pd . data_frame ( { string pnames , string pvals , string tpl_str } , index = pnames ) df . drop_duplicates ( inplace = true ) if df . empty warnings . warn ( string . format ( string . join ( par_cols ) ) , pyemu_warning ) else reach_data . loc [ , notpar_cols ] = string write_df_tpl ( os . path . join ( model_ws , string ) , reach_data , sep = string ) with open ( os . path . join ( model_ws , string ) , string ) as f f . write ( string . format ( nam_file ) ) f . write ( string . format ( model_ws ) ) f . write ( string ) f . write ( string . format ( m . sfr . file_name [ number ] ) ) parnme = parse_tpl_file ( os . path . join ( model_ws , string ) ) assert len ( parnme ) = = df . shape [ number ] df . loc [ , string ] = df . parnme . apply ( lambda x x . split ( string ) [ number ] ) df . loc [ , string ] = number . number df . loc [ , string ] = number . number hpars = df . loc [ df . pargp . apply ( lambda x x . startswith ( string ) ) , string ] df . loc [ hpars , string ] = number . number df . loc [ hpars , string ] = number . number number return df	0
Q_645	generate dictionary of reference idents group by share rank	C_645_0	def _hierarchy ( self ) self . hierarchy = { } for var in self . taxonomy taxslice = self . _slice ( level = self . taxonomy . index ( var ) ) self . hierarchy [ var ] = self . _group ( taxslice )	1
Q_645	generate dictionary of reference idents group by share rank	C_645_1	def format_decimal ( interval , value ) interval = get_significant_decimal ( interval ) if isinstance ( interval , integral ) or isinstance ( value , integral ) return add_separators ( int ( value ) ) if interval = interval return str ( value ) if value = value return str ( value ) decimal_places = len ( str ( interval ) . split ( string ) [ number ] ) my_number_int = str ( value ) . split ( string ) [ number ] my_number_decimal = str ( value ) . split ( string ) [ number ] [ decimal_places ] if len ( set ( my_number_decimal ) ) = = number and my_number_decimal [ _ number ] = = string return my_number_int formatted_decimal = ( add_separators ( int ( my_number_int ) ) + decimal_separator ( ) + my_number_decimal ) return formatted_decimal	0
Q_645	generate dictionary of reference idents group by share rank	C_645_2	def var_to_xml ( val , name , trim_if_too_big = true , additional_in_xml = string , evaluate_full_value = true ) type_name , type_qualifier , is_exception_on_eval , resolver , value = get_variable_details ( val , evaluate_full_value ) try name = quote ( name , string ) except pass xml = string s string s string ( make_valid_xml_value ( name ) , make_valid_xml_value ( type_name ) ) if type_qualifier xml_qualifier = string s string make_valid_xml_value ( type_qualifier ) else xml_qualifier = string if value if len ( value ) > maximum_variable_representation_size and trim_if_too_big value = value [ number maximum_variable_representation_size ] value + = string xml_value = string s string ( make_valid_xml_value ( quote ( value , string ) ) ) else xml_value = string if is_exception_on_eval xml_container = string true string else if resolver is not none xml_container = string true string else xml_container = string return string . join ( ( xml , xml_qualifier , xml_value , xml_container , additional_in_xml , string ) )	0
Q_646	handle the operation	C_646_0	"def run_op ( self , op , sched ) string string t return a generator , set state_completed and set the result to whatever the function returned . * if stop_iteration is raised , set state_completed and return self . * if any other exception is raised , set state_failed , handle error or send it to the caller , return self string string running coro s with itself . something is fishy . string s called with s op r , coroutine state ( s ) should be less than s string state string running string finalized string errored string nop "" ) , op , self . _state_names [ self . state ] , self . _state_names [ self . state_completed ] ) coroutines . ident = self try if self . state = = self . state_running if self . debug traceback . print_stack ( self . coro . gr_frame ) if isinstance ( op , coroutine_exception ) rop = self . coro . throw ( * op . args ) else rop = self . coro . switch ( op and op . finalize ( sched ) ) elif self . state = = self . state_need_init assert op is none rop = self . coro . switch ( * self . f_args , * * self . f_kws ) self . state = self . state_running del self . f_args del self . f_kws else return none except stop_iteration , e self . state = self . state_completed self . result = e . args and e . args [ number ] rop = self except ( keyboard_interrupt , generator_exit , system_exit ) raise except self . state = self . state_failed self . result = none self . exception = sys . exc_info ( ) if not self . caller self . handle_error ( ) rop = self sys . exc_clear ( ) finally coroutines . ident = none return rop"	1
Q_646	handle the operation	C_646_1	def generate_add_user_command ( proposed_user = none , manage_home = none ) command = none if get_platform ( ) in ( string , string ) command = string . format ( sudo_check ( ) , linux_cmd_useradd ) if proposed_user . uid command = string . format ( command , proposed_user . uid ) if proposed_user . gid command = string . format ( command , proposed_user . gid ) if proposed_user . gecos command = string { number } string . format ( command , proposed_user . gecos ) if manage_home if proposed_user . home_dir if os . path . exists ( proposed_user . home_dir ) command = string . format ( command , proposed_user . home_dir ) elif not os . path . exists ( string . format ( proposed_user . name ) ) command = string . format ( command ) if proposed_user . shell command = string . format ( command , proposed_user . shell ) command = string . format ( command , proposed_user . name ) elif get_platform ( ) = = string command = string . format ( sudo_check ( ) , freebsd_cmd_pw ) if proposed_user . uid command = string . format ( command , proposed_user . uid ) if proposed_user . gid command = string . format ( command , proposed_user . gid ) if proposed_user . gecos command = string { number } string . format ( command , proposed_user . gecos ) if manage_home if proposed_user . home_dir command = string . format ( command , proposed_user . home_dir ) else command = string . format ( command ) if proposed_user . shell command = string . format ( command , proposed_user . shell ) command = string . format ( command , proposed_user . name ) if command return shlex . split ( str ( command ) )	0
Q_646	handle the operation	C_646_2	def input_timeout ( msg = string , timeout = number ) import sys import select import time ans = none print ( string timeout ) print ( msg ) if sys . platform . startswith ( string ) import msvcrt start_time = time . time ( ) instr = string while true if msvcrt . kbhit ( ) chr_ = msvcrt . getche ( ) if ord ( chr_ ) = = number ans = instr break elif ord ( chr_ ) > = number instr + = chr_ ellapsed = time . time ( ) _ start_time if ellapsed > timeout ans = none print ( string ) else rlist , o , e = select . select ( [ sys . stdin ] , [ ] , [ ] , timeout ) if rlist ans = sys . stdin . readline ( ) . strip ( ) return ans	0
Q_647	regex substitution function	C_647_0	def substitute ( search , replace , text ) string return re . sub ( re . compile ( str ( search ) ) , replace , text )	1
Q_647	regex substitution function	C_647_1	def copy ( self ) templates = self . prepare_templates ( ) if self . params . interactive keys = list ( self . parser . default ) for var in keys if var . startswith ( string ) continue prompt = string { number } string . format ( var , self . parser . default [ var ] ) if _compat . py2 value = raw_input ( prompt . encode ( string ) ) . decode ( string ) else value = input ( prompt . encode ( string ) ) value = value . strip ( ) if value self . parser . default [ var ] = value self . parser . default [ string ] = tt = string . join ( t . name for t in templates ) logging . warning ( string . format ( tt ) ) self . make_directory ( self . params . target ) logging . debug ( string ) logging . debug ( string . join ( string . format ( * v ) for v in self . parser . default . items ( ) ) ) return [ t . paste ( * * dict ( self . parser . default . items ( ) ) ) for t in templates ]	0
Q_647	regex substitution function	C_647_2	def _convert_ftp_time_to_iso ( ftp_time ) date_time = datetime ( int ( ftp_time [ number ] ) , int ( ftp_time [ number number ] ) , int ( ftp_time [ number number ] ) , int ( ftp_time [ number number ] ) , int ( ftp_time [ number number ] ) , int ( ftp_time [ number number ] ) ) return date_time	0
Q_648	find out if an executable exist in the supply path	C_648_0	def which ( program , environ = none ) def is_exe ( var ) return isfile ( var ) and os . access ( var , os . x_ok ) if program is none raise command_exception ( string ) fpath , fname = split ( program ) if fpath if is_exe ( program ) return program else if environ is none environ = os . environ for var in environ [ string ] . split ( os . pathsep ) exe_file = join ( var , program ) if is_exe ( exe_file ) return exe_file raise command_exception ( string program )	1
Q_648	find out if an executable exist in the supply path	C_648_1	def timestamp_to_rdf_datetime ( timestamp ) if timestamp is none return none else micros = int ( number * timestamp ) return rdfvalue . rdf_datetime . from_microseconds_since_epoch ( micros )	0
Q_648	find out if an executable exist in the supply path	C_648_2	def authorized_get_account_balance ( self , huid ) acc = self . request ( string , safeformat ( string , huid ) ) return int ( acc [ string ] )	0
Q_649	assume a tagint _d numpy array and reshape like	C_649_0	def reshape_to_2d ( arr ) return arr . reshape ( ( arr . shape [ number ] , arr . shape [ number ] * arr . shape [ number ] ) )	1
Q_649	assume a tagint _d numpy array and reshape like	C_649_1	def start ( self ) try self . args . func ( ) except system_exit as e if e . code = number raise except keyboard_interrupt self . log . warning ( string ) except self . log . exception ( string ) finally self . _flush_metrics_q . put ( none , block = true ) self . _flush_metrics_q . put ( none , block = true , timeout = number ) self . log . debug ( string )	0
Q_649	assume a tagint _d numpy array and reshape like	C_649_2	def _sim_wa ( trace , paz , seedresp , water_level , velocity = false ) paz_wa = { string [ _ number . number + number . number , _ number . number _ number . number ] , string [ number + number ] , string number . number , string number } if velocity paz_wa [ string ] = [ number + number , number + number ] trace . detrend ( string ) if paz trace . data = seis_sim ( trace . data , trace . stats . sampling_rate , paz_remove = paz , paz_simulate = paz_wa , water_level = water_level , remove_sensitivity = true ) elif seedresp trace . data = seis_sim ( trace . data , trace . stats . sampling_rate , paz_remove = none , paz_simulate = paz_wa , water_level = water_level , seedresp = seedresp ) else user_warning ( string ) trace . data = seis_sim ( trace . data , trace . stats . sampling_rate , paz_remove = none , paz_simulate = paz_wa , water_level = water_level ) return trace	0
Q_650	add a description name to match	C_650_0	def match_description ( self , description = none , string_match_type = none , match = none ) self . _match_display_text ( string , description , string_match_type , match )	1
Q_650	add a description name to match	C_650_1	def _prepare_calls ( result_file , out_dir , data ) sample = dd . get_sample_name ( data ) out_file = os . path . join ( out_dir , string ( sample ) ) if not utils . file_uptodate ( out_file , result_file ) hla_truth = bwakit . get_hla_truthset ( data ) with file_transaction ( data , out_file ) as tx_out_file with open ( tx_out_file , string ) as out_handle writer = csv . writer ( out_handle ) allele_info = _parse_result_file ( result_file ) if len ( allele_info ) = = number writer . writerow ( [ string , string , string , string , string ] ) else writer . writerow ( [ string , string , string , string , string ] ) for j , ( alleles , score ) in enumerate ( allele_info ) for var , var in alleles truth_alleles = tz . get_in ( [ sample , var ] , hla_truth , [ ] ) if len ( allele_info ) = = number writer . writerow ( [ sample , var , string . join ( var ) , string . join ( truth_alleles ) , bwakit . matches_truth ( var , truth_alleles , data ) ] ) else writer . writerow ( [ sample , var , j , string . join ( var ) , score ] ) return out_file	0
Q_650	add a description name to match	C_650_2	def _photon_integrand ( self , x , egamma ) try return ( self . _sigma_inel ( egamma / x ) * self . _particle_distribution ( ( egamma / x ) ) * self . _fgamma ( x , egamma / x ) / x ) except zero_division_error return np . nan	0
Q_651	handler for parse w message	C_651_0	def _on_message ( self , ws , message ) message = json . loads ( message ) self . logger . debug ( json . dumps ( message ) ) table = message [ string ] if string in message else none action = message [ string ] if string in message else none try if string in message self . logger . debug ( string message [ string ] ) elif action if table not in self . data self . data [ table ] = [ ] if action = = string self . logger . debug ( string table ) self . data [ table ] + = message [ string ] self . keys [ table ] = message [ string ] elif action = = string self . logger . debug ( string ( table , message [ string ] ) ) self . data [ table ] + = message [ string ] elif action = = string self . logger . debug ( string ( table , message [ string ] ) ) for var in message [ string ] item = find_item_by_keys ( self . keys [ table ] , self . data [ table ] , var ) if not item return item . update ( var ) if table = = string and item [ string ] < = number self . data [ table ] . remove ( item ) elif action = = string self . logger . debug ( string ( table , message [ string ] ) ) for delete_data in message [ string ] item = find_item_by_keys ( self . keys [ table ] , self . data [ table ] , delete_data ) self . data [ table ] . remove ( item ) else raise exception ( string action ) except self . logger . error ( traceback . format_exc ( ) )	1
Q_651	handler for parse w message	C_651_1	def up_by_arrival ( * filters , local_dir = string , remote_dir = default_remote_dir ) local_monitor = watch_local_files ( * filters , local_dir = local_dir ) _ , var = next ( local_monitor ) _notify_sync_ready ( len ( var ) , local_dir , remote_dir ) for var , var in local_monitor yield direction . up , var if var _notify_sync ( direction . up , var ) up_by_files ( var , remote_dir ) _notify_sync_ready ( len ( var ) , local_dir , remote_dir )	0
Q_651	handler for parse w message	C_651_2	def fit ( self , x , y ) self . target_encoders = [ none ] * x . shape [ number ] self . target_mean = y . mean ( ) for var , var in enumerate ( x . columns ) self . target_encoders [ var ] = self . _get_target_encoder ( x [ var ] , y ) return self	0
Q_652	pas through to provider proficiency_admin_session	C_652_0	def get_proficiency_form ( self , * args , * * kwargs ) if isinstance ( args [ _ number ] , list ) or string in kwargs return self . get_proficiency_form_for_create ( * args , * * kwargs ) else return self . get_proficiency_form_for_update ( * args , * * kwargs )	1
Q_652	pas through to provider proficiency_admin_session	C_652_1	"def rgb_to_ansi ( color ) if color [ number ] = string ansi_ { number d } "" . format ( n ) except value_error return none"	0
Q_652	pas through to provider proficiency_admin_session	C_652_2	def remove ( self , oid ) virtual_function = self . lookup_by_oid ( oid ) partition = self . parent devno = virtual_function . properties . get ( string , none ) if devno partition . devno_free_if_allocated ( devno ) assert string in partition . properties vf_uris = partition . properties [ string ] vf_uris . remove ( virtual_function . uri ) super ( faked_virtual_function_manager , self ) . remove ( oid )	0
Q_653	"return an iterator over the db "" s pair"	C_653_0	def iteritems ( self , match = none , count = number ) logger . debug ( string , match ) if match is none match = string for var in self . _redis . scan_iter ( match = match , count = count ) yield var , self . get ( var )	1
Q_653	"return an iterator over the db "" s pair"	C_653_1	def _connect_to_device ( self , uuid , key , client ) slug = self . _build_device_slug ( uuid ) message = { string client , string string , string string } self . _logger . info ( string , uuid ) if uuid in self . _connections message [ string ] = false message [ string ] = string self . _publish_status ( slug , message ) return resp = yield self . _manager . connect ( uuid ) message [ string ] = resp [ string ] if resp [ string ] conn_id = resp [ string ] self . _connections [ uuid ] = { string key , string client , string conn_id , string monotonic ( ) , string [ ] , string bytes ( ) , string none , string false , string none } else message [ string ] = resp [ string ] self . _connections [ uuid ] = { } connection = self . _connections [ uuid ] connection [ string ] = self . _manager . register_monitor ( uuid , [ string ] , self . _notify_report ) connection [ string ] = self . _manager . register_monitor ( uuid , [ string ] , self . _notify_trace ) self . _publish_status ( slug , message )	0
Q_653	"return an iterator over the db "" s pair"	C_653_2	def save_vault ( self , vault_form , * args , * * kwargs ) if vault_form . is_for_update ( ) return self . update_vault ( vault_form , * args , * * kwargs ) else return self . create_vault ( vault_form , * args , * * kwargs )	0
Q_654	output it parameter to user stdout	C_654_0	def out ( * args ) for var in args sys . stdout . write ( var ) sys . stdout . write ( os . linesep )	1
Q_654	output it parameter to user stdout	C_654_1	def update_http_method_params ( self ) url_params = { } if self . start url_params . update ( { string int ( calendar . timegm ( self . start . timetuple ( ) ) ) } ) if self . stop url_params . update ( { string int ( calendar . timegm ( self . stop . timetuple ( ) ) ) } ) if self . probe_ids url_params . update ( { string self . probe_ids } ) self . http_method_args [ string ] . update ( url_params )	0
Q_654	output it parameter to user stdout	C_654_2	def hamiltonian ( edges , directed = false , precision = number ) edges = populate_edge_weights ( edges ) incident , nodes = node_to_edge ( edges , directed = false ) dummy = string dummy_edges = edges + [ ( dummy , var , number ) for var in nodes ] if directed dummy_edges + = [ ( var , dummy , number ) for var in nodes ] dummy_edges = reformulate_atsp_as_tsp ( dummy_edges ) tour = tsp ( dummy_edges , precision = precision ) dummy_index = tour . index ( dummy ) tour = tour [ dummy_index ] + tour [ dummy_index ] if directed dummy_star_index = tour . index ( ( dummy , string ) ) assert dummy_star_index in ( number , len ( tour ) _ number ) , tour if dummy_star_index = = len ( tour ) _ number tour = tour [ number ] + tour [ number ] tour = tour [ _ number ] path = tour [ number ] path = [ var for var in path if not isinstance ( var , tuple ) ] else path = tour [ number ] return path	0
Q_655	remove a root catalog	C_655_0	def remove_root_catalog ( self , catalog_id ) if self . _catalog_session is not none return self . _catalog_session . remove_root_catalog ( catalog_id = catalog_id ) return self . _hierarchy_session . remove_root ( id_ = catalog_id )	1
Q_655	remove a root catalog	C_655_1	def show_operand_lines ( rh ) if rh . function = = string rh . print_ln ( string , string ) else rh . print_ln ( string , string ) rh . print_ln ( string , string + string ) rh . print_ln ( string , string ) rh . print_ln ( string , string + string ) if rh . subfunction = string rh . print_ln ( string , string ) rh . print_ln ( string , string + string ) rh . print_ln ( string , string + string ) rh . print_ln ( string , string + string ) rh . print_ln ( string , string + string ) rh . print_ln ( string , string + string ) rh . print_ln ( string , string + string s directory entry . string n string _ logonby < by_users > _ string specifies a list of up to number z / vm userids who can log string n string string on to the virtual machine using their id and password . string n string _ max_mem_size < max_mem > _ string specifies the maximum memory the virtual machine string n string string is allowed to define . string n string _ set_reserved_mem _ string set the additional memory space ( max_mem_size _ pri_mem_size ) string n string string as reserved memory of the virtual machine . string n string < password > _ string specifies the password for the new virtual string n string string machine . string n string < pri_mem_size > _ string specifies the initial memory size for the new virtual string n string string machine . string n string < priv_classes > _ string specifies the privilege classes for the new virtual string n string string machine . string n string _ profile < prof_name > _ string specifies the z / vm profile to include in the string n string string virtual machine string ) rh . print_ln ( string , string + string ) return	0
Q_655	remove a root catalog	C_655_2	def get_cardinality ( self , node = none ) if node return self . get_cpds ( node ) . cardinality [ number ] else cardinalities = defaultdict ( int ) for var in self . cpds cardinalities [ var . variable ] = var . cardinality [ number ] return cardinalities	0
Q_656	return back a graph contain just the root and child	C_656_0	def get_root_graph ( self , root ) children = self . get_children ( root ) g = graph ( ) nodes = [ root ] + children for var in nodes g . add_node ( var ) node_ids = [ x . id for x in nodes ] edges = [ x for x in self . _edges . values ( ) if x . node1 . id in node_ids and x . node2 . id in node_ids ] for e in edges g . add_edge ( e ) return g	1
Q_656	return back a graph contain just the root and child	C_656_1	def set_cell ( self , column_family_id , column , value , timestamp = none ) self . _set_cell ( column_family_id , column , value , timestamp = timestamp , state = none )	0
Q_656	return back a graph contain just the root and child	C_656_2	def is_module ( self , name ) if self . module_prefix . startswith ( self . mount_prefix ) return name . startswith ( self . module_prefix ) return name . startswith ( self . module_prefix ) and not name . startswith ( self . mount_prefix )	0
Q_657	get an asset query session for the give repository	C_657_0	def get_asset_query_session_for_repository ( self , repository_id , proxy ) if not repository_id raise null_argument ( ) if not self . supports_asset_query ( ) raise unimplemented ( ) try from . import sessions except import_error raise operation_failed ( string ) proxy = self . _convert_proxy ( proxy ) try session = sessions . asset_query_session ( repository_id , proxy , runtime = self . _runtime ) except attribute_error raise operation_failed ( string ) return session	1
Q_657	get an asset query session for the give repository	C_657_1	def install_unicast_keys ( self , client_nonce ) pmk = self . pmk anonce = self . anonce snonce = client_nonce amac = mac2str ( self . mac ) smac = mac2str ( self . client ) self . ptk = custom_prf512 ( pmk , amac , smac , anonce , snonce ) self . kck = self . ptk [ number ] self . kek = self . ptk [ number number ] self . tk = self . ptk [ number number ] self . mic_ap_to_sta = self . ptk [ number number ] self . mic_sta_to_ap = self . ptk [ number number ] self . client_iv = count ( )	0
Q_657	get an asset query session for the give repository	C_657_2	def _build_b ( self , force = false ) r if force self . _pure_b = none if self . _pure_b is none b = np . zeros ( shape = ( self . np , ) , dtype = float ) self . _pure_b = b self . b = self . _pure_b . copy ( )	0
Q_658	nonfluent_def ident lparen param_list rparen colon lcurly non_fluent comma type_spec comma default assign_equal range_const rcurly semi	C_658_0	def p_nonfluent_def ( self , p ) if len ( p ) = = number p [ number ] = p_variable ( name = p [ number ] , fluent_type = string , range_type = p [ number ] , param_types = p [ number ] , default = p [ number ] ) else p [ number ] = p_variable ( name = p [ number ] , fluent_type = string , range_type = p [ number ] , default = p [ number ] )	1
Q_658	nonfluent_def ident lparen param_list rparen colon lcurly non_fluent comma type_spec comma default assign_equal range_const rcurly semi	C_658_1	def build_row_dict ( cls , row , dialect , deleted = false , user_id = none , use_dirty = true ) data = { string row . to_archivable_dict ( dialect , use_dirty = use_dirty ) , string deleted , string datetime . now ( ) , string current_version_sql ( as_is = true ) if deleted else row . version_id } for var in row . version_columns data [ var ] = utils . get_column_attribute ( row , var , use_dirty = use_dirty ) if user_id is not none data [ string ] = user_id return data	0
Q_658	nonfluent_def ident lparen param_list rparen colon lcurly non_fluent comma type_spec comma default assign_equal range_const rcurly semi	C_658_2	def get_nested_val ( key_tuple , dict_obj ) if len ( key_tuple ) = = number return dict_obj [ key_tuple [ number ] ] return get_nested_val ( key_tuple [ number ] , dict_obj [ key_tuple [ number ] ] )	0
Q_659	small decorator to make sure we have the git repo or report error	C_659_0	"def _needs_git ( func ) wraps ( func ) def myfunc ( * args , * * kwargs ) if not with_git raise runtime_error ( string t extract info from the string git repos . "" ) return func ( * args , * * kwargs ) return myfunc"	1
Q_659	small decorator to make sure we have the git repo or report error	C_659_1	def set_hvac_mode ( self , index , hvac_mode ) body = { string { string string , string self . thermostats [ index ] [ string ] } , string { string { string hvac_mode } } } log_msg_action = string return self . make_request ( body , log_msg_action )	0
Q_659	small decorator to make sure we have the git repo or report error	C_659_2	def any_slug_field ( field , * * kwargs ) string string str string string letters = ascii_letters + digits + string return xunit . any_string ( letters = letters , max_length = field . max_length )	0
Q_660	recurse up the tree of file_set_states until we find a parent i	C_660_0	def _find_parent ( self , path_elements ) if not self . path_elements return self elif self . path_elements = = path_elements [ number len ( self . path_elements ) ] return self else return self . parent . _find_parent ( path_elements )	1
Q_660	recurse up the tree of file_set_states until we find a parent i	C_660_1	def sample_out_dir ( self ) if self . _sample_out_dir is none self . _sample_out_dir = os . path . join ( self . output_dir , self . _sample_name ) return self . _sample_out_dir	0
Q_660	recurse up the tree of file_set_states until we find a parent i	C_660_2	def pack_into ( self , buf , offset , data , * * kwargs ) try self . pack_into_any ( buf , offset , data , * * kwargs ) except key_error as e raise error ( string . format ( str ( e ) ) )	0
Q_661	find word_ranges all of phrase in token store in token_trie	C_661_0	def find_tracked_words ( self , tokens ) tracked_words = [ ] for var in range ( len ( tokens ) ) for j in range ( var + number , len ( tokens ) + number ) phrase = tokens [ var j ] status = self . has_tokens ( phrase ) if status is not none if status is true tracked_words . append ( token_trie . token ( phrase , var , j _ number ) ) elif status is false break return tracked_words	1
Q_661	find word_ranges all of phrase in token store in token_trie	C_661_1	def _file_local_list ( self , dest ) if os . path . isdir ( dest ) destdir = dest else destdir = os . path . dirname ( dest ) filelist = set ( ) for root , dirs , files in salt . utils . path . os_walk ( destdir , followlinks = true ) for var in files path = os . path . join ( root , var ) filelist . add ( path ) return filelist	0
Q_661	find word_ranges all of phrase in token store in token_trie	C_661_2	def run ( self ) if self . workflow . build_process_failed self . log . info ( string ) return self . session = get_koji_session ( self . workflow , self . koji_fallback ) server_dir = get_koji_upload_dir ( self . workflow ) koji_metadata , output_files = self . combine_metadata_fragments ( ) if is_scratch_build ( ) self . upload_scratch_metadata ( koji_metadata , server_dir , self . session ) return try for var in output_files if var . file self . upload_file ( self . session , var , server_dir ) finally for var in output_files if var . file var . file . close ( ) try build_info = self . session . cg_import ( koji_metadata , server_dir ) except exception self . log . debug ( string , koji_metadata ) raise build_id = build_info . get ( string ) if build_info else none self . log . debug ( string , json . dumps ( build_info , sort_keys = true , indent = number ) ) return build_id	0
Q_662	problem provide data	C_662_0	def set_conditions ( self , variables , constraints ) self . _vars , self . _constraints = variables , [ ] self . _constraints_for_var = { } vars_constraint_count = { } for func , variables , values in constraints c = constraint ( func , variables , values , self . _compute_search_spaces ( variables ) ) self . _constraints . append ( c ) for var in variables self . _constraints_for_var [ var ] = self . _constraints_for_var . get ( var , [ ] ) + [ c ] vars_constraint_count [ var ] = vars_constraint_count . get ( var , number ) + number self . _constraints . sort ( ) self . _variable_expand_order = tuple ( sorted ( self . _vars . keys ( ) , key = vars_constraint_count . get , reverse = true ) )	1
Q_662	problem provide data	C_662_1	def _get_typed_value ( self , key , target_type , type_convert , is_optional = false , is_secret = false , is_local = false , default = none , options = none ) try value = self . _get ( key ) except key_error if not is_optional raise rhea_error ( string . format ( key ) ) return default if isinstance ( value , six . string_types ) try self . _add_key ( key , is_secret = is_secret , is_local = is_local ) self . _check_options ( key = key , value = value , options = options ) return type_convert ( value ) except value_error raise rhea_error ( string string . format ( value , key , target_type ) ) if isinstance ( value , target_type ) self . _add_key ( key , is_secret = is_secret , is_local = is_local ) self . _check_options ( key = key , value = value , options = options ) return value raise rhea_error ( string string . format ( value , key , target_type ) )	0
Q_662	problem provide data	C_662_2	def day_night_duration ( self , daybreak datetime . time = datetime . time ( normal_day_start_h ) , nightfall datetime . time = datetime . time ( normal_day_end_h ) ) _ > tuple [ datetime . timedelta , datetime . timedelta ] daytotal = datetime . timedelta ( ) nighttotal = datetime . timedelta ( ) startdate = self . start . date ( ) enddate = self . end . date ( ) ndays = ( enddate _ startdate ) . days + number for var in range ( ndays ) date = startdate + datetime . timedelta ( days = var ) component = self . component_on_date ( date ) day = interval . daytime ( date , daybreak , nightfall ) daypart = component . intersection ( day ) if daypart is not none daytotal + = daypart . duration ( ) nighttotal + = component . duration ( ) _ daypart . duration ( ) else nighttotal + = component . duration ( ) return daytotal , nighttotal	0
Q_663	get or create the ap_election_meta object for the give row of	C_663_0	def get_or_create_ap_election_meta ( self , row , election ) ap_election_meta . objects . get_or_create ( ap_election_id = row [ string ] , election = election )	1
Q_663	get or create the ap_election_meta object for the give row of	C_663_1	def on_unregistered_type ( self , typekey , packet ) log . msg ( string ( typekey , type ( self ) . _name_ ) ) self . transport . lose_connection ( )	0
Q_663	get or create the ap_election_meta object for the give row of	C_663_2	def _predicate ( self , i ) if isinstance ( i , slice ) if i . step is not none raise type_error ( string ) if ( ( i . start is not none and not isinstance ( i . start , datetime ) ) or ( i . stop is not none and not isinstance ( i . stop , datetime ) ) ) raise type_error ( string . format ( datetime ) ) if i . start is not none and i . stop is not none if i . start > i . stop raise value_error ( string ) else predicate = string . format ( self . _keycol ) elif i . start is not none predicate = string . format ( self . _keycol ) elif i . stop is not none predicate = string . format ( self . _keycol ) else predicate = string multi = true pred = { string i . start , string i . stop } elif isinstance ( i , datetime ) predicate = string . format ( self . _keycol ) multi = false pred = { string i } else raise type_error ( string . format ( datetime ) ) return ( predicate , multi , pred )	0
Q_664	go through list of d_ps to make sure that their destination name	C_664_0	def resolve_filename_conflicts ( self , dialog = true ) resolved = self . wdplv . resolve_filename_conflicts ( ) if resolved and dialog q_message_box . warning ( self , string , string string string , q_message_box . ok , number ) return resolved	1
Q_664	go through list of d_ps to make sure that their destination name	C_664_1	def process_user_input ( self ) user_input = self . get_input ( ) try num = int ( user_input ) except exception return if number < num < len ( self . items ) + number self . current_option = num _ number self . select ( ) return user_input	0
Q_664	go through list of d_ps to make sure that their destination name	C_664_2	def ensure_utf8 ( str_or_unicode ) if isinstance ( str_or_unicode , str ) return str_or_unicode elif isinstance ( str_or_unicode , unicode ) return str_or_unicode . encode ( string ) else raise value_error ( string { number } string string . format ( str_or_unicode , type ( str_or_unicode ) ) )	0
Q_665	initialize the logging system and parse the environment variable	C_665_0	def init ( env_var_name , enable_color_output = false ) global _initialized if _initialized return global _env_var_name _env_var_name = env_var_name if enable_color_output _preformat_levels ( env_var_name + string ) else _preformat_levels ( none ) if env_var_name in os . environ set_debug ( os . environ [ env_var_name ] ) add_limited_log_handler ( stderr_handler ) _initialized = true	1
Q_665	initialize the logging system and parse the environment variable	C_665_1	def get_window_from_xy ( self , xy ) a_transform = self . _get_template_for_given_resolution ( res = self . dst_res , return_ = string ) [ string ] row , col = transform . rowcol ( a_transform , xy [ number ] , xy [ number ] ) ij_containing_xy = none for var , var in enumerate ( self . windows ) ( row_start , row_end ) , ( col_start , col_end ) = rasterio . windows . toranges ( var ) if ( ( col > = col_start ) ( col < col_end ) ) ( ( row > = row_start ) ( row < row_end ) ) ij_containing_xy = var break if ij_containing_xy is none raise value_error ( string xy string ) return ij_containing_xy	0
Q_665	initialize the logging system and parse the environment variable	C_665_2	def create_set ( self , project , json_data = none , * * options ) url = url . articlesets . format ( * * locals ( ) ) if json_data is none return self . request ( url , method = string , data = options ) else if not isinstance ( json_data , ( string_types ) ) json_data = json . dumps ( json_data , default = serialize ) headers = { string string } return self . request ( url , method = string , data = json_data , headers = headers )	0
Q_666	return the current shade position	C_666_0	async def get_current_position ( self , refresh = true ) _ > dict if refresh await self . refresh ( ) position = self . _raw_data . get ( attr_position_data ) return position	1
Q_666	return the current shade position	C_666_1	def _eager_expr_from_flat_schema ( flat_schema ) string string string result = [ ] for var , var in flat_schema . items ( ) if var = = joined result . append ( joinedload ( var ) ) elif var = = subquery result . append ( subqueryload ( var ) ) else raise value_error ( string . format ( var , var ) ) return result	0
Q_666	return the current shade position	C_666_2	def trap_ctrl_c_ctrl_break ( ) _ > none string string string signal . signal ( signal . sigint , ctrl_c_trapper ) signal . signal ( signal . sigterm , sigterm_trapper ) if platform . system ( ) = = string signal . signal ( signal . sigbreak , ctrl_break_trapper )	0
Q_667	unwrap an element x by decode and then deserializing	C_667_0	def _unwrap ( x , deserialize_func , decode_func = base64 . urlsafe_b64decode , compress = true ) return deserialize_func ( decode_func ( x ) , compress )	1
Q_667	unwrap an element x by decode and then deserializing	C_667_1	def memset ( self , allocation , value , size ) c . memset ( allocation . ctypes , value , size )	0
Q_667	unwrap an element x by decode and then deserializing	C_667_2	def _meta_collections ( self ) if self . use_single_meta_collection yield self . meta_collection_name else for var in self . meta_database . collection_names ( include_system_collections = false ) yield var	0
Q_668	render_in_table be call to render fit image in a table	C_668_0	"def render_in_table ( self , relpath = string ) cachekey , html = self . check_cache ( string , relpath ) if html is not none return html if len ( self . imgrec ) = = number var = self . imgrec [ number ] html = string html + = self . render_link_comment ( relpath ) or string html + = string html_img , comment = self . _render_image_rec ( var , relpath , include_size = true ) html + = string . join ( [ string , string html_img , string comment , string ] ) else html = string html + = self . render_link_comment ( relpath ) html + = string ( self . cubesize , len ( self . imgrec ) ) for var , var in enumerate ( self . imgrec ) html_img , comment = self . _render_image_rec ( var , relpath ) comment = string string < tr > string < td > s < / td > string < td > s < / td > string < / tr > "" ] ) return self . write_cache ( cachekey , html )"	1
Q_668	render_in_table be call to render fit image in a table	C_668_1	def get_items_by_search ( self , item_query , item_search ) and_list = list ( ) or_list = list ( ) for var in item_query . _query_terms and_list . append ( { var item_query . _query_terms [ var ] } ) for var in item_query . _keyword_terms or_list . append ( { var item_query . _keyword_terms [ var ] } ) if item_search . _id_list is not none identifiers = [ object_id ( i . identifier ) for i in item_search . _id_list ] and_list . append ( { string { string identifiers } } ) if or_list and_list . append ( { string or_list } ) view_filter = self . _view_filter ( ) if view_filter and_list . append ( view_filter ) if and_list query_terms = { string and_list } collection = json_client_validated ( string , collection = string , runtime = self . _runtime ) if item_search . start is not none and item_search . end is not none result = collection . find ( query_terms ) [ item_search . start item_search . end ] else result = collection . find ( query_terms ) return searches . item_search_results ( result , dict ( item_query . _query_terms ) , runtime = self . _runtime )	0
Q_668	render_in_table be call to render fit image in a table	C_668_2	def parse_dei ( self , xbrl , ignore_errors = number ) dei_obj = dei ( ) if ignore_errors = = number logging . basic_config ( filename = string , level = logging . error , format = string ) logger = logging . get_logger ( _name_ ) else logger = none trading_symbol = xbrl . find_all ( name = re . compile ( string , re . ignorecase re . multiline ) ) dei_obj . trading_symbol = self . data_processing ( trading_symbol , xbrl , ignore_errors , logger , options = { string string , string true } ) company_name = xbrl . find_all ( name = re . compile ( string , re . ignorecase re . multiline ) ) dei_obj . company_name = self . data_processing ( company_name , xbrl , ignore_errors , logger , options = { string string , string true } ) shares_outstanding = xbrl . find_all ( name = re . compile ( string , re . ignorecase re . multiline ) ) dei_obj . shares_outstanding = self . data_processing ( shares_outstanding , xbrl , ignore_errors , logger , options = { string string , string true } ) public_float = xbrl . find_all ( name = re . compile ( string , re . ignorecase re . multiline ) ) dei_obj . public_float = self . data_processing ( public_float , xbrl , ignore_errors , logger , options = { string string , string true } ) return dei_obj	0
Q_669	initiate the log module	C_669_0	def init ( * , threshold_lvl = number , quiet_stdout = false , log_file ) global _logger , _log_lvl _log_lvl = _set_lvl ( threshold_lvl ) _logger = logging . get_logger ( pkg_name ) _logger . set_level ( _log_lvl ) log_h = logging . file_handler ( log_file ) base_fmt = string log_fmt = logging . formatter ( base_fmt ) log_h . set_formatter ( log_fmt ) _logger . add_handler ( log_h ) if not quiet_stdout global _stdout _stdout = true	1
Q_669	initiate the log module	C_669_1	"def create_chapter_from_url ( self , url , title = none ) try request_object = requests . get ( url , headers = self . request_headers , allow_redirects = false ) except ( requests . exceptions . missing_schema , requests . exceptions . connection_error ) raise value_error ( string url ) except requests . exceptions . ssl_error raise value_error ( string t have valid ssl certificate "" url ) unicode_string = request_object . text return self . create_chapter_from_string ( unicode_string , url , title )"	0
Q_669	initiate the log module	C_669_2	def incremental_value ( self , slip_moment , mmax , mag_value , bbar , dbar ) delta_m = mmax _ mag_value a_3 = self . _get_a3 ( bbar , dbar , slip_moment , mmax ) return a_3 * bbar * ( np . exp ( bbar * delta_m ) _ number . number ) * ( delta_m > number . number )	0
Q_670	it obtain the image in another region	C_670_0	def get_image_id_another_region ( glance_url , image_name , token ) url = glance_url + string headers = { string string , string token } try response = requests . get ( url , headers = headers ) response_json = response . json ( ) for var in response_json [ string ] if var [ string ] = = image_name return var [ string ] return none except exception as e return none	1
Q_670	it obtain the image in another region	C_670_1	def get_instances ( cluster , environ , topology , role = none ) params = dict ( cluster = cluster , environ = environ , topology = topology ) if role is not none params [ string ] = role request_url = tornado . httputil . url_concat ( create_url ( physicalplan_url_fmt ) , params ) pplan = yield fetch_url_as_json ( request_url ) instances = pplan [ string ] . keys ( ) raise tornado . gen . return ( instances )	0
Q_670	it obtain the image in another region	C_670_2	def fatal ( * tokens token , * * kwargs any ) _ > none error ( * tokens , * * kwargs ) sys . exit ( number )	0
Q_671	define the slot trigger by framework startup	C_671_0	def on_startup ( self ) logger . debug ( string { number } string on_startup string . format ( self . _class_ . _name_ ) ) self . refresh_nodes . emit ( ) return true	1
Q_671	define the slot trigger by framework startup	C_671_1	def _ip_setter ( self , ipaddr_name , ipaddrs_name , ips ) if isinstance ( ips , six . string_types ) setattr ( self , ipaddr_name , ips ) elif isinstance ( ips , ( list , tuple ) ) and isinstance ( ips [ number ] , ip ) setattr ( self , ipaddr_name , ips [ number ] . ip ) setattr ( self , ipaddrs_name , ips ) elif isinstance ( ips , ip ) setattr ( self , ipaddr_name , ips . ip ) setattr ( self , ipaddrs_name , [ ips ] ) elif ips is none setattr ( self , ipaddr_name , none ) setattr ( self , ipaddrs_name , none ) else raise value_error ( string string ips )	0
Q_671	define the slot trigger by framework startup	C_671_2	def get_artist_by_mbid ( self , mbid ) params = { string mbid } doc = _request ( self , string , params ) . execute ( true ) return artist ( _extract ( doc , string ) , self )	0
Q_672	show all define beamlines	C_672_0	def show_beamlines ( self ) cnt = number blidlist = [ ] for var in self . all_elements try if string in self . all_elements . get ( var ) cnt + = number blidlist . append ( var ) except pass retstr = string . format ( total = cnt , allbl = string . join ( blidlist ) ) return retstr	1
Q_672	show all define beamlines	C_672_1	def get_assets ( self ) return github . paginated_list . paginated_list ( github . git_release_asset . git_release_asset , self . _requester , self . url + string , none )	0
Q_672	show all define beamlines	C_672_2	def _iter_branch ( self , node ) if node = = blank_node raise stop_iteration node_type = self . _get_node_type ( node ) if is_key_value_type ( node_type ) nibbles = key_nibbles_from_key_value_node ( node ) key = b string . join ( [ to_string ( var ) for var in nibbles ] ) if node_type = = node_type_extension sub_tree = self . _iter_branch ( self . _get_inner_node_from_extension ( node ) ) else sub_tree = [ ( to_string ( nibble_terminator ) , node [ number ] ) ] for sub_key , sub_value in sub_tree full_key = ( key + b string + sub_key ) . strip ( b string ) yield ( full_key , sub_value ) elif node_type = = node_type_branch for i in range ( number ) sub_tree = self . _iter_branch ( self . _decode_to_node ( node [ i ] ) ) for sub_key , sub_value in sub_tree full_key = ( str_to_bytes ( str ( i ) ) + b string + sub_key ) . strip ( b string ) yield ( full_key , sub_value ) if node [ number ] yield ( to_string ( nibble_terminator ) , node [ _ number ] )	0
Q_673	process the current token	C_673_0	def _process ( self , name ) if self . token . nature = = name self . token = self . lexer . next_token ( ) else self . _error ( )	1
Q_673	process the current token	C_673_1	def max ( self , values , axis = number ) values = np . asarray ( values ) return self . unique , self . reduce ( values , np . maximum , axis )	0
Q_673	process the current token	C_673_2	def find_general_mappings ( es_major_version ) if es_major_version not in es_supported print ( string ( es_major_version , es_supported ) ) sys . exit ( number ) if es_major_version = = string not_analyze_strings = else not_analyze_strings = return json . loads ( not_analyze_strings )	0
Q_674	return the last n row in the table	C_674_0	def get_last_row ( dbconn , tablename , n = number , uuid = none ) return fetch ( dbconn , tablename , n , uuid , end = true )	1
Q_674	return the last n row in the table	C_674_1	def _log ( self , level , msg , * args , * * kwargs ) if not isinstance ( level , int ) if logging . raise_exceptions raise type_error ( string ) else return if self . logger . is_enabled_for ( level = level ) exc_info = kwargs . get ( string , none ) extra = kwargs . get ( string , none ) stack_info = kwargs . get ( string , false ) record_filter = kwargs . get ( string , none ) tb_info = none if _logone_src try fn , lno , func , tb_info = self . _find_caller ( stack_info = stack_info ) except value_error fn , lno , func = string , number , string else fn , lno , func = string , number , string if exc_info if sys . version_info [ number ] > = number if isinstance ( exc_info , base_exception ) exc_info = type ( exc_info ) , exc_info , exc_info . _traceback_ elif not isinstance ( exc_info , tuple ) exc_info = sys . exc_info ( ) else if not isinstance ( exc_info , tuple ) exc_info = sys . exc_info ( ) if sys . version_info [ number ] > = number record = self . logger . make_record ( self . name , level , fn , lno , msg , args , exc_info , func , extra , tb_info ) else record = self . logger . make_record ( self . name , level , fn , lno , msg , args , exc_info , func , extra ) if record_filter record = record_filter ( record ) self . logger . handle ( record = record )	0
Q_674	return the last n row in the table	C_674_2	def determine_plasma_store_config ( object_store_memory = none , plasma_directory = none , huge_pages = false ) system_memory = ray . utils . get_system_memory ( ) if object_store_memory is none object_store_memory = int ( system_memory * number . number ) if ( object_store_memory > ray_constants . default_object_store_max_memory_bytes ) logger . warning ( string . format ( ray_constants . default_object_store_max_memory_bytes / / number ) + string string ) object_store_memory = ( ray_constants . default_object_store_max_memory_bytes ) if plasma_directory is none if sys . platform = = string or sys . platform = = string shm_avail = ray . utils . get_shared_memory_bytes ( ) if shm_avail > object_store_memory plasma_directory = string else plasma_directory = string logger . warning ( string string string string string string string _ shm _ size string docker run string . format ( shm_avail ) ) else plasma_directory = string if object_store_memory > system_memory raise exception ( string string ) else plasma_directory = os . path . abspath ( plasma_directory ) logger . warning ( string string ) if not os . path . isdir ( plasma_directory ) raise exception ( string . format ( plasma_directory ) ) return object_store_memory , plasma_directory	0
Q_675	check if a give user exist	C_675_0	def user_exists ( self , name ) users = self . data [ string ] for var in users if var [ string ] = = name return true return false	1
Q_675	check if a give user exist	C_675_1	def get_requires_for_build_sdist ( config_settings ) backend = _build_backend ( ) try hook = backend . get_requires_for_build_sdist except attribute_error return [ ] else return hook ( config_settings )	0
Q_675	check if a give user exist	C_675_2	def user_parse ( data ) yield string , data . get ( string ) yield string , data . get ( string ) first_name , _ , last_name = ( data . get ( string ) or string ) . partition ( string ) yield string , first_name yield string , last_name yield string , data . get ( string ) yield string , data . get ( string ) yield string , data . get ( string ) location = data . get ( string , string ) if location split_location = location . split ( string ) yield string , split_location [ number ] . strip ( ) if len ( split_location ) > number yield string , split_location [ number ] . strip ( )	0
Q_676	set metapolicy to permit	C_676_0	def metapolicy ( self , permitted ) if permitted not in valid_site_control raise type_error ( site_control_error . format ( permitted ) ) if permitted = = site_control_none self . domains = { } self . header_domains = { } self . identities = [ ] self . site_control = permitted	1
Q_676	set metapolicy to permit	C_676_1	def unpack_class ( data , magic = none ) with unpack ( data ) as up magic = magic or up . unpack_struct ( _bbbb ) if magic = java_class_magic raise class_unpack_exception ( string ) o = java_class_info ( ) o . unpack ( up , magic = magic ) return o	0
Q_676	set metapolicy to permit	C_676_2	def get ( self , var ) if self . closed raise runtime_error ( string . format ( self ) ) self . stats [ var ] + = number if isinstance ( var , tuple ) and len ( var ) = = number if var [ number ] = = partial fn , a , kw_items = var [ number ] return self . partial ( fn , * a , * * dict ( kw_items ) ) elif var [ number ] = = partial_regardless fn , a , kw_items = var [ number ] return self . partial_regardless ( fn , * a , * * dict ( kw_items ) ) elif var [ number ] = = eager_partial fn , a , kw_items = var [ number ] return self . eager_partial ( fn , * a , * * dict ( kw_items ) ) elif var [ number ] = = eager_partial_regardless fn , a , kw_items = var [ number ] return self . eager_partial_regardless ( fn , * a , * * dict ( kw_items ) ) basenote , name = self . parse_note ( var ) if name is none and basenote in self . values return self . values [ basenote ] try provider_factory = self . lookup ( basenote ) except lookup_error msg = string { } string raise lookup_error ( msg . format ( var ) ) self . instantiating . append ( ( basenote , name ) ) try if self . instantiating . count ( ( basenote , name ) ) > number stack = string . join ( repr ( var ) for var in self . instantiating ) notes = tuple ( self . instantiating ) raise dependency_cycle_error ( stack , notes = notes ) return self . handle_provider ( provider_factory , var ) finally self . instantiating . pop ( )	0
Q_677	set the identifier text on the identifier_lb	C_677_0	def set_identifiertext ( self , index ) dr = qt_core . qt . display_role t = index . model ( ) . index ( index . row ( ) , number , index . parent ( ) ) . data ( dr ) if t is none t = _ number else t = t + number self . identifier_lb . set_text ( string t )	1
Q_677	set the identifier text on the identifier_lb	C_677_1	def resolve_symbol ( self , symbol , b_case_sensitive = false ) if b_case_sensitive for ( symbol_name , symbol_address , symbol_size ) in self . iter_symbols ( ) if symbol = = symbol_name return symbol_address for ( symbol_name , symbol_address , symbol_size ) in self . iter_symbols ( ) try symbol_name = win32 . un_decorate_symbol_name ( symbol_name ) except exception continue if symbol = = symbol_name return symbol_address else symbol = symbol . lower ( ) for ( symbol_name , symbol_address , symbol_size ) in self . iter_symbols ( ) if symbol = = symbol_name . lower ( ) return symbol_address for ( symbol_name , symbol_address , symbol_size ) in self . iter_symbols ( ) try symbol_name = win32 . un_decorate_symbol_name ( symbol_name ) except exception continue if symbol = = symbol_name . lower ( ) return symbol_address	0
Q_677	set the identifier text on the identifier_lb	C_677_2	def is_visible ( self ) if self . _start . y + number < self . _stop . y return true return false	0
Q_678	iterate over gff3 file return gff3 entry	C_678_0	"def iterate ( self , start_line = none , parse_attr = true , headers = false , comments = false ) handle = self . handle split = str . split strip = str . strip if start_line is none line = next ( handle ) else line = start_line if ( isinstance ( line , bytes ) ) def next_line ( i ) return next ( i ) . decode ( string ) line = strip ( line . decode ( string ) ) else next_line = next line = strip ( line ) try while true self . current_line + = number data = gff3_entry ( ) if line . startswith ( string line = strip ( next_line ( handle ) ) continue elif line . startswith ( string line = strip ( next_line ( handle ) ) continue elif line . startswith ( string t string string = string , string , string "" data . attributes [ key ] = value line = strip ( next_line ( handle ) ) yield data except stop_iteration if data . origline yield data else pass except fasta_found pass"	1
Q_678	iterate over gff3 file return gff3 entry	C_678_1	def set ( self , key , value ) if not value = = none self . prefs [ key ] = value else self . prefs . pop ( key ) self . dump ( )	0
Q_678	iterate over gff3 file return gff3 entry	C_678_2	def cp2png ( checkplotin , extrarows = none ) if checkplotin . endswith ( string ) outfile = checkplotin . replace ( string , string ) else outfile = checkplotin . replace ( string , string ) return checkplot_pickle_to_png ( checkplotin , outfile , extrarows = extrarows )	0
Q_679	add a parameter to this app	C_679_0	def add_argument ( self , * args , * * kwargs ) if not ( ( string in kwargs ) and ( kwargs [ string ] = = string ) ) try name = kwargs [ string ] param_type = kwargs [ string ] optional = kwargs [ string ] except key_error as e detail = string e raise key_error ( detail ) if optional and ( string not in kwargs ) detail = string name raise key_error ( detail ) default = none if string in kwargs default = kwargs [ string ] param_help = string if string in kwargs param_help = kwargs [ string ] if param_type not in ( str , int , float , bool , chris_app . path ) detail = string s string param_type raise value_error ( detail ) action = string if param_type = = bool action = string if default else string del kwargs [ string ] del kwargs [ string ] kwargs [ string ] = action param = { string name , string param_type . _name_ , string optional , string args [ number ] , string action , string param_help , string default } self . _parameters . append ( param ) del kwargs [ string ] argument_parser . add_argument ( self , * args , * * kwargs )	1
Q_679	add a parameter to this app	C_679_1	def onpick ( self , event ) if hasattr ( event . artist , string ) idx = event . artist . _mt_legend_item try self . toggle_artist ( self . artists [ idx ] ) except index_error pass return if not event . artist . get_visible ( ) return plot_type = event . artist . _mt_plot_type plot_type . clicked ( event )	0
Q_679	add a parameter to this app	C_679_2	def shortcut_exists ( self , name ) self . db_query ( string string string , ( name , ) ) pth = self . db_fetch_one ( ) if pth is none or len ( pth ) = = number return false else return true	0
Q_680	terminate a process bound to this context	C_680_0	def terminate ( self , pid ) self . _assert_started ( ) log . info ( string pid ) process = self . _processes . pop ( pid , none ) if process log . info ( string process ) self . http . unmount_process ( process ) self . _erase_link ( pid )	1
Q_680	terminate a process bound to this context	C_680_1	def _as_json ( self , response ) content = response . text ( ) if hasattr ( response , string ) else response . text try return json . loads ( content ) except value_error raise deserialization_error ( string )	0
Q_680	terminate a process bound to this context	C_680_2	"def _handle_ctrl_c ( self , * args ) if self . anybar self . anybar . change ( string ) if self . _stop print ( string ) raise system_exit if not self . _stop hline = number * string print ( string + hline + string string re in hurry "" + hline ) self . _stop = true"	0
Q_681	return the header data	C_681_0	def header_data ( self , section , orientation , role ) if orientation = = qt_core . qt . horizontal d = self . _root . data ( section , role ) if d is none and role = = qt_core . qt . display_role return str ( section + number ) return d if orientation = = qt_core . qt . vertical and role = = qt_core . qt . display_role return str ( section + number )	1
Q_681	return the header data	C_681_1	def get_line_data ( self ) if self . branch line_data = { } for var , var in self . data . items ( ) line_data [ var ] = ldf = { } for l1 , _ in list ( var . keys ( ) ) if l1 ldf [ l1 ] = none return line_data else return self . data	0
Q_681	return the header data	C_681_2	def do_array ( self , parent = none , ident = number ) log_debug ( string , ident ) var , classdesc = self . _read_and_exec_opcode ( ident = ident + number , expect = ( self . tc_classdesc , self . tc_proxyclassdesc , self . tc_null , self . tc_reference , ) , ) array = java_array ( classdesc ) self . _add_reference ( array , ident ) ( size , ) = self . _read_struct ( string ) log_debug ( string . format ( size ) , ident ) type_char = classdesc . name [ number ] assert type_char = = self . type_array type_char = classdesc . name [ number ] if type_char = = self . type_object or type_char = = self . type_array for var in range ( size ) var , res = self . _read_and_exec_opcode ( ident = ident + number ) log_debug ( string . format ( res ) , ident ) array . append ( res ) elif type_char = = self . type_byte array = java_byte_array ( self . object_stream . read ( size ) , classdesc ) elif self . use_numpy_arrays import numpy array = numpy . fromfile ( self . object_stream , dtype = java_object_constants . numpy_type_map [ type_char ] , count = size , ) else for var in range ( size ) res = self . _read_value ( type_char , ident ) log_debug ( string . format ( repr ( res ) ) , ident ) array . append ( res ) return array	0
Q_682	return a representation of the id map as a string	C_682_0	def write_string ( self ) map_as_str = [ ] for var , var in self . _map . items ( ) for id_range in var map_as_str . append ( var + string + str ( id_range . first ) + string + str ( id_range . count ) + string ) if len ( map_as_str ) > number map_as_str [ _ number ] = map_as_str [ _ number ] [ _ number ] return string . join ( map_as_str )	1
Q_682	return a representation of the id map as a string	C_682_1	def get_item_form_for_create ( self , item_record_types ) for var in item_record_types if not isinstance ( var , abc_type ) raise errors . invalid_argument ( string ) if item_record_types = = [ ] obj_form = objects . item_form ( bank_id = self . _catalog_id , runtime = self . _runtime , effective_agent_id = self . get_effective_agent_id ( ) , proxy = self . _proxy ) else obj_form = objects . item_form ( bank_id = self . _catalog_id , record_types = item_record_types , runtime = self . _runtime , effective_agent_id = self . get_effective_agent_id ( ) , proxy = self . _proxy ) self . _forms [ obj_form . get_id ( ) . get_identifier ( ) ] = not created return obj_form	0
Q_682	return a representation of the id map as a string	C_682_2	def download ( self ) if os . path . isfile ( self . archive_path ) print ( string self . archive_path ) with open ( self . archive_path , string ) as f content = f . read ( ) else print ( string self . url ) f = urlopen ( self . url ) content = f . read ( ) with open ( self . archive_path , string ) as out_file out_file . write ( content ) current_sha1 = hashlib . sha1 ( content ) . hexdigest ( ) assert current_sha1 = = self . download_sha1 , string current_sha1 print ( string current_sha1 )	0
Q_683	compare the message and input signature and output what the	C_683_0	def verifymsg ( msg , sig ) msg = msg . replace ( string , string ) msg1 = hexstrlify ( bytearray ( string , string ) ) msg2 = tovarint ( len ( msg ) ) msg3 = hexstrlify ( bytearray ( msg , string ) ) msg = hash256 ( msg1 + msg2 + msg3 ) sig = hexstrlify ( base64 . b64decode ( sig ) ) r = int ( sig [ number number ] , number ) s = int ( sig [ number ] , number ) prefix = int ( sig [ number ] , number ) if prefix > number out_compressed = true prefix = prefix _ number else out_compressed = false prefix = prefix _ number m = int ( n * prefix ) if prefix > number else number x = ( r + int ( m / / number ) ) n a = ( pow_mod ( x , number , p ) + number ) p b = pow_mod ( a , ( ( p + number ) / / number ) , p ) if ( b number ) = prefix y = ( _ b p ) else y = b x , y = dechex ( x , number ) , dechex ( y , number ) pubkey = string + x + y negative_msg = dechex ( ( n _ int ( msg , number ) ) , number ) modinv_r = dechex ( modinv ( r , n ) , number ) pubkey = multiplypub ( addpubs ( multiplypub ( pubkey , dechex ( s , number ) , false ) , privtopub ( negative_msg , false ) , false ) , modinv_r , false ) if out_compressed pubkey = compress ( pubkey ) return strlify ( pubkey )	1
Q_683	compare the message and input signature and output what the	C_683_1	def random_id ( length = number , charset = alphanum_chars , first_charset = alpha_chars , sep = string , group = number ) string string string string t = [ ] first_chars = list ( set ( charset ) . intersection ( first_charset ) ) if len ( first_chars ) = = number first_chars = charset t . append ( first_chars [ random . randrange ( len ( first_chars ) ) ] ) for var in range ( len ( t ) , length ) if ( group > number ) and ( var group = = number ) and ( var < length ) t . append ( sep ) t . append ( charset [ random . randrange ( len ( charset ) ) ] ) return string . join ( t )	0
Q_683	compare the message and input signature and output what the	C_683_2	def create_ca_signed_cert ( ca_name , cn , days = number , cacert_path = none , ca_filename = none , cert_path = none , cert_filename = none , digest = string , cert_type = none , type_ext = false , replace = false ) ret = { } set_ca_path ( cacert_path ) if not ca_filename ca_filename = string . format ( ca_name ) if not cert_path cert_path = string . format ( cert_base_path ( ) , ca_name ) if type_ext if not cert_type log . error ( string string ) return ret elif cert_type cn_ext = string . format ( cert_type ) else cn_ext = string csr_filename = string . format ( cn , cn_ext ) if not cert_filename cert_filename = string . format ( cn , cn_ext ) if not replace and os . path . exists ( os . path . join ( os . path . sep . join ( string . format ( cert_base_path ( ) , ca_name , cert_filename ) . split ( string ) ) ) ) return string { number } string . format ( cert_filename ) try maybe_fix_ssl_version ( ca_name , cacert_path = cacert_path , ca_filename = ca_filename ) with salt . utils . files . fopen ( string . format ( cert_base_path ( ) , ca_name , ca_filename ) ) as fhr ca_cert = open_ssl . crypto . load_certificate ( open_ssl . crypto . filetype_pem , fhr . read ( ) ) with salt . utils . files . fopen ( string . format ( cert_base_path ( ) , ca_name , ca_filename ) ) as fhr ca_key = open_ssl . crypto . load_privatekey ( open_ssl . crypto . filetype_pem , fhr . read ( ) ) except io_error ret [ string ] = number ret [ string ] = string { number } string . format ( ca_name ) return ret try csr_path = string . format ( cert_path , csr_filename ) with salt . utils . files . fopen ( csr_path ) as fhr req = open_ssl . crypto . load_certificate_request ( open_ssl . crypto . filetype_pem , fhr . read ( ) ) except io_error ret [ string ] = number ret [ string ] = string { number } string . format ( cert_filename ) return ret exts = [ ] try exts . extend ( req . get_extensions ( ) ) except attribute_error try log . info ( string string string , open_ssl_version ) native_exts_obj = open_ssl . _util . lib . x509_req_get_extensions ( req . _req ) for var in _range ( open_ssl . _util . lib . sk_x509_extension_num ( native_exts_obj ) ) ext = open_ssl . crypto . x509_extension . _new_ ( open_ssl . crypto . x509_extension ) ext . _extension = open_ssl . _util . lib . sk_x509_extension_value ( native_exts_obj , var ) exts . append ( ext ) except exception log . error ( string string string , open_ssl_version ) cert = open_ssl . crypto . x509 ( ) cert . set_version ( number ) cert . set_subject ( req . get_subject ( ) ) cert . gmtime_adj_not_before ( number ) cert . gmtime_adj_not_after ( int ( days ) * number * number * number ) cert . set_serial_number ( _new_serial ( ca_name ) ) cert . set_issuer ( ca_cert . get_subject ( ) ) cert . set_pubkey ( req . get_pubkey ( ) ) cert . add_extensions ( exts ) cert . sign ( ca_key , salt . utils . stringutils . to_str ( digest ) ) cert_full_path = string . format ( cert_path , cert_filename ) with salt . utils . files . fopen ( cert_full_path , string ) as crt crt . write ( salt . utils . stringutils . to_bytes ( open_ssl . crypto . dump_certificate ( open_ssl . crypto . filetype_pem , cert ) ) ) _write_cert_to_database ( ca_name , cert ) return string { number } string { number } / { number } . crt string . format ( cn , cert_path , cert_filename )	0
Q_684	renvoie un caract re repr sentant la cat gorie du lemme	C_684_0	def pos ( self ) if not self . _pos and self . _renvoi lr = self . _lemmatiseur . lemme ( self . _renvoi ) if lr return lr . pos ( ) return self . _pos	1
Q_684	renvoie un caract re repr sentant la cat gorie du lemme	C_684_1	def compute_laplacian ( self , lap_type = string ) r if lap_type = self . lap_type self . _lmax = none self . _u = none self . _e = none self . _coherence = none self . _d = none self . lap_type = lap_type if not self . is_directed ( ) w = self . w else w = utils . symmetrize ( self . w , method = string ) if lap_type = = string d = sparse . diags ( self . dw ) self . l = d _ w elif lap_type = = string d = np . zeros ( self . n_vertices ) disconnected = ( self . dw = = number ) np . power ( self . dw , _ number . number , where = disconnected , out = d ) d = sparse . diags ( d ) self . l = sparse . identity ( self . n_vertices ) _ d * w * d self . l [ disconnected , disconnected ] = number self . l . eliminate_zeros ( ) else raise value_error ( string . format ( lap_type ) )	0
Q_684	renvoie un caract re repr sentant la cat gorie du lemme	C_684_2	def _trace ( self , frame , event , arg_unused ) cur_time = time . time ( ) lineno = frame . f_lineno depth = self . depth filename = inspect . getfile ( frame ) if self . last_exc_back if frame = = self . last_exc_back self . data [ string ] + = ( cur_time _ self . start_time ) self . depth _ = number self . data = self . data_stack . pop ( ) self . last_exc_back = none if event = = string self . depth + = number if self . log print sys . stdout , string ( string * ( depth _ number ) , filename , frame . f_code . co_name ) o_lineno = frame . f_back . f_lineno if self . pause_until is not none if depth = = self . pause_until self . pause_until = none else return self . _trace if o_lineno not in self . data [ string ] self . pause_until = depth return self . _trace self . data_stack . append ( self . data ) call_sig = string ( inspect . getfile ( frame ) , frame . f_code . co_name ) if call_sig not in self . data [ string ] self . data [ string ] [ o_lineno ] [ call_sig ] = self . _get_struct ( frame , event ) self . data = self . data [ string ] [ o_lineno ] [ call_sig ] self . data [ string ] + = number elif event = = string if self . pause_until is none and lineno in self . data [ string ] self . data [ string ] [ lineno ] [ string ] + = number self . data [ string ] [ lineno ] [ string ] + = ( cur_time _ self . start_time ) if self . log print sys . stdout , string ( string * ( depth _ number ) , filename , frame . f_code . co_name , lineno ) elif event = = string timing = ( cur_time _ self . start_time ) if self . pause_until is none self . data [ string ] + = timing self . data = self . data_stack . pop ( ) self . data [ string ] + = timing if self . log print sys . stdout , string ( string * ( depth _ number ) , filename , frame . f_code . co_name , timing ) self . depth _ = number elif event = = string self . last_exc_back = frame . f_back self . last_exc_firstlineno = frame . f_code . co_firstlineno return self . _trace	0
Q_685	perform a preprocess pas of the table to attempt naive conversion of data and to record	C_685_0	def preprocess_worksheet ( self , table , worksheet ) table_conversion = [ ] flags = { } units = { } for var , var in enumerate ( table ) conversion_row = [ ] table_conversion . append ( conversion_row ) if self . skippable_rows and worksheet in self . skippable_rows and var in self . skippable_rows [ worksheet ] self . flag_change ( flags , string , ( var , none ) , worksheet , self . flags [ string ] ) continue for cind , cell in enumerate ( var ) position = ( var , cind ) if self . skippable_columns and worksheet in self . skippable_columns and cind in self . skippable_columns [ worksheet ] conversion = none self . flag_change ( flags , string , position , worksheet , self . flags [ string ] ) else conversion = auto_convert_cell ( self , cell , position , worksheet , flags , units , parens_as_neg = self . parens_as_neg ) conversion_row . append ( conversion ) return table_conversion , flags , units	1
Q_685	perform a preprocess pas of the table to attempt naive conversion of data and to record	C_685_1	def r_get_numbers ( matchgroup , num ) res = [ ] for var in range ( num ) res . append ( float ( matchgroup . next ( ) . group ( ) ) ) return np . array ( res )	0
Q_685	perform a preprocess pas of the table to attempt naive conversion of data and to record	C_685_2	def valid ( self ) if self . expiration_time return self . expiration_time > int ( time . time ( ) ) else return true	0
Q_686	create and write to disk a default site config file	C_686_0	def _create_default_config ( self ) cfg = { string string , string string , string string , string string , string string , string string , string string } file_name = os . path . join ( self . _dirs [ string ] , string ) f = open ( file_name , string ) f . write ( yaml . dump ( cfg , default_flow_style = false ) ) f . close ( ) return cfg	1
Q_686	create and write to disk a default site config file	C_686_1	def make_declarative_base ( self , model , metadata = none ) if not isinstance ( model , declarative_meta ) model = declarative_base ( cls = model , name = string , metadata = metadata , metaclass = default_meta ) if metadata is not none and model . metadata is not metadata model . metadata = metadata if not getattr ( model , string , none ) model . query_class = self . query model . query = _query_property ( self ) return model	0
Q_686	create and write to disk a default site config file	C_686_2	"async def get_postcode ( postcode_like post_code_like ) _ > optional [ postcode ] if isinstance ( postcode_like , postcode ) return postcode_like postcode_like = postcode_like . replace ( string , string ) . upper ( ) try postcode = postcode . get ( postcode . postcode = = postcode_like ) except does_not_exist try postcode = await fetch_postcode_from_string ( postcode_like ) except ( api_error , circuit_breaker_error ) raise caching_error ( f string t be retrieved . "" ) if postcode is not none postcode . save ( ) return postcode"	0
Q_687	make membership request to multicast	C_687_0	def _add_membership_multicast_socket ( self ) self . _membership_request = socket . inet_aton ( self . _multicast_group ) + socket . inet_aton ( self . _multicast_ip ) self . _multicast_socket . setsockopt ( socket . ipproto_ip , socket . ip_add_membership , self . _membership_request )	1
Q_687	make membership request to multicast	C_687_1	def get_property ( self , key ) if not isinstance ( key , basestring ) raise type_error ( string ) value = self . _call ( string , in_p = [ key ] ) return value	0
Q_687	make membership request to multicast	C_687_2	def formalised_address ( self ) try if self . _data_from_search t = self . _data_from_search . find ( string ) . contents [ number ] else t = self . _ad_page_content . find ( string , { string string } ) . find ( string ) . text . strip ( ) except exception as e if self . _debug logging . error ( string + e . args [ number ] ) return s = t . split ( string ) a = s [ number ] . strip ( ) if string in a a = a . split ( ) a = a [ number ] a = string . join ( [ str ( var ) for var in a ] ) return a . lower ( ) . title ( ) . strip ( )	0
Q_688	get an option base on an namespace and an option name	C_688_0	def get_option ( namespace , option_name ) envvalue = _get_env_value ( namespace , option_name ) idx = number final_value = [ ] if envvalue final_value . append ( envvalue ) while _get_env_value ( namespace , option_name , idx ) final_value . append ( _get_env_value ( namespace , option_name , idx ) ) idx + = number return final_value	1
Q_688	get an option base on an namespace and an option name	C_688_1	def _collapse_pc ( master , current , dsn , pc ) logger_ts . info ( string ) _table_name , _variable_name = _get_current_names ( current , dsn , pc ) try _m = re . match ( re_sheet_w_number , _table_name ) _switch = { string string , string string , string string } _ms = _switch [ current [ string ] ] if _ms = = string if _table_name not in master [ dsn ] [ pc ] [ _m . group ( number ) ] [ _ms ] _tmp_table = _collapse_table_root ( current , dsn , pc ) master [ dsn ] [ pc ] [ _m . group ( number ) ] [ _ms ] [ _table_name ] = _tmp_table _tmp_column = _collapse_column ( current , pc ) master [ dsn ] [ pc ] [ _m . group ( number ) ] [ _ms ] [ _table_name ] [ string ] [ _variable_name ] = _tmp_column elif _ms in [ string , string ] if _table_name not in master [ dsn ] [ pc ] [ _m . group ( number ) ] [ string ] [ _m . group ( number ) + _m . group ( number ) ] [ _ms ] _tmp_table = _collapse_table_root ( current , dsn , pc ) master [ dsn ] [ pc ] [ _m . group ( number ) ] [ string ] [ _m . group ( number ) + _m . group ( number ) ] [ _ms ] [ _table_name ] = _tmp_table _tmp_column = _collapse_column ( current , pc ) master [ dsn ] [ pc ] [ _m . group ( number ) ] [ string ] [ _m . group ( number ) + _m . group ( number ) ] [ _ms ] [ _table_name ] [ string ] [ _variable_name ] = _tmp_column except exception as e print ( string . format ( dsn , e ) ) logger_ts . error ( string . format ( dsn , _variable_name , e ) ) return master	0
Q_688	get an option base on an namespace and an option name	C_688_2	def any ( self , values , axis = number ) values = np . asarray ( values ) if not values . dtype = = np . bool values = values = number return self . unique , self . reduce ( values , axis = axis ) > number	0
Q_689	offer to push change if need	C_689_0	def _push ( self ) push_cmds = self . vcs . push_commands ( ) if not push_cmds return if utils . ask ( string ) for var in push_cmds output = utils . system ( var ) logger . info ( output )	1
Q_689	offer to push change if need	C_689_1	def _fake_getinstance ( self , namespace , * * params ) iname = params [ string ] if iname . namespace is none iname . namespace = namespace self . _validate_namespace ( namespace ) if not self . _repo_lite if not self . _class_exists ( iname . classname , namespace ) raise cim_error ( cim_err_invalid_class , _format ( string string , iname . classname , iname ) ) inst = self . _get_instance ( iname , namespace , params [ string ] , params [ string ] , params [ string ] , params [ string ] ) return self . _make_tuple ( [ inst ] )	0
Q_689	offer to push change if need	C_689_2	def ask_exit ( self ) self . exit_now = true payload = dict ( source = string , exit = true , keepkernel = self . keepkernel_on_exit , ) self . payload_manager . write_payload ( payload )	0
Q_690	retrieve any constraint or key across one or more column	C_690_0	"def get_constraints ( self , cursor , table_name ) constraints = { } cursor . execute ( select kc . constraint_name , kc . column_name from information_schema . constraint_column_usage as kc join information_schema . table_constraints as c on kc . table_schema = c . table_schema and kc . table_name = c . table_name and kc . constraint_name = c . constraint_name where c . constraint_type = string and kc . table_schema = s and kc . table_name = s select c2 . relname , array ( select ( select attname from pg_catalog . pg_attribute where attnum = i and attrelid = c . oid ) from unnest ( idx . indkey ) i ) , idx . indisunique , idx . indisprimary from pg_catalog . pg_class c , pg_catalog . pg_class c2 , pg_catalog . pg_index idx , pg_catalog . pg_namespace n where c . oid = idx . indrelid and idx . indexrelid = c2 . oid and c . relnamespace = n . oid and n . nspname = s and c . relname = s string string columns string primary_key string unique string foreign_key string check string index "" true , } return constraints"	1
Q_690	retrieve any constraint or key across one or more column	C_690_1	def instance_query_movie_ids ( self ) _ > list [ str ] completions_with_desc = [ ] for var in utils . natural_sort ( self . movie_database_ids ) if var in self . movie_database movie_entry = self . movie_database [ var ] completions_with_desc . append ( argparse_completer . completion_item ( var , movie_entry [ string ] ) ) self . matches_sorted = true return completions_with_desc	0
Q_690	retrieve any constraint or key across one or more column	C_690_2	def load ( self , page = none , verbose = false ) url = self . _build_query_url ( page , verbose ) response = self . _load ( url , verbose ) response = self . _post_load ( response , verbose ) return response	0
Q_691	input must be hex string and a valid compress public key	C_691_0	def uncompress ( pub ) yp = int ( pub [ number ] , number ) _ number x = int ( pub [ number ] , number ) a = ( pow_mod ( x , number , p ) + number ) p y = pow_mod ( a , ( p + number ) / / number , p ) if y number = yp y = _ y p x = dechex ( x , number ) y = dechex ( y , number ) return string + x + y	1
Q_691	input must be hex string and a valid compress public key	C_691_1	def make_extension_login_method ( extension_key ) def _do_login ( soap_stub ) si = vim . service_instance ( string , soap_stub ) sm = si . content . session_manager if not sm . current_session si . content . session_manager . login_extension_by_certificate ( extension_key ) return _do_login	0
Q_691	input must be hex string and a valid compress public key	C_691_2	def clean_values ( self ) for var in self . rows for index , cell in enumerate ( var ) self . set_cell_value ( var , index , self . clean_value ( self . get_cell_value ( cell ) ) ) return self	0
Q_692	expression _ x_path compile expression	C_692_0	def evaluate ( self , expression , processor_nss = none ) from ft . xml import x_path if not processor_nss context = x_path . context . context ( self . node , processor_nss = self . processor_nss ) else context = x_path . context . context ( self . node , processor_nss = processor_nss ) nodes = expression . evaluate ( context ) return map ( lambda node element_proxy ( self . sw , node ) , nodes )	1
Q_692	expression _ x_path compile expression	C_692_1	def _set_process_restart ( self , v , load = false ) if hasattr ( v , string ) v = v . _utype ( v ) try t = yang_dyn_class ( v , base = process_restart . process_restart , is_container = string , presence = false , yang_name = string , rest_name = string , parent = self , path_helper = self . _path_helper , extmethods = self . _extmethods , register_paths = true , extensions = { u string { u string u string , u string u string number string number string number string number string number string , u string none , u string none , u string u string } } , namespace = string , defining_module = string , yang_type = string , is_config = true ) except ( type_error , value_error ) raise value_error ( { string string string string , string string , string string string container string process _ restart string process _ restart string tailf _ common string info string enable process restart for fault recovery string display _ when string ( ( / local _ node / swbd _ number = string ) or ( / local _ node / swbd _ number = string ) or ( / local _ node / swbd _ number = string ) or ( / local _ node / swbd _ number = string ) or ( / local _ node / swbd _ number = string ) ) string cli _ incomplete _ no string cli _ incomplete _ command string callpoint string ha_callpoint string urn brocade . com mgmt brocade _ ha string brocade _ ha string container string string , } ) self . _process_restart = t if hasattr ( self , string ) self . _set ( )	0
Q_692	expression _ x_path compile expression	C_692_2	def get_domain_events ( self , originator_id , gt = none , gte = none , lt = none , lte = none , limit = none , is_ascending = true , page_size = none ) string string string	0
Q_693	return all the method that be actually a request callback	C_693_0	def callbacks ( cls ) if cls . _callbacks is not none return cls . _callbacks cls . _callbacks = [ ] for var in dir ( cls ) if var in dir ( resource ) continue callback = getattr ( cls , var ) if not hasattr ( callback , string ) continue cls . _callbacks . append ( callback ) return cls . _callbacks	1
Q_693	return all the method that be actually a request callback	C_693_1	def make_job_graph ( infiles , fragfiles , blastcmds ) joblist = [ ] dbjobdict = build_db_jobs ( infiles , blastcmds ) jobnum = len ( dbjobdict ) for var , var in enumerate ( fragfiles [ _ number ] ) for fname2 in fragfiles [ var + number ] jobnum + = number jobs = [ pyani_jobs . job ( string ( blastcmds . prefix , jobnum ) , blastcmds . build_blast_cmd ( var , fname2 . replace ( string , string ) ) , ) , pyani_jobs . job ( string ( blastcmds . prefix , jobnum ) , blastcmds . build_blast_cmd ( fname2 , var . replace ( string , string ) ) , ) , ] jobs [ number ] . add_dependency ( dbjobdict [ var . replace ( string , string ) ] ) jobs [ number ] . add_dependency ( dbjobdict [ fname2 . replace ( string , string ) ] ) joblist . extend ( jobs ) return joblist	0
Q_693	return all the method that be actually a request callback	C_693_2	def _collections_search_request ( self , search_pattern = string , identifier = string , page = number , page_size = number , sort_by = none ) skip = ( page _ number ) * page_size params = { string page_size , string skip , string string , string string , string string , } if search_pattern params [ string ] = if identifier = string params [ string ] = string params [ string ] = self . _escape_lucene_query ( identifier ) if sort_by is not none params [ string ] = string if sort_by = = string params [ string ] = true return self . _get ( urljoin ( self . base_url , string ) , params = params )	0
Q_694	test code call from commandline	C_694_0	def main ( ) model = load_model ( string ) hmm = model . hmms [ string ] for var in hmm . state_names print ( var ) state = model . states [ var ] print ( state . means_ ) print ( model ) model2 = load_model ( string ) print ( model2 )	1
Q_694	test code call from commandline	C_694_1	def wait_for_click ( self , button , time_out = number . number ) button = int ( button ) w = iomediator . waiter ( none , none , button , time_out ) w . wait ( )	0
Q_694	test code call from commandline	C_694_2	def _fit_stacking_model ( self , x , y , cost_mat , max_iter = number ) self . f_staking = cost_sensitive_logistic_regression ( verbose = self . verbose , max_iter = max_iter ) x_stacking = _create_stacking_set ( self . estimators_ , self . estimators_features_ , self . estimators_weight_ , x , self . combination ) self . f_staking . fit ( x_stacking , y , cost_mat ) return self	0
Q_695	save the new status and call all define callback	C_695_0	def set_status ( self , status ) self . status = status for var in self . _update_status_callbacks var ( self )	1
Q_695	save the new status and call all define callback	C_695_1	def cancel ( self ) conn = self . _assert_open ( ) conn . _try_activate_cursor ( self ) self . _session . cancel_if_pending ( )	0
Q_695	save the new status and call all define callback	C_695_2	def _thread_multi_return ( cls , minion_instance , opts , data ) fn_ = os . path . join ( minion_instance . proc_dir , data [ string ] ) if opts [ string ] and not salt . utils . platform . is_windows ( ) salt . log . setup . shutdown_multiprocessing_logging ( ) salt . utils . process . daemonize_if ( opts ) salt . log . setup . setup_multiprocessing_logging ( ) salt . utils . process . appendproctitle ( string . format ( cls . _name_ , data [ string ] ) ) sdata = { string os . getpid ( ) } sdata . update ( data ) log . info ( string , sdata [ string ] ) with salt . utils . files . fopen ( fn_ , string ) as fp_ fp_ . write ( minion_instance . serial . dumps ( sdata ) ) multifunc_ordered = opts . get ( string , false ) num_funcs = len ( data [ string ] ) if multifunc_ordered ret = { string [ none ] * num_funcs , string [ none ] * num_funcs , string [ false ] * num_funcs } else ret = { string { } , string { } , string { } } for var in range ( number , num_funcs ) if not multifunc_ordered ret [ string ] [ data [ string ] [ var ] ] = false try minion_blackout_violation = false if minion_instance . connected and minion_instance . opts [ string ] . get ( string , false ) whitelist = minion_instance . opts [ string ] . get ( string , [ ] ) if data [ string ] [ var ] = string and data [ string ] [ var ] not in whitelist minion_blackout_violation = true elif minion_instance . opts [ string ] . get ( string , false ) whitelist = minion_instance . opts [ string ] . get ( string , [ ] ) if data [ string ] [ var ] = string and data [ string ] [ var ] not in whitelist minion_blackout_violation = true if minion_blackout_violation raise salt_invocation_error ( string minion_blackout string string string ) func = minion_instance . functions [ data [ string ] [ var ] ] args , kwargs = load_args_and_kwargs ( func , data [ string ] [ var ] , data ) minion_instance . functions . pack [ string ] [ string ] = number key = var if multifunc_ordered else data [ string ] [ var ] ret [ string ] [ key ] = func ( * args , * * kwargs ) retcode = minion_instance . functions . pack [ string ] . get ( string , number ) if retcode = = number try func_result = all ( ret [ string ] [ key ] . get ( x , true ) for x in ( string , string ) ) except exception func_result = true if not func_result retcode = number ret [ string ] [ key ] = retcode ret [ string ] [ key ] = retcode = = number except exception as exc trb = traceback . format_exc ( ) log . warning ( string , exc ) if multifunc_ordered ret [ string ] [ var ] = trb else ret [ string ] [ data [ string ] [ var ] ] = trb ret [ string ] = data [ string ] ret [ string ] = data [ string ] ret [ string ] = data [ string ] if string in data ret [ string ] = data [ string ] if minion_instance . connected minion_instance . _return_pub ( ret , timeout = minion_instance . _return_retry_timer ( ) ) if data [ string ] if string in data ret [ string ] = data [ string ] if string in data ret [ string ] = data [ string ] for returner in set ( data [ string ] . split ( string ) ) ret [ string ] = opts [ string ] try minion_instance . returners [ string . format ( returner ) ] ( ret ) except exception as exc log . error ( string , data [ string ] , exc )	0
Q_696	crop a list of string to the specify width height	C_696_0	def _do_crop ( self , lines , width , height , x_crop , y_crop ) lines = crop_or_expand ( lines , height , default = [ self . fillchar * width ] , scheme = self . _schemes [ y_crop ] ) for var , var in enumerate ( lines ) lines [ var ] = crop_or_expand ( var , width , default = self . fillchar , scheme = self . _schemes [ x_crop ] ) return lines	1
Q_696	crop a list of string to the specify width height	C_696_1	"def start ( self ) napalm_logs_publisher_received_messages = counter ( string , string , [ string , string , string ] ) napalm_logs_publisher_whitelist_blacklist_check_fail = counter ( string , string , [ string , string , string ] ) napalm_logs_publisher_messages_published = counter ( string , string , [ string , string , string ] ) self . _setup_ipc ( ) thread = threading . thread ( target = self . _suicide_when_without_parent , args = ( os . getppid ( ) , ) ) thread . start ( ) signal . signal ( signal . sigterm , self . _exit_gracefully ) self . transport . start ( ) self . _up = true while self . _up try bin_obj = self . sub . recv ( ) except zmq . zmq_error as error if self . _up is false log . info ( string ) return else log . error ( error , exc_info = true ) raise napalm_logs_exit ( error ) obj = umsgpack . unpackb ( bin_obj ) if self . _strip_message_details obj . pop ( string , none ) bin_obj = self . serializer_fun ( obj ) napalm_logs_publisher_received_messages . labels ( publisher_type = self . _transport_type , address = self . address , port = self . port ) . inc ( ) if not napalm_logs . ext . check_whitelist_blacklist ( obj [ string ] , whitelist = self . error_whitelist , blacklist = self . error_blacklist ) log . debug ( string error string publishing the oc object "" ) if not self . disable_security and self . _transport_encrypt serialized_obj = self . _prepare ( serialized_obj ) self . transport . publish ( serialized_obj ) napalm_logs_publisher_messages_published . labels ( publisher_type = self . _transport_type , address = self . address , port = self . port ) . inc ( )"	0
Q_696	crop a list of string to the specify width height	C_696_2	def _subscribed ( self ) self . logger . debug ( string ) self . state = string for var in self . message_queue self . send ( var )	0
Q_697	create a variation in direction	C_697_0	def _randomize_direction ( base_heading , sigma ) _ > int val = mission_weather . _gauss ( base_heading , sigma ) val = mission_weather . _normalize_direction ( val ) return val	1
Q_697	create a variation in direction	C_697_1	def create ( self , name , proc_params , ssl_params = { } , auth_key = none , login = none , password = none , auth_source = string , timeout = number , autostart = true , server_id = none , version = none ) name = os . path . split ( name ) [ number ] if server_id is none server_id = str ( uuid4 ( ) ) if server_id in self raise servers_error ( string server_id ) bin_path = self . bin_path ( version ) server = server ( os . path . join ( bin_path , name ) , proc_params , ssl_params , auth_key , login , password , auth_source ) if autostart server . start ( timeout ) self [ server_id ] = server return server_id	0
Q_697	create a variation in direction	C_697_2	def warn ( self , text ) self . logger . warn ( string . format ( self . message_prefix , text ) )	0
Q_698	format the min and max temp elemets into a readable string	C_698_0	def min_max_temp ( temp str , unit str = string ) _ > str if not temp or len ( temp ) < number return string if temp [ number ] = = string temp_type = string elif temp [ number ] = = string temp_type = string else return string temp = temp [ number ] . replace ( string , string ) . replace ( string , string ) . split ( string ) if len ( temp [ number ] ) > number temp [ number ] = temp [ number ] [ number ] + string + temp [ number ] [ number ] temp_value = temperature ( core . make_number ( temp [ number ] ) , unit ) return f string	1
Q_698	format the min and max temp elemets into a readable string	C_698_1	def pearson ( logu , name = none ) with tf . compat . v1 . name_scope ( name , string , [ logu ] ) logu = tf . convert_to_tensor ( value = logu , name = string ) return tf . square ( tf . math . expm1 ( logu ) )	0
Q_698	format the min and max temp elemets into a readable string	C_698_2	def set_logging_level ( level = none ) if level is none level = os . environ . get ( string , string ) if level is not none logger . set_level ( getattr ( logging , level . upper ( ) ) ) return logger . get_effective_level ( )	0
Q_699	flatten will return tagstr tagstr tagstr tagstr tagstr tagstr	C_699_0	def flatten ( lst ) def flatten_aux ( item , accumulated ) if type ( item ) = list accumulated . append ( item ) else for var in item flatten_aux ( var , accumulated ) accumulated = [ ] flatten_aux ( lst , accumulated ) return accumulated	1
Q_699	flatten will return tagstr tagstr tagstr tagstr tagstr tagstr	C_699_1	def _select_item ( self , index ) self . _selected_index = index self . set_current_index ( self . model ( ) . create_index ( index , number ) )	0
Q_699	flatten will return tagstr tagstr tagstr tagstr tagstr tagstr	C_699_2	def _on_github_user ( self , future , access_token , response ) response [ string ] = access_token future . set_result ( response )	0
Q_700	execute all module as _main_ within a give package path	C_700_0	def run_all_in_dir ( dir_path , exclude = [ ] , add_args = ( ) , ignore_errors = true ) print ( string dir_path ) if type ( add_args ) in ( tuple , list ) sys . argv . extend ( add_args ) else sys . argv . append ( add_args ) t = number failed = [ ] for _ , modname , ispkg in pkgutil . walk_packages ( [ dir_path ] , ) if not ispkg and modname = thismodname and modname not in exclude print ( string modname ) t0 = time ( ) try runpy . run_module ( modname , init_globals = none , run_name = string , alter_sys = false ) except exception as var if ignore_errors print ( string var ) else raise var failed . append ( ( modname , var ) ) dt = time ( ) _ t0 t + = dt print ( string dt ) print ( string ) print ( string t ) if failed print ( string ) print ( string len ( failed ) ) print ( string ) for var , var in failed print ( var , var )	1
Q_700	execute all module as _main_ within a give package path	C_700_1	def set_kwargs ( self , * * kwargs ) for var in list ( kwargs . keys ( ) ) f = getattr ( self , string + var ) f ( kwargs [ var ] )	0
Q_700	execute all module as _main_ within a give package path	C_700_2	def compute_results ( ann_inter , est_inter , ann_labels , est_labels , bins , est_file , weight = number . number ) res = { } res [ string ] , res [ string ] , res [ string ] = mir_eval . segment . detection ( ann_inter , est_inter , window = number , trim = false ) res [ string ] , res [ string ] , res [ string ] = mir_eval . segment . detection ( ann_inter , est_inter , window = . number , trim = false ) res [ string ] , res [ string ] , res [ string ] = mir_eval . segment . detection ( ann_inter , est_inter , window = number , trim = true ) res [ string ] , res [ string ] , res [ string ] = mir_eval . segment . detection ( ann_inter , est_inter , window = . number , trim = true ) _ , _ , res [ string ] = mir_eval . segment . detection ( ann_inter , est_inter , window = number , trim = false , beta = weight ) _ , _ , res [ string ] = mir_eval . segment . detection ( ann_inter , est_inter , window = . number , trim = false , beta = weight ) _ , _ , res [ string ] = mir_eval . segment . detection ( ann_inter , est_inter , window = number , trim = true , beta = weight ) _ , _ , res [ string ] = mir_eval . segment . detection ( ann_inter , est_inter , window = . number , trim = true , beta = weight ) res [ string ] = compute_information_gain ( ann_inter , est_inter , est_file , bins = bins ) res [ string ] , res [ string ] = mir_eval . segment . deviation ( ann_inter , est_inter , trim = false ) res [ string ] , res [ string ] = mir_eval . segment . deviation ( ann_inter , est_inter , trim = true ) if est_labels is not none and ( string in est_labels or string in est_labels ) est_labels = none if est_labels is not none and len ( est_labels ) = number ann_labels = list ( ann_labels ) est_labels = list ( est_labels ) ann_inter , ann_labels = mir_eval . util . adjust_intervals ( ann_inter , ann_labels ) est_inter , est_labels = mir_eval . util . adjust_intervals ( est_inter , est_labels , t_min = number . number , t_max = ann_inter . max ( ) ) res [ string ] , res [ string ] , res [ string ] = mir_eval . segment . pairwise ( ann_inter , ann_labels , est_inter , est_labels ) res [ string ] , res [ string ] , res [ string ] = mir_eval . segment . nce ( ann_inter , ann_labels , est_inter , est_labels ) base = os . path . basename ( est_file ) res [ string ] = base [ _ number ] res [ string ] = base . split ( string ) [ number ] return res	0
Q_701	op provide a more verbose syntax for declare operator	C_701_0	def op ( token , func , left = false , right = false ) both = ( left = = right ) return ( token , func , op_both if both else op_left if left else op_right )	1
Q_701	op provide a more verbose syntax for declare operator	C_701_1	def _translate ( env , target = none , source = s_cons . environment . _null , * args , * * kw ) if target is none target = [ ] pot = env . pot_update ( none , source , * args , * * kw ) po = env . po_update ( target , pot , * args , * * kw ) return po	0
Q_701	op provide a more verbose syntax for declare operator	C_701_2	def advance ( self , height , ignore_overflow = false ) if height < = self . remaining_height self . _self_cursor . grow ( height ) elif ignore_overflow self . _self_cursor . grow ( float ( self . remaining_height ) ) else raise container_overflow ( self . page . number )	0
Q_702	get a random name from a give list	C_702_0	def _get_random_name ( name_list ) length = len ( name_list ) index = random . randrange ( length ) return name_list [ index ] [ string ]	1
Q_702	get a random name from a give list	C_702_1	def humanize_seconds ( total_seconds , frmt = string , negative_frmt = none ) if negative_frmt is none negative_frmt = string negative _ time string . format ( frmt ) seconds = abs ( int ( total_seconds ) ) mapping = { string seconds / / number , string seconds number / / number , string seconds number number , } if total_seconds < number result = negative_frmt . format ( * * mapping ) else result = frmt . format ( * * mapping ) return mark_safe ( result )	0
Q_702	get a random name from a give list	C_702_2	def run_as ( user , domain , password , filename , logon_flag = number , work_dir = string , show_flag = properties . sw_shownormal ) ret = auto_it . au3_run_as ( lpcwstr ( user ) , lpcwstr ( domain ) , lpcwstr ( password ) , int ( logon_flag ) , lpcwstr ( filename ) , lpcwstr ( work_dir ) , int ( show_flag ) ) return ret	0
Q_703	deprecate _ use get_next_job	C_703_0	def get_next_job_by_location ( plugin_name , loc , verify_job = true , conn = none ) return get_next_job ( plugin_name , loc , verify_job = verify_job , conn = conn )	1
Q_703	deprecate _ use get_next_job	C_703_1	def get_floatingips ( context , filters = none , fields = none , sorts = [ string ] , limit = none , marker = none , page_reverse = false ) log . info ( string ( context . tenant_id , filters , fields ) ) floating_ips = _get_ips_by_type ( context , ip_types . floating , filters = filters , fields = fields ) return [ v . _make_floating_ip_dict ( var ) for var in floating_ips ]	0
Q_703	deprecate _ use get_next_job	C_703_2	def voltage ( self ) raw = self . value volts = raw * ( _ads1_x15_pga_range [ self . _ads . gain ] / ( number * * ( self . _ads . bits _ number ) _ number ) ) return volts	0
Q_704	get all gate know on the ariane server	C_704_0	def get_gates ( ) logger . debug ( string ) params = session_service . complete_transactional_req ( none ) if params is none if mapping_service . driver_type = driver_factory . driver_rest params = { string string } args = { string params } else args = { string string , string string } else if mapping_service . driver_type = driver_factory . driver_rest params [ string ] = string args = { string params } else args = { string string , string string , string params } response = gate_service . requester . call ( args ) if mapping_service . driver_type = driver_factory . driver_rest response = response . get ( ) ret = none if response . rc = = number ret = [ ] for var in response . response_content [ string ] ret . append ( gate . json_2_gate ( var ) ) elif response . rc = number err_msg = string string + str ( response . response_content ) + string + str ( response . error_message ) + string + str ( response . rc ) + string logger . warning ( err_msg ) if response . rc = = number and ariane_mapping_overload_error . error_msg in response . error_message raise ariane_mapping_overload_error ( string , ariane_mapping_overload_error . error_msg ) return ret	1
Q_704	get all gate know on the ariane server	C_704_1	def write_gif ( dataset , filename , fps = number ) try check_dataset ( dataset ) except value_error as e dataset = try_fix_dataset ( dataset ) check_dataset ( dataset ) delay_time = number / / int ( fps ) def encode ( d ) four_d = isinstance ( dataset , numpy . ndarray ) and len ( dataset . shape ) = = number if four_d or not isinstance ( dataset , numpy . ndarray ) return _make_animated_gif ( d , delay_time = delay_time ) else return _make_gif ( d ) with open ( filename , string ) as outfile outfile . write ( header ) for var in encode ( dataset ) outfile . write ( var ) outfile . write ( trailer )	0
Q_704	get all gate know on the ariane server	C_704_2	def api_server ( connection , server_class ) return server_class ( link = xbahn . connection . link . link ( receive = connection , respond = connection ) )	0
Q_705	write a new nago	C_705_0	def generate_configfile ( cfg_file , defaults = defaults ) _mkdir_for_config ( cfg_file = cfg_file ) with open ( cfg_file , string ) as f f . write ( string ) for var in defaults . keys ( ) set_option ( var , cfg_file = cfg_file , * * defaults [ var ] )	1
Q_705	write a new nago	C_705_1	def tokens2text ( docgraph , token_ids ) return string . join ( docgraph . node [ var ] [ docgraph . ns + string ] for var in token_ids )	0
Q_705	write a new nago	C_705_2	def complete ( response response , project typing . union [ project , none ] , starting project_step = none , force bool = false , limit int = _ number ) _ > list if project is none project = cauldron . project . get_internal_project ( ) starting_index = number if starting starting_index = project . steps . index ( starting ) count = number steps_run = [ ] for var in project . steps if number < limit < = count break if var . index < starting_index continue if not force and not var . is_dirty ( ) if limit < number environ . log ( string . format ( var . definition . name ) ) continue count + = number steps_run . append ( var ) success = source . run_step ( response , project , var , force = true ) if not success or project . stop_condition . halt return steps_run return steps_run	0
Q_706	this method build a array that will contain the string	C_706_0	def get_representation ( self , prefix = string , suffix = string ) res = [ ] if self . hidden res . append ( prefix + string + str ( self . _name ) + string + suffix ) else default = self . default if default is none default = string a = prefix + string a + = str ( self . _name ) + string if isinstance ( default , types . unicode_type ) a + = default else a + = str ( default ) a + = suffix res . append ( a ) return res	1
Q_706	this method build a array that will contain the string	C_706_1	def assign_comments ( self , tree , comments ) comments = list ( comments ) comments . sort ( key = lambda var var . line ) idx_by_line = { number number } for var , var in enumerate ( comments ) if var . line not in idx_by_line idx_by_line [ var . line ] = var idx = [ ] self . comments = [ var . value . strip ( ) for var in comments ] last_comment_line = max ( idx_by_line . keys ( ) ) for var in range ( last_comment_line , number , _ number ) if var in idx_by_line idx . append ( idx_by_line [ var ] ) else idx . append ( idx [ _ number ] ) idx . append ( number ) idx . reverse ( ) self . idx = idx self . _assign_comments ( tree , number )	0
Q_706	this method build a array that will contain the string	C_706_2	def depth ( script , iterations = number , viewpoint = ( number , number , number ) , selected = false ) filter_xml = string . join ( [ string depth smooth string , string step_smooth_num string , string { d } string . format ( iterations ) , string smoothing steps string , string rich_int string , string , string view_point string , string { } string . format ( viewpoint [ number ] ) , string { } string . format ( viewpoint [ number ] ) , string { } string . format ( viewpoint [ number ] ) , string smoothing steps string , string rich_point3f string , string , string selected string , string { } string . format ( str ( selected ) . lower ( ) ) , string affect only selected faces string , string rich_bool string , string , string ] ) util . write_filter ( script , filter_xml ) return none	0
Q_707	view for the link in the verification email send to a new user	C_707_0	def signup_verify ( request , uidb36 = none , token = none ) user = authenticate ( uidb36 = uidb36 , token = token , is_active = false ) if user is not none user . is_active = true user . save ( ) auth_login ( request , user ) info ( request , _ ( string ) ) return login_redirect ( request ) else error ( request , _ ( string ) ) return redirect ( string )	1
Q_707	view for the link in the verification email send to a new user	C_707_1	def _check_email_changed ( cls , username , email ) ret = cls . exec_request ( string . format ( username ) , string , raise_for_status = true ) return ret [ string ] = email	0
Q_707	view for the link in the verification email send to a new user	C_707_2	def vote ( self , voter , author , permlink , voteweight ) vote = vote ( voter , author , permlink , voteweight ) result = self . steemconnect ( ) . broadcast ( [ vote . to_operation_structure ( ) ] ) return result	0
Q_708	run through the operation method update their docstrings if	C_708_0	def _update_docstrings ( self ) ops = self . _details . resource_data [ string ] for var in ops . keys ( ) meth = getattr ( self . _class_ , var , none ) if not meth continue if meth . _doc_ = default_docstring continue api_name = ops [ var ] [ string ] conn_meth = getattr ( self . _connection , to_snake_case ( api_name ) ) if six . py3 meth . _doc_ = conn_meth . _doc_ else meth . _func_ . _doc_ = conn_meth . _doc_	1
Q_708	run through the operation method update their docstrings if	C_708_1	def create_group ( r , name , alloc_policy = none , dry_run = false ) query = { string dry_run , } body = { string name , string alloc_policy } return r . request ( string , string , query = query , content = body )	0
Q_708	run through the operation method update their docstrings if	C_708_2	def find_peaks ( input_map , threshold , min_separation = number . number ) data = input_map . data cdelt = max ( input_map . geom . wcs . wcs . cdelt ) min_separation = max ( min_separation , number * cdelt ) region_size_pix = int ( min_separation / cdelt ) region_size_pix = max ( number , region_size_pix ) deltaxy = utils . make_pixel_distance ( region_size_pix * number + number ) deltaxy * = max ( input_map . geom . wcs . wcs . cdelt ) region = deltaxy < min_separation local_max = maximum_filter ( data , footprint = region ) = = data local_max [ data < threshold ] = false labeled , num_objects = scipy . ndimage . label ( local_max ) slices = scipy . ndimage . find_objects ( labeled ) peaks = [ ] for var in slices skydir = sky_coord . from_pixel ( var [ number ] . start , var [ number ] . start , input_map . geom . wcs ) peaks . append ( { string var [ number ] . start , string var [ number ] . start , string skydir , string data [ var [ number ] . start , var [ number ] . start ] } ) return sorted ( peaks , key = lambda t t [ string ] , reverse = true )	0
Q_709	return this comment as plain text	C_709_0	def get_as_text ( self ) d = { string self . user or self . name , string self . post_date , string self . text , string self . site . domain , string self . get_absolute_url ( ) } return _ ( string ) d	1
Q_709	return this comment as plain text	C_709_1	def migrate ( vm_ , target , live = number , port = number , node = _ number , ssl = none , change_home_server = number ) with _get_xapi_session ( ) as xapi vm_uuid = _get_label_uuid ( xapi , string , vm_ ) if vm_uuid is false return false other_config = { string port , string node , string ssl , string change_home_server } try xapi . vm . migrate ( vm_uuid , target , bool ( live ) , other_config ) return true except exception return false	0
Q_709	return this comment as plain text	C_709_2	def _set_queue_size ( self , v , load = false ) if hasattr ( v , string ) v = v . _utype ( v ) try t = yang_dyn_class ( v , base = yang_list_type ( string , queue_size . queue_size , yang_name = string , rest_name = string , parent = self , is_container = string , user_ordered = false , path_helper = self . _path_helper , yang_keys = string , extensions = { u string { u string u string , u string none , u string none , u string none , u string none , u string none , u string none , u string none , u string u string } } ) , is_container = string , yang_name = string , rest_name = string , parent = self , path_helper = self . _path_helper , extmethods = self . _extmethods , register_paths = true , extensions = { u string { u string u string , u string none , u string none , u string none , u string none , u string none , u string none , u string none , u string u string } } , namespace = string , defining_module = string , yang_type = string , is_config = true ) except ( type_error , value_error ) raise value_error ( { string string string string , string string , string string string traffic_class string queue _ size string queue _ size string list string traffic _ class string tailf _ common string info string configure multicast queue size string cli _ no _ key _ completion string cli _ suppress _ mode string cli _ suppress _ list _ no string cli _ compact _ syntax string cli _ drop _ node _ name string cli _ sequence _ commands string cli _ incomplete _ command string callpoint string intrfc_rx_queue_multicast_qsize string list string queue _ size string queue _ size string tailf _ common string info string configure multicast queue size string cli _ no _ key _ completion string cli _ suppress _ mode string cli _ suppress _ list _ no string cli _ compact _ syntax string cli _ drop _ node _ name string cli _ sequence _ commands string cli _ incomplete _ command string callpoint string intrfc_rx_queue_multicast_qsize string urn brocade . com mgmt brocade _ qos _ mls string brocade _ qos _ mls string list string string , } ) self . _queue_size = t if hasattr ( self , string ) self . _set ( )	0
Q_710	get the url for the video medium that we can send to clarify	C_710_0	def _src_media_url_for_video ( self , video ) src_url = none best_height = number best_source = none video_sources = self . bc_client . get_video_sources ( video [ string ] ) for var in video_sources height = var . get ( string , number ) codec = var . get ( string ) if var . get ( string ) and codec and codec . upper ( ) = = string and height < = number and height > best_height best_source = var if best_source is not none src_url = best_source [ string ] return src_url	1
Q_710	get the url for the video medium that we can send to clarify	C_710_1	def refresh_claims ( self , ids , timeout = string ) with data_store . db . get_mutation_pool ( ) as mutation_pool mutation_pool . queue_refresh_claims ( ids , timeout = timeout )	0
Q_710	get the url for the video medium that we can send to clarify	C_710_2	def execute_no_results ( self , sock_info , generator ) if self . uses_collation raise configuration_error ( string ) if self . uses_array_filters raise configuration_error ( string ) if self . bypass_doc_val and sock_info . max_wire_version > = number raise operation_failure ( string string ) if sock_info . max_wire_version > number if self . ordered return self . execute_command_no_results ( sock_info , generator ) return self . execute_op_msg_no_results ( sock_info , generator ) coll = self . collection write_concern = write_concern ( w = int ( self . ordered ) ) op_id = _randint ( ) next_run = next ( generator ) while next_run run = next_run next_run = next ( generator , none ) needs_ack = self . ordered and next_run is not none try if run . op_type = = _insert self . execute_insert_no_results ( sock_info , run , op_id , needs_ack ) elif run . op_type = = _update for var in run . ops doc = var [ string ] check_keys = true if doc and next ( iter ( doc ) ) . startswith ( string ) check_keys = false coll . _update ( sock_info , var [ string ] , doc , var [ string ] , check_keys , var [ string ] , write_concern = write_concern , op_id = op_id , ordered = self . ordered , bypass_doc_val = self . bypass_doc_val ) else for var in run . ops coll . _delete ( sock_info , var [ string ] , not var [ string ] , write_concern , op_id , self . ordered ) except operation_failure if self . ordered break	0
Q_711	freeze the application by create a static version of it	C_711_0	def freeze ( config , urls_file = none ) if urls_file app = make_app ( config = config , urls_file = urls_file ) else app = make_app ( config = config ) app . logger . info ( string app . config [ string ] ) freezer = freezer ( app ) freezer . register_generator def uri_index ( ) def cleanup_url ( url ) url = url . strip ( ) if url . startswith ( string ) if url . endswith ( string ) return url elif url . endswith ( string ) url = url . strip ( string ) if len ( url ) = = number return ( string , { } ) return ( string , { string url } ) try result = db . execute ( text ( fetch_query_string ( string ) ) ) . fetchall ( ) except ( database_error , statement_error ) as err app . logger . error ( string , err ) return [ ] urls = filter ( none , map ( lambda x cleanup_url ( x [ number ] ) , result ) ) urls_file = app . config . get ( string , none ) if urls_file urls_file = urls_file if urls_file [ number ] = = os . sep else os . path . join ( os . getcwd ( ) , urls_file ) f = open ( urls_file , string ) urls . extend ( filter ( none , map ( cleanup_url , f . readlines ( ) ) ) ) f . close ( ) return urls freezer . register_generator def send_root_file ( ) root_folder = app . config . get ( string , none ) if root_folder and os . path . isdir ( root_folder ) for ( dirpath , dirnames , filenames ) in os . walk ( root_folder , topdown = true ) start = len ( os . path . commonprefix ( ( root_folder , dirpath ) ) ) relative_path = dirpath [ start + number ] for var in filenames yield ( string , { string os . path . join ( relative_path , var ) } ) freezer . register_generator def send_media_file ( ) media_folder = app . config . get ( string , none ) media_path = app . config . get ( string , string ) freeze_all_files = app . config . get ( string , false ) if media_folder and freeze_all_files and os . path . isdir ( media_folder ) and media_path [ number ] = = string for ( dirpath , dirnames , filenames ) in os . walk ( media_folder , topdown = true ) start = len ( os . path . commonprefix ( ( media_folder , dirpath ) ) ) relative_path = dirpath [ start + number ] for var in filenames yield ( string , { string os . path . join ( relative_path , var ) } ) freezer . register_generator def send_theme_file ( ) theme_static_folder = app . config . get ( string , none ) theme_static_path = app . config . get ( string , string ) if theme_static_folder and os . path . isdir ( theme_static_folder ) and theme_static_path [ number ] = = string for ( dirpath , dirnames , filenames ) in os . walk ( theme_static_folder , topdown = true ) start = len ( os . path . commonprefix ( ( theme_static_folder , dirpath ) ) ) relative_path = dirpath [ start + number ] for var in filenames yield ( string , { string os . path . join ( relative_path , var ) } ) freezer . freeze ( )	1
Q_711	freeze the application by create a static version of it	C_711_1	def publish_command_success ( self , duration , reply , command_name , request_id , connection_id , op_id = none ) if op_id is none op_id = request_id event = command_succeeded_event ( duration , reply , command_name , request_id , connection_id , op_id ) for var in self . _command_listeners try var . succeeded ( event ) except exception _handle_exception ( )	0
Q_711	freeze the application by create a static version of it	C_711_2	def _get_broadcast_shape ( shape1 , shape2 ) if shape1 = = shape2 return shape1 length1 = len ( shape1 ) length2 = len ( shape2 ) if length1 > length2 shape = list ( shape1 ) else shape = list ( shape2 ) i = max ( length1 , length2 ) _ number for var , var in zip ( shape1 [ _ number ] , shape2 [ _ number ] ) if var = number and var = number and var = var raise value_error ( string ( shape1 , shape2 ) ) shape [ i ] = max ( var , var ) i _ = number return tuple ( shape )	0
Q_712	run func cleanup first	C_712_0	def publish ( self ) self . cleanup remote = self . remote branch = self . branch return self . m ( string ( remote , branch ) , cmdd = dict ( cmd = string ( remote , branch ) , cwd = self . local ) , more = dict ( remote = remote , branch = branch ) )	1
Q_712	run func cleanup first	C_712_1	def get_statistics ( self , id_or_uri , port_name = string ) uri = self . _client . build_uri ( id_or_uri ) + string if port_name uri = uri + string + port_name return self . _client . get ( uri )	0
Q_712	run func cleanup first	C_712_2	def dentategyrus ( adjusted = true ) if adjusted filename = string url = string adata = read ( filename , backup_url = url , sparse = true , cache = true ) else filename = string url = string adata = read ( filename , backup_url = url , cleanup = true , sparse = true , cache = true ) cleanup ( adata , clean = string , keep = { string , string , string } ) url_louvain = string url_umap = string adata . obs [ string ] = load ( string , url_louvain ) adata . obsm [ string ] = load ( string , url_umap ) adata . obs [ string ] = pd . categorical ( adata . obs [ string ] ) return adata	0
Q_713	"check the user "" s email be unique"	C_713_0	def check_unique ( self ) emails = yield self . view . values ( key = self . email ) user_id = getattr ( self , string , none ) users = { var for var in emails if var = user_id and var } if users raise exceptions . validation_error ( string { } string . format ( self . email ) )	1
Q_713	"check the user "" s email be unique"	C_713_1	def node_list_to_coordinate_lines ( g , node_list , use_geom = true ) edge_nodes = list ( zip ( node_list [ _ number ] , node_list [ number ] ) ) lines = [ ] for var , var in edge_nodes data = min ( g . get_edge_data ( var , var ) . values ( ) , key = lambda x x [ string ] ) if string in data and use_geom xs , ys = data [ string ] . xy lines . append ( list ( zip ( xs , ys ) ) ) else x1 = g . nodes [ var ] [ string ] y1 = g . nodes [ var ] [ string ] x2 = g . nodes [ var ] [ string ] y2 = g . nodes [ var ] [ string ] line = [ ( x1 , y1 ) , ( x2 , y2 ) ] lines . append ( line ) return lines	0
Q_713	"check the user "" s email be unique"	C_713_2	def add ( self , member ) return self . _client . sadd ( self . key_prefix , self . _dumps ( member ) )	0
Q_714	fix _user_classes and some stuff in class	C_714_0	def prepare_classes ( locals ) for ( name , forgetter ) in locals . items ( ) if not ( type ( forgetter ) is types . type_type and issubclass ( forgetter , forgetter ) ) continue for ( key , userclass ) in forgetter . _user_classes . items ( ) if type ( userclass ) is types . string_type resolved = locals [ userclass ] forgetter . _user_classes [ key ] = resolved forgetter . _tables = { } for ( field , sqlfield ) in forgetter . _sql_fields . items ( ) forgetter . _sql_fields [ field ] = forgetter . _check_table ( sqlfield ) new_links = [ ] for var in forgetter . _sql_links ( link1 , link2 ) = var link1 = forgetter . _check_table ( link1 ) link2 = forgetter . _check_table ( link2 ) new_links . append ( ( link1 , link2 ) ) forgetter . _sql_links = new_links forgetter . _prepared = true	1
Q_714	fix _user_classes and some stuff in class	C_714_1	def standard_bytes_header ( self , title , addr , length ) self . save_header ( self . header_type_code , title , length , param1 = addr , param2 = number )	0
Q_714	fix _user_classes and some stuff in class	C_714_2	def tables ( self ) with self . conn . cursor ( ) as cur cur . execute ( self . tables_query ) for var in cur yield var	0
Q_715	return take a regular pattern and find out	C_715_0	def _enumeration_info ( self , pattern ) if not string in pattern or pattern . startswith ( string ) return ( pattern , none ) ( first , rest ) = pattern . split ( string ) rest = rest . replace ( string , string ) if string in rest ( left , right ) = rest . split ( string , number ) return ( first , ( left , right ) ) else return ( first , ( rest , rest ) )	1
Q_715	return take a regular pattern and find out	C_715_1	def create_repository ( self ) schema = self . get_connection ( ) . get_schema_builder ( ) with schema . create ( self . _table ) as table table . string ( string ) table . integer ( string )	0
Q_715	return take a regular pattern and find out	C_715_2	def refresh ( self ) if self . comm . rank = = number self . _blocks = self . list_blocks ( ) else self . _blocks = none self . _blocks = self . comm . bcast ( self . _blocks )	0
Q_716	whether parse within a comment should terminate now	C_716_0	def should_terminate_now ( self , line , waiting_for ) if waiting_for var in ( parser_state . eol , self . _end ) return false if self . _continue_regex return ( re . match ( self . _continue_regex , line ) is none ) return false	1
Q_716	whether parse within a comment should terminate now	C_716_1	def read_adc_difference ( self , differential ) assert number < = differential < = number , string command = number < < number command = ( differential number ) < < number resp = self . _spi . transfer ( [ command , number , number ] ) result = ( resp [ number ] number ) < < number result = ( resp [ number ] number ) < < number result = ( resp [ number ] number ) number return result number	0
Q_716	whether parse within a comment should terminate now	C_716_2	def list_symbols ( self , regex = none , as_of = none , * * kwargs ) if not ( regex or as_of or kwargs ) return self . distinct ( string ) index_query = { } if as_of is not none index_query [ string ] = { string as_of } if regex or as_of index_query [ string ] = { string regex or string } data_query = { } if kwargs for var , var in six . iteritems ( kwargs ) data_query [ string + var ] = var pipeline = [ { string { string pymongo . ascending , string pymongo . descending } } ] if index_query pipeline . append ( { string index_query } ) pipeline . append ( { string { string string , string { string string } } } ) if data_query pipeline . append ( { string data_query } ) pipeline . append ( { string { string number , string string } } ) return sorted ( r [ string ] for r in self . aggregate ( pipeline ) )	0
Q_717	run one of the bayesian model	C_717_0	def run_model ( model , returns_train , returns_test = none , bmark = none , samples = number , ppc = false , progressbar = true ) if model = = string model , trace = model_returns_t_alpha_beta ( returns_train , bmark , samples , progressbar = progressbar ) elif model = = string model , trace = model_returns_t ( returns_train , samples , progressbar = progressbar ) elif model = = string model , trace = model_returns_normal ( returns_train , samples , progressbar = progressbar ) elif model = = string model , trace = model_best ( returns_train , returns_test , samples = samples , progressbar = progressbar ) else raise not_implemented_error ( string string . format ( model ) ) if ppc ppc_samples = pm . sample_ppc ( trace , samples = samples , model = model , size = len ( returns_test ) , progressbar = progressbar ) return trace , ppc_samples [ string ] return trace	1
Q_717	run one of the bayesian model	C_717_1	def stream_item_roundtrip_xpaths ( si , quick = false ) def debug ( s ) logger . warning ( s ) def print_window ( token , size = number ) coffset = token . offsets [ offset_type . chars ] start = max ( number , coffset . first _ size ) end = min ( len ( html ) , coffset . first + coffset . length + size ) debug ( string * number ) debug ( coffset ) debug ( string size ) debug ( html [ start end ] ) debug ( string * number ) def debug_all ( token , xprange , expected , err = none , got = none ) debug ( string * number ) if err is not none debug ( err ) debug ( xprange ) debug ( string s string expected ) if got is not none debug ( string s string got ) debug ( string s string unicode ( token . token , string ) ) print_window ( token , size = number ) print_window ( token , size = number ) print_window ( token , size = number ) print_window ( token , size = number ) debug ( string * number ) def slice_clean_visible ( token ) coffset = token . offsets [ offset_type . chars ] return cleanvis [ coffset . first coffset . first + coffset . length ] def test_token ( token ) coffset = token . offsets . get ( offset_type . chars ) if coffset is none return false xoffset = token . offsets . get ( offset_type . xpath_chars ) if xoffset is none return false crange = ( coffset . first , coffset . first + coffset . length ) xprange = xpath_range . from_offset ( xoffset ) expected = slice_clean_visible ( token ) if expected = unicode ( token . token , string ) return false try got = xprange . slice_node ( html_root ) except invalid_xpath_error as err debug_all ( token , xprange , expected , err = err ) raise xpath_mismatch_error ( html , cleanvis , xprange , crange ) if expected = got debug_all ( token , xprange , expected , got = got ) raise xpath_mismatch_error ( html , cleanvis , xprange , crange ) return true cleanvis = unicode ( si . body . clean_visible , string ) html = unicode ( si . body . clean_html , string ) html_root = xpath_range . html_node ( html ) total , has_valid_xpath = number , number for var in si . body . var . itervalues ( ) for sentence in var if quick for i in xrange ( len ( sentence . tokens ) _ number , _ number , _ number ) if test_token ( sentence . tokens [ i ] ) break else for token in sentence . tokens total + = number if test_token ( token ) has_valid_xpath + = number if not quick logger . info ( string , si . stream_id , has_valid_xpath , total )	0
Q_717	run one of the bayesian model	C_717_2	def with_uvloop_if_possible ( f , * args , * * kwargs ) try import uvloop import asyncio asyncio . set_event_loop_policy ( uvloop . event_loop_policy ( ) ) print ( string ) except import_error print ( string ) return f ( * args , * * kwargs )	0
Q_718	"r tagstr "" input string format of class x y w h return list of list format"	C_718_0	def parse_darknet_ann_str_to_list ( annotations ) r annotations = annotations . split ( string ) ann = [ ] for var in annotations var = var . split ( ) if len ( var ) = = number for i , _v in enumerate ( var ) if i = = number var [ i ] = int ( var [ i ] ) else var [ i ] = float ( var [ i ] ) ann . append ( var ) return ann	1
Q_718	"r tagstr "" input string format of class x y w h return list of list format"	C_718_1	def smeft_evolve ( c_in , scale_in , scale_out , newphys = true , * * kwargs ) sol = _smeft_evolve ( c_in , scale_in , scale_out , newphys = newphys , * * kwargs ) return c_array2dict ( sol . y [ , _ number ] . view ( complex ) )	0
Q_718	"r tagstr "" input string format of class x y w h return list of list format"	C_718_2	def resize ( self , size ) self . instance . resize_volume ( size ) self . size = size	0
Q_719	set the potential mapping for a give column	C_719_0	def set_potential ( self , column_index , potential ) assert ( column_index < self . _num_columns ) potential_sparse = numpy . where ( potential > number ) [ number ] if len ( potential_sparse ) < self . _stimulus_threshold raise exception ( string + string + string ) self . _potential_pools . replace ( column_index , potential_sparse )	1
Q_719	set the potential mapping for a give column	C_719_1	def list_ ( ) ret = { } cmd = string out = _salt_ [ string ] ( cmd ) . splitlines ( ) for var in out comps = var . split ( ) ret [ comps [ number ] ] = comps [ number ] return ret	0
Q_719	set the potential mapping for a give column	C_719_2	def _lookup_global ( self , symbol ) assert symbol . parts namespace = self . namespaces if len ( symbol . parts ) = = number namespace = self . namespaces [ none ] try return self . _lookup_namespace ( symbol , namespace ) except error as orig_exc try namespace = self . namespaces [ none ] return self . _lookup_namespace ( symbol , namespace ) except error raise orig_exc	0
Q_720	return the nth record	C_720_0	def get_record ( self , n = none ) if n is none assert len ( self . fields ) > number n = self . fields [ number ] . num_records _ number assert ( all ( var . num_records > n for var in self . fields ) ) record = [ var . values [ n ] for var in self . fields ] return record	1
Q_720	return the nth record	C_720_1	def find_kwarg ( self , request , names , required , default , kwargs ) val = default found_name = string for var in names if var in kwargs val = kwargs [ var ] found_name = var break if not found_name and required raise value_error ( string . format ( self . var ) ) return found_name , val	0
Q_720	return the nth record	C_720_2	def mouse_move_event ( self , event ) string string string super ( x_chart_scene , self ) . mouse_move_event ( event ) self . update_tracker_item ( event . scene_pos ( ) )	0
Q_721	render the ref anchor of a footnote	C_721_0	def footnote_ref ( self , key , index ) html = ( string footnote _ ref string fnref _ s string string ) ( escape ( key ) , escape ( key ) , index ) return html	1
Q_721	render the ref anchor of a footnote	C_721_1	def update_endpoint ( self , endpoint ) self . _validate_uuid ( endpoint . endpoint_id ) self . _validate_subscriber_id ( endpoint . subscriber_id ) url = string . format ( endpoint . endpoint_id ) response = nws_dao ( ) . put_url ( url , self . _write_headers ( ) , self . _json_body ( endpoint . json_data ( ) ) ) if response . status = number raise data_failure_exception ( url , response . status , response . data ) return response . status	0
Q_721	render the ref anchor of a footnote	C_721_2	def health_check ( self ) logger . debug ( string . format ( namespace = self . namespace ) ) try self . client . head_object ( bucket = self . bucket_name , key = self . data_file ) return true except client_error as e logger . debug ( string )	0
Q_722	invoke task hook for every time spout acks a tuple	C_722_0	def invoke_hook_spout_ack ( self , message_id , complete_latency_ns ) if len ( self . task_hooks ) > number spout_ack_info = spout_ack_info ( message_id = message_id , spout_task_id = self . get_task_id ( ) , complete_latency_ms = complete_latency_ns * system_constants . ns_to_ms ) for var in self . task_hooks var . spout_ack ( spout_ack_info )	1
Q_722	invoke task hook for every time spout acks a tuple	C_722_1	def login ( self , password ) method , url = get_url ( string ) payload = { string self . config . get ( string ) , string self . username , string password , string string } res = getattr ( self . session , method ) ( url , params = payload ) if res . status_code = = number return true hellraiser ( res )	0
Q_722	invoke task hook for every time spout acks a tuple	C_722_2	def serialization_decision ( cls , obj , serialized_classes , serialized_configured_units ) is_declaration = isinstance ( obj , entity ) is_definition = isinstance ( obj , architecture ) if is_declaration unit = obj . origin elif is_definition unit = obj . entity . origin else return true assert isinstance ( unit , unit ) sd = unit . _serialize_decision if sd is none return true else prev_priv = serialized_classes . get ( unit . _class_ , none ) seriazlize , next_priv = sd ( unit , obj , is_declaration , prev_priv ) serialized_classes [ unit . _class_ ] = next_priv return seriazlize	0
Q_723	check for improper use of disallow macro	C_723_0	def check_access ( filename , clean_lines , linenum , nesting_state , error ) line = clean_lines . elided [ linenum ] matched = match ( ( r string r string ) , line ) if not matched return if nesting_state . stack and isinstance ( nesting_state . stack [ _ number ] , _class_info ) if nesting_state . stack [ _ number ] . access = string error ( filename , linenum , string , number , string matched . group ( number ) ) else pass	1
Q_723	check for improper use of disallow macro	C_723_1	def iteritems ( self , key = _absent ) if key is not _absent if key in self items = [ ( var . key , var . value ) for var in self . _map [ key ] ] return iter ( items ) raise key_error ( key ) items = six . iteritems ( self . _map ) return iter ( ( key , nodes [ number ] . value ) for ( key , nodes ) in items )	0
Q_723	check for improper use of disallow macro	C_723_2	def upsert ( self , rows , unique_fields , update_fields , return_rows = false , return_models = false ) if len ( rows ) = = number return model_class = self . tables [ number ] . model rows_with_null_auto_field_value = [ ] auto_field_name = self . get_auto_field_name ( model_class ) if auto_field_name in unique_fields rows_with_null_auto_field_value = [ var for var in rows if getattr ( var , auto_field_name ) is none ] rows = [ var for var in rows if getattr ( var , auto_field_name ) is not none ] return_value = [ ] if rows sql , sql_args = self . get_upsert_sql ( rows , unique_fields , update_fields , auto_field_name = auto_field_name , return_rows = return_rows or return_models ) cursor = self . get_cursor ( ) cursor . execute ( sql , sql_args ) if return_rows or return_models return_value . extend ( self . _fetch_all_as_dict ( cursor ) ) if rows_with_null_auto_field_value sql , sql_args = self . get_upsert_sql ( rows_with_null_auto_field_value , unique_fields , update_fields , auto_field_name = auto_field_name , only_insert = true , return_rows = return_rows or return_models ) cursor = self . get_cursor ( ) cursor . execute ( sql , sql_args ) if return_rows or return_models return_value . extend ( self . _fetch_all_as_dict ( cursor ) ) if return_models model_class = self . tables [ number ] . model model_objects = [ model_class ( * * row_dict ) for row_dict in return_value ] for model_object in model_objects model_object . _state . adding = false model_object . _state . db = string return_value = model_objects return return_value	0
Q_724	check if current position be inside template argument list	C_724_0	def in_template_argument_list ( self , clean_lines , linenum , pos ) while linenum < clean_lines . num_lines ( ) line = clean_lines . elided [ linenum ] match = match ( r string , line [ pos ] ) if not match linenum + = number pos = number continue token = match . group ( number ) pos + = len ( match . group ( number ) ) if token in ( string , string , string ) return false if token in ( string , string , string , string , string ) return true if token = string pos + = number if pos > = len ( line ) linenum + = number pos = number continue ( _ , end_line , end_pos ) = close_expression ( clean_lines , linenum , pos _ number ) if end_pos < number return false linenum = end_line pos = end_pos return false	1
Q_724	check if current position be inside template argument list	C_724_1	"def gene_reader ( fh , format = string ) known_formats = ( string , string , string ) if format not in known_formats print ( string ( format , string . join ( known_formats ) ) , file = sys . stderr ) raise exception ( string ) if format = = string for var in fh f = var . strip ( ) . split ( ) chrom = f [ number ] chrom_start = int ( f [ number ] ) name = f [ number ] strand = f [ number ] cds_start = int ( f [ number ] ) cds_end = int ( f [ number ] ) block_count = int ( f [ number ] ) block_sizes = [ int ( i ) for i in f [ number ] . strip ( string ) . split ( string ) ] block_starts = [ chrom_start + int ( i ) for i in f [ number ] . strip ( string ) . split ( string ) ] gene_exons = [ ] for base , offset in zip ( block_starts , block_sizes ) exon_start = base exon_end = base + offset gene_exons . append ( ( exon_start , exon_end ) ) yield chrom , strand , gene_exons , name genelist = { } grouplist = [ ] if format = = string or format = = string for var in fh if var . startswith ( string t string gtf string "" ) [ number ] else group = fields [ number ] if group not in grouplist grouplist . append ( group ) if group not in genelist genelist [ group ] = ( chrom , strand , [ ] ) exons_i = number genelist [ group ] [ exons_i ] . append ( ( ex_st , ex_end ) ) sp = lambda a , b cmp ( a [ number ] , b [ number ] ) for gene in grouplist chrom , strand , gene_exons = genelist [ gene ] gene_exons = bitset_union ( gene_exons ) yield chrom , strand , gene_exons , gene"	0
Q_724	check if current position be inside template argument list	C_724_2	def remove_members_in_score_range_in ( self , leaderboard_name , min_score , max_score ) pipeline = self . redis_connection . pipeline ( ) pipeline . zremrangebyscore ( leaderboard_name , min_score , max_score ) pipeline . zremrangebyscore ( self . _ties_leaderboard_key ( leaderboard_name ) , min_score , max_score ) pipeline . execute ( )	0
Q_725	select byte from memory	C_725_0	def _raw_read ( self , where int , size = number ) _ > bytes map = self . memory . map_containing ( where ) start = map . _get_offset ( where ) map_type = type ( map ) if map_type is file_map end = map . _get_offset ( where + size ) if end > map . _mapped_size logger . warning ( f string ) raw_data = map . _data [ map . _get_offset ( where ) min ( end , map . _mapped_size ) ] if len ( raw_data ) < end raw_data + = b string * ( end _ len ( raw_data ) ) data = b string for var in sorted ( map . _overlay . keys ( ) ) data + = raw_data [ len ( data ) var ] data + = map . _overlay [ var ] data + = raw_data [ len ( data ) ] elif map_type is anon_map data = bytes ( map . _data [ start start + size ] ) else data = b string . join ( self . memory [ where where + size ] ) assert len ( data ) = = size , string return data	1
Q_725	select byte from memory	C_725_1	def prefix_to_addrmask ( value , sep = string ) prefix = netaddr . ip_network ( value ) return string . format ( prefix . ip , sep , prefix . netmask )	0
Q_725	select byte from memory	C_725_2	async def update ( self ) keys = self . extras . keys ( ) self . extras = { } for var in keys try func = getattr ( self , var , none ) if callable ( func ) func ( ) except pass	0
Q_726	convert a string_based key into a hasher class	C_726_0	def _rectify_hasher ( hasher ) if xxhash is not none if hasher in { string , string , string } return xxhash . xxh32 if hasher in { string , string } return xxhash . xxh64 if hasher is no_param or hasher = = string hasher = default_hasher elif isinstance ( hasher , six . string_types ) if hasher not in hashlib . algorithms_available raise key_error ( string . format ( hasher ) ) else hasher = getattr ( hashlib , hasher ) elif isinstance ( hasher , hash ) return lambda hasher return hasher	1
Q_726	convert a string_based key into a hasher class	C_726_1	def register_actor ( name , actor_handle ) if not isinstance ( name , str ) raise type_error ( string ) if not isinstance ( actor_handle , ray . actor . actor_handle ) raise type_error ( string string ) actor_name = _calculate_key ( name ) pickled_state = pickle . dumps ( actor_handle ) already_exists = _internal_kv_put ( actor_name , pickled_state ) if already_exists actor_handle . _ray_new_actor_handles . pop ( ) raise value_error ( string . format ( name ) )	0
Q_726	convert a string_based key into a hasher class	C_726_2	def add_behavior_for_pclass ( self , cls ) kind = cls . _name_ for var in sorted ( self . versions ) try self . spec . add_behavior_for_pclass ( self . full_name ( var , kind ) , cls ) except no_such_definition pass else return none raise no_such_definition ( kind )	0
Q_727	run an instance of gnn test causal direction	C_727_0	def gnn_instance ( x , idx = number , device = none , nh = number , * * kwargs ) device = settings . get_default ( device = device ) xy = scale ( x ) . astype ( string ) inputx = th . float_tensor ( xy [ , [ number ] ] ) . to ( device ) target = th . float_tensor ( xy [ , [ number ] ] ) . to ( device ) gnnxy = gnn_model ( x . shape [ number ] , device = device , nh = nh ) . to ( device ) gnnyx = gnn_model ( x . shape [ number ] , device = device , nh = nh ) . to ( device ) gnnxy . reset_parameters ( ) gnnyx . reset_parameters ( ) xy = gnnxy . run ( inputx , target , * * kwargs ) yx = gnnyx . run ( target , inputx , * * kwargs ) return [ xy , yx ]	1
Q_727	run an instance of gnn test causal direction	C_727_1	def validate ( self ) try assert self . question = = string , string assert self . question_type = = exercises . perseus_question , string assert self . answers = = [ ] , string assert self . hints = = [ ] , string return super ( perseus_question , self ) . validate ( ) except assertion_error as ae raise invalid_question_exception ( string . format ( self . _dict_ ) )	0
Q_727	run an instance of gnn test causal direction	C_727_2	def progress ( count , total , suffix = string ) bar_len = number filled_len = int ( round ( bar_len * count / float ( total ) ) ) percents = round ( number . number * count / float ( total ) , number ) bar = string * filled_len + string * ( bar_len _ filled_len ) sys . stdout . write ( string ( bar , percents , string , suffix ) ) sys . stdout . flush ( )	0
Q_728	set the license list version raise cardinality_error if	C_728_0	def set_lics_list_ver ( self , doc , value ) if not self . lics_list_ver_set self . lics_list_ver_set = true vers = version . version . from_str ( value ) if vers is not none doc . creation_info . license_list_version = vers return true else raise spdx_value_error ( string ) else raise cardinality_error ( string )	1
Q_728	set the license list version raise cardinality_error if	C_728_1	def correct_scanpy ( adatas , * * kwargs ) if string in kwargs and kwargs [ string ] datasets_dimred , datasets , genes = correct ( [ var . x for var in adatas ] , [ var . var_names . values for var in adatas ] , * * kwargs ) else datasets , genes = correct ( [ var . x for var in adatas ] , [ var . var_names . values for var in adatas ] , * * kwargs ) new_adatas = [ ] for i , var in enumerate ( adatas ) var . x = datasets [ i ] new_adatas . append ( var ) if string in kwargs and kwargs [ string ] return datasets_dimred , new_adatas else return new_adatas	0
Q_728	set the license list version raise cardinality_error if	C_728_2	def _validate_subjects ( cursor , model ) subject_vocab = [ var [ number ] for var in acquire_subject_vocabulary ( cursor ) ] subjects = model . metadata . get ( string , [ ] ) invalid_subjects = [ s for s in subjects if s not in subject_vocab ] if invalid_subjects raise exceptions . invalid_metadata ( string , invalid_subjects )	0
Q_729	load client secret from the give filename	C_729_0	def _load_client_secrets ( filename ) client_type , client_info = clientsecrets . loadfile ( filename ) if client_type = clientsecrets . type_web raise value_error ( string string . format ( client_type ) ) return client_info [ string ] , client_info [ string ]	1
Q_729	load client secret from the give filename	C_729_1	def _authenticate ( self ) response = self . _get ( string . format ( self . domain ) ) self . domain_id = response [ string ] [ string ]	0
Q_729	load client secret from the give filename	C_729_2	def _plot ( self , axes_list ) axes = axes_list [ number ] if self . plot_settings axes . imshow ( self . data [ string ] , cmap = self . plot_settings [ string ] , interpolation = self . plot_settings [ string ] , extent = self . data [ string ] ) axes . set_xlabel ( self . plot_settings [ string ] ) axes . set_ylabel ( self . plot_settings [ string ] ) axes . set_title ( self . plot_settings [ string ] ) self . _update ( axes_list )	0
Q_730	"r tagstr "" calculate the relative permittivity of pure water as a function of"	C_730_0	def permittivity_iapws ( t , rho ) r dipole = number . number e _ number polarizability = number . number e _ number mw = number . number number ih = [ number , number , number , number , number , number , number , number , number , number , number ] jh = [ number . number , number , number . number , number . number , number . number , number . number , number , number , number , number . number , number ] nh = [ number . number , _ number . number , number . number , number . number , _ number . number , _ number . number , number . number e _ number , _ . number e _ number , number . number e _ number , number . number e _ number , _ number . number e _ number ] delta = rho / number . tau = number . number number / t g = ( number + sum ( [ nh [ var ] * delta * * ih [ var ] * tau * * jh [ var ] for var in range ( number ) ] ) + number . number e _ number * delta * ( t / number . _ number ) * * _ number . number ) a = n_a * dipole * * number * ( rho / mw ) * g / epsilon_0 / k / t b = n_a * polarizability * ( rho / mw ) / number . / epsilon_0 epsilon = ( number . + a + number . * b + ( number . + number . * a + number . * b + a * * number + number . * a * b + number . * b * * number ) * * number . number ) / ( number . _ number . * b ) return epsilon	1
Q_730	"r tagstr "" calculate the relative permittivity of pure water as a function of"	C_730_1	def parse_challenge_message ( self , msg2 ) msg2 = base64 . b64decode ( msg2 ) self . challenge_message = challenge_message ( msg2 )	0
Q_730	"r tagstr "" calculate the relative permittivity of pure water as a function of"	C_730_2	def generate_topology ( non_omm_topology , non_element_types = none , residues = none ) if non_element_types is none non_element_types = set ( ) if isinstance ( non_omm_topology , pmd . structure ) return _topology_from_parmed ( non_omm_topology , non_element_types ) elif has_mbuild mb = import_ ( string ) if ( non_omm_topology , mb . compound ) pmd_compound_structure = non_omm_topology . to_parmed ( residues = residues ) return _topology_from_parmed ( pmd_compound_structure , non_element_types ) else raise foyer_error ( string string string parmed . structure string string mbuild . compound string string openmm . app . topology string . format ( non_omm_topology ) )	0
Q_731	"r tagstr "" calculate the viscosity index of a liquid"	C_731_0	def viscosity_index ( nu_40 , nu_100 , rounding = false ) r nu_40 , nu_100 = nu_40 * number , nu_100 * number if nu_100 < number return none elif nu_100 < number l = np . interp ( nu_100 , vi_nus , vi_ls ) h = np . interp ( nu_100 , vi_nus , vi_hs ) else l = number . number * nu_100 * * number + number . number * nu_100 _ number h = number . number * nu_100 * * number + number . number * nu_100 _ number if nu_40 > h vi = ( l _ nu_40 ) / ( l _ h ) * number else n = ( log ( h ) _ log ( nu_40 ) ) / log ( nu_100 ) vi = ( number * * n _ number ) / number . number number + number if rounding vi = _round_whole_even ( vi ) return vi	1
Q_731	"r tagstr "" calculate the viscosity index of a liquid"	C_731_1	def map ( self , func ) for var in self . table for i , cell in enumerate ( var ) var [ i ] = func ( cell )	0
Q_731	"r tagstr "" calculate the viscosity index of a liquid"	C_731_2	def ignore_copy ( func ) def _getattr ( self , name ) if name in [ string , string , string , string , string ] raise attribute_error ( string s string s string ( self . _class_ . _name_ , name ) ) return func ( self , name ) return _getattr	0
Q_732	get the enterprise customer id give an enterprise customer catalog id	C_732_0	def get_enterprise_customer_from_catalog_id ( catalog_id ) try return str ( enterprise_customer_catalog . objects . get ( pk = catalog_id ) . enterprise_customer . uuid ) except enterprise_customer_catalog . does_not_exist return none	1
Q_732	get the enterprise customer id give an enterprise customer catalog id	C_732_1	def _store_cash_balances_per_currency ( self , cash_balances ) cash = self . model . get_cash_asset_class ( ) for var in cash_balances item = cash_balance ( var ) item . parent = cash quantity = cash_balances [ var ] [ string ] item . value = decimal ( quantity ) item . currency = var cash . stocks . append ( item ) self . model . stocks . append ( item )	0
Q_732	get the enterprise customer id give an enterprise customer catalog id	C_732_2	def job_error_message ( self , job , queue , to_be_requeued , exception , trace = none ) return string ( queue . _cached_name , job . pk . get ( ) , job . _cached_identifier , str ( exception ) , string if to_be_requeued else string )	0
Q_733	handle post request _ handle form submission	C_733_0	def post ( self , request , customer_uuid ) enterprise_customer = enterprise_customer . objects . get ( uuid = customer_uuid ) manage_learners_form = manage_learners_form ( request . post , request . files , user = request . user , enterprise_customer = enterprise_customer ) if manage_learners_form . is_valid ( ) email_field_as_bulk_input = split_usernames_and_emails ( manage_learners_form . cleaned_data [ manage_learners_form . fields . email_or_username ] ) is_bulk_entry = len ( email_field_as_bulk_input ) > number mode = manage_learners_form . cleaned_data [ manage_learners_form . fields . mode ] if mode = = manage_learners_form . modes . mode_singular and not is_bulk_entry linked_learners = self . _handle_singular ( enterprise_customer , manage_learners_form ) elif mode = = manage_learners_form . modes . mode_singular linked_learners = self . _handle_bulk_upload ( enterprise_customer , manage_learners_form , request , email_list = email_field_as_bulk_input ) else linked_learners = self . _handle_bulk_upload ( enterprise_customer , manage_learners_form , request ) if manage_learners_form . is_valid ( ) course_details = manage_learners_form . cleaned_data . get ( manage_learners_form . fields . course ) program_details = manage_learners_form . cleaned_data . get ( manage_learners_form . fields . program ) notification_type = manage_learners_form . cleaned_data . get ( manage_learners_form . fields . notify ) notify = notification_type = = manage_learners_form . notification_types . by_email course_id = none if course_details course_id = course_details [ string ] if course_id or program_details course_mode = manage_learners_form . cleaned_data [ manage_learners_form . fields . course_mode ] self . _enroll_users ( request = request , enterprise_customer = enterprise_customer , emails = linked_learners , mode = course_mode , course_id = course_id , program_details = program_details , notify = notify , ) manage_learners_url = reverse ( string + url_names . manage_learners , args = ( customer_uuid , ) ) search_keyword = self . get_search_keyword ( request ) if search_keyword manage_learners_url = manage_learners_url + string + search_keyword return http_response_redirect ( manage_learners_url ) context = self . _build_context ( request , customer_uuid ) context . update ( { self . context_parameters . manage_learners_form manage_learners_form } ) return render ( request , self . template , context )	1
Q_733	handle post request _ handle form submission	C_733_1	"def _build_command ( self ) command = [ self . _vpcs_path ( ) ] command . extend ( [ string , str ( self . _internal_console_port ) ] ) command . extend ( [ string , str ( self . _manager . get_mac_id ( self . id ) ) ] ) command . extend ( [ string , string ] ) command . extend ( [ string ] ) if self . _vpcs_version > = parse_version ( string ) command . extend ( [ string ] ) else log . warn ( string ) if not self . _local_udp_tunnel self . _local_udp_tunnel = self . _create_local_udp_tunnel ( ) nio = self . _local_udp_tunnel [ number ] if nio if isinstance ( nio , nioudp ) command . extend ( [ string , str ( nio . lport ) ] ) command . extend ( [ string , str ( nio . rport ) ] ) try command . extend ( [ string , socket . gethostbyname ( nio . rhost ) ] ) except socket . gaierror as e raise vpcs_error ( string t resolve hostname { } string _ e string _ d "" , nio . tap_device ] ) if self . script_file command . extend ( [ os . path . basename ( self . script_file ) ] ) return command"	0
Q_733	handle post request _ handle form submission	C_733_2	def save_fast_format ( self , filename ) items , _ = zip ( * sorted ( self . items . items ( ) , key = lambda x x [ number ] ) ) items = { string items , string self . unk_index , string self . name } json . dump ( items , open ( string . format ( filename ) , string ) ) np . save ( open ( string . format ( filename ) , string ) , self . vectors )	0
Q_734	snap bound to tile boundary of specific zoom level	C_734_0	def snap_bounds ( bounds = none , pyramid = none , zoom = none ) if not isinstance ( bounds , ( tuple , list ) ) raise type_error ( string ) if len ( bounds ) = number raise value_error ( string ) if not isinstance ( pyramid , buffered_tile_pyramid ) raise type_error ( string ) bounds = bounds ( * bounds ) lb = pyramid . tile_from_xy ( bounds . left , bounds . bottom , zoom , on_edge_use = string ) . bounds rt = pyramid . tile_from_xy ( bounds . right , bounds . top , zoom , on_edge_use = string ) . bounds return bounds ( lb . left , lb . bottom , rt . right , rt . top )	1
Q_734	snap bound to tile boundary of specific zoom level	C_734_1	def email_domain_disposable ( value ) domain = helpers . get_domain_from_email_address ( value ) if domain . lower ( ) in disposable_domains raise validation_error ( message_use_company_email )	0
Q_734	snap bound to tile boundary of specific zoom level	C_734_2	def remove ( self , * filenodes ) for var in filenodes if var . path in ( n . path for n in self . removed ) raise node_already_removed_error ( string string var . path ) if var . path in ( n . path for n in self . changed ) raise node_already_changed_error ( string string var . path ) self . removed . append ( var )	0
Q_735	clip bound by clip	C_735_0	def clip_bounds ( bounds = none , clip = none ) bounds = bounds ( * bounds ) clip = bounds ( * clip ) return bounds ( max ( bounds . left , clip . left ) , max ( bounds . bottom , clip . bottom ) , min ( bounds . right , clip . right ) , min ( bounds . top , clip . top ) )	1
Q_735	clip bound by clip	C_735_1	def sprite_file ( map , sprite ) map = map . render ( ) sprite_maps = _get_cache ( string ) sprite_map = sprite_maps . get ( map ) sprite_name = string . unquoted ( sprite ) . value sprite = sprite_map and sprite_map . get ( sprite_name ) if not sprite_map log . error ( string , map , extra = { string true } ) elif not sprite log . error ( string , sprite_name , sprite_map [ string ] , extra = { string true } ) if sprite return string ( sprite [ number ] [ number ] ) return string . unquoted ( string )	0
Q_735	clip bound by clip	C_735_2	def smiles_to_compound ( smiles , assign_descriptors = true ) it = iter ( smiles ) mol = molecule ( ) try for var in it mol ( var ) result , _ = mol ( none ) except key_error as err raise value_error ( string . format ( err ) ) result . graph . remove_node ( number ) logger . debug ( result ) if assign_descriptors molutil . assign_descriptors ( result ) return result	0
Q_736	convert resource tagstr s table name	C_736_0	def _convert_path ( path , name ) table = os . path . splitext ( path ) [ number ] table = table . replace ( os . path . sep , string ) if name is not none table = string . join ( [ table , name ] ) table = re . sub ( string , string , table ) table = table . lower ( ) return table	1
Q_736	convert resource tagstr s table name	C_736_1	def get_fields ( self , strip_labels = false ) if strip_labels return [ var [ number ] if type ( var ) in ( tuple , list ) else var for var in self . fields ] return self . fields	0
Q_736	convert resource tagstr s table name	C_736_2	def on_batch_begin ( self , last_input , last_target , * * kwargs ) string self . acc_samples + = last_input . shape [ number ] self . acc_batches + = number	0
Q_737	load an asn	C_737_0	def _load_x509 ( certificate ) source = certificate . dump ( ) cf_source = none try cf_source = cf_helpers . cf_data_from_bytes ( source ) sec_key_ref = security . sec_certificate_create_with_data ( core_foundation . k_cf_allocator_default , cf_source ) return certificate ( sec_key_ref , certificate ) finally if cf_source core_foundation . cf_release ( cf_source )	1
Q_737	load an asn	C_737_1	def get_token ( self , code , headers = none , * * kwargs ) self . _check_configuration ( string , string , string , string , string ) url = string ( self . site , quote ( self . token_url ) ) data = { string self . redirect_uri , string self . client_id , string self . client_secret , string code , } data . update ( kwargs ) return self . _make_request ( url , data = data , headers = headers )	0
Q_737	load an asn	C_737_2	def add_annotation ( self , entity , annotation , value ) url = self . base_path + string data = { string entity [ string ] , string annotation [ string ] , string value , string entity [ string ] , string annotation [ string ] } return self . post ( url , data )	0
Q_738	create a shader program	C_738_0	def create ( self ) string string string out_attribs = [ ] if not self . fragment_source if self . geometry_source out_attribs = self . geometry_source . find_out_attribs ( ) else out_attribs = self . vertex_source . find_out_attribs ( ) program = self . ctx . program ( vertex_shader = self . vertex_source . source , geometry_shader = self . geometry_source . source if self . geometry_source else none , fragment_shader = self . fragment_source . source if self . fragment_source else none , tess_control_shader = self . tess_control_source . source if self . tess_control_source else none , tess_evaluation_shader = self . tess_evaluation_source . source if self . tess_evaluation_source else none , varyings = out_attribs , ) program . extra = { string self . meta } return program	1
Q_738	create a shader program	C_738_1	def left_axis_label ( self , label , position = none , rotation = number , offset = number . number number , * * kwargs ) if not position position = ( _ offset , number . / number , number . / number ) self . _labels [ string ] = ( label , position , rotation , kwargs )	0
Q_738	create a shader program	C_738_2	def upload ( self , file_path , description = none ) params = { string string } if description is not none params [ string ] = str ( description ) url = self . _url + string files = { } files [ string ] = file_path return self . _post ( url = url , param_dict = params , files = files , security_handler = self . _security_handler , proxy_url = self . _proxy_url , proxy_port = self . _proxy_port )	0
Q_739	return the phonix code for a word	C_739_0	def encode ( self , word , max_length = number , zero_pad = true ) def _start_repl ( word , src , tar , post = none ) if post for var in post if word . startswith ( src + var ) return tar + word [ len ( src ) ] elif word . startswith ( src ) return tar + word [ len ( src ) ] return word def _end_repl ( word , src , tar , pre = none ) if pre for var in pre if word . endswith ( var + src ) return word [ _ len ( src ) ] + tar elif word . endswith ( src ) return word [ _ len ( src ) ] + tar return word def _mid_repl ( word , src , tar , pre = none , post = none ) if pre or post if not pre return word [ number ] + _all_repl ( word [ number ] , src , tar , pre , post ) elif not post return _all_repl ( word [ _ number ] , src , tar , pre , post ) + word [ _ number ] return _all_repl ( word , src , tar , pre , post ) return ( word [ number ] + _all_repl ( word [ number _ number ] , src , tar , pre , post ) + word [ _ number ] ) def _all_repl ( word , src , tar , pre = none , post = none ) if pre or post if post post = post else post = frozenset ( ( string , ) ) if pre pre = pre else pre = frozenset ( ( string , ) ) for var , j in ( ( var , j ) for var in pre for j in post ) word = word . replace ( var + src + j , var + tar + j ) return word else return word . replace ( src , tar ) repl_at = ( _start_repl , _end_repl , _mid_repl , _all_repl ) sdx = string word = unicode_normalize ( string , text_type ( word . upper ( ) ) ) word = word . replace ( string , string ) word = string . join ( c for c in word if c in self . _uc_set ) if word for trans in self . _substitutions word = repl_at [ trans [ number ] ] ( word , * trans [ number ] ) if word [ number ] in self . _uc_vy_set sdx = string + word [ number ] . translate ( self . _trans ) else sdx = word [ number ] + word [ number ] . translate ( self . _trans ) sdx = self . _delete_consecutive_repeats ( sdx ) sdx = sdx . replace ( string , string ) if max_length = _ number max_length = min ( max ( number , max_length ) , number ) else max_length = number if zero_pad sdx + = string * max_length if not sdx sdx = string return sdx [ max_length ]	1
Q_739	return the phonix code for a word	C_739_1	def count_per_m ( self ) ( self . _count_per_m , value ) = self . get_cached_attr_int ( self . _count_per_m , string ) return value	0
Q_739	return the phonix code for a word	C_739_2	def as_cache_key ( self , ireq ) name , version , extras = as_tuple ( ireq ) if not extras extras_string = string else extras_string = string . format ( string . join ( extras ) ) return name , string . format ( version , extras_string )	0
Q_740	return the specify script command	C_740_0	def get_script ( self , scriptname ) script = self . description . get ( string , { } ) . get ( scriptname , none ) if script is not none if isinstance ( script , str ) or isinstance ( script , type ( u string ) ) import shlex script = shlex . split ( script ) if len ( script ) and script [ number ] . lower ( ) . endswith ( string ) if not os . path . isabs ( script [ number ] ) absscript = os . path . abspath ( os . path . join ( self . path , script [ number ] ) ) logger . debug ( string , script [ number ] , absscript ) script [ number ] = absscript import sys script = [ sys . executable ] + script return script	1
Q_740	return the specify script command	C_740_1	def load_multiple ( paths = none , first_data_line = string , filters = string , text = string , default_directory = string , quiet = true , header_only = false , transpose = false , * * kwargs ) if paths = = none paths = _s . dialogs . load_multiple ( filters , text , default_directory ) if paths is none return datas = [ ] for var in paths if _os . var . isfile ( var ) datas . append ( load ( var = var , first_data_line = first_data_line , filters = filters , text = text , default_directory = default_directory , header_only = header_only , transpose = transpose , * * kwargs ) ) return datas	0
Q_740	return the specify script command	C_740_2	def flush ( self ) if self . _cache is none return [ ] with self . _cache as c , self . _out as out c . expire ( ) now = self . _timer ( ) for var in c . values ( ) if ( not self . _in_flush_all ) and ( not self . _should_expire ( var ) ) if ( not var . is_in_flight ) and var . _op_aggregator is not none var . is_in_flight = true var . last_refresh_timestamp = now out . append ( var . extract_request ( ) ) flushed_items = list ( out ) out . clear ( ) for req in flushed_items assert isinstance ( req , sc_messages . servicecontrol_services_allocate_quota_request ) return flushed_items	0
Q_741	launch a debugger for the specify program	C_741_0	def debug ( self , builddir , program ) try signal . signal ( signal . sigint , _ignore_signal ) if self . get_script ( string ) is not none return self . _debug_with_script ( builddir , program ) elif string in self . description logger . warning ( string + string , self . get_name ( ) ) return self . _debug_deprecated ( builddir , program ) else return string self finally signal . signal ( signal . sigint , signal . sig_dfl )	1
Q_741	launch a debugger for the specify program	C_741_1	def parse ( xml_input , encoding = none , expat = expat , process_namespaces = false , namespace_separator = string , disable_entities = true , * * kwargs ) handler = _dict_sax_handler ( namespace_separator = namespace_separator , * * kwargs ) if isinstance ( xml_input , _unicode ) if not encoding encoding = string xml_input = xml_input . encode ( encoding ) if not process_namespaces namespace_separator = none parser = expat . parser_create ( encoding , namespace_separator ) try parser . ordered_attributes = true except attribute_error pass parser . start_namespace_decl_handler = handler . start_namespace_decl parser . start_element_handler = handler . start_element parser . end_element_handler = handler . end_element parser . character_data_handler = handler . characters parser . buffer_text = true if disable_entities try feature = string parser . _reader . set_feature ( feature , true ) except attribute_error parser . default_handler = lambda x none parser . external_entity_ref_handler = lambda * x number if hasattr ( xml_input , string ) parser . parse_file ( xml_input ) elif isgenerator ( xml_input ) for var in xml_input parser . parse ( var , false ) parser . parse ( b string , true ) else parser . parse ( xml_input , true ) return handler . item	0
Q_741	launch a debugger for the specify program	C_741_2	def ystep ( self ) r rho = self . nb * self . rho m_axu = np . mean ( self . ax + self . u , axis = _ number ) self . y [ ] = self . prox_g ( m_axu , rho )	0
Q_742	enable all module in the current module list	C_742_0	def enable_mods ( self ) r = self . local_renderer for var in r . env . mods_enabled with self . settings ( warn_only = true ) self . enable_mod ( var )	1
Q_742	enable all module in the current module list	C_742_1	def builds ( self ) api_version = self . _get_api_version ( string ) if api_version = = string from . v2018_02_01_preview . operations import builds_operations as operation_class else raise not_implemented_error ( string . format ( api_version ) ) return operation_class ( self . _client , self . config , serializer ( self . _models_dict ( api_version ) ) , deserializer ( self . _models_dict ( api_version ) ) )	0
Q_742	enable all module in the current module list	C_742_2	def read_flags_from_files ( self , argv , force_gnu = true ) rest_of_args = argv new_argv = [ ] while rest_of_args current_arg = rest_of_args [ number ] rest_of_args = rest_of_args [ number ] if self . _is_flag_file_directive ( current_arg ) if current_arg = = string or current_arg = = string if not rest_of_args raise exceptions . illegal_flag_value_error ( string ) flag_filename = os . path . expanduser ( rest_of_args [ number ] ) rest_of_args = rest_of_args [ number ] else flag_filename = self . extract_filename ( current_arg ) new_argv . extend ( self . _get_flag_file_lines ( flag_filename ) ) else new_argv . append ( current_arg ) if current_arg = = string break if not current_arg . startswith ( string ) if not force_gnu and not self . _dict_ [ string ] break else if ( string not in current_arg and rest_of_args and not rest_of_args [ number ] . startswith ( string ) ) fl = self . flag_dict ( ) name = current_arg . lstrip ( string ) if name in fl and not fl [ name ] . boolean current_arg = rest_of_args [ number ] rest_of_args = rest_of_args [ number ] new_argv . append ( current_arg ) if rest_of_args new_argv . extend ( rest_of_args ) return new_argv	0
Q_743	print header for vcf file	C_743_0	def vcfheader ( data , names , ofile ) if data . paramsdict [ string ] reference = data . paramsdict [ string ] else reference = string header = string string string . format ( date = time . strftime ( string ) , version = _version_ , reference = os . path . basename ( reference ) , mincov = data . paramsdict [ string ] , maxsh = data . paramsdict [ string ] , names = string . join ( names ) ) ofile . write ( header )	1
Q_743	print header for vcf file	C_743_1	def pillar ( tgt , delimiter = default_target_delim ) matchers = salt . loader . matchers ( _opts_ ) try return matchers [ string ] ( tgt , delimiter = delimiter , opts = _opts_ ) except exception as exc log . exception ( exc ) return false	0
Q_743	print header for vcf file	C_743_2	def get_results_range ( self ) specs = results_range_dict ( result = string ) sample = self . get_sample ( ) if not sample return specs service_uid = self . get_service_uid ( ) sample_range = sample . get_results_range_dict ( ) return sample_range . get ( service_uid , specs )	0
Q_744	return the reflection of a point through origin	C_744_0	def reflect ( x , y , x0 , y0 , d = number . number , a = number ) string string string return coordinates ( x0 , y0 , d * distance ( x0 , y0 , x , y ) , a + angle ( x0 , y0 , x , y ) )	1
Q_744	return the reflection of a point through origin	C_744_1	def gen_keywords ( * args union [ ansi_colors , ansi_styles ] , * * kwargs union [ ansi_colors , ansi_styles ] ) _ > tuple fields tuple = tuple ( ) values tuple = tuple ( ) for var in args fields + = var . _fields values + = var for prefix , var in kwargs . items ( ) fields + = tuple ( map ( lambda x string . join ( [ prefix , x ] ) , var . _fields ) ) values + = var return namedtuple ( string , fields ) ( * values )	0
Q_744	return the reflection of a point through origin	C_744_2	def _has_x ( self , kwargs ) return ( ( string in kwargs ) or ( self . _element_x in kwargs ) or ( self . _type = = number and self . _element_1mx in kwargs ) )	0
Q_745	update an exist floating ip	C_745_0	def update_floatingip ( context , id , content ) log . info ( string ( id , context . tenant_id , content ) ) if string not in content raise n_exc . bad_request ( resource = string , msg = string ) requested_ports = [ ] if content . get ( string ) requested_ports = [ { string content . get ( string ) } ] flip = _update_flip ( context , id , ip_types . floating , requested_ports ) return v . _make_floating_ip_dict ( flip )	1
Q_745	update an exist floating ip	C_745_1	def copy ( self ) clone = self . _class_ ( ) clone . fields = self . copy_fields_dict ( self . fields ) clone . default_fields = self . copy_fields_dict ( self . default_fields ) clone . overloaded_fields = self . overloaded_fields . copy ( ) clone . underlayer = self . underlayer clone . explicit = self . explicit clone . raw_packet_cache = self . raw_packet_cache clone . raw_packet_cache_fields = self . copy_fields_dict ( self . raw_packet_cache_fields ) clone . wirelen = self . wirelen clone . post_transforms = self . post_transforms [ ] clone . payload = self . payload . copy ( ) clone . payload . add_underlayer ( clone ) clone . time = self . time return clone	0
Q_745	update an exist floating ip	C_745_2	def groups_set_purpose ( self , * , channel str , purpose str , * * kwargs ) _ > slack_response kwargs . update ( { string channel , string purpose } ) return self . api_call ( string , json = kwargs )	0
Q_746	remove the provider with the give id or term uri	C_746_0	def remove_provider ( self , id ) if id in self . providers p = self . providers . get ( id , false ) del self . providers [ id ] del self . concept_scheme_uri_map [ p . concept_scheme . uri ] return p elif id in self . concept_scheme_uri_map id = self . concept_scheme_uri_map [ id ] return self . remove_provider ( id ) else return false	1
Q_746	remove the provider with the give id or term uri	C_746_1	def splitparams ( path ) if string in path i = path . find ( string , path . rfind ( string ) ) else i = path . find ( string ) if i < number return path , string return path [ i ] , path [ i + number ]	0
Q_746	remove the provider with the give id or term uri	C_746_2	def non_decreasing ( values ) return all ( var < = var for var , var in zip ( values , values [ number ] ) )	0
Q_747	obtain the license in a dictionary form key by url	C_747_0	def obtain_licenses ( ) with db_connect ( ) as db_conn with db_conn . cursor ( ) as cursor cursor . execute ( string string code string version string name string url string is_valid_for_publication string string ) licenses = { var [ number ] var [ number ] for var in cursor . fetchall ( ) } return licenses	1
Q_747	obtain the license in a dictionary form key by url	C_747_1	def create_and_push_expression ( token , expressions ) if token = = string right_expr = expressions . pop ( ) expressions . append ( and ( expressions . pop ( ) , right_expr ) ) elif token = = string right_expr = expressions . pop ( ) expressions . append ( or ( expressions . pop ( ) , right_expr ) ) elif token = = string expressions . append ( not ( expressions . pop ( ) ) ) else expressions . append ( literal ( token ) )	0
Q_747	obtain the license in a dictionary form key by url	C_747_2	"def prokka_barplot ( self ) keys = ordered_dict ( ) keys [ string ] = { string string } keys [ string ] = { string string } keys [ string ] = { string string } keys [ string ] = { string string } keys [ string ] = { string string } keys [ string ] = { string string } keys [ string ] = { string string } plot_config = { string string , string string , string string cpswitch_counts_label string features "" } return bargraph . plot ( self . prokka , keys , plot_config )"	0
Q_748	filter tag accord exclude_tags option	C_748_0	def apply_exclude_tags ( self , all_tags ) filtered = copy . deepcopy ( all_tags ) for var in all_tags if var [ string ] not in self . options . exclude_tags self . warn_if_tag_not_found ( var , string ) else filtered . remove ( var ) return filtered	1
Q_748	filter tag accord exclude_tags option	C_748_1	def _calculate_minimum_silent_period ( baudrate ) _check_numerical ( baudrate , minvalue = number , description = string ) bittimes_per_charactertime = number minimum_silent_charactertimes = number . number bittime = number / float ( baudrate ) return bittime * bittimes_per_charactertime * minimum_silent_charactertimes	0
Q_748	filter tag accord exclude_tags option	C_748_2	def ctr_mass ( geom , masses ) import numpy as np from . base import safe_cast as scast if len ( geom . shape ) = number raise value_error ( string ) if len ( masses . shape ) = number raise value_error ( string ) if not geom . shape [ number ] number = = number raise value_error ( string ) if geom . shape [ number ] = number * masses . shape [ number ] and geom . shape [ number ] = masses . shape [ number ] raise value_error ( string ) if geom . shape [ number ] = = number * masses . shape [ number ] masses = masses . repeat ( number ) ctr = np . multiply ( geom , masses ) . reshape ( ( geom . shape [ number ] / / number , number ) ) . sum ( axis = number ) . squeeze ( ) / ( masses . sum ( ) / number ) return ctr	0
Q_749	parse a litezip file structure to a data structure give the path	C_749_0	def parse_litezip ( path ) struct = [ parse_collection ( path ) ] struct . extend ( [ parse_module ( var ) for var in path . iterdir ( ) if var . is_dir ( ) and var . name . startswith ( string ) ] ) return tuple ( sorted ( struct ) )	1
Q_749	parse a litezip file structure to a data structure give the path	C_749_1	def stop ( self ) self . _hw_virtualization = false yield from self . _stop_ubridge ( ) yield from self . _stop_remote_console ( ) vm_state = yield from self . _get_vm_state ( ) if vm_state = = string or vm_state = = string or vm_state = = string if self . acpi_shutdown result = yield from self . _control_vm ( string ) trial = number while true vm_state = yield from self . _get_vm_state ( ) if vm_state = = string break yield from asyncio . sleep ( number ) trial + = number if trial > = number yield from self . _control_vm ( string ) break self . status = string log . debug ( string . format ( result ) ) else result = yield from self . _control_vm ( string ) self . status = string log . debug ( string . format ( result ) ) log . info ( string { name } string . format ( name = self . name , id = self . id ) ) yield from asyncio . sleep ( number . number ) try yield from self . _modify_vm ( string ) except virtual_box_error as e log . warn ( string . format ( e ) ) for var in range ( number , self . _adapters ) nio = self . _ethernet_adapters [ var ] . get_nio ( number ) if nio yield from self . _modify_vm ( string . format ( var + number ) ) yield from self . _modify_vm ( string . format ( var + number ) ) yield from self . _modify_vm ( string . format ( var + number ) ) yield from super ( ) . stop ( )	0
Q_749	parse a litezip file structure to a data structure give the path	C_749_2	def from_jd ( jd ) start = number jd = trunc ( jd ) + number . number greg = gregorian . from_jd ( jd ) leap = isleap ( greg [ number ] ) year = greg [ number ] _ saka_epoch greg0 = gregorian . to_jd ( greg [ number ] , number , number ) yday = jd _ greg0 if leap caitra = number else caitra = number if yday < start year _ = number yday + = caitra + ( number * number ) + ( number * number ) + number + start yday _ = start if yday < caitra month = number day = yday + number else mday = yday _ caitra if ( mday < ( number * number ) ) month = trunc ( mday / number ) + number day = ( mday number ) + number else mday _ = number * number month = trunc ( mday / number ) + number day = ( mday number ) + number return ( year , month , int ( day ) )	0
Q_750	call fortran function to load cdf variable data	C_750_0	def _call_multi_fortran_z ( self , names , data_types , rec_nums , dim_sizes , input_type_code , func , epoch = false , data_offset = none , epoch16 = false ) idx , = np . where ( data_types = = input_type_code ) if len ( idx ) > number max_rec = rec_nums [ idx ] . max ( ) sub_names = np . array ( names ) [ idx ] sub_sizes = dim_sizes [ idx ] status , data = func ( self . fname , sub_names . tolist ( ) , sub_sizes , sub_sizes . sum ( ) , max_rec , len ( sub_names ) ) if status = = number if data_offset is not none data = data . astype ( int ) idx , idy , = np . where ( data < number ) data [ idx , idy ] + = data_offset if epoch data _ = number data = data . astype ( string ) if epoch16 data [ number number , ] _ = number data = data [ number number , ] * number + data [ number number , ] / number . e3 data = data . astype ( string ) sub_sizes / = number self . _process_return_multi_z ( data , sub_names , sub_sizes ) else raise io_error ( fortran_cdf . statusreporter ( status ) )	1
Q_750	call fortran function to load cdf variable data	C_750_1	def get_filter_config ( platform , filter_name , filter_options = none , terms = none , prepend = true , pillar_key = string , pillarenv = none , saltenv = none , merge_pillar = true , only_lower_merge = false , revision_id = none , revision_no = none , revision_date = true , revision_date_format = string ) if not filter_options filter_options = [ ] if not terms terms = [ ] if merge_pillar and not only_lower_merge acl_pillar_cfg = _get_pillar_cfg ( pillar_key , saltenv = saltenv , pillarenv = pillarenv ) filter_pillar_cfg = _lookup_element ( acl_pillar_cfg , filter_name ) filter_options = filter_options or filter_pillar_cfg . pop ( string , none ) if filter_pillar_cfg pillar_terms = filter_pillar_cfg . get ( string , [ ] ) terms = _merge_list_of_dict ( terms , pillar_terms , prepend = prepend ) filters = [ ] filters . append ( { filter_name { string _make_it_list ( { } , filter_name , filter_options ) , string terms } } ) return get_policy_config ( platform , filters = filters , pillar_key = pillar_key , pillarenv = pillarenv , saltenv = saltenv , merge_pillar = merge_pillar , only_lower_merge = true , revision_id = revision_id , revision_no = revision_no , revision_date = revision_date , revision_date_format = revision_date_format )	0
Q_750	call fortran function to load cdf variable data	C_750_2	def update ( customer , * * data ) if isinstance ( customer , resources . customer ) customer = customer . id http_client = http_client ( ) response , _ = http_client . patch ( routes . url ( routes . customer_resource , resource_id = customer ) , data ) return resources . customer ( * * response )	0
Q_751	read value from an excel range into a dictionary	C_751_0	def read_range ( filepath , range_expr , sheet = none , dict_generator = none ) def default_generator ( cells ) for var , var in enumerate ( cells ) for col_ind , cell in enumerate ( var ) yield ( var , col_ind ) , cell . value book = opxl . load_workbook ( filepath , data_only = true ) if _is_range_address ( range_expr ) sheet_names = [ name . upper ( ) for name in book . sheetnames ] index = sheet_names . index ( sheet . upper ( ) ) cells = book . worksheets [ index ] [ range_expr ] else cells = _get_namedrange ( book , range_expr , sheet ) if isinstance ( cells , opxl . cell . cell ) return cells . value if dict_generator is none dict_generator = default_generator gen = dict_generator ( cells ) return { keyval [ number ] keyval [ number ] for keyval in gen }	1
Q_751	read value from an excel range into a dictionary	C_751_1	def normal_range ( mean , sd , treshold = number . number ) bottom = mean _ sd * treshold top = mean + sd * treshold return ( bottom , top )	0
Q_751	read value from an excel range into a dictionary	C_751_2	def get_files_crc32 ( self ) if self . files_crc32 = = { } for var in self . get_files ( ) buffer = self . zip . read ( var ) self . files_crc32 [ var ] = crc32 ( buffer ) return self . files_crc32	0
Q_752	true if only one lambda expression be include	C_752_0	def has_lambda ( src ) module_node = ast . parse ( dedent ( src ) ) lambdaexp = [ var for var in ast . walk ( module_node ) if isinstance ( var , ast . lambda ) ] return bool ( lambdaexp )	1
Q_752	true if only one lambda expression be include	C_752_1	def set_contributor_details ( self , contdetails ) if not isinstance ( contdetails , bool ) raise twitter_search_exception ( number ) self . arguments . update ( { string string if contdetails else string } )	0
Q_752	true if only one lambda expression be include	C_752_2	def _get_serv ( ret = none , commit = false ) _options = _get_options ( ret ) try conn = psycopg2 . connect ( host = _options . get ( string ) , user = _options . get ( string ) , password = _options . get ( string ) , database = _options . get ( string ) , port = _options . get ( string ) ) except psycopg2 . operational_error as exc raise salt . exceptions . salt_master_error ( string . format ( exc = exc ) ) cursor = conn . cursor ( ) try yield cursor except psycopg2 . database_error as err error = err . args sys . stderr . write ( six . text_type ( error ) ) cursor . execute ( string ) raise err else if commit cursor . execute ( string ) else cursor . execute ( string ) finally conn . close ( )	0
Q_753	append the item to the metadata	C_753_0	def append ( self , key , value = marker , replace = true ) return self . add_item ( key , value , replace = replace )	1
Q_753	append the item to the metadata	C_753_1	def _add_tab ( self , file_path ) for var in range ( self . tab_bar . count ( ) ) widget = self . pages . widget ( var ) if not widget . is_static and file_path = = widget . file_path return var tab = subtitle_editor ( file_path , self . _subtitle_data , self ) new_index = self . tab_bar . add_tab ( self . _create_tab_name ( tab . name , tab . history . is_clean ( ) ) ) tab . history . clean_changed . connect ( lambda clean self . _clean_state_for_file_changed ( file_path , clean ) ) self . pages . add_widget ( tab ) return new_index	0
Q_753	append the item to the metadata	C_753_2	def _unordered_categories ( df , columns ) for var in columns df [ var ] = df [ var ] . astype ( categorical_dtype ( ordered = false ) ) return df	0
Q_754	return a dict of md status define in the line	C_754_0	def get_md_status ( self , line ) ret = { } splitted = split ( string , line ) if len ( splitted ) < number ret [ string ] = none ret [ string ] = none ret [ string ] = none else ret [ string ] = splitted [ _ number ] ret [ string ] = splitted [ _ number ] ret [ string ] = splitted [ _ number ] return ret	1
Q_754	return a dict of md status define in the line	C_754_1	def linregress ( x , y , return_stats = false ) a1 , a0 , r_value , p_value , stderr = scipy . stats . linregress ( x , y ) retval = a1 , a0 if return_stats retval + = r_value , p_value , stderr return retval	0
Q_754	return a dict of md status define in the line	C_754_2	def set_proto_message_event ( pb_message_event , span_data_message_event ) pb_message_event . type = span_data_message_event . type pb_message_event . id = span_data_message_event . id pb_message_event . uncompressed_size = span_data_message_event . uncompressed_size_bytes pb_message_event . compressed_size = span_data_message_event . compressed_size_bytes	0
Q_755	calculate dynamic viscosity at the specify temperature and	C_755_0	def calculate ( self , * * state ) def phi ( i , j , mu_i , mu_j ) m_i = m ( i ) m_j = m ( j ) result = ( number . number + ( mu_i / mu_j ) * * number . number * ( m_j / m_i ) * * number . number ) * * number . number result / = ( number . number / sqrt ( number . number ) ) result / = ( number . number + m_i / m_j ) * * number . number return result t = state [ string ] var = state [ string ] x_total = sum ( [ var for var , var in var . items ( ) if var in materials ] ) var = { var var [ var ] / x_total for var in var . keys ( ) if var in materials } result = number . number mu = { i materials [ i ] . mu ( t = t ) for i in var . keys ( ) } for i in var . keys ( ) sum_i = number . number for j in var . keys ( ) if j = = i continue sum_i + = var [ j ] * phi ( i , j , mu [ i ] , mu [ j ] ) result + = var [ i ] * mu [ i ] / ( var [ i ] + sum_i ) return result	1
Q_755	calculate dynamic viscosity at the specify temperature and	C_755_1	def import_phantom_module ( xml_file ) import lxml . etree as etree object_cache = { } tree = etree . parse ( xml_file ) root = tree . getroot ( ) all_nodes = dict ( [ ( var . attrib [ string ] , var ) for var in root ] ) def _get_bases ( node , recurse = false ) bases = [ x . attrib [ string ] for x in node . findall ( string ) ] if recurse j = number while true try b = bases [ j ] except index_error break if b in all_nodes bases . extend ( _get_bases ( all_nodes [ b ] ) ) j + = number return bases type_index = [ string , string , string , string ] def base_cmp ( a , b ) x = cmp ( type_index . index ( a . tag ) , type_index . index ( b . tag ) ) if x = number return x if a . tag = = string and b . tag = = string a_bases = _get_bases ( a , recurse = true ) b_bases = _get_bases ( b , recurse = true ) x = cmp ( len ( a_bases ) , len ( b_bases ) ) if x = number return x if a . attrib [ string ] in b_bases return _ number if b . attrib [ string ] in a_bases return number return cmp ( a . attrib [ string ] . count ( string ) , b . attrib [ string ] . count ( string ) ) nodes = root . getchildren ( ) nodes . sort ( base_cmp ) for node in nodes name = node . attrib [ string ] doc = ( node . text or string ) . decode ( string ) + string if doc = = string doc = string parent = name while true parent = string . join ( parent . split ( string ) [ _ number ] ) if not parent break if parent in object_cache break obj = imp . new_module ( parent ) object_cache [ parent ] = obj sys . modules [ parent ] = obj if node . tag = = string obj = imp . new_module ( name ) obj . _doc_ = doc sys . modules [ name ] = obj elif node . tag = = string bases = [ object_cache [ b ] for b in _get_bases ( node ) if b in object_cache ] bases . append ( object ) init = lambda self none init . _doc_ = doc obj = type ( name , tuple ( bases ) , { string doc , string init } ) obj . _name_ = name . split ( string ) [ _ number ] elif node . tag = = string funcname = node . attrib [ string ] . split ( string ) [ _ number ] argspec = node . attrib . get ( string ) if argspec argspec = re . sub ( string , string , argspec ) doc = string ( funcname , argspec , doc ) obj = lambda number obj . _argspec_is_invalid_ = true if sys . version_info [ number ] > = number obj . _name_ = funcname else obj . _name_ = funcname obj . _name_ = name obj . _doc_ = doc if inspect . isclass ( object_cache [ parent ] ) obj . _objclass_ = object_cache [ parent ] else class dummy ( object ) pass obj = dummy ( ) obj . _name_ = name obj . _doc_ = doc if inspect . isclass ( object_cache [ parent ] ) obj . _get_ = lambda none object_cache [ name ] = obj if parent if inspect . ismodule ( object_cache [ parent ] ) obj . _module_ = parent setattr ( object_cache [ parent ] , name . split ( string ) [ _ number ] , obj ) for node in root obj = object_cache . get ( node . attrib [ string ] ) if obj is none continue for ref in node . findall ( string ) if node . tag = = string if ref . attrib [ string ] . startswith ( node . attrib [ string ] + string ) setattr ( obj , ref . attrib [ string ] , object_cache . get ( ref . attrib [ string ] ) ) else setattr ( obj , ref . attrib [ string ] , object_cache . get ( ref . attrib [ string ] ) )	0
Q_755	calculate dynamic viscosity at the specify temperature and	C_755_2	def update_product_version ( id , * * kwargs ) content = update_product_version_raw ( id , * * kwargs ) if content return utils . format_json ( content )	0
Q_756	this pipe get filename or file object from previous pipe and read the	C_756_0	def readline ( prev , filename = none , mode = string , trim = str . rstrip , start = number , end = sys . maxsize ) if prev is none if filename is none raise exception ( string ) elif is_str_type ( filename ) file_list = [ filename , ] else file_list = filename else file_list = prev for var in file_list if isinstance ( var , file_type ) fd = var else fd = open ( var , mode ) try if start < = number and end = = sys . maxsize for line in fd yield trim ( line ) else for line_no , line in enumerate ( fd , number ) if line_no < start continue yield trim ( line ) if line_no > = end break finally if fd = var fd . close ( )	1
Q_756	this pipe get filename or file object from previous pipe and read the	C_756_1	def comm_new_best_to_bgp_peers ( self , new_best_path ) comm_attr = new_best_path . get_pattr ( bgp_attr_type_communities ) if comm_attr comm_attr_na = comm_attr . has_comm_attr ( bgp_path_attribute_communities . no_advertise ) if comm_attr_na log . debug ( string string , comm_attr_na ) return qualified_peers = self . _collect_peers_of_interest ( new_best_path ) for var in qualified_peers var . communicate_path ( new_best_path )	0
Q_756	this pipe get filename or file object from previous pipe and read the	C_756_2	def packages ( self , login = none , platform = none , package_type = none , type_ = none , access = none ) logger . debug ( string ) method = self . _anaconda_client_api . user_packages return self . _create_worker ( method , login = login , platform = platform , package_type = package_type , type_ = type_ , access = access )	0
Q_757	aggregate all process_commands and the designate output file into a	C_757_0	def _launch_process_group ( self , process_commands , streams_path ) processes = [ ] for var , var in process_commands job_timestamp = time . strftime ( string ) basename = string ( self . batch_name , job_timestamp , var ) stdout_path = os . path . join ( streams_path , string ( basename , var ) ) stderr_path = os . path . join ( streams_path , string ( basename , var ) ) process = { string var , string var , string stdout_path , string stderr_path } processes . append ( process ) json_path = os . path . join ( self . root_directory , self . json_name ( var ) ) with open ( json_path , string ) as json_file json . dump ( processes , json_file , sort_keys = true , indent = number ) p = subprocess . popen ( [ self . script_path , json_path , self . batch_name , str ( len ( processes ) ) , str ( self . max_concurrency ) ] ) if p . wait ( ) = number raise environment_error ( string p . poll ( ) )	1
Q_757	aggregate all process_commands and the designate output file into a	C_757_1	def parse ( feedparser , args ) entries = [ ] args = irc3 . utils . config ( args ) max_date = datetime . datetime . now ( ) _ datetime . timedelta ( days = number ) for var in args [ string ] try with open ( var + string ) as fd updated = fd . read ( ) . strip ( ) except ( os_error , io_error ) updated = string feed = feedparser . parse ( var ) for e in feed . entries if e . updated < = updated continue try updated_parsed = e . updated_parsed except attribute_error continue if datetime . datetime ( * updated_parsed [ number ] ) < max_date continue e [ string ] = var e [ string ] = args entries . append ( ( e . updated , e ) ) if entries entries = sorted ( entries , key = itemgetter ( number ) ) with open ( var + string , string ) as fd fd . write ( str ( entries [ _ number ] [ number ] ) ) return entries	0
Q_757	aggregate all process_commands and the designate output file into a	C_757_2	def get_filter_args_for_all_events_from_channel ( token_network_address token_network_address , channel_identifier channel_id , contract_manager contract_manager , from_block block_specification = genesis_block_number , to_block block_specification = string , ) _ > dict event_filter_params = get_filter_args_for_specific_event_from_channel ( token_network_address = token_network_address , channel_identifier = channel_identifier , event_name = channel_event . opened , contract_manager = contract_manager , from_block = from_block , to_block = to_block , ) event_filter_params [ string ] = [ none , event_filter_params [ string ] [ number ] ] return event_filter_params	0
Q_758	simple replacement for numpy linspace	C_758_0	def linspace ( self , start , stop , n ) if n = = number return [ start ] l = [ number . number ] * n nm1 = n _ number nm1inv = number . number / nm1 for var in range ( n ) l [ var ] = nm1inv * ( start * ( nm1 _ var ) + stop * var ) return l	1
Q_758	simple replacement for numpy linspace	C_758_1	def set_key_state ( self , key , state ) key . state = state self . renderer . draw_key ( self . surface , key )	0
Q_758	simple replacement for numpy linspace	C_758_2	def n_times ( n , f , * args , * * kwargs ) r for var in xrange ( n ) f ( * args , * * kwargs )	0
Q_759	load all plugins from dgit extension	C_759_0	def discover_all_plugins ( self ) for var in pkg_resources . iter_entry_points ( string ) m = var . load ( ) m . setup ( self )	1
Q_759	load all plugins from dgit extension	C_759_1	def file_remove_tags ( object_id , input_params = { } , always_retry = true , * * kwargs ) return dxhttp_request ( string object_id , input_params , always_retry = always_retry , * * kwargs )	0
Q_759	load all plugins from dgit extension	C_759_2	def get_work_items ( self , * * filters ) filter_string = string for var , var in filters . iteritems ( ) filter_string + = string s string ( var , var ) url = string string filter_string try response = self . get ( url ) except requests . exceptions . read_timeout return string if response . status_code = number return none output = xmltodict . parse ( response . text ) if string not in output [ string ] return none else workitems = [ ] if isinstance ( output [ string ] [ string ] , list ) for wi in output [ string ] [ string ] workitems . append ( workitem ( wi ) ) else workitems . append ( workitem ( output [ string ] [ string ] ) ) return workitems	0
Q_760	create a panda data_frame from a numpy ndarray	C_760_0	def pprint ( arr , columns = ( string , string ) , names = ( string , string ) , max_rows = number , precision = number ) if max_rows is true pd . set_option ( string , number ) elif type ( max_rows ) is int pd . set_option ( string , max_rows ) pd . set_option ( string , precision ) df = pd . data_frame ( arr . flatten ( ) , index = arr [ string ] . flatten ( ) , columns = columns ) df . columns = names return df . style . format ( { names [ number ] string , names [ number ] string } )	1
Q_760	create a panda data_frame from a numpy ndarray	C_760_1	def delete ( table , where = ( ) , * * kwargs ) string string string where = dict ( where , * * kwargs ) . items ( ) sql , args = make_sql ( string , table , where = where ) return execute ( sql , args ) . rowcount	0
Q_760	create a panda data_frame from a numpy ndarray	C_760_2	"def _read_single_query_result ( rs , field_names ) rf = string_io . string_io ( rs ) def readline ( ) l = rf . readline ( ) if not l . endswith ( string ) raise eof_error ( ) return l . strip ( ) result = result ( ) try l = readline ( ) assert l . startswith ( string query_str = l [ len ( string string string l = readline ( ) if l . startswith ( string assert len ( field_names ) = = len ( fns ) l = readline ( ) assert l . endswith ( string ) nhits = int ( l [ len ( string t "" ) assert len ( field_vals ) = = len ( field_names ) fields = dict ( zip ( field_names , field_vals ) ) result . hits . append ( hit ( fields ) ) nhits _ = number return result , rf . read ( ) except eof_error return none , rs"	0
Q_761	convert give duration in millisecond into a human_readable representation i	C_761_0	def get_human_readable_time ( time_ms ) millis = time_ms number secs = ( time_ms / / number ) number mins = ( time_ms / / number ) number hours = ( time_ms / / number ) number days = ( time_ms / / number ) res = string if days > number res + = string days elif days = = number res + = string if hours > number or ( hours = = number and res ) res + = string hours elif hours = = number res + = string if mins > number or ( mins = = number and res ) res + = string mins elif mins = = number res + = string if days = = number and hours = = number res + = string secs if not res res = string millis return res . strip ( )	1
Q_761	convert give duration in millisecond into a human_readable representation i	C_761_1	def _handle_results ( self , success , result , expected_failures = tuple ( ) ) if not success and isinstance ( result , expected_failures ) return [ ] elif success return dict_factory ( * result . results ) if result else [ ] else raise result	0
Q_761	convert give duration in millisecond into a human_readable representation i	C_761_2	def plot_sampler_fingerprint ( sampler , hyperprior , weights = none , cutoff_weight = none , nbins = none , labels = none , burn = number , chain_mask = none , temp_idx = number , points = none , plot_samples = false , sample_color = string , point_color = none , point_lw = number , title = string , rot_x_labels = false , figsize = none ) try k = sampler . flatchain . shape [ _ number ] except attribute_error k = sampler . shape [ _ number ] if isinstance ( sampler , emcee . ensemble_sampler ) if chain_mask is none chain_mask = scipy . ones ( sampler . chain . shape [ number ] , dtype = bool ) flat_trace = sampler . chain [ chain_mask , burn , ] flat_trace = flat_trace . reshape ( ( _ number , k ) ) elif isinstance ( sampler , emcee . pt_sampler ) if chain_mask is none chain_mask = scipy . ones ( sampler . nwalkers , dtype = bool ) flat_trace = sampler . chain [ temp_idx , chain_mask , burn , ] flat_trace = flat_trace . reshape ( ( _ number , k ) ) elif isinstance ( sampler , scipy . ndarray ) if sampler . ndim = = number if chain_mask is none chain_mask = scipy . ones ( sampler . shape [ number ] , dtype = bool ) flat_trace = sampler [ temp_idx , chain_mask , burn , ] flat_trace = flat_trace . reshape ( ( _ number , k ) ) if weights is not none weights = weights [ temp_idx , chain_mask , burn ] weights = weights . ravel ( ) elif sampler . ndim = = number if chain_mask is none chain_mask = scipy . ones ( sampler . shape [ number ] , dtype = bool ) flat_trace = sampler [ chain_mask , burn , ] flat_trace = flat_trace . reshape ( ( _ number , k ) ) if weights is not none weights = weights [ chain_mask , burn ] weights = weights . ravel ( ) elif sampler . ndim = = number flat_trace = sampler [ burn , ] flat_trace = flat_trace . reshape ( ( _ number , k ) ) if weights is not none weights = weights [ burn ] weights = weights . ravel ( ) if cutoff_weight is not none and weights is not none mask = weights > = cutoff_weight * weights . max ( ) flat_trace = flat_trace [ mask , ] weights = weights [ mask ] else raise value_error ( string ( type ( sampler ) , ) ) if labels is none labels = [ string ] * k u = scipy . asarray ( [ hyperprior . elementwise_cdf ( var ) for var in flat_trace ] , dtype = float ) . t if nbins is none lq , uq = scipy . stats . scoreatpercentile ( u , [ number , number ] , axis = number ) h = number . number * ( uq _ lq ) / u . shape [ number ] * * ( number . number / number . number ) n = scipy . asarray ( scipy . ceil ( number . number / h ) , dtype = int ) else try iter ( nbins ) n = nbins except type_error n = nbins * scipy . ones ( u . shape [ number ] ) hist = [ scipy . stats . histogram ( uv , numbins = nv , defaultlimits = [ number , number ] , weights = weights ) for uv , nv in zip ( u , n ) ] max_ct = max ( [ max ( h . count ) for h in hist ] ) min_ct = min ( [ min ( h . count ) for h in hist ] ) f = plt . figure ( figsize = figsize ) a = f . add_subplot ( number , number , number ) for i , ( h , pn ) in enumerate ( zip ( hist , labels ) ) a . imshow ( scipy . atleast_2d ( scipy . asarray ( h . count [ _ number ] , dtype = float ) ) . t , cmap = string , interpolation = string , vmin = min_ct , vmax = max_ct , extent = ( i , i + number , number , number ) , aspect = string ) if plot_samples for var in u for i , uv in enumerate ( var ) a . plot ( [ i , i + number ] , [ uv , uv ] , sample_color , alpha = number . number ) if points is not none points = scipy . atleast_2d ( scipy . asarray ( points , dtype = float ) ) u_points = [ hyperprior . elementwise_cdf ( var ) for var in points ] if point_color is none c_cycle = itertools . cycle ( [ string , string , string , string , string , string , string ] ) else c_cycle = itertools . cycle ( scipy . atleast_1d ( point_color ) ) for var in u_points c = c_cycle . next ( ) for i , uv in enumerate ( var ) a . plot ( [ i , i + number ] , [ uv , uv ] , color = c , lw = point_lw ) a . set_xlim ( number , len ( hist ) ) a . set_ylim ( number , number ) a . set_xticks ( number . number + scipy . arange ( number , len ( hist ) , dtype = float ) ) a . set_xticklabels ( labels ) if rot_x_labels plt . setp ( a . xaxis . get_majorticklabels ( ) , rotation = number ) a . set_xlabel ( string ) a . set_ylabel ( string ) a . set_title ( title ) return f , a	0
Q_762	shut down the specify virtual machine	C_762_0	def shutdown_roles ( self , service_name , deployment_name , role_names , post_shutdown_action = string ) _validate_not_none ( string , service_name ) _validate_not_none ( string , deployment_name ) _validate_not_none ( string , role_names ) _validate_not_none ( string , post_shutdown_action ) return self . _perform_post ( self . _get_roles_operations_path ( service_name , deployment_name ) , _xml_serializer . shutdown_roles_operation_to_xml ( role_names , post_shutdown_action ) , as_async = true )	1
Q_762	shut down the specify virtual machine	C_762_1	def stack ( self , column_name , new_column_name = none , drop_na = false , new_column_type = none ) column_name = str ( column_name ) if column_name not in self . column_names ( ) raise value_error ( string string string ) stack_column_type = self [ column_name ] . dtype if ( stack_column_type not in [ dict , array . array , list ] ) raise type_error ( string ) if new_column_type is not none if type ( new_column_type ) is type new_column_type = [ new_column_type ] if ( stack_column_type in [ list , array . array ] ) and len ( new_column_type ) = number raise value_error ( string ) if ( stack_column_type in [ dict ] ) and len ( new_column_type ) = number raise value_error ( string ) if ( new_column_name is not none ) if stack_column_type = = dict if ( type ( new_column_name ) is not list ) raise type_error ( string ) elif ( len ( new_column_name ) = number ) raise type_error ( string ) else if ( type ( new_column_name ) = str ) raise type_error ( string ) new_column_name = [ new_column_name ] for var in new_column_name if ( var in self . column_names ( ) ) and ( var = column_name ) raise value_error ( string string string ) else if stack_column_type = = dict new_column_name = [ string , string ] else new_column_name = [ string ] head_row = s_array ( self [ column_name ] . head ( number ) ) . dropna ( ) if ( len ( head_row ) = = number ) raise value_error ( string ) if new_column_type is none if stack_column_type = = dict keys = [ ] values = [ ] for row in head_row for val in row keys . append ( val ) if val is not none values . append ( row [ val ] ) new_column_type = [ infer_type_of_list ( keys ) , infer_type_of_list ( values ) ] else values = [ v for v in itertools . chain . from_iterable ( head_row ) ] new_column_type = [ infer_type_of_list ( values ) ] with cython_context ( ) return s_frame ( _proxy = self . _proxy_ . stack ( column_name , new_column_name , new_column_type , drop_na ) )	0
Q_762	shut down the specify virtual machine	C_762_2	def has_repo ( self , repo ) url = self . _build_url ( string , repo , base_url = self . _api ) return self . _boolean ( self . _get ( url ) , number , number )	0
Q_763	receive a batch of message at once	C_763_0	def fetch_next ( self , max_batch_size = none , timeout = none ) self . _can_run ( ) wrapped_batch = [ ] max_batch_size = max_batch_size or self . _handler . _prefetch try timeout_ms = number * timeout if timeout else number batch = self . _handler . receive_message_batch ( max_batch_size = max_batch_size , timeout = timeout_ms ) for var in batch message = self . _build_message ( var ) wrapped_batch . append ( message ) except exception as e self . _handle_exception ( e ) return wrapped_batch	1
Q_763	receive a batch of message at once	C_763_1	def get_property_value_for_brok ( self , prop , tab ) entry = tab [ prop ] value = getattr ( self , prop , entry . default ) pre_op = entry . brok_transformation if pre_op is not none value = pre_op ( self , value ) return value	0
Q_763	receive a batch of message at once	C_763_2	def get_org ( self , name ) if name not in self . orgs self . _raise_org_not_found ( name ) return self . _get_org ( name )	0
Q_764	open stream create output and finally write the stream to output	C_764_0	def output_stream ( plugin , stream ) global output success_open = false for var in range ( args . retry_open ) try stream_fd , prebuffer = open_stream ( stream ) success_open = true break except stream_error as err log . error ( string , var + number , args . retry_open , stream , err ) if not success_open console . exit ( string , stream , args . retry_open ) output = create_output ( plugin ) try output . open ( ) except ( io_error , os_error ) as err if isinstance ( output , player_output ) console . exit ( string , args . player , err ) else console . exit ( string , args . output , err ) with closing ( output ) log . debug ( string ) read_stream ( stream_fd , output , prebuffer ) return true	1
Q_764	open stream create output and finally write the stream to output	C_764_1	def broadcast ( self , msg ) if getattr ( msg , string , false ) for var in msg self . _send ( string s string self . _escape ( str ( var ) ) ) else self . _send ( string s string self . _escape ( str ( msg ) ) )	0
Q_764	open stream create output and finally write the stream to output	C_764_2	def allow_unconfirmed_email ( view_function ) wraps ( view_function ) def decorator ( * args , * * kwargs ) g . _flask_user_allow_unconfirmed_email = true try user_manager = current_app . user_manager allowed = _is_logged_in_with_confirmed_email ( user_manager ) if not allowed return user_manager . unauthenticated_view ( ) return view_function ( * args , * * kwargs ) finally g . _flask_user_allow_unconfirmed_email = false return decorator	0
Q_765	find a swap circuit that implement a permutation for this layer	C_765_0	def _layer_permutation ( self , layer_partition , layout , qubit_subset , coupling , trials ) return _layer_permutation ( layer_partition , self . initial_layout , layout , qubit_subset , coupling , trials , self . qregs , self . rng )	1
Q_765	find a swap circuit that implement a permutation for this layer	C_765_1	def _read_para_notification ( self , code , cbit , clen , * , desc , length , version ) _resv = self . _read_fileng ( number ) _code = self . _read_unpack ( number ) _data = self . _read_fileng ( number ) _type = _notification_type . get ( _code ) if _type is none if number < = _code < = number _type = string elif number < = _code < = number _type = string elif number < = _code < = number _type = string elif number < = _code < = number _type = string elif number < = _code < = number _type = string else raise protocol_error ( f string ) notification = dict ( type = desc , critical = cbit , length = clen , msg_type = _type , data = _data , ) _plen = length _ clen if _plen self . _read_fileng ( _plen ) return notification	0
Q_765	find a swap circuit that implement a permutation for this layer	C_765_2	def _write_var_attrs ( self , f , var_num , var_attrs , z_var ) if ( not isinstance ( var_attrs , dict ) ) raise type_error ( string ) for var , var in var_attrs . items ( ) if ( var in self . gattrs ) print ( string , var , string ) continue if not ( var in self . attrs ) attr_num , offset = self . _write_adr ( f , false , var ) if ( len ( self . attrs ) = = number ) self . _update_offset_value ( self . grd_offset + number , number , offset ) else attr_num = self . attrs . index ( var ) offset = self . attrsinfo [ attr_num ] [ number ] if ( var is none ) continue data_type = number if ( isinstance ( var , list ) or isinstance ( var , tuple ) ) items = len ( var ) if ( items = = number ) data_type = cdf . _datatype_token ( var [ number ] ) if ( data_type > number ) data = var [ number ] if ( cdf . _checklistof_nums ( data ) ) if ( isinstance ( data , list ) or isinstance ( data , tuple ) ) num_elems = len ( data ) else num_elems = number else if ( data_type = = cdf . cdf_char or data_type = = cdf . cdf_uchar ) if isinstance ( data , ( list , tuple ) ) items = len ( data ) odata = data data = str ( string ) for x in range ( number , items ) if ( x > number ) data + = str ( string ) data + = odata [ x ] else data = odata [ x ] num_elems = len ( data ) elif ( data_type = = cdf . cdf_epoch or data_type = = cdf . cdf_epoch16 or data_type = = cdf . cdf_time_tt2000 ) cvalue = [ ] if isinstance ( data , ( list , tuple ) ) num_elems = len ( data ) for x in range ( number , num_elems ) cvalue . append ( cdfepoch . cd_fepoch . parse ( data [ x ] ) ) data = cvalue else data = cdfepoch . cd_fepoch . parse ( data ) num_elems = number else data = var if isinstance ( var , ( list , tuple ) ) num_elems , data_type = cdf . _datatype_define ( var [ number ] ) if ( data_type = = cdf . cdf_char or data_type = = cdf . cdf_uchar ) data = str ( string ) for x in range ( number , len ( var ) ) if ( x > number ) data + = str ( string ) data + = var [ x ] else data = var [ x ] num_elems = len ( data ) else num_elems , data_type = cdf . _datatype_define ( var ) offset = self . _write_aedr ( f , false , attr_num , var_num , data , data_type , num_elems , z_var ) self . _update_aedr_link ( f , attr_num , z_var , var_num , offset )	0
Q_766	generate sawtooth wave sample_pulse	C_766_0	def sawtooth ( duration int , amp complex , period float = none , phase float = number , name str = none ) _ > sample_pulse if period is none period = duration return _sampled_sawtooth_pulse ( duration , amp , period , phase = phase , name = name )	1
Q_766	generate sawtooth wave sample_pulse	C_766_1	def process_order ( self , order ) try dt_orders = self . _orders_by_modified [ order . dt ] except key_error self . _orders_by_modified [ order . dt ] = ordered_dict ( [ ( order . id , order ) , ] ) self . _orders_by_id [ order . id ] = order else self . _orders_by_id [ order . id ] = dt_orders [ order . id ] = order move_to_end ( dt_orders , order . id , last = true ) move_to_end ( self . _orders_by_id , order . id , last = true )	0
Q_766	generate sawtooth wave sample_pulse	C_766_2	def context_exists ( self , name ) contexts = self . data [ string ] for var in contexts if var [ string ] = = name return true return false	0
Q_767	ensure the client be authorize to use the grant type request	C_767_0	def validate_grant_type ( self , client_id , grant_type , client , request , * args , * * kwargs ) if self . _usergetter is none and grant_type = = string log . debug ( string ) return false default_grant_types = ( string , string , string , string , ) if hasattr ( client , string ) if grant_type not in client . allowed_grant_types return false else if grant_type not in default_grant_types return false if grant_type = = string if not hasattr ( client , string ) log . debug ( string ) return false request . user = client . user return true	1
Q_767	ensure the client be authorize to use the grant type request	C_767_1	def python_parser ( version , co , out = sys . stdout , showasm = false , parser_debug = parser_default_debug , is_pypy = false ) assert iscode ( co ) from uncompyle6 . scanner import get_scanner scanner = get_scanner ( version , is_pypy ) tokens , customize = scanner . ingest ( co ) maybe_show_asm ( showasm , tokens ) p = get_python_parser ( version , parser_debug ) return parse ( p , tokens , customize )	0
Q_767	ensure the client be authorize to use the grant type request	C_767_2	def child_allocation ( self ) sum = decimal ( number ) if self . classes for var in self . classes sum + = var . child_allocation else sum = self . allocation return sum	0
Q_768	add certificate to chain	C_768_0	def add_extra_chain_cert ( self , certobj ) if not isinstance ( certobj , x509 ) raise type_error ( string ) copy = _lib . x509_dup ( certobj . _x509 ) add_result = _lib . ssl_ctx_add_extra_chain_cert ( self . _context , copy ) if not add_result _lib . x509_free ( copy ) _raise_current_error ( )	1
Q_768	add certificate to chain	C_768_1	def get_jobs ( self , job_ids = none ) if job_ids is not none and len ( job_ids ) = = number return [ ] q = self . session . query ( job ) if job_ids is not none q = q . filter ( job . unique . in_ ( job_ids ) ) return sorted ( list ( q ) , key = lambda job job . unique )	0
Q_768	add certificate to chain	C_768_2	def _predict_response_size ( mode , functioncode , payload_to_slave ) min_payload_length = number byterange_for_given_size = slice ( number , number ) number_of_payload_bytes_in_write_confirmation = number number_of_payload_bytes_for_bytecountfield = number rtu_to_ascii_payload_factor = number number_of_rtu_response_startbytes = number number_of_rtu_response_endbytes = number number_of_ascii_response_startbytes = number number_of_ascii_response_endbytes = number _check_mode ( mode ) _check_functioncode ( functioncode , none ) _check_string ( payload_to_slave , description = string , minlength = min_payload_length ) if functioncode in [ number , number , number , number ] response_payload_size = number_of_payload_bytes_in_write_confirmation elif functioncode in [ number , number , number , number ] given_size = _two_byte_string_to_num ( payload_to_slave [ byterange_for_given_size ] ) if functioncode = = number or functioncode = = number number_of_inputs = given_size response_payload_size = number_of_payload_bytes_for_bytecountfield + number_of_inputs / / number + ( number if number_of_inputs number else number ) elif functioncode = = number or functioncode = = number number_of_registers = given_size response_payload_size = number_of_payload_bytes_for_bytecountfield + number_of_registers * _number_of_bytes_per_register else raise value_error ( string . format ( functioncode , payload_to_slave ) ) if mode = = mode_ascii return number_of_ascii_response_startbytes + response_payload_size * rtu_to_ascii_payload_factor + number_of_ascii_response_endbytes else return number_of_rtu_response_startbytes + response_payload_size + number_of_rtu_response_endbytes	0
Q_769	set the list of prefer client certificate signer for this server	C_769_0	def set_client_ca_list ( self , certificate_authorities ) name_stack = _lib . sk_x509_name_new_null ( ) _openssl_assert ( name_stack = _ffi . null ) try for var in certificate_authorities if not isinstance ( var , x509_name ) raise type_error ( string string ( type ( var ) . _name_ , ) ) copy = _lib . x509_name_dup ( var . _name ) _openssl_assert ( copy = _ffi . null ) push_result = _lib . sk_x509_name_push ( name_stack , copy ) if not push_result _lib . x509_name_free ( copy ) _raise_current_error ( ) except exception _lib . sk_x509_name_free ( name_stack ) raise _lib . ssl_ctx_set_client_ca_list ( self . _context , name_stack )	1
Q_769	set the list of prefer client certificate signer for this server	C_769_1	def next ( self , length ) return segment ( self . strip , length , self . offset + self . length )	0
Q_769	set the list of prefer client certificate signer for this server	C_769_2	def bb_photlam_arcsec ( wave , temperature ) lam = wave * number . number e _ number return f * llam_si ( lam , temperature ) / ( hs * c / lam )	0
Q_770	convert one or more parse forensic report to flat csv format include	C_770_0	def parsed_forensic_reports_to_csv ( reports ) fields = [ string , string , string , string , string , string , string , string , string , string , string , string , string , string , string , string , string , string , string , string , string ] if type ( reports ) = = ordered_dict reports = [ reports ] csv_file = string_io ( ) csv_writer = dict_writer ( csv_file , fieldnames = fields ) csv_writer . writeheader ( ) for var in reports row = var . copy ( ) row [ string ] = var [ string ] [ string ] row [ string ] = var [ string ] [ string ] row [ string ] = var [ string ] [ string ] row [ string ] = var [ string ] [ string ] del row [ string ] row [ string ] = var [ string ] [ string ] row [ string ] = string . join ( var [ string ] ) authentication_mechanisms = var [ string ] row [ string ] = string . join ( authentication_mechanisms ) del row [ string ] del row [ string ] csv_writer . writerow ( row ) return csv_file . getvalue ( )	1
Q_770	convert one or more parse forensic report to flat csv format include	C_770_1	def get_request_value ( request , key , default = none ) if request . method in [ string , string ] return request . query_params . get ( key , request . data . get ( key , default ) ) return request . data . get ( key , request . query_params . get ( key , default ) )	0
Q_770	convert one or more parse forensic report to flat csv format include	C_770_2	def open ( self , mode = string ) if self . file self . close ( ) raise string if mode = = string self . file = open ( self . path , string ) elif mode = = string self . file = open ( self . path , string ) else self . file = open ( self . path , string ) self . _csv = csv . dict_writer ( self . file , fieldnames = self . fields , delimiter = string , quotechar = string , quoting = csv . quote_minimal , extrasaction = string ) if self . file . tell ( ) = = number self . _csv . writeheader ( )	0
Q_771	generate a random legend for a give list of component	C_771_0	def random ( cls , components , width = false , colour = none ) try list_of_decors = [ decor . random ( var ) for var in [ i [ number ] for i in components . unique if i [ number ] ] ] except try list_of_decors = [ decor . random ( var ) for var in components . copy ( ) ] except list_of_decors = [ decor . random ( components ) ] if colour is not none for d in list_of_decors d . colour = colour if width for i , d in enumerate ( list_of_decors ) d . width = i + number return cls ( list_of_decors )	1
Q_771	generate a random legend for a give list of component	C_771_1	def format_table ( self , width = none , min_label_width = number , min_progress_width = number ) if len ( self . _lines ) = = number return [ ] if width is none width = shutil . get_terminal_size ( ) [ number ] labelw , progw , summaryw = self . calculate_field_widths ( width = width , min_label_width = min_label_width , min_progress_width = min_progress_width ) output = [ var . format_status ( label_width = labelw , progress_width = progw , summary_width = summaryw ) for var in self . _lines ] return output	0
Q_771	generate a random legend for a give list of component	C_771_2	def run ( config_obj , wcsmap = none ) if config_obj [ string ] [ _ number ] = string config_obj [ string ] + = string scale_pars = config_obj [ string ] user_wcs_pars = config_obj [ string ] _fname , _sciextn = fileutil . parse_filename ( config_obj [ string ] ) _inimg = fileutil . open_image ( _fname , memmap = false ) _expin = fileutil . get_keyword ( config_obj [ string ] , scale_pars [ string ] , handle = _inimg ) _scihdu = fileutil . get_extn ( _inimg , _sciextn ) _insci = _scihdu . data . copy ( ) _inexptime = number . number if scale_pars [ string ] = = string if scale_pars [ string ] in _inimg [ string ] . header _inexptime = _inimg [ string ] . header [ scale_pars [ string ] ] elif string in _inimg [ string ] . header _inexptime = _inimg [ string ] . header [ string ] else raise value_error ( string string config_obj [ string ] ) if _inexptime = number . number or _inexptime = number . number np . divide ( _insci , _inexptime , _insci ) _inimg . close ( ) del _inimg source_wcs = stwcs . wcsutil . hstwcs ( config_obj [ string ] ) if source_wcs . wcs . is_unity ( ) print ( string . format ( config_obj [ string ] ) ) blot_wcs = none _refname , _refextn = fileutil . parse_filename ( config_obj [ string ] ) if os . path . exists ( _refname ) blot_wcs = stwcs . wcsutil . hstwcs ( config_obj [ string ] ) if blot_wcs . wcs . is_unity ( ) print ( string . format ( config_obj [ string ] ) ) if user_wcs_pars [ string ] blot_wcs = wcs_functions . build_hstwcs ( user_wcs_pars [ string ] , user_wcs_pars [ string ] , user_wcs_pars [ string ] , user_wcs_pars [ string ] , user_wcs_pars [ string ] , user_wcs_pars [ string ] , user_wcs_pars [ string ] , user_wcs_pars [ string ] ) config_obj [ string ] = none if blot_wcs is none blot_wcs = stwcs . distortion . utils . output_wcs ( [ source_wcs ] , undistort = false ) out_wcs = blot_wcs . copy ( ) _outsci = do_blot ( _insci , source_wcs , out_wcs , _expin , coeffs = config_obj [ string ] , interp = config_obj [ string ] , sinscl = config_obj [ string ] , stepsize = config_obj [ string ] , wcsmap = wcsmap ) if scale_pars [ string ] = = string if scale_pars [ string ] = = string _outscale = fileutil . get_keyword ( config_obj [ string ] , scale_pars [ string ] ) else _outscale = float ( scale_pars [ string ] ) print ( string . format ( _outscale ) ) np . multiply ( _outsci , _outscale , _outsci ) if config_obj [ string ] skyval = _scihdu . header [ string ] else skyval = config_obj [ string ] print ( string . format ( skyval ) ) _outsci + = skyval del _scihdu outputimage . write_single_fits ( _outsci , blot_wcs , config_obj [ string ] , config_obj [ string ] )	0
Q_772	get the mapping for a logical interface from a device type	C_772_0	def get_mappings_on_device_type_for_logical_interface ( self , type_id , logical_interface_id , draft = false ) if draft req = api_client . one_device_type_mapping_url ( self . host , string , type_id , logical_interface_id ) else req = api_client . one_device_type_mapping_url ( self . host , string , type_id , logical_interface_id ) resp = requests . get ( req , auth = self . credentials , verify = self . verify ) if resp . status_code = = number self . logger . debug ( string ) else raise ibmiotf . api_exception ( resp . status_code , string , resp ) return resp . json ( )	1
Q_772	get the mapping for a logical interface from a device type	C_772_1	def post ( method , hmc , uri , uri_parms , body , logon_required , wait_for_completion ) assert wait_for_completion is true storage_group_oid = uri_parms [ number ] storage_group_uri = string + storage_group_oid try storage_group = hmc . lookup_by_uri ( storage_group_uri ) except key_error raise invalid_resource_error ( method , uri ) body2 = body . copy ( ) sv_requests = body2 . pop ( string , none ) storage_group . update ( body2 ) sv_uris = [ ] if sv_requests for var in sv_requests check_required_fields ( method , uri , var , [ string ] ) operation = var [ string ] if operation = = string sv_props = var . copy ( ) del sv_props [ string ] if string in sv_props raise bad_request_error ( method , uri , number , string element _ uri string string ) sv_uri = storage_group . storage_volumes . add ( sv_props ) sv_uris . append ( sv_uri ) elif operation = = string check_required_fields ( method , uri , var , [ string ] ) sv_uri = var [ string ] storage_volume = hmc . lookup_by_uri ( sv_uri ) storage_volume . update_properties ( sv_props ) elif operation = = string check_required_fields ( method , uri , var , [ string ] ) sv_uri = var [ string ] storage_volume = hmc . lookup_by_uri ( sv_uri ) storage_volume . delete ( ) else raise bad_request_error ( method , uri , number , string operation string string operation ) return { string sv_uris , }	0
Q_772	get the mapping for a logical interface from a device type	C_772_2	def open_tunnel ( addr , server , keyfile = none , password = none , paramiko = none , timeout = number ) lport = select_random_ports ( number ) [ number ] transport , addr = addr . split ( string ) ip , rport = addr . split ( string ) rport = int ( rport ) if paramiko is none paramiko = sys . platform = = string if paramiko tunnelf = paramiko_tunnel else tunnelf = openssh_tunnel tunnel = tunnelf ( lport , rport , server , remoteip = ip , keyfile = keyfile , password = password , timeout = timeout ) return string lport , tunnel	0
Q_773	"show short help for all command in category """	C_773_0	def show_category ( self , category , args ) n2cmd = self . proc . commands names = list ( n2cmd . keys ( ) ) if len ( args ) = = number and args [ number ] = = string self . section ( string category ) cmds = [ var for var in names if category = = n2cmd [ var ] . category ] cmds . sort ( ) self . msg_nocr ( self . columnize_commands ( cmds ) ) return self . msg ( string categories [ category ] ) self . section ( string ) names . sort ( ) for name in names if category = n2cmd [ name ] . category continue self . msg ( string ( name , n2cmd [ name ] . short_help , ) ) pass return	1
Q_773	"show short help for all command in category """	C_773_1	async def _deploy ( self , charm_url , application , series , config , constraints , endpoint_bindings , resources , storage , channel = none , num_units = none , placement = none , devices = none ) log . info ( string , charm_url ) config = { var str ( var ) for var , var in config . items ( ) } config = yaml . dump ( { application config } , default_flow_style = false ) app_facade = client . application_facade . from_connection ( self . connection ( ) ) app = client . application_deploy ( charm_url = charm_url , application = application , series = series , channel = channel , config_yaml = config , constraints = parse_constraints ( constraints ) , endpoint_bindings = endpoint_bindings , num_units = num_units , resources = resources , storage = storage , placement = placement , devices = devices , ) result = await app_facade . deploy ( [ app ] ) errors = [ r . error . message for r in result . results if r . error ] if errors raise juju_error ( string . join ( errors ) ) return await self . _wait_for_new ( string , application )	0
Q_773	"show short help for all command in category """	C_773_2	def parameters ( self ) lines = lines ( ) lines . add ( number , string ) lines . add ( number , string ) for var in self . model . parameters if var lines . add ( number , string ( objecttools . classname ( var ) , var . name ) ) for var in self . model . parameters if var print ( string var . name ) lines . add ( number , string ) lines . add ( number , string objecttools . classname ( var ) ) for par in var try ctype = type2_str [ par . type ] + ndim2_str [ par . ndim ] except key_error ctype = par . type + ndim2_str [ par . ndim ] lines . add ( number , string ( ctype , par . name ) ) return lines	0
Q_774	make a post request to the give path with data in it body	C_774_0	def post_data ( self , path , data , content_type , * * params ) params = jsonify_parameters ( params ) url = ensure_trailing_slash ( self . url + path . lstrip ( string ) ) return self . _json_request ( string , url , params = params , data = data , headers = { string content_type } )	1
Q_774	make a post request to the give path with data in it body	C_774_1	def batch_results ( self , results ) logging . debug ( string ) queue = self . mgr . get_queue ( self . qname_out ) for var in results queue . put ( var , block = true ) logging . debug ( string )	0
Q_774	make a post request to the give path with data in it body	C_774_2	def set_aliases ( self , aliases , lang = string , append = true ) if self . fast_run and not self . require_write self . require_write = self . fast_run_container . check_language_data ( qid = self . wd_item_id , lang_data = aliases , lang = lang , lang_data_type = string ) if self . require_write self . init_data_load ( ) else return if string not in self . wd_json_representation self . wd_json_representation [ string ] = { } if not append or lang not in self . wd_json_representation [ string ] self . wd_json_representation [ string ] [ lang ] = [ ] for var in aliases found = false for current_aliases in self . wd_json_representation [ string ] [ lang ] if var . strip ( ) . lower ( ) = current_aliases [ string ] . strip ( ) . lower ( ) continue else found = true break if not found self . wd_json_representation [ string ] [ lang ] . append ( { string lang , string var } )	0
Q_775	handle argument for the tagstr command	C_775_0	def _main ( argv ) parser = argparse . argument_parser ( description = description , formatter_class = argparse . raw_description_help_formatter , ) parser . add_argument ( string , string , default = url_base , help = string url_base , ) parser . add_argument ( string , string , default = none , help = string , ) parser . add_argument ( string , string , default = string , help = string , ) parser . add_argument ( string , string , help = string ) parser . add_argument ( string , string , action = string , help = string , ) parser . add_argument ( string , help = string , ) parser . add_argument ( string , nargs = string , default = none , help = string , ) args = parser . parse_args ( argv ) if args . save_token if not args . token raise value_error ( string ) luminoso_client . save_token ( args . token , domain = urlparse ( args . base_url ) . netloc ) client = luminoso_client . connect ( url = args . base_url , token = args . token ) name = args . project_name if name is none name = input ( string ) if not name print ( string ) return result = upload_docs ( client , args . input_filename , args . language , name , account = args . account_id , progress = true , ) print ( string . format ( result [ string ] , result [ string ] ) )	1
Q_775	handle argument for the tagstr command	C_775_1	def replace_orders ( self , market_id , instructions , customer_ref = none , market_version = none , async_ = none , session = none , lightweight = none ) params = clean_locals ( locals ( ) ) method = string ( self . uri , string ) ( response , elapsed_time ) = self . request ( method , params , session ) return self . process_response ( response , resources . replace_orders , elapsed_time , lightweight )	0
Q_775	handle argument for the tagstr command	C_775_2	def validate ( text , file , schema_type ) content = none if text print ( string ) content = text if file print ( string ) content = file . read ( ) if content is none click . secho ( string , fg = string ) exit ( number ) try if schema_type = = string validator = dependencies_schema_validator ( ) elif schema_type = = string validator = actions_schema_validator ( ) else raise exception ( string ) validator . validate_json ( content ) click . secho ( string , fg = string ) except exception as e click . secho ( string , fg = string ) raise e	0
Q_776	create a optimize function receive gradient	C_776_0	def optimize_function ( params , config = none ) gs = [ dim_to_var ( var . ndim ) for var in params ] updates , _ = optimize_updates ( params , gs , config ) return theano . function ( gs , [ ] , updates = updates )	1
Q_776	create a optimize function receive gradient	C_776_1	def component_type ( self ) cls = javabridge . call ( self . jobject , string , string ) comptype = javabridge . call ( cls , string , string ) return javabridge . call ( comptype , string , string )	0
Q_776	create a optimize function receive gradient	C_776_2	def update_blob_metadata ( blob , response , config , bucket , names ) manifest = os . path . basename ( response [ string ] [ string ] ) manifest = json . loads ( bucket . blob ( manifest ) . download_as_string ( ) ) metadata = { string manifest [ string ] [ number ] [ string ] [ number ] [ string ] , string response [ string ] [ string ] , string manifest [ string ] , string config [ string ] [ string ] [ string ] , string config [ string ] [ string ] [ string ] , string string . join ( config [ string ] [ number ] [ string ] ) , string config [ string ] [ number ] [ string ] , string blob . media_link , string blob . self_link , string blob . size , string names [ string ] , string string } blob . metadata = metadata blob . _properties [ string ] = metadata blob . patch ( )	0
Q_777	prune the tagstr or emptry string value from dictionary item	C_777_0	def _prune_dict_null_str ( dictionary ) for var , var in list ( dictionary . items ( ) ) if var is none or str ( var ) = = string del dictionary [ var ] if isinstance ( var , dict ) dictionary [ var ] = _prune_dict_null_str ( dictionary [ var ] ) return dictionary	1
Q_777	prune the tagstr or emptry string value from dictionary item	C_777_1	def ordered_list ( text_array ) text_list = [ ] for var , var in enumerate ( text_array ) text_list . append ( ( esc_format ( var + number ) + string ) . ljust ( number ) + string + esc_format ( var ) ) return string . join ( text_list )	0
Q_777	prune the tagstr or emptry string value from dictionary item	C_777_2	def get_groups_url ( certfile , group ) gms = string + _server + _gms certfile . seek ( number ) buf = certfile . read ( ) x509 = crypto . load_certificate ( crypto . filetype_pem , buf ) sep = string dn = string parts = [ ] for var in x509 . get_issuer ( ) . get_components ( ) if var [ number ] in parts continue parts . append ( var [ number ] ) dn = var [ number ] + string + var [ number ] + sep + dn sep = string return gms + string + group + string + urllib . quote ( dn )	0
Q_778	obtain peak from a novelty curve use an adaptive threshold	C_778_0	def pick_peaks ( nc , l = number ) offset = nc . mean ( ) / number . nc = filters . gaussian_filter1d ( nc , sigma = number ) th = filters . median_filter ( nc , size = l ) + offset peaks = [ ] for var in range ( number , nc . shape [ number ] _ number ) if nc [ var _ number ] < nc [ var ] and nc [ var ] > nc [ var + number ] if nc [ var ] > th [ var ] peaks . append ( var ) return peaks	1
Q_778	obtain peak from a novelty curve use an adaptive threshold	C_778_1	def get_event_discounts ( self , id , * * data ) return self . get ( string . format ( id ) , data = data )	0
Q_778	obtain peak from a novelty curve use an adaptive threshold	C_778_2	def _fromfile ( self , filename , astype = none ) assert os . path . exists ( filename ) , string + string + filename ext = filename . split ( string ) [ _ number ] . lower ( ) if ext in [ string , string ] self . log ( string + filename ) if astype is none astype = jco m = astype . from_binary ( filename ) self . log ( string + filename ) elif ext in [ string , string ] self . log ( string + filename ) if astype is none astype = matrix m = astype . from_ascii ( filename ) self . log ( string + filename ) elif ext in [ string ] self . log ( string + filename ) if astype is none astype = cov if _istextfile ( filename ) m = astype . from_ascii ( filename ) else m = astype . from_binary ( filename ) self . log ( string + filename ) elif ext in [ string ] self . log ( string + filename ) if astype is none astype = cov m = astype . from_uncfile ( filename ) self . log ( string + filename ) else raise exception ( string + string + str ( ext ) ) return m	0
Q_779	format output use format_name	C_779_0	def format_output ( data , headers , format_name , * * kwargs ) formatter = tabular_output_formatter ( format_name = format_name ) return formatter . format_output ( data , headers , * * kwargs )	1
Q_779	format output use format_name	C_779_1	def process_account ( account_info ) log = logging . get_logger ( string ) log . info ( string , account_info ) session = get_session ( account_info ) client = session . client ( string , config = s3config ) buckets = client . list_buckets ( ) [ string ] connection . hset ( string , account_info [ string ] , json . dumps ( account_info ) ) for var in buckets connection . hset ( string , bucket_id ( account_info , var [ string ] ) , var [ string ] . isoformat ( ) ) account_buckets = account_info . pop ( string , none ) buckets = [ n [ string ] for n in buckets if not account_buckets or n [ string ] in account_buckets ] account_not_buckets = account_info . pop ( string , none ) buckets = [ n for n in buckets if not account_not_buckets or n not in account_not_buckets ] log . info ( string , len ( buckets ) , account_info [ string ] ) for bucket_set in chunks ( buckets , number ) invoke ( process_bucket_set , account_info , bucket_set )	0
Q_779	format output use format_name	C_779_2	def destroy ( name , conn = none , call = none ) if call = = string raise salt_cloud_system_exit ( string string ) _utils_ [ string ] ( string , string , string . format ( name ) , args = { string name } , sock_dir = _opts_ [ string ] , transport = _opts_ [ string ] ) if not conn conn = get_conn ( ) node = show_instance ( name , conn = conn , call = string ) log . info ( string , name ) ret = conn . delete_server ( name ) if ret log . info ( string , name ) _utils_ [ string ] ( string , string , string . format ( name ) , args = { string name } , sock_dir = _opts_ [ string ] , transport = _opts_ [ string ] ) if _opts_ . get ( string , false ) is true _utils_ [ string ] ( getattr ( node , _opts_ . get ( string , string ) ) [ number ] ) if _opts_ . get ( string , false ) is true _utils_ [ string ] ( name , _active_provider_name_ . split ( string ) [ number ] , _opts_ ) _utils_ [ string ] ( name ) return true log . error ( string , name ) return false	0
Q_780	add a new phenotype term to a case	C_780_0	def add_phenotype ( self , institute , case , user , link , var = none , omim_term = none , is_group = false ) hpo_results = [ ] try if var hpo_results = [ var ] elif omim_term log . debug ( string . format ( omim_term ) ) disease_obj = self . disease_term ( omim_term ) if disease_obj for var in disease_obj . get ( string , [ ] ) hpo_results . append ( var ) else raise value_error ( string ) except value_error as e raise e existing_terms = set ( term [ string ] for term in case . get ( string , [ ] ) ) updated_case = case phenotype_terms = [ ] for var in hpo_results log . debug ( string . format ( var ) ) hpo_obj = self . var ( var ) if hpo_obj is none raise value_error ( string var ) phenotype_id = hpo_obj [ string ] description = hpo_obj [ string ] if phenotype_id not in existing_terms phenotype_term = dict ( phenotype_id = phenotype_id , feature = description ) phenotype_terms . append ( phenotype_term ) log . info ( string string . format ( case [ string ] ) ) self . create_event ( institute = institute , case = case , user = user , link = link , category = string , verb = string , subject = case [ string ] , content = phenotype_id ) if is_group updated_case = self . case_collection . find_one_and_update ( { string case [ string ] } , { string { string { string phenotype_terms } , string { string phenotype_terms } , } , } , return_document = pymongo . return_document . after ) else updated_case = self . case_collection . find_one_and_update ( { string case [ string ] } , { string { string { string phenotype_terms } , } , } , return_document = pymongo . return_document . after ) log . debug ( string ) return updated_case	1
Q_780	add a new phenotype term to a case	C_780_1	def check_rest_server_quick ( rest_port ) response = rest_get ( check_status_url ( rest_port ) , number ) if response and response . status_code = = number return true , response return false , none	0
Q_780	add a new phenotype term to a case	C_780_2	def kld_error ( res , error = string , rstate = none , return_new = false , approx = false ) logp2 = res . logwt _ res . logz [ _ number ] if error = = string new_res = jitter_run ( res , rstate = rstate , approx = approx ) elif error = = string new_res , samp_idx = resample_run ( res , rstate = rstate , return_idx = true ) logp2 = logp2 [ samp_idx ] elif error = = string new_res , samp_idx = resample_run ( res , rstate = rstate , return_idx = true ) new_res = jitter_run ( new_res ) logp2 = logp2 [ samp_idx ] else raise value_error ( string error string { number } string . format ( error ) ) logp1 = new_res . logwt _ new_res . logz [ _ number ] kld = np . cumsum ( np . exp ( logp1 ) * ( logp1 _ logp2 ) ) if return_new return kld , new_res else return kld	0
Q_781	"r tagstr """	C_781_0	def log ( wave ) r pexdoc . exh . addex ( value_error , string , bool ( ( min ( wave . _dep_vector ) < = number ) ) ) return _operation ( wave , string , string , np . log )	1
Q_781	"r tagstr """	C_781_1	def disassociate_network_acl ( subnet_id = none , vpc_id = none , subnet_name = none , vpc_name = none , region = none , key = none , keyid = none , profile = none ) if not _exactly_one ( ( subnet_name , subnet_id ) ) raise salt_invocation_error ( string string ) if all ( ( vpc_name , vpc_id ) ) raise salt_invocation_error ( string string ) try if subnet_name subnet_id = _get_resource_id ( string , subnet_name , region = region , key = key , keyid = keyid , profile = profile ) if not subnet_id return { string false , string { string string . format ( subnet_name ) } } if vpc_name or vpc_id vpc_id = check_vpc ( vpc_id , vpc_name , region , key , keyid , profile ) conn = _get_conn ( region = region , key = key , keyid = keyid , profile = profile ) association_id = conn . disassociate_network_acl ( subnet_id , vpc_id = vpc_id ) return { string true , string association_id } except boto_server_error as e return { string false , string _utils_ [ string ] ( e ) }	0
Q_781	"r tagstr """	C_781_2	def ask_confirmation ( ) while true print ( string , end = string ) choice = input ( ) . lower ( ) if choice in [ string , string ] return true elif choice in [ string , string ] return false else print ( string yes string no string )	0
Q_782	add an exist label to this card	C_782_0	def _add_label_from_class ( self , label = none ) return self . fetch_json ( uri_path = self . base_uri + string , http_method = string , query_params = { string label . id } )	1
Q_782	add an exist label to this card	C_782_1	def block_to_graphviz_string ( block = none , namer = _graphviz_default_namer ) graph = net_graph ( block , split_state = true ) node_index_map = { } rstring = for var , var in enumerate ( graph ) label = namer ( var , is_edge = false ) rstring + = string ( var , label ) node_index_map [ var ] = var for _from in graph for _to in graph [ _from ] from_index = node_index_map [ _from ] to_index = node_index_map [ _to ] edge = graph [ _from ] [ _to ] is_to_splitmerge = true if hasattr ( _to , string ) and _to . op in string else false label = namer ( edge , is_to_splitmerge = is_to_splitmerge ) rstring + = string ( from_index , to_index , label ) rstring + = string return rstring	0
Q_782	add an exist label to this card	C_782_2	def _fail ( self , message , text , i ) raise value_error ( string . format ( message , text [ i i + number ] ) )	0
Q_783	turn on specify filter for specify analyte	C_783_0	def on ( self , analyte = none , filt = none ) if isinstance ( analyte , str ) analyte = [ analyte ] if isinstance ( filt , ( int , float ) ) filt = [ filt ] elif isinstance ( filt , str ) filt = self . fuzzmatch ( filt , multi = true ) if analyte is none analyte = self . analytes if filt is none filt = list ( self . index . values ( ) ) for var in analyte for f in filt if isinstance ( f , ( int , float ) ) f = self . index [ int ( f ) ] try self . switches [ var ] [ f ] = true except key_error f = self . fuzzmatch ( f , multi = false ) self . switches [ var ] [ f ] = true return	1
Q_783	turn on specify filter for specify analyte	C_783_1	def visit_dictcomp ( self , node ) return string ( node . key . accept ( self ) , node . value . accept ( self ) , string . join ( var . accept ( self ) for var in node . generators ) , )	0
Q_783	turn on specify filter for specify analyte	C_783_2	def joule_thomson ( t , v , cp , d_v_d_t = none , beta = none ) r if d_v_d_t return ( t * d_v_d_t _ v ) / cp elif beta return v / cp * ( beta * t _ number . ) else raise exception ( string )	0
Q_784	serial call to set pulse input ratio on a line	C_784_0	def set_pulse_input_ratio ( self , line_in , new_cnst , password = string ) result = false self . set_context ( string ) try if not self . request_a ( ) self . write_cmd_msg ( string ) else if not self . serial_cmd_pwd_auth ( password ) self . write_cmd_msg ( string ) else req_const = binascii . hexlify ( str ( new_cnst ) . zfill ( number ) ) line_const = binascii . hexlify ( str ( line_in _ number ) ) req_str = string + line_const + string + req_const + string req_str + = self . calc_crc16 ( req_str [ number ] . decode ( string ) ) self . m_serial_port . write ( req_str . decode ( string ) ) if self . m_serial_port . get_response ( self . get_context ( ) ) . encode ( string ) = = string self . write_cmd_msg ( string ) result = true self . serial_post_end ( ) except ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) self . set_context ( string ) return result	1
Q_784	serial call to set pulse input ratio on a line	C_784_1	def cross ( self , vector ) return vector ( ( self . y * vector . z _ self . z * vector . y ) , ( self . z * vector . x _ self . x * vector . z ) , ( self . x * vector . y _ self . y * vector . x ) )	0
Q_784	serial call to set pulse input ratio on a line	C_784_2	def set_embeddings ( self , embeddings ) if self . embeddings_ is not none raise exception ( string ) assert embeddings . shape [ number ] = = self . corpus_ . get_num_terms ( ) self . embeddings_ = embeddings . t self . vocab_ = self . corpus_ . get_terms ( ) return self	0
Q_785	"like takewhile but take a peekable iterable and doesn "" t"	C_785_0	def takewhile_peek ( predicate , iterable ) while true try if not predicate ( iterable . peek ( ) ) break yield next ( iterable ) except stop_iteration break	1
Q_785	"like takewhile but take a peekable iterable and doesn "" t"	C_785_1	def get_groups ( self , env , token ) groups = none memcache_client = cache_from_env ( env ) if memcache_client memcache_key = string ( self . reseller_prefix , token ) cached_auth_data = memcache_client . get ( memcache_key ) if cached_auth_data expires , groups = cached_auth_data if expires < time ( ) groups = none s3_auth_details = env . get ( string ) if s3_auth_details if not self . s3_support self . logger . warning ( string ) return none if self . swauth_remote self . logger . warning ( string string ) return none try account , user = s3_auth_details [ string ] . split ( string , number ) signature_from_user = s3_auth_details [ string ] msg = s3_auth_details [ string ] except exception self . logger . debug ( string ( s3_auth_details , ) ) return none path = quote ( string ( self . auth_account , account , user ) ) resp = self . make_pre_authed_request ( env , string , path ) . get_response ( self . app ) if resp . status_int / / number = number return none if string in resp . headers account_id = resp . headers [ string ] else path = quote ( string ( self . auth_account , account ) ) resp2 = self . make_pre_authed_request ( env , string , path ) . get_response ( self . app ) if resp2 . status_int / / number = number return none account_id = resp2 . headers [ string ] path = env [ string ] env [ string ] = path . replace ( string ( account , user ) , account_id , number ) detail = json . loads ( resp . body ) if detail creds = detail . get ( string ) try auth_encoder , creds_dict = swauth . authtypes . validate_creds ( creds ) except value_error as e self . logger . error ( string e . args [ number ] ) return none password = creds_dict [ string ] if isinstance ( password , six . text_type ) password = password . encode ( string ) if isinstance ( msg , six . text_type ) msg = msg . encode ( string ) valid_signature = base64 . encodestring ( hmac . new ( password , msg , sha1 ) . digest ( ) ) . strip ( ) if signature_from_user = valid_signature return none groups = [ var [ string ] for var in detail [ string ] ] if string in groups groups . remove ( string ) groups . append ( account_id ) groups = string . join ( groups ) return groups if not groups if self . swauth_remote with timeout ( self . swauth_remote_timeout ) conn = http_connect ( self . swauth_remote_parsed . hostname , self . swauth_remote_parsed . port , string , string ( self . swauth_remote_parsed . path , quote ( token ) ) , ssl = ( self . swauth_remote_parsed . scheme = = string ) ) resp = conn . getresponse ( ) resp . read ( ) conn . close ( ) if resp . status / / number = number return none expires_from_now = float ( resp . getheader ( string ) ) groups = resp . getheader ( string ) if memcache_client memcache_client . set ( memcache_key , ( time ( ) + expires_from_now , groups ) , time = expires_from_now ) else object_name = self . _get_concealed_token ( token ) path = quote ( string ( self . auth_account , object_name [ _ number ] , object_name ) ) resp = self . make_pre_authed_request ( env , string , path ) . get_response ( self . app ) if resp . status_int / / number = number return none detail = json . loads ( resp . body ) if detail [ string ] < time ( ) self . make_pre_authed_request ( env , string , path ) . get_response ( self . app ) return none groups = [ var [ string ] for var in detail [ string ] ] if string in groups groups . remove ( string ) groups . append ( detail [ string ] ) groups = string . join ( groups ) if memcache_client memcache_client . set ( memcache_key , ( detail [ string ] , groups ) , time = float ( detail [ string ] _ time ( ) ) ) return groups	0
Q_785	"like takewhile but take a peekable iterable and doesn "" t"	C_785_2	def mutate_unsampled_call_args ( self , module , method , wrapped , instance , args , kwargs , transaction ) return args , kwargs	0
Q_786	give a partition_dict result if the partition miss swap	C_786_0	def _swap_on_miss ( partition_result ) before , item , after = partition_result return ( before , item , after ) if item else ( after , item , before )	1
Q_786	give a partition_dict result if the partition miss swap	C_786_1	def create_bulk ( cls , name , interfaces = none , nodes = number , cluster_mode = string , primary_mgt = none , backup_mgt = none , primary_heartbeat = none , log_server_ref = none , domain_server_address = none , location_ref = none , default_nat = false , enable_antivirus = false , enable_gti = false , comment = none , snmp = none , * * kw ) primary_heartbeat = primary_mgt if not primary_heartbeat else primary_heartbeat physical_interfaces = [ ] for var in interfaces if string not in var raise create_engine_failed ( string string name ) if var . get ( string , none ) = = string tunnel_interface = tunnel_interface ( * * var ) physical_interfaces . append ( { string tunnel_interface } ) else cluster_interface = cluster_physical_interface ( primary_mgt = primary_mgt , backup_mgt = backup_mgt , primary_heartbeat = primary_heartbeat , * * var ) physical_interfaces . append ( { string cluster_interface } ) if snmp snmp_agent = dict ( snmp_agent_ref = snmp . get ( string , string ) , snmp_location = snmp . get ( string , string ) ) snmp_agent . update ( snmp_interface = add_snmp ( interfaces , snmp . get ( string , [ ] ) ) ) try engine = super ( firewall_cluster , cls ) . _create ( name = name , node_type = string , physical_interfaces = physical_interfaces , domain_server_address = domain_server_address , log_server_ref = log_server_ref , location_ref = location_ref , nodes = nodes , enable_gti = enable_gti , enable_antivirus = enable_antivirus , default_nat = default_nat , snmp_agent = snmp_agent if snmp else none , comment = comment ) engine . update ( cluster_mode = cluster_mode ) return element_creator ( cls , json = engine ) except ( element_not_found , create_element_failed ) as e raise create_engine_failed ( e )	0
Q_786	give a partition_dict result if the partition miss swap	C_786_2	def user_add_to_group_action ( model , request ) var = request . params . get ( string ) if not var group_ids = request . params . getall ( string ) else group_ids = [ var ] try user = model . model validate_add_users_to_groups ( model , [ user . id ] , group_ids ) groups = user . root . groups for var in group_ids groups [ var ] . add ( user . name ) groups ( ) model . parent . invalidate ( user . name ) localizer = get_localizer ( request ) message = localizer . translate ( _ ( string , default = string { uid } string { gid } string , mapping = { string user . id , string string . join ( group_ids ) } ) ) return { string true , string message } except manage_membership_error as e if e . reason is not lm_target_gid_not_allowed raise exception ( u string ) localizer = get_localizer ( request ) message = localizer . translate ( _ ( string , default = ( string { uid } string { gid } string string ) , mapping = { string user . id , string e . data } ) ) return { string false , string message } except exception as e return { string false , string str ( e ) }	0
Q_787	create service start server	C_787_0	def service_factory ( app , host , port , report_message = string , provider_cls = http_service_provider ) service = service ( app ) server = provider_cls ( service , host , port , report_message ) server . serve_forever ( )	1
Q_787	create service start server	C_787_1	def _imm_repr ( self ) return ( type ( self ) . _name_ + ( string if _imm_is_persist ( self ) else string ) + string . join ( [ k + string + str ( v ) for ( k , v ) in six . iteritems ( imm_params ( self ) ) ] ) + string )	0
Q_787	create service start server	C_787_2	def new ( cls , package , slide_part ) notes_master_part = package . presentation_part . notes_master_part notes_slide_part = cls . _add_notes_slide_part ( package , slide_part , notes_master_part ) notes_slide = notes_slide_part . notes_slide notes_slide . clone_master_placeholders ( notes_master_part . notes_master ) return notes_slide_part	0
Q_788	check request return false if use ssl or local connection	C_788_0	def check_secure ( ) if this . request . is_secure ( ) return true elif this . request . meta [ string ] in [ string , string , ] return true raise meteor_error ( number , string )	1
Q_788	check request return false if use ssl or local connection	C_788_1	def extract_filezip ( path_to_file , dest_path , target_zipfiles = none ) target_zipfiles = [ string ] if target_zipfiles is none else target_zipfiles files = [ ] _ , ext = os . path . splitext ( path_to_file ) if ext = = string file = open ( path_to_file , string ) with zipfile . zip_file ( file ) as zip_file regexp = string . join ( target_zipfiles ) if target_zipfiles else string search_regex = re . compile ( regexp ) lista = [ m . group ( ) for var in zip_file . namelist ( ) for m in [ search_regex . search ( var ) ] if m ] for zp_file in lista try zip_file . extract ( zp_file , dest_path ) files . append ( os . path . join ( dest_path , zp_file ) ) except exception as ex msg = string . format ( zp_file , path_to_file , dest_path , ex ) logger . error ( msg ) file . close ( ) else logger . warning ( string ) return files	0
Q_788	check request return false if use ssl or local connection	C_788_2	def legend_value ( request_context , series_list , * value_types ) value_funcs = { string lambda s safe_div ( safe_sum ( s ) , safe_len ( s ) ) , string safe_sum , string safe_min , string safe_max , string safe_last , } system = none if value_types [ _ number ] in ( string , string ) system = value_types [ _ number ] value_types = value_types [ _ number ] for var in value_types value_func = value_funcs . get ( var , lambda s string ) if system is none for series in series_list series . name + = string ( var , value_func ( series ) ) else for series in series_list value = value_func ( series ) formatted = none if value is not none formatted = string format_units ( value , system = system ) series . name = string ( series . name , var , formatted ) return series_list	0
Q_789	return a batch client with same setting of the client	C_789_0	def batch_client ( self , size = number ) batch_client = batch_client ( self . host , self . port , self . prefix , size ) self . _configure_client ( batch_client ) return batch_client	1
Q_789	return a batch client with same setting of the client	C_789_1	def diffusion_coeff_counts ( self ) return [ ( var , len ( list ( var ) ) ) for var , var in itertools . groupby ( self . diffusion_coeff ) ]	0
Q_789	return a batch client with same setting of the client	C_789_2	def max_1day_precipitation_amount ( pr , freq = string ) r out = pr . resample ( time = freq ) . max ( dim = string , keep_attrs = true ) return utils . convert_units_to ( out , string , string )	0
Q_790	increment the cursor to the next character	C_790_0	def incpos ( self , length int = number ) _ > int if length < number raise value_error ( string ) i = number while ( i < length ) if self . _cursor . index < self . _len if self . peek_char = = string self . _cursor . step_next_line ( ) self . _cursor . step_next_char ( ) i + = number return self . _cursor . index	1
Q_790	increment the cursor to the next character	C_790_1	def subscribe_async ( self , subject , * * kwargs ) kwargs [ string ] = true sid = yield self . subscribe ( subject , * * kwargs ) raise tornado . gen . return ( sid )	0
Q_790	increment the cursor to the next character	C_790_2	def newer ( self , source , target ) if not os . path . exists ( source ) raise distlib_exception ( string r string os . path . abspath ( source ) ) if not os . path . exists ( target ) return true return os . stat ( source ) . st_mtime > os . stat ( target ) . st_mtime	0
Q_791	create a v_card3	C_791_0	def create_vcard3_str ( name , surname , displayname , email = string , org = string , title = string , url = string , note = string ) vcard = [ ] vcard + = [ string ] vcard + = [ string ] if name and surname name = name . strip ( ) vcard + = [ string . format ( name , surname ) ] if not displayname displayname = string . format ( name , surname ) vcard + = [ string . format ( displayname ) ] if email vcard + = [ string . format ( email ) ] if org vcard + = [ string . format ( org ) ] if title vcard + = [ string . format ( title ) ] if url vcard + = [ string . format ( url ) ] if note vcard + = [ string . format ( note ) ] vcard + = [ string ] return string . join ( [ var . strip ( ) for var in vcard ] )	1
Q_791	create a v_card3	C_791_1	def execute_request ( server_url , creds , namespace , classname ) print ( string ( server_url , namespace , classname ) ) try conn = wbem_connection ( server_url , creds , default_namespace = namespace , no_verification = true ) instances = conn . enumerate_instances ( classname ) print ( string ( type ( instances ) , len ( instances ) ) ) for var in instances print ( string var . path ) print ( var . tomof ( ) ) except error as err if isinstance ( err , cim_error ) print ( string ( err . status_code_name , err . status_description ) ) else print ( string err ) sys . exit ( number )	0
Q_791	create a v_card3	C_791_2	def _get_iterator ( self ) try return self . job_param_source_iter ( self . config ) except type_error try return self . job_param_source_iter ( ) except type_error return self . job_param_source_iter	0
Q_792	post notification to all register observer	C_792_0	def post_notification ( self , ntype , sender , * args , * * kwargs ) if ( ntype = = none or sender = = none ) raise notification_error ( string ) if ( ( ntype not in self . registered_types and none not in self . registered_types ) or ( sender not in self . registered_senders and none not in self . registered_senders ) ) return for var in self . _observers_for_notification ( ntype , sender ) var ( ntype , sender , * args , * * kwargs )	1
Q_792	post notification to all register observer	C_792_1	def _try_load_module ( path , import_name = none ) logger = logging . get_logger ( _name_ ) obj_name = none if len ( path ) > number and string in path [ number ] path , _ , obj_name = path . rpartition ( string ) folder , basename = os . path . split ( path ) if folder = = string folder = string if basename = = string or not os . path . exists ( path ) raise argument_error ( string , path = path ) basename , ext = os . path . splitext ( basename ) if ext not in ( string , string , string ) raise argument_error ( string , path = path ) if import_name is none import_name = basename else logger . debug ( string , import_name ) try fileobj = none fileobj , pathname , description = imp . find_module ( basename , [ folder ] ) if basename in sys . modules mod = sys . modules [ basename ] else mod = imp . load_module ( import_name , fileobj , pathname , description ) if obj_name is not none if obj_name not in mod . _dict_ raise argument_error ( string s string s string ( obj_name , basename ) , path = path ) mod = mod . _dict_ [ obj_name ] return basename , mod finally if fileobj is not none fileobj . close ( )	0
Q_792	post notification to all register observer	C_792_2	def derivatives_ctrlpts ( * * kwargs ) r1 = kwargs . get ( string ) r2 = kwargs . get ( string ) deriv_order = kwargs . get ( string ) degree = kwargs . get ( string ) knotvector = kwargs . get ( string ) ctrlpts = kwargs . get ( string ) dimension = kwargs . get ( string ) r = r2 _ r1 pk = [ [ [ none for var in range ( dimension ) ] for var in range ( r + number ) ] for var in range ( deriv_order + number ) ] for i in range ( number , r + number ) pk [ number ] [ i ] [ ] = [ elem for elem in ctrlpts [ r1 + i ] ] for k in range ( number , deriv_order + number ) tmp = degree _ k + number for i in range ( number , r _ k + number ) pk [ k ] [ i ] [ ] = [ tmp * ( elem1 _ elem2 ) / ( knotvector [ r1 + i + degree + number ] _ knotvector [ r1 + i + k ] ) for elem1 , elem2 in zip ( pk [ k _ number ] [ i + number ] , pk [ k _ number ] [ i ] ) ] return pk	0
Q_793	transform a list of string into a single string with column	C_793_0	def columnize ( items , separator = string , displaywidth = number ) if not items return string matrix , info = compute_item_matrix ( items , separator_size = len ( separator ) , displaywidth = displaywidth ) fmatrix = [ filter ( none , var ) for var in matrix ] sjoin = lambda var separator . join ( [ y . ljust ( w , string ) for y , w in zip ( var , info [ string ] ) ] ) return string . join ( map ( sjoin , fmatrix ) ) + string	1
Q_793	transform a list of string into a single string with column	C_793_1	def _enable_session_activity ( self , app ) user_logged_in . connect ( login_listener , app ) user_logged_out . connect ( logout_listener , app ) from . views . settings import blueprint from . views . security import security , revoke_session blueprint . route ( string , methods = [ string ] ) ( security ) blueprint . route ( string , methods = [ string ] ) ( revoke_session )	0
Q_793	transform a list of string into a single string with column	C_793_2	def merge_config ( cfg_chunks ) root = lxml . etree . xml ( string s string r53_xmlns , parser = xml_parser ) for var in cfg_chunks for rrset in var . iterfind ( string r53_xmlns ) root . append ( rrset ) return root	0
Q_794	this be the actual zest	C_794_0	def create_zipfile ( context ) if not prerequisites_ok ( ) return subprocess . call ( [ string , string ] ) for var in glob . glob ( string ) first_part = var . split ( string ) [ number ] new_name = string ( first_part , context [ string ] ) target = os . path . join ( context [ string ] , new_name ) shutil . copy ( var , target ) print ( string ( var , target ) )	1
Q_794	this be the actual zest	C_794_1	def doi_main ( files ) logger_doi_main . info ( string ) print ( string . format ( str ( len ( files [ string ] ) ) , string ) ) force = prompt_force ( ) for var in files [ string ] dir_tmp = create_tmp_dir ( ) unzipper ( var [ string ] , dir_tmp ) if force print ( string . format ( var [ string ] ) ) logger_doi_main . info ( string . format ( var [ string ] ) ) process_lpd ( var [ string ] , dir_tmp ) os . chdir ( var [ string ] ) zipper ( path_name_ext = var [ string ] , root_dir = dir_tmp , name = var [ string ] ) shutil . rmtree ( dir_tmp ) if not force if resolved_flag ( open_bag ( os . path . join ( dir_tmp , var [ string ] ) ) ) print ( string . format ( var [ string ] ) ) logger_doi_main . info ( string . format ( var [ string ] ) ) shutil . rmtree ( dir_tmp ) else print ( string . format ( var [ string ] ) ) logger_doi_main . info ( string . format ( var [ string ] ) ) process_lpd ( var [ string ] , dir_tmp ) os . chdir ( var [ string ] ) zipper ( path_name_ext = var [ string ] , root_dir = dir_tmp , name = var [ string ] ) shutil . rmtree ( dir_tmp ) logger_doi_main . info ( string ) print ( string ) return	0
Q_794	this be the actual zest	C_794_2	def underlying_likelihood ( self , binary_outcomes , modelparams , expparams ) original_mps = modelparams [ . , self . _orig_mps_slice ] return self . underlying_model . likelihood ( binary_outcomes , original_mps , expparams )	0
Q_795	override to handle rich data type like svg	C_795_0	def _handle_display_data ( self , msg ) if not self . _hidden and self . _is_from_this_session ( msg ) source = msg [ string ] [ string ] data = msg [ string ] [ string ] metadata = msg [ string ] [ string ] if data . has_key ( string ) self . log . debug ( string , msg . get ( string , string ) ) svg = data [ string ] self . _append_svg ( svg , true ) elif data . has_key ( string ) self . log . debug ( string , msg . get ( string , string ) ) png = decodestring ( data [ string ] . encode ( string ) ) self . _append_png ( png , true ) elif data . has_key ( string ) and self . _jpg_supported self . log . debug ( string , msg . get ( string , string ) ) jpg = decodestring ( data [ string ] . encode ( string ) ) self . _append_jpg ( jpg , true ) else return super ( rich_i_python_widget , self ) . _handle_display_data ( msg )	1
Q_795	override to handle rich data type like svg	C_795_1	def _update_datasource ( self , source , data ) if isinstance ( source , column_data_source ) if self . handles [ string ] source . trigger ( string , source . data , data ) else source . data . update ( data ) else source . graph_layout = data	0
Q_795	override to handle rich data type like svg	C_795_2	def mro ( * bases ) seqs = [ list ( var . _mro_ ) for var in bases ] + [ list ( bases ) ] res = [ ] while true non_empty = list ( filter ( none , seqs ) ) if not non_empty return tuple ( res ) for seq in non_empty candidate = seq [ number ] not_head = [ s for s in non_empty if candidate in s [ number ] ] if not_head candidate = none else break if not candidate raise type_error ( string ) res . append ( candidate ) for seq in non_empty if seq [ number ] = = candidate del seq [ number ]	0
Q_796	"return a tuple with the process "" r and vms size"	C_796_0	def get_memory_info ( self ) rss , vms = _psutil_osx . get_process_memory_info ( self . pid ) [ number ] return nt_meminfo ( rss , vms )	1
Q_796	"return a tuple with the process "" r and vms size"	C_796_1	def _percent_match ( date_classes , tokens ) match_count = [ number ] * len ( date_classes ) for var , var in enumerate ( date_classes ) for token in tokens if var . is_match ( token ) match_count [ var ] + = number percentages = tuple ( [ float ( m ) / len ( tokens ) for m in match_count ] ) return percentages	0
Q_796	"return a tuple with the process "" r and vms size"	C_796_2	def iter_commits ( self , sha = none , path = none , author = none , number = _ number , etag = none , since = none , until = none ) params = { string sha , string path , string author , string timestamp_parameter ( since ) , string timestamp_parameter ( until ) } self . _remove_none ( params ) url = self . _build_url ( string , base_url = self . _api ) return self . _iter ( int ( number ) , url , repo_commit , params , etag )	0
Q_797	flush task or queue result wait in zmq queue	C_797_0	def _flush_results ( self , sock ) idents , msg = self . session . recv ( sock , mode = zmq . noblock ) while msg is not none if self . debug pprint ( msg ) msg_type = msg [ string ] [ string ] handler = self . _queue_handlers . get ( msg_type , none ) if handler is none raise exception ( string msg . msg_type ) else handler ( msg ) idents , msg = self . session . recv ( sock , mode = zmq . noblock )	1
Q_797	flush task or queue result wait in zmq queue	C_797_1	def nearest_pois ( self , distance , category , num_pois = number , max_distance = none , imp_name = none , include_poi_ids = false ) if max_distance is none max_distance = distance if category not in self . poi_category_names assert number , string if num_pois > self . max_pois assert number , string imp_num = self . _imp_name_to_num ( imp_name ) dists , poi_ids = self . net . find_all_nearest_pois ( distance , num_pois , category . encode ( string ) , imp_num ) dists [ dists = = _ number ] = max_distance df = pd . data_frame ( dists , index = self . node_ids ) df . columns = list ( range ( number , num_pois + number ) ) if include_poi_ids df2 = pd . data_frame ( poi_ids , index = self . node_ids ) df2 . columns = [ string var for var in range ( number , num_pois + number ) ] for col in df2 . columns s = df2 [ col ] . astype ( string ) df2 [ col ] = self . poi_category_indexes [ category ] . values [ s ] df2 . loc [ s = = _ number , col ] = np . nan df = pd . concat ( [ df , df2 ] , axis = number ) return df	0
Q_797	flush task or queue result wait in zmq queue	C_797_2	def disasm_symbol_app ( _parser , _ , args ) parser = argparse . argument_parser ( prog = _parser . prog , description = _parser . description , ) parser . add_argument ( string , string , choices = asm_syntax . _members_ . keys ( ) , default = none , ) parser . add_argument ( string , help = string ) parser . add_argument ( string , help = string ) args = parser . parse_args ( args ) if args . syntax is not none syntax = asm_syntax . _members_ [ args . syntax ] else syntax = none elf = elf ( args . file ) symbol = elf . get_symbol ( args . symbol ) print ( string . join ( disasm ( symbol . content , symbol . value , syntax = syntax , target = elf ) ) )	0
Q_798	be the give identifier define in one of the namespaces which shadow	C_798_0	def is_shadowed ( identifier , ip ) return ( identifier in ip . user_ns or identifier in ip . user_global_ns or identifier in ip . ns_table [ string ] )	1
Q_798	be the give identifier define in one of the namespaces which shadow	C_798_1	def sls ( mods , saltenv = string , test = none , exclude = none , * * kwargs ) st_kwargs = _salt_ . kwargs _opts_ [ string ] = _grains_ _pillar_ . update ( kwargs . get ( string , { } ) ) opts = salt . utils . state . get_sls_opts ( _opts_ , * * kwargs ) st_ = salt . client . ssh . state . ssh_high_state ( opts , _pillar_ , _salt_ , _context_ [ string ] ) st_ . push_active ( ) mods = _parse_mods ( mods ) high_data , errors = st_ . render_highstate ( { saltenv mods } ) if exclude if isinstance ( exclude , six . string_types ) exclude = exclude . split ( string ) if string in high_data high_data [ string ] . extend ( exclude ) else high_data [ string ] = exclude high_data , ext_errors = st_ . state . reconcile_extend ( high_data ) errors + = ext_errors errors + = st_ . state . verify_high ( high_data ) if errors return errors high_data , req_in_errors = st_ . state . requisite_in ( high_data ) errors + = req_in_errors high_data = st_ . state . apply_exclude ( high_data ) if errors return errors chunks = st_ . state . compile_high_data ( high_data ) file_refs = salt . client . ssh . state . lowstate_file_refs ( chunks , _merge_extra_filerefs ( kwargs . get ( string , string ) , opts . get ( string , string ) ) ) roster = salt . roster . roster ( opts , opts . get ( string , string ) ) roster_grains = roster . opts [ string ] _cleanup_slsmod_low_data ( chunks ) trans_tar = salt . client . ssh . state . prep_trans_tar ( _context_ [ string ] , chunks , file_refs , _pillar_ , st_kwargs [ string ] , roster_grains ) trans_tar_sum = salt . utils . hashutils . get_hash ( trans_tar , opts [ string ] ) cmd = string . format ( opts [ string ] , test , trans_tar_sum , opts [ string ] ) single = salt . client . ssh . single ( opts , cmd , fsclient = _context_ [ string ] , minion_opts = _salt_ . minion_opts , * * st_kwargs ) single . shell . send ( trans_tar , string . format ( opts [ string ] ) ) stdout , stderr , _ = single . cmd_block ( ) try os . remove ( trans_tar ) except ( os_error , io_error ) pass try return salt . utils . json . loads ( stdout ) except exception as e log . error ( string , stdout , stderr ) log . error ( six . text_type ( e ) ) return stdout	0
Q_798	be the give identifier define in one of the namespaces which shadow	C_798_2	def fcoe_get_interface_output_fcoe_intf_list_fcoe_intf_last_counters_cleared ( self , * * kwargs ) config = et . element ( string ) fcoe_get_interface = et . element ( string ) config = fcoe_get_interface output = et . sub_element ( fcoe_get_interface , string ) fcoe_intf_list = et . sub_element ( output , string ) fcoe_intf_fcoe_port_id_key = et . sub_element ( fcoe_intf_list , string ) fcoe_intf_fcoe_port_id_key . text = kwargs . pop ( string ) fcoe_intf_last_counters_cleared = et . sub_element ( fcoe_intf_list , string ) fcoe_intf_last_counters_cleared . text = kwargs . pop ( string ) callback = kwargs . pop ( string , self . _callback ) return callback ( config )	0
Q_799	return a canvas from a grayscale image	C_799_0	def from_grayscale ( im , channels_on = ( true , true , true ) ) xdim , ydim = im . shape canvas = np . zeros ( ( xdim , ydim , number ) , dtype = np . uint8 ) for var , var in enumerate ( channels_on ) if var canvas [ , , var ] = im return canvas . view ( annotated_image )	1
Q_799	return a canvas from a grayscale image	C_799_1	def init_service_processes ( self ) processes = [ ] for var in getattr ( self . context . config . settings , string , ( ) ) process_cls = import_object ( var [ number ] ) wait_unless_ready , timeout = var [ number ] , var [ number ] self . logger . info ( string s string , process_cls . _name_ ) processes . append ( process_wrapper ( process_cls , ( self . context , ) , wait_unless_ready = wait_unless_ready , timeout = timeout ) ) return processes	0
Q_799	return a canvas from a grayscale image	C_799_2	def t_hexconstant ( self , t ) r string t . value = int ( t . value , number ) t . type = string return t	0
